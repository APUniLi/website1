[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"website online version Tidy Finance R, book currently development intended eventual print release via Chapman & Hall/CRC. book result joint effort Christoph Scheuch, Stefan Voigt, Patrick Weiss.grateful kind feedback every aspect book. please get touch us via contact@tidy-finance.org spot typos, discover issues deserve attention, suggestions additional chapters sections. Additionally, let us know found text helpful. look forward hearing !","code":""},{"path":"index.html","id":"why-does-this-book-exist","chapter":"Preface","heading":"Why does this book exist?","text":"Financial economics vibrant area research, central part businesses activities, least implicitly relevant everyday life. Despite relevance society vast number empirical studies financial phenomenons, one quickly learns actual implementation models solve problems area financial economics typically rather opaque.\ngraduate students, particularly surprised lack public code seminal papers even textbooks key concepts financial economics. lack transparent code leads numerous replication efforts (failures), also constitutes waste resources problems already solved countless others secrecy.book aims lift curtain reproducible finance providing fully transparent code base many common financial applications. hope inspire others share code publicly take part journey towards reproducible research future.","code":""},{"path":"index.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who should read this book?","text":"write book three audiences:Students want acquire basic tools required conduct financial research ranging undergrad graduate level. book’s structure simple enough material sufficient self-study purposes.Instructors look materials teach courses empirical finance financial economics. provide plenty examples focus intuitive explanations can easily adjusted expanded. end chapter provide exercises hope inspire students dig deeper.Data analysts statisticians work issues dealing financial data need practical tools succeed.","code":""},{"path":"index.html","id":"what-will-you-learn","chapter":"Preface","heading":"What will you learn?","text":"book currently divided 5 parts:Chapter 1 introduces important concepts around approach Tidy Finance revolves.Chapters 2-4 provide tools organize data prepare common data sets used financial research. Although many important data behind paywalls, start describing different open source data download . move prepare two popular datasets financial research: CRSP Compustat. , cover corporate bond data TRACE. reuse data chapters subsequent chapters. Chapter 5 contains overview common alternative data provides direct access vie R packages exist.Chapters 6-11 deal key concepts empirical asset pricing beta estimation, portfolio sorts, performance analysis, asset pricing regressions.Chapters 12-15 apply linear models panel data machine learning methods problems factor selection option pricing.Chapters 16-17 provide approaches parametric, constrained portfolio optimization, backtesting procedures.chapter self-contained can read individually. Yet data chapters provide important background necessary data management chapters.","code":""},{"path":"index.html","id":"what-wont-you-learn","chapter":"Preface","heading":"What won’t you learn?","text":"book empirical work. assume basic knowledge statistics econometrics, provide detailed treatments underlying theoretical models methods applied book. Instead, find references seminal academic work journal articles textbooks detailed treatments.\nbelieve comparative advantage provide thorough implementation typical approaches portfolio sorts, backtesting procedures, regressions, machine learning methods, related topics empirical finance. enrich implementations discussions needy-greedy choices face conducting empirical analyses. hence refrain deriving theoretical models extensively discussing statistical properties well-established tools.book close spirit books provide fully reproducible code financial applications. view complementary work want highlight differences:Regenstein Jr (2018) provides excellent introduction discussion different tools standard applications finance (e.g., compute returns sample standard deviations time series stock returns). book, contrast, clear focus applications state---art academic research finance. thus fill niche allows aspiring researchers instructors rely well-designed code base.Coqueret Guida (2020) constitutes great compendium book respect applications related return prediction portfolio formation. book primarily targets practitioners hands-focus. book, contrast, relies typical databases used financial research focuses preparation datasets academic applications. addition, chapter machine learning focuses factor selection instead return prediction.Although emphasize importance reproducible workflow principles, provide introductions core tools relied create maintain book:Version control systems Git vital managing programming project. Originally designed organize collaboration software developers, even solo data analysts benefit adopting version control. Git also makes simple publicly share code allow others reproduce findings. refer Bryan (2022) gentle introduction (sometimes painful) life Git. Good communication results key ingredient reproducible transparent research. compile book, heavily draw suite fantastic open source tools. First, Wickham (2016) provides highly customizable, yet easy use system creating data visualizations. Wickham Grolemund (2016) provides intuitive introduction creating graphics using approach. Second, daily work compile book, used markdown-based authoring framework described Xie, Allaire, Grolemund (2018) Xie, Dervieux, Riederer (2020). Markdown documents fully reproducible support dozens static dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. provide introductions tools, resources already provide easily accessible tutorials.Good writing also important presentation findings. neither claim experts domain, try sound particularly academic. contrary, deliberately use colloquial language describe methods results presented book order allow readers relate easily mainly technical content. desire guidance respect proper academic writing financial economics, recommend Kiesling (2003), Cochrane (2005), Jacobsen (2014) provide essential tips (condensed pages).","code":""},{"path":"index.html","id":"why-r","chapter":"Preface","heading":"Why R?","text":"believe R among best choices programming language area finance. favorite features include:R free open-source can use academic professional contexts.diverse active online community works broad range tools.massive set actively maintained packages kinds applications exists, e.g., data manipulation, visualization, machine learning, etc.Powerful tools communication, e.g., Rmarkdown shiny, readily available.RStudio one best development environments interactive data analysis.Strong foundations functional programming provided.Smooth integration programming languages, e.g., SQL, Python, C, C++, Fortran, etc.information R great, refer Wickham et al. (2019).","code":""},{"path":"index.html","id":"why-tidy","chapter":"Preface","heading":"Why tidy?","text":"start working data, quickly realize spend lot time reading, cleaning, transforming data. fact, often said 80% data analysis spent preparing data. tidying data, want structure data sets facilitate analyses. Wickham (2014) puts :[T]idy datasets alike, every messy dataset messy way. Tidy datasets provide standardized way link structure dataset (physical layout) semantics (meaning).essence, tidy data follows three principles:Every column variable.Every row observation.Every cell single value.Throughout book, try follow principles best can. want learn tidy data principles informal manner, refer vignette part Wickham Girlich (2022).addition data layer, also tidy coding principles outlined tidy tools manifesto try follow:Reuse existing data structures.Compose simple functions pipe.Embrace functional programming.Design humans.particular, heavily draw set packages called tidyverse (Wickham et al. 2019). tidyverse consistent set packages data analysis tasks, ranging importing wrangling visualizing modeling data grammar. addition explicit tidy principles, tidyverse benefits: () master one package, easier master others, (ii) core packages developed maintained Public Benefit Company Posit.\ncore packages contained tidyverse : ggplot2 (Wickham 2016), dplyr (Wickham, François, et al. 2022), tidyr (Wickham Girlich 2022), readr (Wickham, Hester, Bryan 2022), purrr (Henry Wickham 2020), tibble (Müller Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021).Throughout book use native pipe |>, powerful tool clearly express sequence operations. Readers familiar tidyverse may used predecessor %>% part magrittr package. applications, native magrittr pipe behave identically, opt one simpler part base R. thorough discussion subtle differences two pipes, refer second edition Wickham Grolemund (2016).","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"continue, make sure software need book:Install R RStudio. get walk-installation every major operating system, follow steps outlined summary. whole process done clicks. wonder difference: R open-source language environment statistical computing graphics, free download use. R runs computations, RStudio integrated development environment provides interface adding many convenient features tools. suggest coding RStudio.Open RStudio install tidyverse. sure works? find helpful information install packages brief summary.new R, recommend starting following sources:gentle good introduction workings R can found form weighted dice project. done setting R machine, try follow instructions project.main book tidyverse, Wickham Grolemund (2016) available online free: R Data Science explains majority tools use book.instructor searching effectively teach R data science methods, recommend take look excellent data science toolbox Mine Cetinkaya-Rundel.RStudio provides range excellent cheat sheets extensive information use tidyverse packages.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"Preface","heading":"About the authors","text":"met Vienna Graduate School Finance us graduated different focus shared passion: coding R. continue sharpen R skills part current occupations:Christoph Scheuch Director Product social trading platform wikifolio.com. responsible product planning, execution, monitoring manages team data scientists analyze user behavior develop data-driven products. Christoph also external lecturer Vienna University Economics Business teaches finance students manage empirical projects.Stefan Voigt Assistant Professor Finance Department Economics University Copenhagen research fellow Danish Finance Institute. research focuses blockchain technology, high-frequency trading, financial econometrics. Stefan’s research published leading finance econometrics journals. teaches parts book courses empirical finance students practitioners.Patrick Weiss postdoctoral researcher Vienna University Economics Business external lecturer Reykjavik University. research activity centers around intersection empirical asset pricing corporate finance. Patrick especially passionate empirical asset pricing published research top journal financial economics.","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC -NC-SA 4.0.code samples book licensed Creative Commons CC0 1.0 Universal (CC0 1.0), .e., public domain.\n","code":""},{"path":"index.html","id":"colophon","chapter":"Preface","heading":"Colophon","text":"book written RStudio using bookdown (Xie 2016). website hosted GitHub Pages. complete source available GitHub.\ngenerated plots book using ggplot2 classic dark--light theme (theme_bw()).version book built R version 4.2.1 (2022-06-23, Funny-Looking Kid) following packages:","code":""},{"path":"introduction-to-tidy-finance.html","id":"introduction-to-tidy-finance","chapter":"1 Introduction to Tidy Finance","heading":"1 Introduction to Tidy Finance","text":"main aim chapter familiarize tidyverse. start downloading visualizing stock data Yahoo!Finance. move simple portfolio choice problem construct efficient frontier. examples introduce approach Tidy Finance.","code":""},{"path":"introduction-to-tidy-finance.html","id":"working-with-stock-market-data","chapter":"1 Introduction to Tidy Finance","heading":"1.1 Working with stock market data","text":"start session, load required packages.\nThroughout entire book, always use tidyverse (Wickham et al. 2019).\nchapter, also load convenient tidyquant package (Dancho Vaughan 2022a) download price data. package provides convenient wrapper various quantitative functions compatible tidyverse.typically install package can load .\ncase done yet, call install.packages(\"tidyquant\").\ntrouble using tidyquant, check corresponding documentation.first download daily prices one stock market ticker, e.g., Apple stock, AAPL, directly data provider Yahoo!Finance.\ndownload data, can use command tq_get.\nknow use , make sure read help file calling ?tq_get.\nespecially recommend taking look examples section documentation. request daily data period 20 year. tq_get downloads stock market data Yahoo!Finance specify another data source.\nfunction returns tibble eight quite self-explanatory columns: symbol, date, market prices open, high, low close, daily volume (number traded shares), adjusted price USD.\nadjusted prices corrected anything might affect stock price market closes, e.g., stock splits dividends.\nactions affect quoted prices, direct impact investors hold stock. Therefore, often rely adjusted prices comes analyzing returns investor earned holding stock continuously.Next, use ggplot2 package (Wickham 2016) visualize time series adjusted prices. package takes care visualization tasks based principles grammar graphics (Wilkinson 2012).\nFigure 1.1: Prices USD, adjusted divident payments stock splits.\n Instead analyzing prices, compute daily net returns defined \\((p_t - p_{t-1}) / p_{t-1} = p_t / p_{t-1} - 1\\) \\(p_t\\) adjusted day \\(t\\) price.\ncontext, function lag() helpful, returns previous value vector.resulting tibble contains three columns last contains daily returns (ret).\nNote first entry naturally contains missing value (NA) previous price.\nObviously, use lag() meaningless time series ordered ascending dates.\ncommand arrange() provides convenient way order observations correct way application. case want order observations descending dates, can use arrange(desc(date)).upcoming examples, remove missing values require separate treatment computing, e.g., sample averages. general, however, make sure understand NA values occur carefully examine can simply get rid observations.Next, visualize distribution daily returns histogram. convenience, multiply returns 100 get returns percent visualizations.\nAdditionally, add dashed red line indicates 5 percent quantile daily returns histogram, (crude) proxy worst return stock probability least 5 percent.\n5 percent quantile closely connected (historical) Value--risk, risk measure commonly monitored regulators. refer Tsay (2010) thorough introduction stylized facts returns.\nFigure 1.2: dotted vertical line indicates historical 5 percent quantile.\n, bins = 100 determines number bins used illustration hence implicitly width bins.\nproceeding, make sure understand use geom geom_vline() add dashed line indicates 5 percent quantile daily returns.\ntypical task proceeding data compute summary statistics main variables interest.see maximum daily return 13.905 percent. Perhaps surprisingly, daily average return close slightly 0.\nline illustration , large losses day minimum returns indicate strong asymmetry distribution returns.\ncan also compute summary statistics year individually imposing group_by(year = year(date)), call year(date) returns year. specifically, lines code compute summary statistics individual groups data, defined year. summary statistics therefore allow eyeball analysis time-series dynamics return distribution.case wonder: additional argument .names = \"{.fn}\" across() determines name output columns. specification rather flexible allows almost arbitrary column names, can useful reporting. print() function simply controls output options R console.","code":"\nlibrary(tidyverse)\nlibrary(tidyquant)\nprices <- tq_get(\"AAPL\",\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\nprices# A tibble: 5,535 × 8\n  symbol date        open  high   low close    volume adjusted\n  <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800    0.853\n2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600    0.781\n3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600    0.793\n4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800    0.724\n5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400    0.759\n# … with 5,530 more rows\nprices |>\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Apple stock prices between beginning of 2000 and end of 2021\"\n  )\nreturns <- prices |>\n  arrange(date) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret)\nreturns# A tibble: 5,535 × 3\n  symbol date           ret\n  <chr>  <date>       <dbl>\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# … with 5,530 more rows\nreturns <- returns |>\n  drop_na(ret)\nquantile_05 <- quantile(returns |> pull(ret) * 100, probs = 0.05)\n\nreturns |>\n  ggplot(aes(x = ret * 100)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns in percent\"\n  )\nreturns |>\n  mutate(ret = ret * 100) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           <dbl>        <dbl>         <dbl>         <dbl>\n1          0.130         2.52         -51.9          13.9\nreturns |>\n  mutate(ret = ret * 100) |>\n  group_by(year = year(date)) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)# A tibble: 22 × 5\n    year daily_mean daily_sd daily_min daily_max\n   <dbl>      <dbl>    <dbl>     <dbl>     <dbl>\n 1  2000   -0.346       5.49    -51.9      13.7 \n 2  2001    0.233       3.93    -17.2      12.9 \n 3  2002   -0.121       3.05    -15.0       8.46\n 4  2003    0.186       2.34     -8.14     11.3 \n 5  2004    0.470       2.55     -5.58     13.2 \n 6  2005    0.349       2.45     -9.21      9.12\n 7  2006    0.0949      2.43     -6.33     11.8 \n 8  2007    0.366       2.38     -7.02     10.5 \n 9  2008   -0.265       3.67    -17.9      13.9 \n10  2009    0.382       2.14     -5.02      6.76\n11  2010    0.183       1.69     -4.96      7.69\n12  2011    0.104       1.65     -5.59      5.89\n13  2012    0.130       1.86     -6.44      8.87\n14  2013    0.0472      1.80    -12.4       5.14\n15  2014    0.145       1.36     -7.99      8.20\n16  2015    0.00199     1.68     -6.12      5.74\n17  2016    0.0575      1.47     -6.57      6.50\n18  2017    0.164       1.11     -3.88      6.10\n19  2018   -0.00573     1.81     -6.63      7.04\n20  2019    0.266       1.65     -9.96      6.83\n21  2020    0.281       2.94    -12.9      12.0 \n22  2021    0.133       1.58     -4.17      5.39"},{"path":"introduction-to-tidy-finance.html","id":"scaling-up-the-analysis","chapter":"1 Introduction to Tidy Finance","heading":"1.2 Scaling up the analysis","text":"next step, generalize code computations can handle arbitrary vector tickers (e.g., constituents index). Following tidy principles, quite easy download data, plot price time series, tabulate summary statistics arbitrary number assets.tidyverse magic starts: tidy data makes extremely easy generalize computations many assets like. following code takes vector tickers, e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\"), automates download well plot price time series.\nend, create table summary statistics arbitrary number assets. perform analysis data current constituents Dow Jones Industrial Average index. Conveniently, tidyquant provides function get stocks stock index single call (similarly, tq_exchange(\"NASDAQ\") delivers stocks currently listed NASDAQ exchange). resulting tibble contains 163553 daily observations 30 different corporations.\nfigure illustrates time series downloaded adjusted prices constituents Dow Jones index. Make sure understand every single line code! (arguments aes()? alternative geoms use visualize time series? Hint: know answers try change code see difference intervention causes).\nFigure 1.3: Prices USD, adjusted dividend payments stock splits.\nnotice small differences relative code used ? tq_get(ticker) returns tibble several symbols well. need illustrate tickers simultaneously include color = symbol ggplot2 aesthetics. way, generate separate line ticker. course, simply many lines graph properly identify individual stocks, illustrates point well.holds stock returns. computing returns, use group_by(symbol) mutate() command performed symbol individually. logic also applies computation summary statistics: group_by(symbol) key aggregating time series ticker-specific variables interest.Note now also equipped tools download price data ticker listed S&P 500 index number lines code. Just use ticker <- tq_index(\"SP500\"), provides tibble contains symbol (currently) part S&P 500. However, don’t try prepared wait couple minutes quite data download!","code":"\nticker <- tq_index(\"DOW\")\nticker# A tibble: 30 × 8\n  symbol company          ident…¹ sedol weight sector share…² local…³\n  <chr>  <chr>            <chr>   <chr>  <dbl> <chr>    <dbl> <chr>  \n1 UNH    UnitedHealth Gr… 91324P… 2917… 0.113  Healt… 5738247 USD    \n2 GS     Goldman Sachs G… 38141G… 2407… 0.0686 Finan… 5738247 USD    \n3 HD     Home Depot Inc.  437076… 2434… 0.0590 Consu… 5738247 USD    \n4 MCD    McDonald's Corp… 580135… 2550… 0.0543 Consu… 5738247 USD    \n5 MSFT   Microsoft Corpo… 594918… 2588… 0.0528 Infor… 5738247 USD    \n# … with 25 more rows, and abbreviated variable names ¹​identifier,\n#   ²​shares_held, ³​local_currency\nindex_prices <- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2022-12-31\"\n)\nindex_prices |>\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of DOW index constituents\"\n  ) +\n  theme(legend.position = \"none\")\nall_returns <- index_prices |>\n  group_by(symbol) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret) |>\n  drop_na(ret)\n\nall_returns |>\n  mutate(ret = ret * 100) |>\n  group_by(symbol) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   <chr>       <dbl>    <dbl>     <dbl>     <dbl>\n 1 AAPL       0.123      2.51     -51.9      13.9\n 2 AMGN       0.0467     1.97     -13.4      15.1\n 3 AXP        0.0512     2.30     -17.6      21.9\n 4 BA         0.0533     2.23     -23.8      24.3\n 5 CAT        0.0647     2.04     -14.5      14.7\n 6 CRM        0.113      2.69     -27.1      26.0\n 7 CSCO       0.0290     2.38     -16.2      24.4\n 8 CVX        0.0519     1.76     -22.1      22.7\n 9 DIS        0.0442     1.94     -18.4      16.0\n10 DOW        0.0418     2.63     -21.7      20.9\n11 GS         0.0529     2.32     -19.0      26.5\n12 HD         0.0519     1.94     -28.7      14.1\n13 HON        0.0480     1.94     -17.4      28.2\n14 IBM        0.0249     1.65     -15.5      12.0\n15 INTC       0.0289     2.36     -22.0      20.1\n16 JNJ        0.0401     1.22     -15.8      12.2\n17 JPM        0.0550     2.43     -20.7      25.1\n18 KO         0.0325     1.32     -10.1      13.9\n19 MCD        0.0526     1.48     -15.9      18.1\n20 MMM        0.0369     1.50     -12.9      12.6\n21 MRK        0.0342     1.68     -26.8      13.0\n22 MSFT       0.0514     1.93     -15.6      19.6\n23 NKE        0.0713     1.92     -19.8      15.5\n24 PG         0.0360     1.34     -30.2      12.0\n25 TRV        0.0541     1.84     -20.8      25.6\n26 UNH        0.0989     1.98     -18.6      34.8\n27 V          0.0910     1.90     -13.6      15.0\n28 VZ         0.0238     1.51     -11.8      14.6\n29 WBA        0.0260     1.81     -15.0      16.6\n30 WMT        0.0301     1.50     -11.4      11.7"},{"path":"introduction-to-tidy-finance.html","id":"other-forms-of-data-aggregation","chapter":"1 Introduction to Tidy Finance","heading":"1.3 Other forms of data aggregation","text":"course, aggregation across variables symbol can make sense well. instance, suppose interested answering question: days high aggregate trading volume likely followed days high aggregate trading volume? provide initial analysis question, take downloaded data compute aggregate daily trading volume Dow Jones constituents USD.\nRecall column volume denoted number traded shares.\nThus, multiply trading volume daily closing price get proxy aggregate trading volume USD. Scaling 1e9 (R can handle scientific notation) denotes daily trading volume billion USD.\nFigure 1.4: Total daily trading volume billion USD.\nfigure indicates clear upwards trend aggregated daily trading volume. particular since outbreak COVID-19 pandemic markets process huge trading volume, analyzed instance Goldstein, Koijen, Mueller (2021).\nOne way illustrate persistence trading volume plot volume day \\(t\\) volume day \\(t-1\\) example . add dotted 45°-line indicate hypothetical one--one relation geom_abline(), addressing potential differences axes’ scales.\nFigure 1.5: Total daily trading volume billion USD.\nunderstand warning ## Warning: Removed 1 rows containing missing values (geom_point). comes means? Purely eye-balling reveals days high trading volume often followed similarly high trading volume days.","code":"\nvolume <- index_prices |>\n  group_by(date) |>\n  summarize(volume = sum(volume * close / 1e9))\n\nvolume |>\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of DOW index constitutens\"\n  )\nvolume |>\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of DOW index constituents\"\n  )Warning: Removed 1 rows containing missing values (geom_point)."},{"path":"introduction-to-tidy-finance.html","id":"portfolio-choice-problems","chapter":"1 Introduction to Tidy Finance","heading":"1.4 Portfolio choice problems","text":"previous part, show download stock market data inspect graphs summary statistics.\nNow, move typical question Finance, namely, optimally allocate wealth across different assets. standard framework optimal portfolio selection considers investors prefer higher future returns dislike future return volatility (defined square root return variance): mean-variance investor (Markowitz 1952). essential tool evaluate portfolios mean-variance context efficient frontier, set portfolios satisfy condition portfolio exists higher expected return volatility (square-root variance, .e., risk), see, e.g., Merton (1972).\ncompute visualize efficient frontier several stocks.\nFirst, extract asset’s monthly returns.\norder keep things simple, work balanced panel exclude DOW constituents observe price every single trading day since year 2000., floor_date() function lubridate package (Grolemund Wickham 2011) provides useful functions work dates time.Next, transform returns tidy tibble \\((T \\times N)\\) matrix one column \\(N\\) tickers one row \\(T\\) trading days compute sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] \\(r_t\\) \\(N\\) vector returns date \\(t\\) sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'.\\]\nachieve using pivot_wider() new column names column symbol setting values ret.\ncompute vector sample average returns sample variance-covariance matrix, consider proxies parameters distribution future stock returns.\nThus, simplicity refer \\(\\Sigma\\) \\(\\mu\\) instead explictly highlighting sample moments estimates. later chapters, discuss issues arise take estimation uncertainty account., compute minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) well expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) portfolio.\nRecall minimum variance portfolio vector portfolio weights solution \n\\[\\omega_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } \\sum\\limits_{=1}^Nw_i = 1.\\]\nconstraint weights sum one simply implies funds distributed across available asset universe, possibility retain cash.\neasy show analytically, \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\) \\(\\iota\\) vector ones \\(\\Sigma^{-1}\\) inverse \\(\\Sigma\\).command solve(, b) returns solution system equations \\(Ax = b\\). b provided, example , defaults identity matrix solve(Sigma) delivers \\(\\Sigma^{-1}\\) (unique solution exists).\nNote monthly volatility minimum variance portfolio order magnitude daily standard deviation individual components. Thus, diversification benefits terms risk reduction tremendous!Next, set find weights portfolio achieves, example, three times expected return minimum variance portfolio.\nHowever, mean-variance investors interested portfolio achieves required return rather efficient portfolio, .e., portfolio lowest standard deviation.\nwonder solution \\(\\omega_\\text{eff}\\) comes : efficient portfolio chosen investor aims achieve minimum variance given minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, objective function choose \\(\\omega_\\text{eff}\\) solution \n\\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]code implements analytic solution optimization problem benchmark return \\(\\bar\\mu\\) set 3 times expected return minimum variance portfolio. encourage verify correct.","code":"\nindex_prices <- index_prices |>\n  group_by(symbol) |>\n  mutate(n = n()) |>\n  ungroup() |>\n  filter(n == max(n)) |>\n  select(-n)\n\nreturns <- index_prices |>\n  mutate(month = floor_date(date, \"month\")) |>\n  group_by(symbol, month) |>\n  summarize(price = last(adjusted), .groups = \"drop_last\") |>\n  mutate(ret = price / lag(price) - 1) |>\n  drop_na(ret) |>\n  select(-price)\nreturns_matrix <- returns |>\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |>\n  select(-month)\n\nSigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(Sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\n\ntibble(\n  average_ret = as.numeric(t(mvp_weights) %*% mu),\n  volatility = as.numeric(sqrt(t(mvp_weights) %*% Sigma %*% mvp_weights))\n)# A tibble: 1 × 2\n  average_ret volatility\n        <dbl>      <dbl>\n1     0.00773     0.0314\nmu_bar <- 3 * t(mvp_weights) %*% mu\n\nC <- as.numeric(t(iota) %*% solve(Sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(Sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(Sigma) %*% mu)\n\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights +\n  lambda_tilde / 2 * (solve(Sigma) %*% mu - D * mvp_weights)"},{"path":"introduction-to-tidy-finance.html","id":"the-efficient-frontier","chapter":"1 Introduction to Tidy Finance","heading":"1.5 The efficient frontier","text":" two mutual fund separation theorem states soon two efficient portfolios (minimum variance portfolio \\(w_{mvp}\\) efficient portfolio higher required level expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), can characterize entire efficient frontier combining two portfolios.\n, linear combination two portfolio weights represent efficient portfolio.\ncode implements construction efficient frontier, characterizes highest expected return achievable level risk. understand code better, make sure familiarize inner workings loop.code proceeds two steps: First, compute vector combination weights \\(c\\) evaluate resulting linear combination \\(c\\\\mathbb{R}\\):\\[w^* = cw_\\text{eff}(\\bar\\mu) + (1-c)w_{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] \\(\\lambda^* = 2\\frac{c\\bar\\mu + (1-c)\\tilde\\mu - D/C}{E-D^2/C}\\).\nFinally, simple visualize efficient frontier alongside two efficient portfolios within one, powerful figure using ggplot2. also add individual stocks call.\ncompute annualized returns based simple assumption monthly returns independent identically distributed. Thus, average annualized return just 12 times expected monthly return.\nFigure 1.6: big dots indicate location minimum variance efficient tangency portfolios, respectively. small dots indicate location individual constituents.\nline indicates efficient frontier: set portfolios mean-variance efficient investor choose . Compare performance relative individual assets (dots) - become clear diversifying yields massive performance gains (least long take parameters \\(\\Sigma\\) \\(\\mu\\) given).","code":"\nc <- seq(from = -0.4, to = 1.9, by = 0.01)\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\n\nfor (i in seq_along(c)) {\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights\n  res$mu[i] <- 12 * 100 * t(w) %*% mu\n  res$sd[i] <- 12 * sqrt(100) * sqrt(t(w) %*% Sigma %*% w)\n}\nres |>\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point(\n    data = res |> filter(c %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point(\n    data = tibble(\n      mu = 12 * 100 * mu,\n      sd = 12 * 10 * sqrt(diag(Sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Efficient frontier for DOW index constituents\"\n  )"},{"path":"introduction-to-tidy-finance.html","id":"exercises","chapter":"1 Introduction to Tidy Finance","heading":"1.6 Exercises","text":"Download daily prices another stock market ticker choice Yahoo!Finance tq_get() tidyquant package. Plot two time series ticker’s un-adjusted adjusted closing prices. Explain differences.Compute daily net returns asset visualize distribution daily returns histogram. Also, use geom_vline() add dashed red line indicates 5 percent quantile daily returns within histogram. Compute summary statistics (mean, standard deviation, minimum maximum) daily returnsTake code generalize can perform computations arbitrary vector tickers (e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\")). Automate download, plot price time series, create table return summary statistics arbitrary number assets.Consider research question: days high aggregate trading volume often also days large absolute price changes? Find appropriate visualization analyze question.Compute monthly returns downloaded stock market prices. Compute vector historical average returns sample variance-covariance matrix. Compute minimum variance portfolio weights portfolio volatility average returns. Visualize mean-variance efficient frontier. Choose one assets identify portfolio yields historical volatility achieves highest possible average return.portfolio choice analysis, restricted sample assets trading every single day since 2000. decision problem want infer future expected portfolio performance results?efficient frontier characterizes portfolios highest expected return different levels risk, .e., standard deviation. Identify portfolio highest expected return per standard deviation. Hint: ratio expected return standard deviation important concept Finance.","code":""},{"path":"accessing-managing-financial-data.html","id":"accessing-managing-financial-data","chapter":"2 Accessing & managing financial data","heading":"2 Accessing & managing financial data","text":"chapter, suggest way organize financial data. Everybody, experience data, also familiar storing data various formats like CSV, XLS, XLSX, delimited value stores. Reading saving data can become cumbersome case using different data formats, across different projects across different programming languages. Moreover, storing data delimited files often leads problems respect column type consistency. instance, date-type columns frequently lead inconsistencies across different data formats programming languages.chapter shows import different open source data sets. Specifically, data comes application programming interface (API) Yahoo!Finance, downloaded standard CSV files, XLSX file stored public Google Drive repository, macroeconomic time series. store data single database, serves source data subsequent chapters. conclude chapter providing tips managing databases.First, load global packages use throughout chapter. Later , load packages sections need .package lubridate provides convenient tools work dates time (Grolemund Wickham 2011). package scales (Wickham Seidel 2022) provides useful scale functions visualizations.Moreover, initially define date range fetch store financial data, making future data updates tractable. case need another time frame, need adjust dates. data starts 1960 since asset pricing studies use data 1962 .","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nstart_date <- ymd(\"1960-01-01\")\nend_date <- ymd(\"2021-12-31\")"},{"path":"accessing-managing-financial-data.html","id":"fama-french-data","chapter":"2 Accessing & managing financial data","heading":"2.1 Fama-French data","text":"start downloading famous Fama-French factors (e.g., Fama French 1993) portfolio returns commonly used empirical asset pricing. Fortunately, neat package Nelson Areal allows us easily access data: frenchdata package provides functions download read data sets Prof. Kenneth French finance data library (Areal 2021).can use main function package download monthly Fama-French factors. set 3 Factors includes return time series market, size, value factors alongside risk-free rates. Note manual work correctly parse columns scale appropriately raw Fama-French data comes unpractical data format. precise descriptions variables, suggest consulting Prof. Kenneth French finance data library directly. site, check raw data files appreciate time can save thanks frenchdata.straightforward download corresponding daily Fama-French factors function.subsequent chapter, also use 10 monthly industry portfolios, let us fetch data, .worth taking look available portfolio return time series Kenneth French’s homepage. check sets calling get_french_data_list().","code":"\nlibrary(frenchdata)\nfactors_ff_monthly_raw <- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff_monthly <- factors_ff_monthly_raw$subsets$data[[1]] |>\n  transmute(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(month >= start_date & month <= end_date)\nfactors_ff_daily_raw <- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff_daily <- factors_ff_daily_raw$subsets$data[[1]] |>\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(date >= start_date & date <= end_date)\nindustries_ff_monthly_raw <- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly <- industries_ff_monthly_raw$subsets$data[[1]] |>\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |>\n  mutate(across(where(is.numeric), ~ . / 100)) |>\n  select(month, everything(), -date) |>\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"q-factors","chapter":"2 Accessing & managing financial data","heading":"2.2 q-factors","text":"recent years, academic discourse experienced rise alternative factor models, e.g., form Hou, Xue, Zhang (2014) q-factor model. refer extended background information provided original authors information. q factors can downloaded directly authors’ homepage within read_csv().also need adjust data. First, discard information use remainder book. , rename columns “R_”-prescript using regular expressions write column names lower case. always try sticking consistent style naming objects, try illustrate - emphasis try. can check style guides available online, e.g., Hadley Wickham’s tidyverse style guide.","code":"\nfactors_q_monthly_link <-\n  \"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2021.csv\"\n\nfactors_q_monthly <- read_csv(factors_q_monthly_link) |>\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |>\n  select(-R_F, -R_MKT, -year) |>\n  rename_with(~ str_remove(., \"R_\")) |>\n  rename_with(~ str_to_lower(.)) |>\n  mutate(across(-month, ~ . / 100)) |>\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"macroeconomic-predictors","chapter":"2 Accessing & managing financial data","heading":"2.3 Macroeconomic predictors","text":"next data source set macroeconomic variables often used predictors equity premium. Welch Goyal (2008) comprehensively reexamine performance variables suggested academic literature good predictors equity premium. authors host data updated 2021 Amit Goyal’s website. Since data .xlsx-file stored public Google drive location, need additional packages access data directly R session. Therefore, load readxl read .xlsx-file (Wickham Bryan 2022) googledrive Google drive connection (D’Agostino McGowan Bryan 2021).Usually, need authenticate interact Google drive directly R. Since data stored via public link, can proceed without authentication.drive_download() function googledrive package allows us download data store locally.Next, read new data transform columns variables later use:dividend price ratio (dp), difference log dividends log prices, dividends 12-month moving sums dividends paid S&P 500 index, prices monthly averages daily closing prices (John Y. Campbell Shiller 1988; John Y. Campbell Yogo 2006).Dividend yield (dy), difference log dividends log lagged prices (Ball 1978).Earnings price ratio (ep), difference log earnings log prices, earnings 12-month moving sums earnings S&P 500 index (John Y. Campbell Shiller 1988).Dividend payout ratio (de), difference log dividends log earnings (Lamont 1998).Stock variance (svar), sum squared daily returns S&P 500 index (Guo 2006).Book--market ratio (bm), ratio book value market value Dow Jones Industrial Average (Kothari Shanken 1997)Net equity expansion (ntis),ratio 12-month moving sums net issues NYSE listed stocks divided total end--year market capitalization NYSE stocks (John Y. Campbell, Hilscher, Szilagyi 2008).Treasury bills (tbl), 3-Month Treasury Bill: Secondary Market Rate economic research data base Federal Reserve Bank St. Louis (John Y. Campbell 1987).Long term yield (lty), long-term government bond yield Ibbotson’s Stocks, Bonds, Bills Inflation Yearbook (Welch Goyal 2008).Long term rate returns (ltr), long-term government bond returns Ibbotson’s Stocks, Bonds, Bills Inflation Yearbook (Welch Goyal 2008).Term spread (tms), difference long term yield government bonds Treasury bill (John Y. Campbell 1987).Default yield spread (dfy), difference BAA AAA-rated corporate bond yields (Fama French 1989).Inflation (infl), Consumer Price Index (Urban Consumers) Bureau Labor Statistics (John Y. Campbell Vuolteenaho 2004).can consult material Amit Goyal’s website variable definitions required data transformations.Finally, reading macro predictors memory, remove raw data file temporary storage.","code":"\nlibrary(readxl)\nlibrary(googledrive)\ndrive_deauth()\nmacro_predictors_link <-\n  \"https://docs.google.com/spreadsheets/d/1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2\"\n\ndrive_download(\n  macro_predictors_link,\n  path = \"data/macro_predictors.xlsx\"\n)\nmacro_predictors <- read_xlsx(\n  \"data/macro_predictors.xlsx\",\n  sheet = \"Monthly\"\n) |>\n  mutate(month = ym(yyyymm)) |>\n  mutate(across(where(is.character), as.numeric)) |>\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |>\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |>\n  filter(month >= start_date & month <= end_date) |>\n  drop_na()\nfile.remove(\"data/macro_predictors.xlsx\")[1] TRUE"},{"path":"accessing-managing-financial-data.html","id":"other-macroeconomic-data","chapter":"2 Accessing & managing financial data","heading":"2.4 Other macroeconomic data","text":"Federal Reserve bank St. Louis provides Federal Reserve Economic Data (FRED), extensive database macroeconomic data. total, 817,000 US international time series 108 different sources. illustration, use already familiar tidyquant package fetch consumer price index (CPI) data can found CPIAUCNS key.download time series, just look FRED website extract corresponding key address. instance, produce price index gold ores can found PCU2122212122210 key. tidyquant package provides access around 10,000 time series FRED database. desired time series included, recommend working fredr package (Boysel Vaughan 2021). Note need get API key use functionality. refer package documentation details.","code":"\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date,\n  to = end_date\n) |>\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)]\n  )"},{"path":"accessing-managing-financial-data.html","id":"setting-up-a-database","chapter":"2 Accessing & managing financial data","heading":"2.5 Setting up a database","text":"Now downloaded (freely available) data web memory R session, let us set database store information future use. use data stored database throughout following chapters, alternatively implement different strategy replace respective code.many ways set organize database, depending use case. purpose, efficient way use SQLite database, C-language library implements small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note SQL (Structured Query Language) standard language accessing manipulating databases, heavily inspired dplyr functions. refer tutorial information SQL.two packages make working SQLite R simple: RSQLite (Müller et al. 2022) embeds SQLite database engine R dbplyr (Wickham, Girlich, Ruiz 2022) database back-end dplyr. packages allow set database remotely store tables use remote database tables -memory data frames automatically converting dplyr SQL. Check RSQLite dbplyr vignettes information.SQLite database easily created - code really , need external software. Note use extended_types=TRUE option enable date types storing fetching data, otherwise date columns stored retrieved integers. use resulting file tidy_finance.sqlite subfolder data subsequent chapters retrieve data.Next, create remote table monthly Fama-French factor data. function dbWriteTable(), copies data SQLite-database.can use remote table -memory data frame building connection via tbl().dplyr calls evaluated lazily, .e., data memory R session, actually, database work. can see noticing output show number rows. fact, following code chunk fetches top 10 rows database printing.want whole table memory, need collect() . see regularly load data memory next chapters.last couple code chunks really organize simple database! can also share SQLite database across devices programming languages.move next data source, let us also store five tables new SQLite database.now , need access data stored database follow three steps: () Establish connection SQLite database, (ii) call table want extract, (iii) collect data. convenience, following steps show need compact fashion.","code":"\nlibrary(RSQLite)\nlibrary(dbplyr)\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n  dbWriteTable(tidy_finance,\n    \"factors_ff_monthly\",\n    value = factors_ff_monthly,\n    overwrite = TRUE\n  )\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\nfactors_ff_monthly_db |>\n  select(month, rf)# Source:   SQL [?? x 2]\n# Database: sqlite 3.39.3 [data/tidy_finance.sqlite]\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with more rows\nfactors_ff_monthly_db |>\n  select(month, rf) |>\n  collect()# A tibble: 744 × 2\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with 739 more rows\n  dbWriteTable(tidy_finance,\n    \"factors_ff_daily\",\n    value = factors_ff_daily,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"industries_ff_monthly\",\n    value = industries_ff_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"factors_q_monthly\",\n    value = factors_q_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"macro_predictors\",\n    value = macro_predictors,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"cpi_monthly\",\n    value = cpi_monthly,\n    overwrite = TRUE\n  )\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly <- factors_q_monthly |> collect()"},{"path":"accessing-managing-financial-data.html","id":"managing-sqlite-databases","chapter":"2 Accessing & managing financial data","heading":"2.6 Managing SQLite databases","text":"Finally, end data chapter, revisit SQLite database . drop database objects tables delete data tables, database file size remains unchanged SQLite just marks deleted objects free reserves space future uses. result, database file always grows size.optimize database file, can run VACUUM command database, rebuilds database frees unused space. can execute command database using dbSendQuery() function.VACUUM command actually performs couple additional cleaning steps, can read tutorial.Apart cleaning , might interested listing tables currently database. can via dbListTables() function.function comes handy unsure correct naming tables database.","code":"\ndbSendQuery(tidy_finance, \"VACUUM\")<SQLiteResult>\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\ndbListTables(tidy_finance)Warning: Closing open result set, pending rows [1] \"beta\"                  \"compustat\"            \n [3] \"cpi_monthly\"           \"crsp_daily\"           \n [5] \"crsp_monthly\"          \"factors_ff_daily\"     \n [7] \"factors_ff_monthly\"    \"factors_q_monthly\"    \n [9] \"industries_ff_monthly\" \"macro_predictors\"     \n[11] \"mergent\"               \"trace_enhanced\"       "},{"path":"accessing-managing-financial-data.html","id":"exercises-1","chapter":"2 Accessing & managing financial data","heading":"2.7 Exercises","text":"Download monthly Fama-French factors manually Ken French’s data library read via read_csv(). Validate get data via frenchdata package.Download Fama-French 5 factors using frenchdata package. Use get_french_data_list() find corresponding table name. successful download conversion column format used , compare resulting rf, mkt_excess, smb hml columns factors_ff_monthly. Explain differences might find.","code":""},{"path":"wrds-crsp-and-compustat.html","id":"wrds-crsp-and-compustat","chapter":"3 WRDS, CRSP, and Compustat","heading":"3 WRDS, CRSP, and Compustat","text":"chapter shows connect Wharton Research Data Services (WRDS), popular provider financial economic data research applications. use connection download commonly used data stock firm characteristics, CRSP Compustat. Unfortunately, data freely available, students researchers typically access WRDS university libraries. Assuming access WRDS, show prepare merge databases store SQLite-database introduced previous chapter. conclude chapter providing tips working WRDS database.First, load packages use throughout chapter. Later , load packages sections need .use date range previous chapter ensure consistency.","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(RSQLite)\nlibrary(dbplyr)\nstart_date <- ymd(\"1960-01-01\")\nend_date <- ymd(\"2021-12-31\")"},{"path":"wrds-crsp-and-compustat.html","id":"accessing-wrds","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.1 Accessing WRDS","text":"WRDS widely used source asset firm-specific financial data used academic settings. WRDS data platform provides data validation, flexible delivery options, access many different data sources. data WRDS also organized SQL database, although use PostgreSQL engine. database engine just easy handle R SQLite. use RPostgres package establish connection WRDS database (Wickham, Ooms, Müller 2022). Note also use odbc package connect PostgreSQL database, need install appropriate drivers . RPostgres already contains suitable driver.establish connection, use function dbConnect() following arguments. Note need replace user password fields credentials. defined system variables purpose book obviously want (allowed) share credentials rest world.remote connection WRDS useful. Yet, database contains many different tables. can check WRDS homepage identify table’s name looking (go beyond exposition). Alternatively, can also query data structure function dbSendQuery(). interested, exercise based WRDS’ tutorial “Querying WRDS Data using R”. Furthermore, penultimate section chapter shows investigate structure databases.","code":"\nlibrary(RPostgres)\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)"},{"path":"wrds-crsp-and-compustat.html","id":"downloading-and-preparing-crsp","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.2 Downloading and preparing CRSP","text":"Center Research Security Prices (CRSP) provides widely used data US stocks. use wrds connection object just created first access monthly CRSP return data. Actually, need three tables get desired data: () CRSP monthly security file,identifying information,(iii) delisting information.use three remote tables fetch data want put local database. Just , idea let WRDS database work just download data actually need. apply common filters data selection criteria narrow data interest: () keep data time windows interest, (ii) keep US-listed stocks identified via share codes shrcd 10 11, (iii) keep months within permno-specific start dates namedt end dates nameendt. addition, add delisting codes returns. can read great textbook Bali, Engle, Murray (2016) extensive discussion filters apply code .Now, relevant monthly return data memory proceed preparing data future analyses. perform preparation step current stage since want avoid executing mutations every time use data subsequent chapters.first additional variable create market capitalization (mktcap), product number outstanding shares shrout last traded price month altprc. Note contrast returns `ret, two variables adjusted ex-post corporate actions like stock splits. Moreover, altprc negative whenever last traded price exist CRSP decides report mid quote last available orderbook instead. Hence, take absolute value market cap. also keep market cap millions USD just convenience want print huge numbers figures tables. addition, set zero market cap missing makes conceptually little sense (.e., firm bankrupt).next variable frequently use one-month lagged market capitalization. Lagged market capitalization typically used compute value-weighted portfolio returns, demonstrate later chapter. simple consistent way add column lagged market cap values add one month observation join information monthly CRSP data.wonder use lag() function, e.g., via crsp_monthly |> group_by(permno) |> mutate(mktcap_lag = lag(mktcap)), take look exercises.Next, follow Bali, Engle, Murray (2016) transforming listing exchange codes explicit exchange names.Similarly, transform industry codes industry descriptions following Bali, Engle, Murray (2016). Notice also categorizations industries (e.g., Fama French 1997) commonly used.also construct returns adjusted delistings described Bali, Engle, Murray (2016). delisting security usually results company ceases operations, declares bankruptcy, merges, meet listing requirements, seeks become private. adjustment tries reflect returns investors bought stock month delisting held delisting date. transformation, can drop delisting returns codes.Next, compute excess returns subtracting monthly risk-free rate provided Fama-French data. base analyses excess returns, can drop adjusted returns risk-free rate tibble. Note ensure excess returns bounded -1 less -100% return make conceptually sense. can adjust returns, connect database load tibble factors_ff_monthly.Since excess returns market capitalization crucial analyses, can safely exclude observations missing returns market capitalization.Finally, store monthly CRSP file database.","code":"\nmsf_db <- tbl(wrds, in_schema(\"crsp\", \"msf\"))\nmsenames_db <- tbl(wrds, in_schema(\"crsp\", \"msenames\"))\nmsedelist_db <- tbl(wrds, in_schema(\"crsp\", \"msedelist\"))\ncrsp_monthly <- msf_db |>\n  filter(date >= start_date & date <= end_date) |>\n  inner_join(\n    msenames_db |>\n      filter(shrcd %in% c(10, 11)) |>\n      select(permno, exchcd, siccd, namedt, nameendt),\n    by = c(\"permno\")\n  ) |>\n  filter(date >= namedt & date <= nameendt) |>\n  mutate(month = floor_date(date, \"month\")) |>\n  left_join(\n    msedelist_db |>\n      select(permno, dlstdt, dlret, dlstcd) |>\n      mutate(month = floor_date(dlstdt, \"month\")),\n    by = c(\"permno\", \"month\")\n  ) |>\n  select(\n    permno, # Security identifier\n    date, # Date of the observation\n    month, # Month of the observation\n    ret, # Return\n    shrout, # Shares outstanding (in thousands)\n    altprc, # Last traded price in a month\n    exchcd, # Exchange code\n    siccd, # Industry code\n    dlret, # Delisting return\n    dlstcd # Delisting code\n  ) |>\n  collect() |>\n  mutate(\n    month = ymd(month),\n    shrout = shrout * 1000\n  )\ncrsp_monthly <- crsp_monthly |>\n  mutate(\n    mktcap = abs(shrout * altprc) / 1000000,\n    mktcap = na_if(mktcap, 0)\n  )\nmktcap_lag <- crsp_monthly |>\n  mutate(month = month %m+% months(1)) |>\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(mktcap_lag, by = c(\"permno\", \"month\"))\ncrsp_monthly <- crsp_monthly |>\n  mutate(exchange = case_when(\n    exchcd %in% c(1, 31) ~ \"NYSE\",\n    exchcd %in% c(2, 32) ~ \"AMEX\",\n    exchcd %in% c(3, 33) ~ \"NASDAQ\",\n    TRUE ~ \"Other\"\n  ))\ncrsp_monthly <- crsp_monthly |>\n  mutate(industry = case_when(\n    siccd >= 1 & siccd <= 999 ~ \"Agriculture\",\n    siccd >= 1000 & siccd <= 1499 ~ \"Mining\",\n    siccd >= 1500 & siccd <= 1799 ~ \"Construction\",\n    siccd >= 2000 & siccd <= 3999 ~ \"Manufacturing\",\n    siccd >= 4000 & siccd <= 4899 ~ \"Transportation\",\n    siccd >= 4900 & siccd <= 4999 ~ \"Utilities\",\n    siccd >= 5000 & siccd <= 5199 ~ \"Wholesale\",\n    siccd >= 5200 & siccd <= 5999 ~ \"Retail\",\n    siccd >= 6000 & siccd <= 6799 ~ \"Finance\",\n    siccd >= 7000 & siccd <= 8999 ~ \"Services\",\n    siccd >= 9000 & siccd <= 9999 ~ \"Public\",\n    TRUE ~ \"Missing\"\n  ))\ncrsp_monthly <- crsp_monthly |>\n  mutate(ret_adj = case_when(\n    is.na(dlstcd) ~ ret,\n    !is.na(dlstcd) & !is.na(dlret) ~ dlret,\n    dlstcd %in% c(500, 520, 580, 584) |\n      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,\n    dlstcd == 100 ~ ret,\n    TRUE ~ -1\n  )) |>\n  select(-c(dlret, dlstcd))\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly |> select(month, rf),\n    by = \"month\"\n  ) |>\n  mutate(\n    ret_excess = ret_adj - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) |>\n  select(-ret_adj, -rf)\ncrsp_monthly <- crsp_monthly |>\n  drop_na(ret_excess, mktcap, mktcap_lag)\n  dbWriteTable(tidy_finance,\n    \"crsp_monthly\",\n    value = crsp_monthly,\n    overwrite = TRUE\n  )"},{"path":"wrds-crsp-and-compustat.html","id":"first-glimpse-of-the-crsp-sample","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.3 First glimpse of the CRSP sample","text":"move data sources, let us look descriptive statistics CRSP sample, main source stock returns.figure shows monthly number securities listing exchange time. NYSE longest history data, NASDAQ lists considerable large number stocks. number stocks listed AMEX decreasing steadily last couple decades. end 2021, 2,779 stocks primary listing NASDAQ, 1,395 NYSE, 145 AMEX 1 belongs category.\nFigure 3.1: Number stocks CRSP sample listed US exchanges.\nNext, look aggregate market capitalization grouped respective listing exchanges. ensure look meaningful data comparable time, adjust nominal values inflation. fact, can use tables already database calculate aggregate market caps listing exchange plotting just memory. values end 2021 USD ensure inter-temporal comparability. NYSE-listed stocks far largest market capitalization, followed NASDAQ-listed stocks.\nFigure 3.2: Market capitalization measured billion USD, adjusted consumer price index changes values horizontal axis reflect buying power billion USD December 2021.\ncourse, performing computation database really meaningful can easily pull required data memory. code chunk slower performing steps tables already memory. However, just want illustrate can perform many things database loading data memory. proceed, load monthly CPI data.Next, look descriptive statistics industry. figure plots number stocks sample SIC industry classifiers. sample period, largest share stocks apparently manufacturing, albeit number peaked somewhere 90s. number firms associated public administration seems category rise recent years, even surpassing manufacturing end sample period.\nFigure 3.3: Number stocks CRSP sample associated different industries.\nalso compute market cap stocks belonging respective industries. values terms billions end 2021 USD. points time, manufacturing firms comprise largest portion market capitalization. Towards end sample, however, financial firms services begin make substantial portion market cap.\nFigure 3.4: Market capitalization measured billion USD, adjusted consumer price index changes values y-axis reflect buying power billion USD December 2021.\n","code":"\ncrsp_monthly |>\n  count(exchange, date) |>\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ntbl(tidy_finance, \"crsp_monthly\") |>\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), by = \"month\") |>\n  group_by(month, exchange) |>\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi,\n    .groups = \"drop\"\n  ) |>\n  collect() |>\n  mutate(month = ymd(month)) |>\n  ggplot(aes(\n    x = month, y = mktcap / 1000,\n    color = exchange, linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ncpi_monthly <- tbl(tidy_finance, \"cpi_monthly\") |>\n  collect()\ncrsp_monthly_industry <- crsp_monthly |>\n  left_join(cpi_monthly, by = \"month\") |>\n  group_by(month, industry) |>\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry |>\n  ggplot(aes(\n    x = month,\n    y = securities,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ncrsp_monthly_industry |>\n  ggplot(aes(\n    x = month,\n    y = mktcap / 1000,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market cap by industry in billions of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)"},{"path":"wrds-crsp-and-compustat.html","id":"daily-crsp-data","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.4 Daily CRSP data","text":"turn accounting data, provide proposal downloading daily CRSP data. monthly data typically fit memory can downloaded meaningful amount time, usually true daily return data. daily CRSP data file substantially larger monthly data can exceed 20GB. two important implications: hold daily return data memory (hence possible copy entire data set local database), experience, download usually crashes (never stops) much data WRDS cloud prepare send R session.solution challenge. many big data problems, can split big task several smaller tasks easy handle. , instead downloading data many stocks , download data small batches stock consecutively. operations can implemented ()-loops, download, prepare, store data single stock iteration. operation might nonetheless take couple hours, patient either way (often run code overnight). keep track progress, can use txtProgressBar(). Eventually, end 68 million rows daily return data. Note store identifying information actually need, namely permno, date, month alongside excess returns. thus ensure local database contains data actually use can load full daily data memory later. Notice also use function dbWriteTable() option append new data existing table, process second following batches.","code":"\ndsf_db <- tbl(wrds, in_schema(\"crsp\", \"dsf\"))\n\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\") |>\n  collect()\n\npermnos <- tbl(tidy_finance, \"crsp_monthly\") |>\n  distinct(permno) |>\n  pull()\n\nprogress <- txtProgressBar(\n  min = 0,\n  max = length(permnos),\n  initial = 0,\n  style = 3\n)\n\nfor (j in 1:length(permnos)) {\n  permno_sub <- permnos[j]\n  crsp_daily_sub <- dsf_db |>\n    filter(permno == permno_sub &\n      date >= start_date & date <= end_date) |>\n    select(permno, date, ret) |>\n    collect() |>\n    drop_na()\n\n  if (nrow(crsp_daily_sub) > 0) {\n    crsp_daily_sub <- crsp_daily_sub |>\n      mutate(month = floor_date(date, \"month\")) |>\n      left_join(factors_ff_daily |>\n        select(date, rf), by = \"date\") |>\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) |>\n      select(permno, date, month, ret_excess)\n\n    dbWriteTable(tidy_finance,\n        \"crsp_daily\",\n        value = crsp_daily_sub,\n        overwrite = ifelse(j == 1, TRUE, FALSE),\n        append = ifelse(j != 1, TRUE, FALSE)\n      )\n  }\n  setTxtProgressBar(progress, j)\n}\n\nclose(progress)\n\ncrsp_daily_db <- tbl(tidy_finance, \"crsp_daily\")"},{"path":"wrds-crsp-and-compustat.html","id":"preparing-compustat-data","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.5 Preparing Compustat data","text":"Firm accounting data important source information use portfolio analyses subsequent chapters. commonly used source firm financial information Compustat provided S&P Global Market Intelligence, global data vendor provides financial, statistical, market information active inactive companies throughout world. US Canadian companies, annual history available back 1950 quarterly well monthly histories date back 1962.access Compustat data, can tap WRDS, hosts funda table contains annual firm-level information North American companies.follow typical filter conventions pull data actually need: () get records industrial data format, (ii) standard format (.e., consolidated information standard presentation), (iii) data desired time window.Next, calculate book value preferred stock equity inspired variable definition Ken French’s data library. Note set negative zero equity missing common practice working book--market ratios (see Fama French 1992 details).keep last available information firm-year group. Note datadate defines time corresponding financial data refers (e.g., annual report December 31, 2021). Therefore, datadate date data made available public. Check exercises insights peculiarities datadate.last step, already done preparing firm fundamentals. Thus, can store local database.","code":"\nfunda_db <- tbl(wrds, in_schema(\"comp\", \"funda\"))\ncompustat <- funda_db |>\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" &\n      consol == \"C\" &\n      datadate >= start_date & datadate <= end_date\n  ) |>\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk, # Preferred stock par value\n    capx, # Capital investment\n    oancf # Operating cash flow\n  ) |>\n  collect()\ncompustat <- compustat |>\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be <= 0, as.numeric(NA), be)\n  )\ncompustat <- compustat |>\n  mutate(year = year(datadate)) |>\n  group_by(gvkey, year) |>\n  filter(datadate == max(datadate)) |>\n  ungroup()\n  dbWriteTable(tidy_finance,\n    \"compustat\",\n    value = compustat,\n    overwrite = TRUE\n  )"},{"path":"wrds-crsp-and-compustat.html","id":"merging-crsp-with-compustat","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.6 Merging CRSP with Compustat","text":"Unfortunately, CRSP Compustat use different keys identify stocks firms. CRSP uses permno stocks, Compustat uses gvkey identify firms. Fortunately, curated matching table WRDS allows us merge CRSP Compustat, create connection CRSP-Compustat Merged table (provided CRSP).linking table contains links CRSP Compustat identifiers various approaches. However, need make sure keep relevant correct links, following description outlined Bali, Engle, Murray (2016). Note also currently active links end date, just enter current date via today().use links create new table mapping stock identifier, firm identifier, month. add links Compustat gvkey monthly stock data.last step, update previously prepared monthly CRSP file linking information local database.close chapter, let us look interesting descriptive statistic data. book value equity plays crucial role many asset pricing applications, interesting know many stocks information available. Hence, figure plots share securities book equity values exchange. turns coverage pretty bad AMEX- NYSE-listed stocks 60s hovers around 80% periods thereafter. can ignore erratic coverage securities belong category since handful anyway sample.\nFigure 3.5: End--year share securities book equity values listing exchange.\n","code":"\nccmxpf_linktable_db <- tbl(\n  wrds,\n  in_schema(\"crsp\", \"ccmxpf_linktable\")\n)\nccmxpf_linktable <- ccmxpf_linktable_db |>\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) |>\n  select(permno = lpermno, gvkey, linkdt, linkenddt) |>\n  collect() |>\n  mutate(linkenddt = replace_na(linkenddt, today()))\nccmxpf_linktable# A tibble: 31,770 × 4\n  permno gvkey  linkdt     linkenddt \n   <dbl> <chr>  <date>     <date>    \n1  25881 001000 1970-11-13 1978-06-30\n2  10015 001001 1983-09-20 1986-07-31\n3  10023 001002 1972-12-14 1973-06-05\n4  10031 001003 1983-12-07 1989-08-16\n5  54594 001004 1972-04-24 2022-09-24\n# … with 31,765 more rows\nccm_links <- crsp_monthly |>\n  inner_join(ccmxpf_linktable, by = \"permno\") |>\n  filter(!is.na(gvkey) & (date >= linkdt & date <= linkenddt)) |>\n  select(permno, gvkey, date)\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(ccm_links, by = c(\"permno\", \"date\"))\n  dbWriteTable(tidy_finance,\n    \"crsp_monthly\",\n    value = crsp_monthly,\n    overwrite = TRUE\n  )\ncrsp_monthly |>\n  group_by(permno, year = year(month)) |>\n  filter(date == max(date)) |>\n  ungroup() |>\n  left_join(compustat, by = c(\"gvkey\", \"year\")) |>\n  group_by(exchange, year) |>\n  summarize(\n    share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),\n    .groups = \"drop\"\n  ) |>\n  ggplot(aes(x = year, y = share, color = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_cartesian(ylim = c(0, 1))"},{"path":"wrds-crsp-and-compustat.html","id":"some-tricks-for-postgresql-databases","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.7 Some tricks for PostgreSQL databases","text":"mentioned , WRDS database runs PostgreSQL rather SQLite. Finding right tables data needs can tricky WRDS PostgreSQL instance, tables organized schemas. wonder purpose schemas , check documetation. instance, want find tables live crsp schema, runThis operation returns list tables belong crsp family WRSD, e.g. <Id> schema = crsp, table = msenames. Similarly, can fetch list tables belong comp family viaIf want get schemas, run","code":"\ndbListObjects(wrds, Id(schema = \"crsp\"))\ndbListObjects(wrds, Id(schema = \"comp\"))\ndbListObjects(wrds)"},{"path":"wrds-crsp-and-compustat.html","id":"exercises-2","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.8 Exercises","text":"Check structure WRDS database sending queries spirit “Querying WRDS Data using R” verify output dbListObjects(). many tables associated CRSP? Can identify stored within msp500?Compute mkt_cap_lag using lag(mktcap) rather joins . Filter rows lag-based market capitalization measure different one computed . different?main part, look distribution market capitalization across exchanges industries. Now, plot average market capitalization firms exchange industry. find?datadate refers date fiscal year corresponding firm refers . Count number observations Compustat month date variable. find? finding suggest pooling observations fiscal year?Go back original Compustat data funda_db extract rows firm multiple rows fiscal year. reason observations?Repeat analysis market capitalization book equity, computed Compustat data. , use matched sample plot book equity market capitalization. two variables related?","code":""},{"path":"trace-and-fisd.html","id":"trace-and-fisd","chapter":"4 TRACE and FISD","heading":"4 TRACE and FISD","text":"chapter, dive US corporate bond market. Bond markets far diverse stock markets, issuers multiple bonds outstanding simultaneously potentially different indentures. market segment exciting due size (roughly $10 trillion outstanding), heterogeneity issuers (opposed government bonds), market structure (mostly --counter trades), data availability. introduce use bond characteristics FISD trade reports TRACE provide code download clean TRACE R.Many researchers study liquidity US corporate bond market O’Hara Zhou (2021). cover bond returns , can compute TRACE data. Instead, refer studies topic Bessembinder et al. (2008), Bai, Bali, Wen (2019), Kelly, Palhares, Pruitt (2021) survey Huang Shi (2021). Moreover, WRDS includes bond returns computed TRACE data monthly frequency.current chapter relies set packages.Compared previous chapters, load devtools package (Wickham, Hester, et al. 2022) source code provided public via gist.","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(dbplyr)\nlibrary(RSQLite)\nlibrary(RPostgres)\nlibrary(devtools)"},{"path":"trace-and-fisd.html","id":"bond-data-from-wrds","chapter":"4 TRACE and FISD","heading":"4.1 Bond data from WRDS","text":"bond databases need available WRDS establish RPostgres connection described previous chapter. Additionally, connect local SQLite-database store data download.","code":"\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)"},{"path":"trace-and-fisd.html","id":"mergent-fisd","chapter":"4 TRACE and FISD","heading":"4.2 Mergent FISD","text":"research US corporate bonds, Mergent Fixed Income Securities Database (FISD) primary resource bond characteristics. detailed manual WRDS, cover necessary subjects . FISD data comes two main variants, namely, centered issuers issues. either case, useful identifiers CUSIPs. 9-digit CUSIPs identify securities issued issuers. issuers can identified first six digits security CUSIP, also called 6-digit CUSIP. stocks bonds CUSIPs. connection , principle, allow matching easily, due changing issuer details, approach yields small coverage.use issue-centered version FISD identify subset US corporate bonds meet standard criteria (Bessembinder, Maxwell, Venkataraman 2006). WRDS table fisd_mergedissue contains information need 9-digit CUSIP level.\nDue diversity corporate bonds, details indenture vary significantly. focus common bonds make majority trading volume market without diverging much indentures.following chunk connects data selects bond sample remove certain bond types less commonly (see, e.g., Dick-Nielsen, Feldhütter, Lando 2012; O’Hara Zhou 2021, among many others).also pull issuer information fisd_mergedissuer regarding industry country firm issued particular bond. , filter include US-domiciled firms’ bonds. match data issuer_id.Finally, save bond characteristics local database. selection bonds also constitutes sample collect trade reports TRACE .FISD database also contains data. issue-based file contains information covenants, .e., restrictions included bond indentures limit specific actions firms (e.g., Handler, Jankowitsch, Weiss 2021). Moreover, FISD also provides information bond ratings. need either .","code":"\nmergent <- tbl(\n  wrds,\n  in_schema(\"fisd\", \"fisd_mergedissue\")\n) |>\n  filter(\n    security_level == \"SEN\", # senior bonds\n    slob == \"N\", # secured lease obligation\n    is.na(security_pledge), # unsecured bonds\n    asset_backed == \"N\", # not asset backed\n    defeased == \"N\", # not defeased\n    bond_type %in% c(\n      \"CDEB\", # US Corporate Debentures\n      \"CMTN\", # US Corporate MTN (Medium Term Note)\n      \"CMTZ\", # US Corporate MTN Zero\n      \"CZ\", # US Corporate Zero,\n      \"USBN\" # US Corporate Bank Note\n    ), \n    pay_in_kind != \"Y\", # not payable in kind\n    yankee == \"N\", # no foreign issuer\n    canadian == \"N\", # not Canadian\n    foreign_currency == \"N\", # USD\n    coupon_type %in% c(\n      \"F\", # fixed coupon\n      \"Z\" # zero coupon\n    ), \n    is.na(fix_frequency),\n    coupon_change_indicator == \"N\",\n    interest_frequency %in% c(\n      \"0\", # per year\n      \"1\",\n      \"2\",\n      \"4\",\n      \"12\"\n    ),\n    rule_144a == \"N\", # publicly traded\n    private_placement == \"N\",\n    defaulted == \"N\", # not defaulted\n    is.na(filing_date),\n    is.na(settlement),\n    convertible == \"N\", # not convertible\n    is.na(exchange),\n    putable == \"N\", # not putable\n    unit_deal == \"N\", # not issued with another security\n    exchangeable == \"N\", # not exchangeable\n    perpetual == \"N\", # not perpetual\n    preferred_security == \"N\" # not preferred\n  ) |> \n  select(\n    complete_cusip, maturity,\n    offering_amt, offering_date,\n    dated_date, \n    interest_frequency, coupon,\n    last_interest_date, \n    issue_id, issuer_id\n  ) |>\n  collect()\nmergent_issuer <- tbl(wrds, in_schema(\"fisd\", \"fisd_mergedissuer\")) |>\n  select(issuer_id, sic_code, country_domicile) |>\n  collect()\n\nmergent <- mergent |>\n  inner_join(mergent_issuer, by = \"issuer_id\") |>\n  filter(country_domicile == \"USA\") |>\n  select(-country_domicile)\n  dbWriteTable(\n    conn = tidy_finance,\n    name = \"mergent\",\n    value = mergent,\n    overwrite = TRUE\n  )"},{"path":"trace-and-fisd.html","id":"trace","chapter":"4 TRACE and FISD","heading":"4.3 TRACE","text":"Financial Industry Regulatory Authority (FINRA) provides Trade Reporting Compliance Engine (TRACE). TRACE, dealers trade corporate bonds must report trades individually. Hence, observe trade messages TRACE contain information bond traded, trade time, price, volume. TRACE comes two variants; standard enhanced TRACE. show download clean enhanced TRACE contains uncapped volume, crucial quantity missing standard distribution. Moreover, enhanced TRACE also provides information respective parties’ roles direction trade report. items become essential cleaning messages.repeatedly talk cleaning TRACE? Trade messages submitted within short time window trade executed (less 15 minutes). messages can contain errors, reporters subsequently correct cancel trade altogether. cleaning needs described Dick-Nielsen (2009) detail, Dick-Nielsen (2014) shows clean enhanced TRACE data using SAS. go cleaning steps , since code lengthy serves educational purpose. However, downloading cleaning enhanced TRACE data straightforward setup.store code cleaning enhanced TRACE R following Github gist. function. appendix also contains code reference. need source code gist, can source_gist(). Alternatively, can also go gist, download , source() respective R-file. clean_enhanced_trace() function takes vector CUSIPs, connection WRDS explained Chapter 3, start end date, respectively.TRACE database considerably large. Therefore, download subsets data . Specifying many CUSIPs long time horizon result long download times potential failure due size request WRDS. size limit depends many parameters, give guideline . working complete TRACE data CUSIPs , splitting data 100 parts takes roughly two hours using setup. applications book, need data around Paris Agreement December 2015 download data ten sets, define .Finally, run loop style Chapter 3 download daily returns CRSP. CUSIP sets defined , call cleaning function save resulting output. add new data existing dataframe batch two following batches.","code":"\nsource_gist(\"3a05b3ab281563b2e94858451c2eb3a4\")\nmergent_cusips <- mergent |>\n  pull(complete_cusip)\n\nmergent_parts <- split(\n  mergent_cusips,\n  rep(1:10, \n      length.out = length(mergent_cusips))\n)\nfor (j in 1:length(mergent_parts)) {\n  trace_enhanced <- clean_enhanced_trace(\n    cusips = mergent_parts[[j]],\n    connection = wrds,\n    start_date = ymd(\"2014-01-01\"),\n    end_date = ymd(\"2016-11-30\")\n  )\n\n  dbWriteTable(\n      conn = tidy_finance,\n      name = \"trace_enhanced\",\n      value = trace_enhanced,\n      overwrite = ifelse(j == 1, TRUE, FALSE),\n      append = ifelse(j != 1, TRUE, FALSE)\n    )\n\n}"},{"path":"trace-and-fisd.html","id":"insights-into-corporate-bonds","chapter":"4 TRACE and FISD","heading":"4.4 Insights into corporate bonds","text":"many news outlets readily provide information stocks underlying firms, corporate bonds covered frequently. Additionally, TRACE database contains trade-level information, potentially new students. Therefore, provide insights showing summary statistics.start looking number bonds outstanding time compare number bonds traded sample. First, compute number bonds outstanding quarter 1990 2018.Next, look bonds traded quarter shorter period around Paris Agreement 2014 2016. Notice load complete trace database sample, single part environment download.Finally, plot two time series figure .\nFigure 4.1: number corporate bonds outstanding quarter 1990 2018, reported Mergent FISD. number traded bonds enhanced TRACE per quarter 2014 2016.\nsee number bonds outstanding varies time, peak turn millennium. 2010, number steadily increased, eventually surpassing previous maximum. sample period trade data, see fraction bonds trading quarter roughly 60%. flip side, relatively small number traded bonds means many bonds trade entire quarter. lack trading activity illustrates generally low level liquidity corporate bond market, can hard trade specific bonds.\nlack liquidity mean corporate bond markets irrelevant terms size? 7,500 traded bonds quarter, hard say market small. However, let us also investigate characteristics issued corporate bonds. particular, consider maturity (years), coupon, offering amount (million USD).see average bond offering amount 350 million USD, considered small. average bond maturity 10 years pays around 6% coupons.Finally, let us compute summary statistics trades market. end, show summary based aggregate information daily. particular, consider trade size (million USD) number trades.average, nearly 26 billion USD corporate bonds traded daily nearly 13,000 transactions. can conclude corporate bond market indeed significant terms trading volume activity.","code":"\nbonds_outstanding <- expand_grid(\"date\" = seq(as.Date(\"1990-01-01\"),\n                                              as.Date(\"2018-12-01\"), \n                                              by = \"quarter\"), \n                                 \"complete_cusip\" = mergent$complete_cusip) |> \n  left_join(mergent |> select(complete_cusip, \n                              offering_date,\n                              maturity), \n            by = \"complete_cusip\") |> \n  mutate(offering_date = floor_date(offering_date),\n         maturity = floor_date(maturity)) |> \n  filter(date >= offering_date & date <= maturity) |> \n  count(date) |> \n  mutate(type = \"outstanding\")\ntrace_enhanced <- tbl(tidy_finance, \"trace_enhanced\") |>\n  collect()\n\nbonds_traded <- trace_enhanced |> \n  mutate(date = floor_date(trd_exctn_dt, \"quarters\")) |> \n  group_by(date) |> \n  summarize(n = length(unique(cusip_id)),\n            type = \"traded\",\n            .groups = \"drop\") \nbonds_outstanding |> \n  bind_rows(bonds_traded) |> \n  ggplot(aes(x = date, y = n, color = type, linetype = type)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Number of bonds outstanding and traded each quarter\"\n  )\nmergent |>\n  mutate(maturity = as.numeric(maturity - offering_date) / 365,\n         offering_amt = offering_amt / 10^3) |> \n  pivot_longer(cols = c(maturity, coupon, offering_amt),\n               names_to = \"measure\") |>\n  drop_na() |> \n  group_by(measure) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )# A tibble: 3 × 8\n  measure        mean     sd   min   q05    q50     q95    max\n  <chr>         <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl>  <dbl>\n1 coupon         6.21   2.43 0     2.25    6.38    9.86    39 \n2 maturity      10.1    9.13 0.252 1.26    8.02   30.0    100.\n3 offering_amt 357.   545.   0.001 0.875 200    1250    15000 \ntrace_enhanced |> \n  group_by(trd_exctn_dt) |> \n  summarize(trade_size = sum(entrd_vol_qt * rptd_pr / 100) / 10^6,\n            trade_number = n(),\n            .groups = \"drop\") |> \n  pivot_longer(cols = c(trade_size, trade_number),\n               names_to = \"measure\") |> \n  group_by(measure) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )# A tibble: 2 × 8\n  measure        mean    sd   min    q05    q50    q95    max\n  <chr>         <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 trade_number 25921. 5460. 438   17851. 26025  34458. 40889 \n2 trade_size   12968. 3574.  17.2  6138. 13408. 17851. 20905."},{"path":"trace-and-fisd.html","id":"exercises-3","chapter":"4 TRACE and FISD","heading":"4.5 Exercises","text":"Summarize amount outstanding bonds time describe resulting graph.Compute number days bond traded (accounting bonds maturity issuance). Start looking number bonds traded day graph similar one . many bonds trade 75% trading days?WRDS provides information Mergent FISD. particular, also provide rating information fisd_ratings. Download ratings bond sample. , plot distribution ratings histogram.Download TRACE data end September 2021. Hint: want download data completely new, rather adding new information. Can find reason ?","code":""},{"path":"other-data-providers.html","id":"other-data-providers","chapter":"5 Other data providers","heading":"5 Other data providers","text":"previous chapters, introduced many ways get financial data researchers regularly use. showed load data R Yahoo!Finance commonly used file types, comma-separated files, Excel files, etc. , introduced remotely connecting WRDS downloading data . However, subset vast amounts data available days.short chapter, aim provide overview common alternative data providers direct access via R packages exists. list requires constant adjustments data providers access methods change. However, want emphasize two main insights: First, number R packages provide access (financial) data large. large actually survey exhaustively. Instead, can cover tip iceberg. Second, R provides functionalities access basically form files data available online. Thus, even desired data source come well-established R package, chances high data can retrieved establishing API connection scrapping content altogether.non-exhaustive list , restrict listing data sources accessed easy--use R packages. inspiration potential data sources, recommend reading R task view empirical finance. inspiration (general social sciences) can found .feel like miss fantastic financial data source, just edit table table get touch us via contact@tidy-finance.org - thank much support!","code":""},{"path":"other-data-providers.html","id":"exercises-4","chapter":"5 Other data providers","heading":"5.1 Exercises","text":"Select one data sources table retrieve data: Browse homepage data provider package documentation find inspiration type data available download data R session.Generate summary statistics data retrieved provide useful visualization. possibilities endless: Maybe interesting economic event want analyse stock market responses Twitter activity?Simfin provides excellent data coverage. Use API find information Simfin provides overlaps CRSP/Compustat dataset tidy_finance.sqlite database introduced Chapters 2-4.","code":""},{"path":"beta-estimation.html","id":"beta-estimation","chapter":"6 Beta estimation","heading":"6 Beta estimation","text":"chapter, introduce important concept financial economics: exposure individual stock changes market portfolio. According Capital Asset Pricing Model (CAPM) Sharpe (1964), Lintner (1965), Mossin (1966), cross-sectional variation expected asset returns function covariance excess return asset excess return market portfolio. regression coefficient excess market returns excess stock returns usually called market beta. show estimation procedure market betas. go details foundations market beta simply refer treatment CAPM information. Instead, provide details functions use compute results. particular, leverage useful computational concepts: rolling-window estimation parallelization.use following packages throughout chapter:Compared previous chapters, introduce slider (Vaughan 2021) sliding window functions, furrr (Vaughan Dancho 2022) apply mapping functions parallel.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(slider)\nlibrary(furrr)"},{"path":"beta-estimation.html","id":"estimating-beta-using-monthly-returns","chapter":"6 Beta estimation","heading":"6.1 Estimating beta using monthly returns","text":"estimation procedure based rolling-window estimation may use either monthly daily returns different window lengths. First, let us start loading monthly CRSP data SQLite-database introduced previous Chapters 2-4.estimate CAPM regression coeffients\\[\nr_{, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{, t}\n\\]\nregress stock excess returns ret_excess excess returns market portfolio mkt_excess.\nR provides simple solution estimate (linear) models function lm(). lm() requires formula input specified compact symbolic form. expression form y ~ model interpreted specification response y modeled linear predictor specified symbolically model. model consists series terms separated + operators. addition standard linear models, lm() provides lot flexibility. check documentation information. start, restrict data time series observations CRSP correspond Apple’s stock (.e., permno 14593 Apple) compute \\(\\hat\\alpha_i\\) well \\(\\hat\\beta_i\\).lm() returns object class lm contains information usually care linear models. summary() returns overview estimated parameters. coefficients(fit) return estimated coefficients. output indicates Apple moves excessively market estimated \\(\\hat\\beta_i\\) one (\\(\\hat\\beta_i \\approx 1.4\\)).","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  select(permno, month, industry, ret_excess, mkt_excess)\nfit <- lm(ret_excess ~ mkt_excess,\n  data = crsp_monthly |>\n    filter(permno == \"14593\")\n)\n\nsummary(fit)\nCall:\nlm(formula = ret_excess ~ mkt_excess, data = filter(crsp_monthly, \n    permno == \"14593\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5169 -0.0598  0.0001  0.0636  0.3944 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.01034    0.00521    1.99    0.048 *  \nmkt_excess   1.39419    0.11576   12.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.114 on 490 degrees of freedom\nMultiple R-squared:  0.228, Adjusted R-squared:  0.227 \nF-statistic:  145 on 1 and 490 DF,  p-value: <2e-16"},{"path":"beta-estimation.html","id":"rolling-window-estimation","chapter":"6 Beta estimation","heading":"6.2 Rolling-window estimation","text":"estimated regression coefficients example, scale estimation \\(\\beta_i\\) whole different level perform rolling-window estimations entire CRSP sample. following function implements CAPM regression data frame (part thereof) containing least min_obs observations avoid huge fluctuations time series short. condition violated, , time series short, function returns missing value.Next, define function rolling estimation. slide_period function able handle months window input straightforward manner. thus avoid using time-series package (e.g., zoo) converting data fit package functions, rather stay world tidyverse.following function takes input data slides across month vector, considering total months months. function essentially performs three steps: () arrange rows, (ii) compute betas sliding across months, (iii) return tibble months corresponding beta estimates (particularly useful case daily data).\ndemonstrate , can also apply function daily returns data.attack whole CRSP sample, let us focus couple examples well-known firms.want estimate rolling betas Apple, can use mutate().\ntake total 5 years data require least 48 months return data compute betas.\nCheck exercises want ot compute beta different time periods.actually quite simple perform rolling-window estimation arbitrary number stocks, visualize following code chunk.\nFigure 6.1: CAPM betas estimated monthly data rolling window length 5 years based adjusted excess returns CRSP. use market excess returns Kenneth French data library.\n","code":"\nestimate_capm <- function(data, min_obs = 1) {\n  if (nrow(data) < min_obs) {\n    beta <- as.numeric(NA)\n  } else {\n    fit <- lm(ret_excess ~ mkt_excess, data = data)\n    beta <- as.numeric(coefficients(fit)[2])\n  }\n  return(beta)\n}\nroll_capm_estimation <- function(data, months, min_obs) {\n  data <- data |>\n    arrange(month)\n\n  betas <- slide_period_vec(\n    .x = data,\n    .i = data$month,\n    .period = \"month\",\n    .f = ~ estimate_capm(., min_obs),\n    .before = months - 1,\n    .complete = FALSE\n  )\n\n  return(tibble(\n    month = unique(data$month),\n    beta = betas\n  ))\n}\nexamples <- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n)\nbeta_example <- crsp_monthly |>\n  filter(permno == examples$permno[1]) |>\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) |>\n  drop_na()\nbeta_example# A tibble: 445 × 6\n  permno month      industry      ret_excess mkt_excess  beta\n   <dbl> <date>     <chr>              <dbl>      <dbl> <dbl>\n1  14593 1984-12-01 Manufacturing     0.170      0.0184  2.05\n2  14593 1985-01-01 Manufacturing    -0.0108     0.0799  1.90\n3  14593 1985-02-01 Manufacturing    -0.152      0.0122  1.88\n4  14593 1985-03-01 Manufacturing    -0.112     -0.0084  1.89\n5  14593 1985-04-01 Manufacturing    -0.0467    -0.0096  1.90\n# … with 440 more rows\nbeta_examples <- crsp_monthly |>\n  inner_join(examples, by = \"permno\") |>\n  group_by(permno) |>\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) |>\n  ungroup() |>\n  select(permno, company, month, beta) |>\n  drop_na()\n\nbeta_examples |>\n  ggplot(aes(x = month, y = beta, color = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of data\"\n  )"},{"path":"beta-estimation.html","id":"parallelized-rolling-window-estimation","chapter":"6 Beta estimation","heading":"6.3 Parallelized rolling-window estimation","text":"Even though now just apply function using group_by() whole CRSP sample, advise computationally quite expensive.\nRemember perform rolling-window estimations across stocks time periods.\nHowever, estimation problem ideal scenario employ power parallelization.\nParallelization means split tasks perform rolling-window estimations across different workers (cores local machine).First, nest() data permno. Nested data means now list permno corresponding time series data industry label. get one row output unique combination non-nested variables permno industry.Alternatively, created nested data excluding variables want nest, following code chunk. However, many applications desirable explicitly state variables nested data list-column, reader can track ends .Next, want apply roll_capm_estimation() function stock. situation ideal use case map(), takes list vector input returns object length input. case, map() returns single data frame time series beta estimates stock. Therefore, use unnest() transform list outputs tidy data frame.However, instead, want perform estimations rolling betas different stocks parallel. Windows machine, makes sense define multisession, means separate R processes running background machine perform individual jobs. check documentation plan(), can also see ways resolve parallelization different environments.Using eight cores, estimation sample around 25k stocks takes around 20 minutes. course, can speed things considerably cores available share workload powerful cores. Notice difference code ? need replace map() future_map().","code":"\ncrsp_monthly_nested <- crsp_monthly |>\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_monthly_nested# A tibble: 30,071 × 3\n  permno industry      data              \n   <dbl> <chr>         <list>            \n1  10000 Manufacturing <tibble [16 × 3]> \n2  10001 Utilities     <tibble [378 × 3]>\n3  10002 Finance       <tibble [324 × 3]>\n4  10003 Finance       <tibble [118 × 3]>\n5  10005 Mining        <tibble [65 × 3]> \n# … with 30,066 more rows\ncrsp_monthly_nested <- crsp_monthly |>\n  nest(data = -c(permno, industry))\ncrsp_monthly_nested |>\n  inner_join(examples, by = \"permno\") |>\n  mutate(beta = map(\n    data,\n    ~ roll_capm_estimation(., months = 60, min_obs = 48)\n  )) |>\n  unnest(beta) |>\n  select(permno, month, beta_monthly = beta) |>\n  drop_na()# A tibble: 1,410 × 3\n  permno month      beta_monthly\n   <dbl> <date>            <dbl>\n1  10107 1990-03-01         1.39\n2  10107 1990-04-01         1.38\n3  10107 1990-05-01         1.43\n4  10107 1990-06-01         1.43\n5  10107 1990-07-01         1.45\n# … with 1,405 more rows\nplan(multisession, workers = availableCores())\nbeta_monthly <- crsp_monthly_nested |>\n  mutate(beta = future_map(\n    data, ~ roll_capm_estimation(., months = 60, min_obs = 48)\n  )) |>\n  unnest(c(beta)) |>\n  select(permno, month, beta_monthly = beta) |>\n  drop_na()"},{"path":"beta-estimation.html","id":"estimating-beta-using-daily-returns","chapter":"6 Beta estimation","heading":"6.4 Estimating beta using daily returns","text":"provide descriptive statistics beta estimates, implement estimation daily CRSP sample well.\nDepending application, might either use longer horizon beta estimates based monthly data shorter horizon estimates based daily returns.First, load daily CRSP data.\nNote sample large compared monthly data, make sure enough memory available.also need daily Fama-French market excess returns.make sure keep relevant data save memory space.\nHowever, note machine might enough memory read whole daily CRSP sample. case, refer exercises try working loops Chapter 3.Just like , nest data permno parallelization.estimation looks like couple examples using map().\ndaily data, use function take 3 months data require least 50 daily return observations months.\nrestrictions help us retrieve somewhat smooth coefficient estimates.sake completeness, tell session use multiple workers parallelization.code chunk beta estimation using daily returns now looks similar one monthly data. whole estimation takes around 30 minutes using eight cores 16gb memory.","code":"\ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\") |>\n  collect()\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\") |>\n  collect()\ncrsp_daily <- crsp_daily |>\n  inner_join(factors_ff_daily, by = \"date\") |>\n  select(permno, month, ret_excess, mkt_excess)\ncrsp_daily_nested <- crsp_daily |>\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_daily_nested |>\n  inner_join(examples, by = \"permno\") |>\n  mutate(beta_daily = map(\n    data,\n    ~ roll_capm_estimation(., months = 3, min_obs = 50)\n  )) |>\n  unnest(c(beta_daily)) |>\n  select(permno, month, beta_daily = beta) |>\n  drop_na()# A tibble: 1,591 × 3\n  permno month      beta_daily\n   <dbl> <date>          <dbl>\n1  10107 1986-05-01      0.898\n2  10107 1986-06-01      0.906\n3  10107 1986-07-01      0.822\n4  10107 1986-08-01      0.900\n5  10107 1986-09-01      1.01 \n# … with 1,586 more rows\nplan(multisession, workers = availableCores())\nbeta_daily <- crsp_daily_nested |>\n  mutate(beta_daily = future_map(\n    data, ~ roll_capm_estimation(., months = 3, min_obs = 50)\n  )) |>\n  unnest(c(beta_daily)) |>\n  select(permno, month, beta_daily = beta) |>\n  drop_na()"},{"path":"beta-estimation.html","id":"comparing-beta-estimates","chapter":"6 Beta estimation","heading":"6.5 Comparing beta estimates","text":"typical value stock betas? get feeling, illustrate dispersion estimated \\(\\hat\\beta_i\\) across different industries across time . first figure shows typical business models across industries imply different exposure general market economy. However, barely firms exhibit negative exposure market factor.\nFigure 6.2: box plots show average firm-specific beta estimates industry.\nNext, illustrate time-variation cross-section estimated betas. figure shows monthly deciles estimated betas (based monthly data) indicates interesting pattern: First, betas seem vary time sense periods, clear trend across deciles. Second, sample exhibits periods dispersion across stocks increases sense lower decile decreases upper decile increases, indicates stocks correlation market increases others decreases. Note also : stocks negative betas rare exception.\nFigure 6.3: line corresponds monthly cross-sectional quantile estimated CAPM beta.\ncompare difference daily monthly data, combine beta estimates single table. , use table plot comparison beta estimates example stocks.\nFigure 6.4: CAPM betas computed using 5 years monthly 3 months daily data. two lines show monthly estimates based rolling window exemplary stocks.\nestimates look expected. can see, really depends estimation window data frequency beta estimates turn .Finally, write estimates database can use later chapters.Whenever perform kind estimation, also makes sense rough plausibility tests. possible check plot share stocks beta estimates time.\ndescriptive helps us discover potential errors data preparation estimation procedure.\ninstance, suppose gap output betas.\ncase, go back check previous steps find went wrong.\nFigure 6.5: two lines show share securities beta estimates using 5 years monthly 3 months daily data.\nfigure indicate troubles, let us move next check.also encourage everyone always look distributional summary statistics variables. can easily spot outliers weird distributions looking tables.summary statistics also look plausible two estimation procedures.Finally, since two different estimators theoretical object, expect estimators least positively correlated (although perfectly estimators based different sample periods frequencies).Indeed, find positive correlation beta estimates. subsequent chapters, mainly use estimates based monthly data readers able replicate due potential memory limitations might arise daily data.","code":"\ncrsp_monthly |>\n  left_join(beta_monthly, by = c(\"permno\", \"month\")) |>\n  drop_na(beta_monthly) |>\n  group_by(industry, permno) |>\n  summarize(beta = mean(beta_monthly)) |>\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Firm-specific beta distributions by industry\"\n  )\nbeta_monthly |>\n  drop_na(beta_monthly) |>\n  group_by(month) |>\n  summarize(\n    x = quantile(beta_monthly, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1),\n    .groups = \"drop\"\n  ) |>\n  ggplot(aes(x = month, y = x, color = as_factor(quantile))) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Monthly deciles of estimated betas\",\n  )\nbeta <- beta_monthly |>\n  full_join(beta_daily, by = c(\"permno\", \"month\")) |>\n  arrange(permno, month)\n\nbeta |>\n  inner_join(examples, by = \"permno\") |>\n  pivot_longer(cols = c(beta_monthly, beta_daily)) |>\n  drop_na() |>\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Comparison of beta estimates using monthly and daily data\"\n  )\n  dbWriteTable(tidy_finance,\n    \"beta\",\n    value = beta,\n    overwrite = TRUE\n  )\nbeta_long <- crsp_monthly |>\n  left_join(beta, by = c(\"permno\", \"month\")) |>\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long |>\n  group_by(month, name) |>\n  summarize(share = sum(!is.na(value)) / n()) |>\n  ggplot(aes(x = month, y = share, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\nbeta_long |>\n  select(name, value) |>\n  drop_na() |>\n  group_by(name) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  )# A tibble: 2 × 9\n  name          mean    sd   min    q05   q50   q95   max       n\n  <chr>        <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>   <int>\n1 beta_daily   0.749 0.926 -43.7 -0.447 0.686  2.23  56.6 3233745\n2 beta_monthly 1.10  0.713 -13.0  0.125 1.03   2.32  10.3 2102936\nbeta |>\n  select(beta_daily, beta_monthly) |>\n  cor(use = \"complete.obs\")             beta_daily beta_monthly\nbeta_daily        1.000        0.323\nbeta_monthly      0.323        1.000"},{"path":"beta-estimation.html","id":"exercises-5","chapter":"6 Beta estimation","heading":"6.6 Exercises","text":"Compute beta estimates based monthly data using 1, 3, 5 years data impose minimum number observations 10, 28, 48 months return data, respectively. strongly correlated estimated betas?Compute beta estimates based monthly data using 5 years data impose different numbers minimum observations. share permno-month observations successful beta estimates vary across different requirements? find high correlation across estimated betas?Instead using future_map(), perform beta estimation loop (using either monthly daily data) subset 100 permnos choice. Verify get results parallelized code .Filter stocks negative betas. stocks frequently exhibit negative betas, resemble estimation errors?Compute beta estimates multi-factor models Fama-French 3 factor model. purpose, extend regression\n\\[\nr_{, t} - r_{f, t} = \\alpha_i + \\sum\\limits_{j=1}^k\\beta_{,k}(r_{j, t}-r_{f,t})+\\varepsilon_{, t}\n\\]\n\\(r_{j, t}\\) \\(k\\) factor returns. Thus, estimate 4 parameters (\\(alpha_i\\) slope coefficients). Provide summary statistics cross-section firms exposure different factors.","code":""},{"path":"univariate-portfolio-sorts.html","id":"univariate-portfolio-sorts","chapter":"7 Univariate portfolio sorts","heading":"7 Univariate portfolio sorts","text":"chapter, dive portfolio sorts, one widely used statistical methodologies empirical asset pricing (e.g., Bali, Engle, Murray 2016). key application portfolio sorts examine whether one variables can predict future excess returns. general, idea sort individual stocks portfolios, stocks within portfolio similar respect sorting variable, firm size. different portfolios represent well-diversified investments differ level sorting variable. can attribute differences return distribution impact sorting variable.\nstart introducing univariate portfolio sorts (sort based one characteristic) tackle bivariate sorting Chapter 9.univariate portfolio sort considers one sorting variable \\(x_{t-1,}\\).\n, \\(\\) denotes stock \\(t-1\\) indicates characteristic observable investors time \\(t\\).\nobjective assess cross-sectional relation \\(x_{t-1,}\\) , typically, stock excess returns \\(r_{t,}\\) time \\(t\\) outcome variable.\nillustrate portfolio sorts work, use estimates market betas previous chapter sorting variable.current chapter relies following set packages.Compared previous chapters, introduce lmtest (Zeileis Hothorn 2002) inference estimated coefficients, sandwich (Zeileis 2006) different covariance matrix estimators.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(sandwich)"},{"path":"univariate-portfolio-sorts.html","id":"data-preparation","chapter":"7 Univariate portfolio sorts","heading":"7.1 Data preparation","text":"start loading required data SQLite-database introduced Chapters 2-4. particular, use monthly CRSP sample asset universe.\nform portfolios, use Fama-French market factor returns compute risk-adjusted performance (.e., alpha).\nbeta tibble market betas computed previous chapter.","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  select(permno, month, ret_excess, mktcap_lag) |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") |>\n  collect()"},{"path":"univariate-portfolio-sorts.html","id":"sorting-by-market-beta","chapter":"7 Univariate portfolio sorts","heading":"7.2 Sorting by market beta","text":"Next, merge sorting variable return data. use one-month lagged betas sorting variable ensure sorts rely information available create portfolios.\nlag stock beta one month, add one month current date join resulting information return data.\nprocedure ensures month \\(t\\) information available month \\(t+1\\).\nmay tempted simply use call crsp_monthly |> group_by(permno) |> mutate(beta_lag = lag(beta))) instead.\nprocedure, however, work correctly non-explicit missing values time series.first step conduct portfolio sorts calculate periodic breakpoints can use group stocks portfolios.\nsimplicity, start median lagged market beta single breakpoint.\ncompute value-weighted returns two resulting portfolios, means lagged market capitalization determines weight weighted.mean().","code":"\nbeta_lag <- beta |>\n  mutate(month = month %m+% months(1)) |>\n  select(permno, month, beta_lag = beta_monthly) |>\n  drop_na()\n\ndata_for_sorts <- crsp_monthly |>\n  inner_join(beta_lag, by = c(\"permno\", \"month\"))\nbeta_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(\n    breakpoint = median(beta_lag),\n    portfolio = case_when(\n      beta_lag <= breakpoint ~ \"low\",\n      beta_lag > breakpoint ~ \"high\"\n    )\n  ) |>\n  group_by(month, portfolio) |>\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\")"},{"path":"univariate-portfolio-sorts.html","id":"performance-evaluation","chapter":"7 Univariate portfolio sorts","heading":"7.3 Performance evaluation","text":"can construct long-short strategy based two portfolios: buy high-beta portfolio , time, short low-beta portfolio. Thereby, overall position market net-zero, .e., need invest money realize strategy absence frictions.compute average return corresponding standard error test whether long-short portfolio yields average positive negative excess returns. asset pricing literature, one typically adjusts autocorrelation using Newey West (1987) \\(t\\)-statistics test null hypothesis average portfolio excess returns equal zero. One necessary input Newey-West standard errors chosen bandwidth based number lags employed estimation. seems researchers often default choosing pre-specified lag length 6 months, instead recommend data-driven approach. automatic selection advocated Newey West (1994) available sandwich package. implement test, compute average return via lm() employ coeftest() function. want implement typical 6-lag default setting, can enforce passing arguments lag = 6, prewhite = FALSE coeftest() function code passes NeweyWest().results indicate reject null hypothesis average returns equal zero. portfolio strategy using median breakpoint hence yield abnormal returns. finding surprising reconsider CAPM? certainly . CAPM yields high beta stocks yield higher expected returns. portfolio sort implicitly mimics investment strategy finances high beta stocks shorting low beta stocks. Therefore, one expect average excess returns yield return risk-free rate.","code":"\nbeta_longshort <- beta_portfolios |>\n  pivot_wider(month, names_from = portfolio, values_from = ret) |>\n  mutate(long_short = high - low)\nmodel_fit <- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest)\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 0.000287   0.001312    0.22     0.83"},{"path":"univariate-portfolio-sorts.html","id":"functional-programming-for-portfolio-sorts","chapter":"7 Univariate portfolio sorts","heading":"7.4 Functional programming for portfolio sorts","text":"Now take portfolio sorts next level. want able sort stocks arbitrary number portfolios. case, functional programming handy: employ curly-curly-operator give us flexibility concerning variable use sorting, denoted var. use quantile() compute breakpoints n_portfolios. , assign portfolios stocks using findInterval() function. output following function new column contains number portfolio stock belongs.can use function sort stocks ten portfolios month using lagged betas compute value-weighted returns portfolio. Note transform portfolio column factor variable provides convenience figure construction .","code":"\nassign_portfolio <- function(data, var, n_portfolios) {\n  breakpoints <- data |>\n    summarize(breakpoint = quantile({{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  assigned_portfolios <- data |>\n    mutate(portfolio = findInterval({{ var }},\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n\n  return(assigned_portfolios)\n}\nbeta_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(\n    portfolio = assign_portfolio(\n      data = cur_data(),\n      var = beta_lag,\n      n_portfolios = 10\n    ),\n    portfolio = as.factor(portfolio)\n  ) |>\n  group_by(portfolio, month) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )"},{"path":"univariate-portfolio-sorts.html","id":"more-performance-evaluation","chapter":"7 Univariate portfolio sorts","heading":"7.5 More performance evaluation","text":"next step, compute summary statistics beta portfolio. Namely, compute CAPM-adjusted alphas, beta beta portfolio, average returns.figure illustrates CAPM alphas beta-sorted portfolios. shows low beta portfolios tend exhibit positive alphas, high beta portfolios exhibit negative alphas.\nFigure 7.1: Portfolios sorted deciles month based estimated CAPM beta. bar charts indicate CAPM alpha resulting portfolio returns entire CRSP period.\nresults suggest negative relation beta future stock returns, contradicts predictions CAPM. According CAPM, returns increase beta across portfolios risk-adjusted returns statistically indistinguishable zero.","code":"\nbeta_portfolios_summary <- beta_portfolios |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  group_by(portfolio) |>\n  summarize(\n    alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),\n    beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),\n    ret = mean(ret)\n  )\nbeta_portfolios_summary |>\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"CAPM alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")"},{"path":"univariate-portfolio-sorts.html","id":"the-security-market-line-and-beta-portfolios","chapter":"7 Univariate portfolio sorts","heading":"7.6 The security market line and beta portfolios","text":"CAPM predicts portfolios lie security market line (SML). slope SML equal market risk premium reflects risk-return trade-given time. figure illustrates security market line: see (surprisingly) high beta portfolio returns high correlation market returns. However, seems like average excess returns high beta stocks lower security market line implies “appropriate” compensation high market risk.\nFigure 7.2: Excess returns computed CAPM alphas beta-sorted portfolios. horizontal axis indicates CAPM beta resulting beta-sorted portfolio return time series. dashed line indicates slope coefficient linear regression excess returns portfolio betas.\nprovide evidence CAPM predictions, form long-short strategy buys high-beta portfolio shorts low-beta portfolio., resulting long-short strategy exhibit statistically significant returns.However, long-short portfolio yields statistically significant negative CAPM-adjusted alpha, although, controlling effect beta, average excess stock returns zero according CAPM. results thus provide evidence support CAPM. negative value documented -called betting beta factor (Frazzini Pedersen 2014). Betting beta corresponds strategy shorts high beta stocks takes (levered) long position low beta stocks. borrowing constraints prevent investors taking positions SML instead incentivized buy high beta stocks, leads relatively higher price (therefore lower expected returns implied CAPM) high beta stocks. result, betting--beta strategy earns providing liquidity capital constraint investors lower risk aversion.figure shows annual returns extreme beta portfolios mainly interested . figure illustrates consistent striking patterns last years - portfolio exhibits periods positive negative annual returns.\nFigure 7.3: construct portfolios sorting stocks high low based estimated CAPM beta. Long short indicates strategy goes long high beta stocks short low beta stocks.\nOverall, chapter shows functional programming can leveraged form arbitrary number portfolios using sorting variable evaluate performance resulting portfolios. next chapter, dive deeper many degrees freedom arise context portfolio analysis.","code":"\nsml_capm <- lm(ret ~ 1 + beta, data = beta_portfolios_summary)$coefficients\n\nbeta_portfolios_summary |>\n  ggplot(aes(x = beta, y = ret, color = portfolio)) +\n  geom_point() +\n  geom_abline(\n    intercept = mean(factors_ff_monthly$rf),\n    slope = mean(factors_ff_monthly$mkt_excess),\n    linetype = \"solid\"\n  ) +\n  geom_abline(\n    intercept = sml_capm[1],\n    slope = sml_capm[2],\n    linetype = \"dashed\"\n  ) +\n  scale_y_continuous(\n    labels = percent,\n    limit = c(0, mean(factors_ff_monthly$mkt_excess) * 2)\n  ) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\nbeta_longshort <- beta_portfolios |>\n  ungroup() |>\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) |>\n  filter(portfolio %in% c(\"low\", \"high\")) |>\n  pivot_wider(month, names_from = portfolio, values_from = ret) |>\n  mutate(long_short = high - low) |>\n  left_join(factors_ff_monthly, by = \"month\")\ncoeftest(lm(long_short ~ 1, data = beta_longshort),\n  vcov = NeweyWest\n)\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.00232    0.00322    0.72     0.47\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort),\n  vcov = NeweyWest\n)\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.00447    0.00256   -1.75    0.081 .  \nmkt_excess   1.16555    0.09562   12.19   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nbeta_longshort |>\n  group_by(year = year(month)) |>\n  summarize(\n    low = prod(1 + low),\n    high = prod(1 + high),\n    long_short = prod(1 + long_short)\n  ) |>\n  pivot_longer(cols = -year) |>\n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )"},{"path":"univariate-portfolio-sorts.html","id":"exercises-6","chapter":"7 Univariate portfolio sorts","heading":"7.7 Exercises","text":"Take two long-short beta strategies based different numbers portfolios compare returns. significant difference returns? Sharpe ratios compare strategies? Find one additional portfolio evaluation statistic compute .plotted alphas ten beta portfolios . Write function tests estimates significance. portfolios significant alphas?analysis based betas monthly returns. However, also computed betas daily returns. Re-run analysis point differences results.Given results chapter, can define long-short strategy yields positive abnormal returns (.e., alphas)? Plot cumulative excess return strategy market excess return comparison.","code":""},{"path":"size-sorts-and-p-hacking.html","id":"size-sorts-and-p-hacking","chapter":"8 Size sorts and p-hacking","heading":"8 Size sorts and p-hacking","text":"chapter, continue portfolio sorts univariate setting. Yet, consider firm size sorting variable, gives rise well-known return factor: size premium. size premium arises buying small stocks selling large stocks. Prominently, Fama French (1993) include factor three-factor model. Apart , asset managers commonly include size key firm characteristic making investment decisions.also introduce new choices formation portfolios. particular, discuss listing exchanges, industries, weighting regimes, periods. choices matter portfolio returns result different size premiums Walter, Weber, Weiss (2022). Exploiting ideas generate favorable results called p-hacking.\narguably thin line p-hacking conducting robustness tests. purpose illustrate substantial variation can arise along evidence-generating process.chapter relies following set packages:Compared previous chapters, introduce rlang package (Henry Wickham 2022) advanced parsing functional expressions.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(furrr)\nlibrary(rlang)"},{"path":"size-sorts-and-p-hacking.html","id":"data-preparation-1","chapter":"8 Size sorts and p-hacking","heading":"8.1 Data preparation","text":"First, retrieve relevant data SQLite-database introduced Chapters 2-4. Firm size defined market equity asset pricing applications retrieve CRSP. use Fama-French factor returns performance evaluation.","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()"},{"path":"size-sorts-and-p-hacking.html","id":"size-distribution","chapter":"8 Size sorts and p-hacking","heading":"8.2 Size distribution","text":"build size portfolios, investigate distribution variable firm size. Visualizing data valuable starting point understand input analysis. figure shows fraction total market capitalization concentrated largest firm. produce graph, create monthly indicators track whether stock belongs largest x percent firms.\n, aggregate firms within bucket compute buckets’ share total market capitalization.figure shows largest 1 percent firms cover 50 percent total market capitalization, holding just 25 percent largest firms CRSP universe essentially replicates market portfolio. distribution firm size thus implies largest firms market dominate many small firms whenever use value-weighted benchmarks.\nFigure 8.1: report aggregate market capitalization stocks belong 1, 5, 10, 25 percent quantile largest firms monthly cross-section relative market capitalization stocks month.\nNext, firm sizes also differ across listing exchanges. Stocks’ primary listings important past potentially still relevant today. graph shows New York Stock Exchange (NYSE) still largest listing exchange terms market capitalization. recently, NASDAQ gained relevance listing exchange. know small peak NASDAQ’s market cap around year 2000 ?\nFigure 8.2: Years horizontal axis corresponding share total market capitalization per listing exchange vertical axis.\nFinally, consider distribution firm size across listing exchanges create summary statistics. function summary() include statistics interested , create function create_summary() adds standard deviation number observations. , apply current month CRSP data listing exchange. also add row add_row() overall summary statistics.resulting table shows firms listed NYSE December 2021 significantly larger average firms listed exchanges. Moreover, NASDAQ lists largest number firms. discrepancy firm sizes across listing exchanges motivated researchers form breakpoints exclusively NYSE sample apply breakpoints stocks. following, use distinction update portfolio sort procedure.","code":"\ncrsp_monthly |>\n  group_by(month) |>\n  mutate(\n    top01 = if_else(mktcap >= quantile(mktcap, 0.99), 1, 0),\n    top05 = if_else(mktcap >= quantile(mktcap, 0.95), 1, 0),\n    top10 = if_else(mktcap >= quantile(mktcap, 0.90), 1, 0),\n    top25 = if_else(mktcap >= quantile(mktcap, 0.75), 1, 0),\n    total_market_cap = sum(mktcap)\n  ) |>\n  summarize(\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap\n  ) |>\n  pivot_longer(cols = -month) |>\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |>\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\ncrsp_monthly |>\n  group_by(month, exchange) |>\n  summarize(mktcap = sum(mktcap)) |>\n  mutate(share = mktcap / sum(mktcap)) |>\n  ggplot(aes(x = month, y = share, fill = exchange, color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\ncreate_summary <- function(data, column_name) {\n  data |>\n    select(value = {{ column_name }}) |>\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q50 = quantile(value, 0.50),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |>\n  filter(month == max(month)) |>\n  group_by(exchange) |>\n  create_summary(mktcap) |>\n  add_row(crsp_monthly |>\n    filter(month == max(month)) |>\n    create_summary(mktcap) |>\n    mutate(exchange = \"Overall\"))# A tibble: 5 × 9\n  exchange   mean     sd      min     q05     q50    q95    max     n\n  <chr>     <dbl>  <dbl>    <dbl>   <dbl>   <dbl>  <dbl>  <dbl> <int>\n1 AMEX       415.  2181.     7.57    12.6    75.8  1218. 2.57e4   145\n2 NASDAQ    8649. 90038.     7.01    29.1   428.  18781. 2.90e6  2779\n3 NYSE     17858. 48619.    23.9    195.   3434.  80748. 4.73e5  1395\n4 Other    13906.    NA  13906.   13906.  13906.  13906. 1.39e4     1\n5 Overall  11348. 77458.     7.01    34.0   794.  40647. 2.90e6  4320"},{"path":"size-sorts-and-p-hacking.html","id":"univariate-size-portfolios-with-flexible-breakpoints","chapter":"8 Size sorts and p-hacking","heading":"8.3 Univariate size portfolios with flexible breakpoints","text":"Chapter 7, construct portfolios varying number breakpoints different sorting variables. , extend framework compute breakpoints subset data, instance, based selected listing exchanges. published asset pricing articles, many scholars compute sorting breakpoints NYSE-listed stocks. NYSE-specific breakpoints applied entire universe stocks.replicate NYSE-centered sorting procedure, introduce exchanges argument assign_portfolio() function. exchange-specific argument enters filter filter(exchange %% exchanges). example, exchanges = 'NYSE' specified, stocks listed NYSE used compute breakpoints. Alternatively, specify exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"), keeps stocks listed either exchanges. Overall, regular expressions powerful tool, touch specific case .","code":"\nassign_portfolio <- function(n_portfolios,\n                             exchanges,\n                             data) {\n  breakpoints <- data |>\n    filter(exchange %in% exchanges) |>\n    summarize(breakpoint = quantile(\n      mktcap_lag,\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  assigned_portfolios <- data |>\n    mutate(portfolio = findInterval(mktcap_lag,\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n  return(assigned_portfolios)\n}"},{"path":"size-sorts-and-p-hacking.html","id":"weighting-schemes-for-portfolios","chapter":"8 Size sorts and p-hacking","heading":"8.4 Weighting schemes for portfolios","text":"Apart computing breakpoints different samples, researchers often use different portfolio weighting schemes. far, weighted portfolio constituent relative market equity previous period. protocol called value-weighting. alternative protocol equal-weighting, assigns stock’s return weight, .e., simple average constituents’ returns. Notice equal-weighting difficult practice portfolio manager needs rebalance portfolio monthly value-weighting truly passive investment.implement two weighting schemes function compute_portfolio_returns() takes logical argument weight returns firm value. statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns value_weighted = TRUE. Additionally, long-short portfolio long smallest firms short largest firms, consistent research showing small firms outperform larger counterparts. Apart two changes, function similar procedure Chapter 7.see function compute_portfolio_returns() works, consider simple median breakpoint example value-weighted returns. interested effect restricting listing exchanges estimation size premium. first function call, compute returns based breakpoints listing exchanges. , computed returns based breakpoints NYSE-listed stocks.table shows size premium 60 percent larger consider stocks NYSE form breakpoint month. NYSE-specific breakpoints larger, 50 percent stocks entire universe resulting small portfolio NYSE firms larger average. impact choice negligible.","code":"\ncompute_portfolio_returns <- function(n_portfolios = 10,\n                                      exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data |>\n    group_by(month) |>\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = cur_data()\n    )) |>\n    group_by(month, portfolio) |>\n    summarize(\n      ret = if_else(value_weighted,\n        weighted.mean(ret_excess, mktcap_lag),\n        mean(ret_excess)\n      ),\n      .groups = \"drop_last\"\n    ) |>\n    summarize(size_premium = ret[portfolio == min(portfolio)] -\n      ret[portfolio == max(portfolio)]) |>\n    summarize(size_premium = mean(size_premium))\n}\nret_all <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(\n  Exchanges = c(\"NYSE, NASDAQ & AMEX\", \"NYSE\"),\n  Premium = as.numeric(c(ret_all, ret_nyse)) * 100\n)# A tibble: 2 × 2\n  Exchanges           Premium\n  <chr>                 <dbl>\n1 NYSE, NASDAQ & AMEX  0.0975\n2 NYSE                 0.166 "},{"path":"size-sorts-and-p-hacking.html","id":"p-hacking-and-non-standard-errors","chapter":"8 Size sorts and p-hacking","heading":"8.5 P-hacking and non-standard errors","text":"Since choice listing exchange significant impact, next step investigate effect data processing decisions researchers make along way.\nparticular, portfolio sort analysis decide least number portfolios, listing exchanges form breakpoints, equal- value-weighting.\n, one may exclude firms active finance industry restrict analysis parts time series.\nvariations choices discuss part scholarly articles published top finance journals.\nrefer Walter, Weber, Weiss (2022) extensive set decision nodes discretion researchers.intention application show different ways form portfolios result different estimated size premiums. Despite effects multitude choices, correct way. also noted none procedures wrong, aim simply illustrate changes can arise due variation evidence-generating process (Menkveld et al. 2021). term non-standard errors refers variation due (suitable) choices made researchers. Interestingly, large scale study, Menkveld et al. (2021) find magnitude non-standard errors similar estimation uncertainty based chosen model shows important adjust seemingly innocent choices data preparation evaluation workflow. malicious perspective, modeling choices give researcher multiple chances find statistically significant results. Yet considered p-hacking, renders statistical inference due multiple testing invalid (Harvey, Liu, Zhu 2016).Nevertheless, multitude options creates problem since single correct way sorting portfolios. researcher convince reader results come p-hacking exercise? circumvent dilemma, academics encouraged present evidence different sorting schemes robustness tests report multiple approaches show result depend single choice. Thus, robustness premiums key feature.conduct series robustness tests also interpreted p-hacking exercise. , examine size premium different specifications presented table p_hacking_setup. function expand_grid() produces table possible permutations arguments. Note use argument data exclude financial firms truncate time series.speed computation parallelize (many) different sorting procedures, beta estimation Chapter 6. Finally, report resulting size premiums descending order. indeed substantial size premiums possible data, particular use equal-weighted portfolios.","code":"\np_hacking_setup <- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n  value_weighted = c(TRUE, FALSE),\n  data = parse_exprs(\n    'crsp_monthly; \n     crsp_monthly |> filter(industry != \"Finance\");\n     crsp_monthly |> filter(month < \"1990-06-01\");\n     crsp_monthly |> filter(month >=\"1990-06-01\")'\n  )\n)\nplan(multisession, workers = availableCores())\n\np_hacking_setup <- p_hacking_setup |>\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = eval_tidy(..4)\n    )\n  ))\n\np_hacking_results <- p_hacking_setup |>\n  mutate(data = map_chr(data, deparse)) |>\n  unnest(size_premium) |>\n  arrange(desc(size_premium))\np_hacking_results# A tibble: 48 × 5\n  n_portfolios exchanges value_weighted data                  size_…¹\n         <dbl> <list>    <lgl>          <chr>                   <dbl>\n1           10 <chr [3]> FALSE          \"filter(crsp_monthly…  0.0186\n2           10 <chr [3]> FALSE          \"filter(crsp_monthly…  0.0182\n3           10 <chr [3]> FALSE          \"crsp_monthly\"         0.0163\n4           10 <chr [3]> FALSE          \"filter(crsp_monthly…  0.0139\n5           10 <chr [3]> TRUE           \"filter(crsp_monthly…  0.0115\n# … with 43 more rows, and abbreviated variable name ¹​size_premium"},{"path":"size-sorts-and-p-hacking.html","id":"the-size-premium-variation","chapter":"8 Size sorts and p-hacking","heading":"8.6 The size-premium variation","text":"provide graph shows different premiums. plot also shows relation average Fama-French SMB (small minus big) premium used literature include dotted vertical line.\nFigure 8.3: dashed vertical line indicates average Fama-French SMB premium.\n","code":"\np_hacking_results |>\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of size premiums for different sorting choices\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb)),\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)"},{"path":"size-sorts-and-p-hacking.html","id":"exercises-7","chapter":"8 Size sorts and p-hacking","heading":"8.7 Exercises","text":"gained several insights size distribution . However, analyze average size across listing exchanges industries. listing exchanges/industries largest firms? Plot average firm size three listing exchanges time. conclude?compute breakpoints take look exposition . might cover potential data errors. Plot breakpoints ten size portfolios time. , take difference two extreme portfolios plot . Describe results.returns analyse account differences exposure market risk, .e., CAPM beta. Change function compute_portfolio_returns() output CAPM alpha beta instead average excess return.saw spread returns p-hacking exercise, show choices led largest effects. Find way investigate choice variable largest impact estimated size premium.computed several size premiums, follow definition Fama French (1993). approaches comes closest SMB premium?","code":""},{"path":"value-and-bivariate-sorts.html","id":"value-and-bivariate-sorts","chapter":"9 Value and bivariate sorts","heading":"9 Value and bivariate sorts","text":"chapter, extend univariate portfolio analysis bivariate sorts, means assign stocks portfolios based two characteristics. Bivariate sorts regularly used academic asset pricing literature basis Fama French three factors. However, scholars also use sorts three grouping variables. Conceptually, portfolio sorts easily applicable higher dimensions.form portfolios firm size book--market ratio. calculate book--market ratios, accounting data required, necessitates additional steps portfolio formation. end, demonstrate form portfolios two sorting variables using -called independent dependent portfolio sorts.current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(sandwich)"},{"path":"value-and-bivariate-sorts.html","id":"data-preparation-2","chapter":"9 Value and bivariate sorts","heading":"9.1 Data preparation","text":"First, load necessary data SQLite-database introduced Chapters 2-4. conduct portfolio sorts based CRSP sample keep necessary columns memory. use data sources firm size Chapter 8., utilize accounting data. common source accounting data Compustat. need book equity data application, select database. Additionally, convert variable datadate monthly value, consider monthly returns need account exact date. achieve , use function floor_date().","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\ncrsp_monthly <- crsp_monthly |>\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |>\n  drop_na()\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\n\nbe <- compustat |>\n  select(gvkey, datadate, be) |>\n  drop_na() |>\n  mutate(month = floor_date(ymd(datadate), \"month\"))"},{"path":"value-and-bivariate-sorts.html","id":"book-to-market-ratio","chapter":"9 Value and bivariate sorts","heading":"9.2 Book-to-market ratio","text":"fundamental problem handling accounting data look-ahead bias - must include data forming portfolio public knowledge time. course, researchers information looking past agents moment. However, abnormal excess returns trading strategy rely information advantage differential result informed agents’ trades. Hence, lag accounting information.continue lag market capitalization firm size one month. , compute book--market ratio, relates firm’s book equity market equity. Firms high (low) book--market ratio called value (growth) firms. matching accounting market equity information month, lag book--market six months. sufficiently conservative approach accounting information usually released well six months pass. However, asset pricing literature, even longer lags used well.1Having variables, .e., firm size lagged one month book--market lagged six months, merge sorting variables returns using sorting_date-column created purpose. final step data preparation deals differences frequency variables. Returns firm size recorded monthly. Yet accounting information released annual basis. Hence, match book--market one month per year eleven empty observations. solve frequency issue, carry latest book--market ratio firm subsequent months, .e., fill missing observations current report. done via fill()-function sorting date firm (identify permno gvkey) firm basis (group_by() usual). filter observations accounting data older year. last step, remove rows missing entries returns matched annual report.last step preparation portfolio sorts computation breakpoints. continue use function allowing specification exchanges use breakpoints. Additionally, reintroduce argument var function defining different sorting variables via curly-curly.data preparation steps, present bivariate portfolio sorts independent dependent basis.","code":"\nme <- crsp_monthly |>\n  mutate(sorting_date = month %m+% months(1)) |>\n  select(permno, sorting_date, me = mktcap)\n\nbm <- be |>\n  inner_join(crsp_monthly |>\n    select(month, permno, gvkey, mktcap), by = c(\"gvkey\", \"month\")) |>\n  mutate(\n    bm = be / mktcap,\n    sorting_date = month %m+% months(6),\n    comp_date = sorting_date\n  ) |>\n  select(permno, gvkey, sorting_date, comp_date, bm)\n\ndata_for_sorts <- crsp_monthly |>\n  left_join(bm, by = c(\"permno\",\n    \"gvkey\",\n    \"month\" = \"sorting_date\"\n  )) |>\n  left_join(me, by = c(\"permno\", \"month\" = \"sorting_date\")) |>\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap_lag, me, bm, exchange, comp_date\n  )\n\ndata_for_sorts <- data_for_sorts |>\n  arrange(permno, gvkey, month) |>\n  group_by(permno, gvkey) |>\n  fill(bm, comp_date) |>\n  filter(comp_date > month %m-% months(12)) |>\n  select(-comp_date) |>\n  drop_na()\nassign_portfolio <- function(data, var, n_portfolios, exchanges) {\n  breakpoints <- data |>\n    filter(exchange %in% exchanges) |>\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  assigned_portfolios <- data |>\n    mutate(portfolio = findInterval({{ var }},\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n\n  return(assigned_portfolios)\n}"},{"path":"value-and-bivariate-sorts.html","id":"independent-sorts","chapter":"9 Value and bivariate sorts","heading":"9.3 Independent sorts","text":"Bivariate sorts create portfolios within two-dimensional space spanned two sorting variables. possible assess return impact either sorting variable return differential trading strategy invests portfolios either end respective variables spectrum. create five--five matrix using book--market firm size sorting variables example . end 25 portfolios. Since interested value premium (.e., return differential high low book--market firms), go long five portfolios highest book--market firms short five portfolios lowest book--market firms. five portfolios end due size splits employed alongside book--market splits.implement independent bivariate portfolio sort, assign monthly portfolios sorting variables separately create variables portfolio_bm portfolio_me, respectively. , separate portfolios combined final sort stored portfolio_combined. assigning portfolios, compute average return within portfolio month. Additionally, keep book--market portfolio makes computation value premium easier. alternative disaggregate combined portfolio separate step. Notice weigh stocks within portfolio market capitalization, .e., decide value-weight returns.Equipped monthly portfolio returns, ready compute value premium. However, still decide invest five high five low book--market portfolios. common approach weigh portfolios equally, yet another researcher’s choice. , compute return differential high low book--market portfolios show average value premium.resulting annualized value premium 4.608 percent.","code":"\nvalue_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = str_c(portfolio_bm, portfolio_me)\n  ) |>\n  group_by(month, portfolio_combined) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\nvalue_premium <- value_portfolios |>\n  group_by(month, portfolio_bm) |>\n  summarize(ret = mean(ret), .groups = \"drop_last\") |>\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n    ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)[1] 0.384"},{"path":"value-and-bivariate-sorts.html","id":"dependent-sorts","chapter":"9 Value and bivariate sorts","heading":"9.4 Dependent sorts","text":"previous exercise, assigned portfolios without considering second variable assignment. protocol called independent portfolio sorts. alternative, .e., dependent sorts, creates portfolios second sorting variable within bucket first sorting variable. example , sort firms five size buckets, within buckets, assign firms five book--market portfolios. Hence, monthly breakpoints specific size group. decision independent dependent portfolio sorts another choice researcher. Notice dependent sorts ensure equal amount stocks within portfolio.implement dependent sorts, first create size portfolios calling assign_portfolio() var = . , group data month size portfolio assigning book--market portfolio. rest implementation . Finally, compute value premium.value premium dependent sorts 3.948 percent per year.Overall, show conduct bivariate portfolio sorts chapter. one case, sort portfolios independently . Yet also discuss create dependent portfolio sorts. Along lines Chapter 8, see many choices researcher make implement portfolio sorts, bivariate sorts increase number choices.","code":"\nvalue_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(portfolio_me = assign_portfolio(\n    data = cur_data(),\n    var = me,\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) |>\n  group_by(month, portfolio_me) |>\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = str_c(portfolio_bm, portfolio_me)\n  ) |>\n  group_by(month, portfolio_combined) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\n\nvalue_premium <- value_portfolios |>\n  group_by(month, portfolio_bm) |>\n  summarize(ret = mean(ret), .groups = \"drop_last\") |>\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n    ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)[1] 0.329"},{"path":"value-and-bivariate-sorts.html","id":"exercises-8","chapter":"9 Value and bivariate sorts","heading":"9.5 Exercises","text":"Chapter 8, examine distribution market equity. Repeat analysis book equity book--market ratio (alongside plot breakpoints, .e., deciles).investigate portfolios, focus returns exclusively. However, also interest understand characteristics portfolios. Write function compute average characteristics size book--market across 25 independently dependently sorted portfolios.size premium, also value premium constructed follow Fama French (1993). Implement p-hacking setup Chapter 8 find premium comes closest HML premium.","code":""},{"path":"replicating-fama-and-french-factors.html","id":"replicating-fama-and-french-factors","chapter":"10 Replicating Fama and French factors","heading":"10 Replicating Fama and French factors","text":"chapter, provide replication famous Fama French factor portfolios. Fama French three-factor model (see Fama French 1993) cornerstone asset pricing. top market factor represented traditional CAPM beta, model includes size value factors explain cross section returns. introduce factors Chapter 9, definition remains . Size SMB factor (small-minus-big) long small firms short large firms. value factor HML (high-minus-low) long high book--market firms short low book--market counterparts.current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)"},{"path":"replicating-fama-and-french-factors.html","id":"data-preparation-3","chapter":"10 Replicating Fama and French factors","heading":"10.1 Data preparation","text":"use CRSP Compustat data sources, need exactly variables compute size value factors way Fama French . Hence, nothing new load data SQLite-database introduced Chapters 2-4.Yet start merging data set computing premiums, differences Chapter 9. First, Fama French form portfolios June year \\(t\\), whereby returns July first monthly return respective portfolio. firm size, consequently use market capitalization recorded June. held constant June year \\(t+1\\).Second, Fama French also different protocol computing book--market ratio. use market equity end year \\(t - 1\\) book equity reported year \\(t-1\\), .e., datadate within last year. Hence, book--market ratio can based accounting information 18 months old. Market equity also necessarily reflect time point book equity.implement time lags, employ temporary sorting_date-column. Notice combine information, want single observation per year stock since interested computing breakpoints held constant entire year. ensure call distinct() end chunk .","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\ndata_ff <- crsp_monthly |>\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |>\n  drop_na()\n\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\n\nbe <- compustat |>\n  select(gvkey, datadate, be) |>\n  drop_na()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\nme_ff <- data_ff |>\n  filter(month(month) == 6) |>\n  mutate(sorting_date = month %m+% months(1)) |>\n  select(permno, sorting_date, me_ff = mktcap)\n\nme_ff_dec <- data_ff |>\n  filter(month(month) == 12) |>\n  mutate(sorting_date = ymd(str_c(year(month) + 1, \"0701)\"))) |>\n  select(permno, gvkey, sorting_date, bm_me = mktcap)\n\nbm_ff <- be |>\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |>\n  select(gvkey, sorting_date, bm_be = be) |>\n  drop_na() |>\n  inner_join(me_ff_dec, by = c(\"gvkey\", \"sorting_date\")) |>\n  mutate(bm_ff = bm_be / bm_me) |>\n  select(permno, sorting_date, bm_ff)\n\nvariables_ff <- me_ff |>\n  inner_join(bm_ff, by = c(\"permno\", \"sorting_date\")) |>\n  drop_na() |>\n  distinct(permno, sorting_date, .keep_all = TRUE)"},{"path":"replicating-fama-and-french-factors.html","id":"portfolio-sorts","chapter":"10 Replicating Fama and French factors","heading":"10.2 Portfolio sorts","text":"Next, construct portfolios adjusted assign_portfolio() function. Fama French rely NYSE-specific breakpoints, form two portfolios size dimension median three portfolios dimension book--market 30%- 70%-percentiles, use independent sorts. sorts book--market require adjustment function Chapter 9 seq() produce produce right breakpoints. Instead n_portfolios, now specify percentiles, take breakpoint-sequence object specified function’s call. Specifically, give percentiles = c(0, 0.3, 0.7, 1) function. Additionally, perform inner_join() return data ensure use traded stocks computing breakpoints first step.Next, merge portfolios return data rest year. implement step, create new column sorting_date return data setting date sort July \\(t-1\\) month June (year \\(t\\)) earlier July year \\(t\\) month July later.","code":"\nassign_portfolio <- function(data, var, percentiles) {\n  breakpoints <- data |>\n    filter(exchange == \"NYSE\") |>\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = {{ percentiles }},\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  assigned_portfolios <- data |>\n    mutate(portfolio = findInterval({{ var }},\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n\n  return(assigned_portfolios)\n}\n\nportfolios_ff <- variables_ff |>\n  inner_join(data_ff, by = c(\"permno\" = \"permno\", \"sorting_date\" = \"month\")) |>\n  group_by(sorting_date) |>\n  mutate(\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me_ff,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm_ff,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) |>\n  select(permno, sorting_date, portfolio_me, portfolio_bm)\nportfolios_ff <- data_ff |>\n  mutate(sorting_date = case_when(\n    month(month) <= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) >= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |>\n  inner_join(portfolios_ff, by = c(\"permno\", \"sorting_date\"))"},{"path":"replicating-fama-and-french-factors.html","id":"fama-and-french-factor-returns","chapter":"10 Replicating Fama and French factors","heading":"10.3 Fama and French factor returns","text":"Equipped return data assigned portfolios, can now compute value-weighted average return six portfolios. , form Fama French factors. size factor (.e., SMB), go long three small portfolios short three large portfolios taking average across either group. value factor (.e., HML), go long two high book--market portfolios short two low book--market portfolios, weighting equally.","code":"\nfactors_ff_monthly_replicated <- portfolios_ff |>\n  mutate(portfolio = str_c(portfolio_me, portfolio_bm)) |>\n  group_by(portfolio, month) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\",\n    portfolio_me = unique(portfolio_me),\n    portfolio_bm = unique(portfolio_bm)\n  ) |>\n  group_by(month) |>\n  summarize(\n    smb_replicated = mean(ret[portfolio_me == 1]) -\n      mean(ret[portfolio_me == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )"},{"path":"replicating-fama-and-french-factors.html","id":"replication-evaluation","chapter":"10 Replicating Fama and French factors","heading":"10.4 Replication evaluation","text":"previous section, replicated size value premiums following procedure outlined Fama French. However, follow procedure strictly. final question : close get? answer question looking two time-series estimates regression analysis using lm(). good job, see non-significant intercept (rejecting notion systematic error), coefficient close 1 (indicating high correlation), adjusted R-squared close 1 (indicating high proportion explained variance).results SMB factor quite convincing three criteria outlined met coefficient R-squared 99%.replication HML factor also success, although slightly lower level coefficient R-squared around 95%.evidence hence allows us conclude relatively good job replicating original Fama-French premiums, although see underlying code.\nperspective, perfect match possible additional information maintainers original data.","code":"\ntest <- factors_ff_monthly |>\n  inner_join(factors_ff_monthly_replicated, by = \"month\") |>\n  mutate(\n    smb_replicated = round(smb_replicated, 4),\n    hml_replicated = round(hml_replicated, 4)\n  )\nsummary(lm(smb ~ smb_replicated, data = test))\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020352 -0.001590  0.000007  0.001563  0.014636 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.000129   0.000131   -0.98     0.33    \nsmb_replicated  0.995395   0.004341  229.31   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00352 on 724 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.26e+04 on 1 and 724 DF,  p-value: <2e-16\nsummary(lm(hml ~ hml_replicated, data = test))\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.023350 -0.002984 -0.000091  0.002280  0.028783 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.000318   0.000219    1.45     0.15    \nhml_replicated 0.965370   0.007478  129.09   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00587 on 724 degrees of freedom\nMultiple R-squared:  0.958, Adjusted R-squared:  0.958 \nF-statistic: 1.67e+04 on 1 and 724 DF,  p-value: <2e-16"},{"path":"replicating-fama-and-french-factors.html","id":"exercises-9","chapter":"10 Replicating Fama and French factors","heading":"10.5 Exercises","text":"Fama French (1993) claim sample excludes firms appeared Compustat two years. Implement additional filter compare improvements replication effort.homepage, Kenneth French provides instructions construct common variables used portfolio sorts. Pick one , e.g. OP (operating profitability) try replicate portfolio sort return time series provided homepage.","code":""},{"path":"fama-macbeth-regressions.html","id":"fama-macbeth-regressions","chapter":"11 Fama-MacBeth regressions","heading":"11 Fama-MacBeth regressions","text":"chapter, present simple implementation Fama MacBeth (1973), regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions widely used empirical asset pricing studies. use individual stocks test assets estimate risk premium associated three factors included Fama French (1993).Researchers use two-stage regression approach estimate risk premiums various markets, predominately stock market.\nEssentially, two-step Fama-MacBeth regressions exploit linear relationship expected returns exposure (priced) risk factors.\nbasic idea regression approach project asset returns factor exposures characteristics resemble exposure risk factor cross-section time period.\n, second step, estimates aggregated across time test risk factor priced.\nprinciple, Fama-MacBeth regressions can used way portfolio sorts introduced previous chapters. Fama-MacBeth procedure simple two-step approach:\nfirst step uses exposures (characteristics) explanatory variables \\(T\\) cross-sectional regressions. example, \\(r_{,t+1}\\) denote excess returns asset \\(\\) month \\(t+1\\), famous Fama-French three factor model implies following return generating process (see also John Y. Campbell et al. (1998)):\n\\[\\begin{aligned}r_{,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{,t} + \\lambda^{HML}_t \\beta^{HML}_{,t} + \\epsilon_{,t}\\text{, t}.\\end{aligned}\\]\n, interested compensation \\(\\lambda^{f}_t\\) exposure risk factor \\(\\beta^{f}_{,t}\\) time point, .e., risk premium. Note terminology: \\(\\beta^{f}_{,t}\\) asset-specific characteristic, e.g., factor exposure accounting variable. linear relationship expected returns characteristic given month, expect regression coefficient reflect relationship, .e., \\(\\lambda_t^{f}\\neq0\\).second step, time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) estimates \\(\\hat\\lambda^{f}_t\\) can interpreted risk premium specific risk factor \\(f\\). follow Zaffaroni Zhou (2022) consider standard cross-sectional regression predict future returns. characteristics replaced time \\(t+1\\) variables, regression approach rather captures risk attributes.move implementation, want highlight characteristics, e.g., \\(\\hat\\beta^{f}_{}\\), often estimated separate step applying actual Fama-MacBeth methodology. can think step 0. might thus worry errors \\(\\hat\\beta^{f}_{}\\) impact risk premiums’ standard errors. Measurement error \\(\\hat\\beta^{f}_{}\\) indeed affects risk premium estimates, .e., lead biased estimates. literature provides adjustments bias (see, e.g., Shanken 1992; Kim 1995; H.-Y. Chen, Lee, Lee 2015, among others) also shows bias goes zero \\(T \\\\infty\\). refer Gagliardini, Ossola, Scaillet (2016) -depth discussion also covering case time-varying betas. Moreover, plan use Fama-MacBeth regressions individual stocks: Hou, Xue, Zhang (2020) advocates using weighed-least squares estimate coefficients biased towards small firms. Without adjustment, high number small firms drive coefficient estimates.current chapter relies set packages.Compared previous chapters, introduce broom package (Robinson, Hayes, Couch 2022) tidy estimation output many estimated linear models.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(broom)"},{"path":"fama-macbeth-regressions.html","id":"data-preparation-4","chapter":"11 Fama-MacBeth regressions","heading":"11.1 Data preparation","text":"illustrate Fama MacBeth (1973) monthly CRSP sample use three characteristics explain cross-section returns: market capitalization, book--market ratio, CAPM beta (.e., covariance excess stock returns market excess returns). collect data SQLite-database introduced Chapters 2-4.use Compustat CRSP data compute book--market ratio (log) market capitalization.\nFurthermore, also use CAPM betas based monthly returns computed previous chapters.","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") |>\n  collect()\ncharacteristics <- compustat |>\n  mutate(month = floor_date(ymd(datadate), \"month\")) |>\n  left_join(crsp_monthly, by = c(\"gvkey\", \"month\")) |>\n  left_join(beta, by = c(\"permno\", \"month\")) |>\n  transmute(gvkey,\n    bm = be / mktcap,\n    log_mktcap = log(mktcap),\n    beta = beta_monthly,\n    sorting_date = month %m+% months(6)\n  )\n\ndata_fama_macbeth <- crsp_monthly |>\n  left_join(characteristics, by = c(\"gvkey\", \"month\" = \"sorting_date\")) |>\n  group_by(permno) |>\n  arrange(month) |>\n  fill(c(beta, bm, log_mktcap), .direction = \"down\") |>\n  ungroup() |>\n  left_join(crsp_monthly |>\n    select(permno, month, ret_excess_lead = ret) |>\n    mutate(month = month %m-% months(1)),\n  by = c(\"permno\", \"month\")\n  ) |>\n  select(permno, month, ret_excess_lead, beta, log_mktcap, bm) |>\n  drop_na()"},{"path":"fama-macbeth-regressions.html","id":"cross-sectional-regression","chapter":"11 Fama-MacBeth regressions","heading":"11.2 Cross-sectional regression","text":"Next, run cross-sectional regressions characteristics explanatory variables month.\nregress returns test assets particular time point asset’s characteristics.\n, get estimate risk premiums \\(\\hat\\lambda^{f}_t\\) point time. ","code":"\nrisk_premiums <- data_fama_macbeth |>\n  nest(data = c(ret_excess_lead, beta, log_mktcap, bm, permno)) |>\n  mutate(estimates = map(\n    data,\n    ~ tidy(lm(ret_excess_lead ~ beta + log_mktcap + bm, data = .x))\n  )) |>\n  unnest(estimates)"},{"path":"fama-macbeth-regressions.html","id":"time-series-aggregation","chapter":"11 Fama-MacBeth regressions","heading":"11.3 Time-series aggregation","text":"Now risk premiums’ estimates period, can average across time-series dimension get expected risk premium characteristic. Similarly, manually create \\(t\\)-test statistics regressor, can compare usual critical values 1.96 2.576 two-tailed significance tests.common adjust autocorrelation reporting standard errors risk premiums. Chapter 7, typical procedure computing Newey West (1987) standard errors. recommend data-driven approach Newey West (1994) using NeweyWest() function, note can enforce typical 6 lag settings via NeweyWest(., lag = 6, prewhite = FALSE).Finally, let us interpret results. Stocks higher book--market ratios earn higher expected future returns, line value premium. negative value log market capitalization reflects size premium smaller stocks. Consistent results earlier chapters, detect relation beta future stock returns.","code":"\nprice_of_risk <- risk_premiums |>\n  group_by(factor = term) |>\n  summarize(\n    risk_premium = mean(estimate) * 100,\n    t_statistic = mean(estimate) / sd(estimate) * sqrt(n())\n  )\nregressions_for_newey_west <- risk_premiums |>\n  select(month, factor = term, estimate) |>\n  nest(data = c(month, estimate)) |>\n  mutate(\n    model = map(data, ~ lm(estimate ~ 1, .)),\n    mean = map(model, tidy)\n  )\n\nprice_of_risk_newey_west <- regressions_for_newey_west |>\n  mutate(newey_west_se = map_dbl(model, ~ sqrt(NeweyWest(.)))) |>\n  unnest(mean) |>\n  mutate(t_statistic_newey_west = estimate / newey_west_se) |>\n  select(factor,\n    risk_premium = estimate,\n    t_statistic_newey_west\n  )\n\nleft_join(price_of_risk,\n  price_of_risk_newey_west |>\n    select(factor, t_statistic_newey_west),\n  by = \"factor\"\n)# A tibble: 4 × 4\n  factor      risk_premium t_statistic t_statistic_newey_west\n  <chr>              <dbl>       <dbl>                  <dbl>\n1 (Intercept)      1.70         6.68                   5.76  \n2 beta             0.00763      0.0730                 0.0651\n3 bm               0.141        3.03                   2.59  \n4 log_mktcap      -0.114       -3.20                  -2.94  "},{"path":"fama-macbeth-regressions.html","id":"exercises-10","chapter":"11 Fama-MacBeth regressions","heading":"11.4 Exercises","text":"Download sample test assets Kenneth French’s homepage reevaluate risk premiums industry portfolios instead individual stocks.Use individual stocks weighted-least squares based firm’s size suggested Hou, Xue, Zhang (2020). , repeat Fama-MacBeth regressions without weighting scheme adjustment drop smallest 20 percent firms month. Compare results three approaches.Implement rolling-window regression time-series estimation factor exposure. Skip one month rolling period including exposures cross-sectional regression avoid look-ahead bias. , adapt cross-sectional regression compute average risk premiums.","code":""},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"fixed-effects-and-clustered-standard-errors","chapter":"12 Fixed effects and clustered standard errors","heading":"12 Fixed effects and clustered standard errors","text":"chapter, provide intuitive introduction two popular concepts fixed effects regressions clustered standard errors. working regressions empirical finance, sooner later confronted discussions around deal omitted variables bias dependence residuals. concepts introduce chapter designed address concerns.focus classical panel regression common corporate finance literature (e.g., Fazzari et al. 1988; Erickson Whited 2012; Gulen Ion 2015): firm investment modeled function increases firm cash flow firm investment opportunities.Typically, investment regression uses quarterly balance sheet data provided via Compustat allows richer dynamics regressors opportunities construct variables. focus implementation fixed effects clustered standard errors, use annual Compustat data previous chapters leave estimation using quarterly data exercise. demonstrate regression based annual data yields qualitatively similar results estimations based quarterly data literature, namely confirming positive relationships investment two regressors.current chapter relies following set packages.Compared previous chapters, introduce fixest (Bergé 2018) fixed effects regressions, implementation standard error clusters, tidy estimation output.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(fixest)"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"data-preparation-5","chapter":"12 Fixed effects and clustered standard errors","heading":"12.1 Data preparation","text":"use CRSP annual Compustat data sources SQLite-database introduced Chapters 2-4. particular, Compustat provides balance sheet income statement data firm level, CRSP provides market valuations. classical investment regressions model capital investment firm function operating cash flows Tobin’s q, measure firm’s investment opportunities. start constructing investment cash flows usually normalized lagged total assets firm. following code chunk, construct panel firm-year observations, cross-sectional information firms well time-series information firm.Tobin’s q ratio market value capital replacement costs. one common regressors corporate finance applications (e.g., Fazzari et al. 1988; Erickson Whited 2012). follow implementation Gulen Ion (2015) compute Tobin’s q market value equity (mktcap) plus book value assets () minus book value equity () plus deferred taxes (txdb), divided book value assets (). Finally, keep observations variables interest non-missing, reported book value assets strictly positive.variable construction typically leads extreme values likely related data issues (e.g., reporting errors), many papers include winsorization variables interest. Winsorization involves replacing values extreme outliers quantiles respective end. following function implements winsorization percentage cut applied either end distributions. specific example, winsorize main variables (´investment´, ´cash_flows´, ´tobins_q´) 1 percent level.proceeding estimations, highly recommend tabulating summary statistics variables enter regression. simple tables allow check plausibility numerical variables, well spot obvious errors outliers. Additionally, panel data, plotting time series variable’s mean number observations useful exercise spot potential problems.","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\ndata_investment <- compustat |>\n  mutate(month = floor_date(datadate, \"month\")) |>\n  left_join(compustat |>\n    select(gvkey, year, at_lag = at) |>\n    mutate(year = year + 1),\n  by = c(\"gvkey\", \"year\")\n  ) |>\n  filter(at > 0, at_lag > 0) |>\n  mutate(\n    investment = capx / at_lag,\n    cash_flows = oancf / at_lag\n  )\n\ndata_investment <- data_investment |>\n  left_join(data_investment |>\n    select(gvkey, year, investment_lead = investment) |>\n    mutate(year = year - 1),\n  by = c(\"gvkey\", \"year\")\n  )\ndata_investment <- data_investment |>\n  left_join(crsp |>\n    select(gvkey, month, mktcap),\n  by = c(\"gvkey\", \"month\")\n  ) |>\n  mutate(tobins_q = (mktcap + at - be + txdb) / at)\n\ndata_investment <- data_investment |>\n  select(gvkey, year, investment_lead, cash_flows, tobins_q) |>\n  drop_na()\nwinsorize <- function(x, cut) {\n  x <- replace(\n    x,\n    x > quantile(x, 1 - cut, na.rm = T),\n    quantile(x, 1 - cut, na.rm = T)\n  )\n  x <- replace(\n    x,\n    x < quantile(x, cut, na.rm = T),\n    quantile(x, cut, na.rm = T)\n  )\n  return(x)\n}\n\ndata_investment <- data_investment |>\n  mutate(across(\n    c(investment_lead, cash_flows, tobins_q),\n    ~ winsorize(., 0.01)\n  ))\ndata_investment |>\n  pivot_longer(\n    cols = c(investment_lead, cash_flows, tobins_q),\n    names_to = \"measure\"\n  ) |>\n  group_by(measure) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )# A tibble: 3 × 9\n  measure      mean     sd    min      q05    q50   q95    max      n\n  <chr>       <dbl>  <dbl>  <dbl>    <dbl>  <dbl> <dbl>  <dbl>  <int>\n1 cash_flows 0.0145 0.266  -1.50  -4.57e-1 0.0649 0.273  0.480 124178\n2 investmen… 0.0584 0.0778  0      7.27e-4 0.0333 0.208  0.467 124178\n3 tobins_q   1.99   1.69    0.571  7.92e-1 1.38   5.33  10.9   124178"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"fixed-effects","chapter":"12 Fixed effects and clustered standard errors","heading":"12.2 Fixed effects","text":"illustrate fixed effects regressions, use fixest package, computationally powerful flexible respect model specifications. start basic investment regression using simple model\n\\[ \\text{Investment}_{,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{,t}+\\beta_2\\text{Tobin's q}_{,t}+\\varepsilon_{,t},\\]\n\\(\\varepsilon_t\\) ..d. normally distributed across time firms. use feols()-function estimate simple model output structure regressions - also use lm().expected, regression output shows significant coefficients variables. Higher cash flows investment opportunities associated higher investment. However, simple model actually may lot omitted variables, coefficients likely biased. lot unexplained variation simple model (indicated rather low adjusted R-squared), bias coefficients potentially severe, true values zero. Note clear cutoffs decide R-squared high low, depends context application comparison different models data.One way tackle issue omitted variable bias get rid much unexplained variation possible including fixed effects - .e., model parameters fixed specific groups (e.g., Wooldridge 2010). essence, group mean fixed effects regressions. simplest group can form investment regression firm level. firm fixed effects regression \n\\[ \\text{Investment}_{,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{,t}+\\beta_2\\text{Tobin's q}_{,t}+\\varepsilon_{,t},\\]\n\\(\\alpha_i\\) firm fixed effect captures firm-specific mean investment across years. fact, also compute firms’ investments deviations firms’ average investments estimate model without fixed effects. idea firm fixed effect remove firm’s average investment, might affected firm-specific variables observe. example, firms specific industry might invest average. observe young firm large investments small concurrent cash flows, happen years. sort variation unwanted related unobserved variables can bias estimates direction.include firm fixed effect, use gvkey (Compustat’s firm identifier) follows:regression output shows lot unexplained variation firm level taken care including firm fixed effect adjusted R-squared rises 50%. fact, interesting look within R-squared shows explanatory power firm’s cash flow Tobin’s q top average investment firm. can also see coefficients changed slightly magnitude sign.another source variation can get rid setting: average investment across firms might vary time due macroeconomic factors affect firms, economic crises. including year fixed effects, can take effect unobservables vary time. two-way fixed effects regression \n\\[ \\text{Investment}_{,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{,t}+\\beta_2\\text{Tobin's q}_{,t}+\\varepsilon_{,t},\\]\n\\(\\alpha_t\\) time fixed effect. can think higher investments economic expansion simultaneously high cash flows.inclusion time fixed effects marginally affect R-squared coefficients, can interpret good thing indicates coefficients driven omitted variable varies time.can improve robustness regression results? Ideally, want get rid unexplained variation firm-year level, means need include variables vary across firm time likely correlated investment. Note include firm-year fixed effects setting cash flows Tobin’s q colinear fixed effects, estimation becomes void.discuss properties estimation errors, want point regression tables heart every empirical analysis compare multiple models. Fortunately, etable() provides convenient way tabulate regression output (many parameters customize even print output LaTeX). recommend printing \\(t\\)-statistics rather standard errors regression tables latter typically hard interpret across coefficients vary size. also print p-values sometimes misinterpreted signal importance observed effects (Wasserstein Lazar 2016). \\(t\\)-statistics provide consistent way interpret changes estimation uncertainty across different model specifications.","code":"\nmodel_ols <- feols(\n  fml = investment_lead ~ cash_flows + tobins_q,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_olsOLS estimation, Dep. Var.: investment_lead\nObservations: 124,178 \nStandard-errors: IID \n            Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)  0.04243   0.000342   124.1 < 2.2e-16 ***\ncash_flows   0.05143   0.000835    61.6 < 2.2e-16 ***\ntobins_q     0.00767   0.000132    58.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.076041   Adj. R2: 0.044434\nmodel_fe_firm <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmOLS estimation, Dep. Var.: investment_lead\nObservations: 124,178 \nFixed-effects: gvkey: 13,899\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(>|t|)    \ncash_flows   0.0146   0.000963    15.1 < 2.2e-16 ***\ntobins_q     0.0113   0.000136    82.6 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.05008     Adj. R2: 0.533303\n                Within R2: 0.059427\nmodel_fe_firmyear <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmyearOLS estimation, Dep. Var.: investment_lead\nObservations: 124,178 \nFixed-effects: gvkey: 13,899,  year: 34\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(>|t|)    \ncash_flows   0.0182   0.000941    19.3 < 2.2e-16 ***\ntobins_q     0.0102   0.000135    75.5 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.048805     Adj. R2: 0.556619\n                 Within R2: 0.051548\netable(model_ols, model_fe_firm, model_fe_firmyear,\n  coefstat = \"tstat\"\n)                        model_ols     model_fe_firm\nDependent Var.:   investment_lead   investment_lead\n                                                   \n(Intercept)     0.0424*** (124.1)                  \ncash_flows      0.0514*** (61.56) 0.0146*** (15.14)\ntobins_q        0.0077*** (58.17) 0.0113*** (82.60)\nFixed-Effects:  ----------------- -----------------\ngvkey                          No               Yes\nyear                           No                No\n_______________ _________________ _________________\nVCOV type                     IID               IID\nObservations              124,178           124,178\nR2                        0.04445           0.58554\nWithin R2                      --           0.05943\n\n                model_fe_firmyear\nDependent Var.:   investment_lead\n                                 \n(Intercept)                      \ncash_flows      0.0182*** (19.30)\ntobins_q        0.0102*** (75.51)\nFixed-Effects:  -----------------\ngvkey                         Yes\nyear                          Yes\n_______________ _________________\nVCOV type                     IID\nObservations              124,178\nR2                        0.60637\nWithin R2                 0.05155\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"clustering-standard-errors","chapter":"12 Fixed effects and clustered standard errors","heading":"12.3 Clustering standard errors","text":"Apart biased estimators, usually deal potentially complex dependencies residuals . dependencies residuals invalidate ..d. assumption OLS lead biased standard errors. biased OLS standard errors, reliably interpret statistical significance estimated coefficients.setting, residuals may correlated across years given firm (time-series dependence), , alternatively, residuals may correlated across different firms (cross-section dependence). One common approaches dealing dependence use clustered standard errors (Petersen 2008). idea behind clustering correlation residuals within cluster can form. number clusters grows, cluster-robust standard errors become consistent (Donald Lang 2007; Wooldridge 2010). natural requirement clustering standard errors practice hence sufficiently large number clusters. Typically, around least 30 50 clusters seen sufficient (Cameron, Gelbach, Miller 2011).Instead relying iid assumption, can use cluster option feols-function . code chunk applies one-way clustering firm well two-way clustering firm year.\ntable shows comparison different assumptions behind standard errors. first column, can see highly significant coefficients cash flows Tobin’s q. clustering standard errors firm level, \\(t\\)-statistics coefficients drop half, indicating high correlation residuals within firms. additionally cluster year, see drop, particularly Tobin’s q, . Even relaxing assumptions behind standard errors, coefficients still comfortably significant \\(t\\) statistics well usual critical values 1.96 2.576 two-tailed significance tests.Inspired Abadie et al. (2017), want close chapter highlighting choosing right dimensions clustering design problem. Even data informative whether clustering matters standard errors, tell whether adjust standard errors clustering. Clustering aggregate levels can hence lead unnecessarily inflated standard errors.","code":"\nmodel_cluster_firm <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = \"gvkey\",\n  data = data_investment\n)\n\nmodel_cluster_firmyear <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = c(\"gvkey\", \"year\"),\n  data = data_investment\n)\netable(model_fe_firmyear, model_cluster_firm, model_cluster_firmyear,\n  coefstat = \"tstat\"\n)                model_fe_firmyear model_cluster_f..\nDependent Var.:   investment_lead   investment_lead\n                                                   \ncash_flows      0.0182*** (19.30) 0.0182*** (11.18)\ntobins_q        0.0102*** (75.51) 0.0102*** (35.63)\nFixed-Effects:  ----------------- -----------------\ngvkey                         Yes               Yes\nyear                          Yes               Yes\n_______________ _________________ _________________\nVCOV type                     IID         by: gvkey\nObservations              124,178           124,178\nR2                        0.60637           0.60637\nWithin R2                 0.05155           0.05155\n\n                model_cluster_f..\nDependent Var.:   investment_lead\n                                 \ncash_flows      0.0182*** (9.459)\ntobins_q        0.0102*** (16.36)\nFixed-Effects:  -----------------\ngvkey                         Yes\nyear                          Yes\n_______________ _________________\nVCOV type        by: gvkey & year\nObservations              124,178\nR2                        0.60637\nWithin R2                 0.05155\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"exercises-11","chapter":"12 Fixed effects and clustered standard errors","heading":"12.4 Exercises","text":"Estimate two-way fixed effects model two-way clustered standard errors using quarterly Compustat data WRDS. Note can access quarterly data via tbl(wrds, in_schema(\"comp\", \"fundq\")).Following Peters Taylor (2017), compute Tobin’s q market value outstanding equity mktcap plus book value debt (dltt + dlc) minus current assets atc everything divided book value property, plant equipment ppegt. correlation measures Tobin’s q? impact two-way fixed effects regressions?","code":""},{"path":"difference-in-differences.html","id":"difference-in-differences","chapter":"13 Difference in differences","heading":"13 Difference in differences","text":"chapter, illustrate concept difference differences (DD) estimators evaluating effects climate change regulation pricing bonds across firms. DD estimators typically used recover treatment effects natural quasi-natural experiments trigger sharp changes environment specific group. Instead looking differences just one group (e.g., effect treated group), DD investigates treatment effects looking difference differences two groups. experiments usually exploited address endogeneity concerns (e.g., Roberts Whited 2013). identifying assumption outcome variable change equally groups without treatment. assumption also often referred assumption parallel trends. Moreover, ideally also want random assignment treatment control groups. Due lobbying activities, randomness often violated (financial) economics.context setting, investigate impact Paris Agreement, signed December 12, 2015, bond yields polluting firms. first estimate treatment effect agreement using panel regression techniques discuss Chapter 12. present two methods illustrate treatment effect time graphically. Although demonstrate treatment effect agreement anticipated bond market participants well advance, techniques present can also applied many settings.approach use replicates results Seltzer, Starks, Zhu (2022) partly. Specifically, borrow industry definitions grouping firms green brown types. Overall, literature ESG effects corporate bond markets already large continues grow (recent examples, see, e.g., Halling, Yu, Zechner (2021), Handler, Jankowitsch, Pasler (2022), Huynh Xia (2021), among many others).current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\nlibrary(fixest)\nlibrary(broom)"},{"path":"difference-in-differences.html","id":"data-preparation-6","chapter":"13 Difference in differences","heading":"13.1 Data preparation","text":"use TRACE Mergent FISD data sources SQLite-database introduced Chapters 2-4. start analysis preparing sample bonds. consider bonds time maturity one year signing PA. restriction also excludes bonds issued agreement. also consider first two digits SIC industry code, use identify polluting industries (line Seltzer, Starks, Zhu 2022).Next, aggregate individual transactions reported TRACE monthly panel bond yields. consider bond yields bond’s last trading day month. Therefore, first aggregate bond data daily frequency apply common restrictions literature (see, e.g., Bessembinder et al. 2008). weigh transaction volume reflect trade’s relative importance avoid emphasizing small trades. Moreover, consider transactions reported prices rptd_pr larger 25 (exclude bonds close default) bond-day observations five trades corresponding day (exclude prices based , potentially non-representative transactions). combining bond-specific information Mergent FISD bond sample aggregated TRACE data, arrive main sample analysis.can run first regression, need define treated indicator, product post_period (.e., months signing PA) polluter indicator defined .usual, tabulate summary statistics variables enter regression check validity variable definitions.","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nmergent <- tbl(tidy_finance, \"mergent\") |>\n  collect()\n\ntrace_enhanced <- tbl(tidy_finance, \"trace_enhanced\") |>\n  collect()\ntreatment_date <- ymd(\"2015-12-12\")\n\nbonds <- mergent |>\n  mutate(\n    time_to_maturity = as.numeric(maturity - treatment_date),\n    time_to_maturity = time_to_maturity / 365,\n    sic_code = as.integer(substr(sic_code, 1, 2)),\n    log_offering_amt = log(offering_amt)\n  ) |>\n  filter(time_to_maturity >= 1) |>\n  select(\n    cusip_id = complete_cusip,\n    time_to_maturity, log_offering_amt, sic_code\n  )\n\npolluting_industries <- c(\n  49, 13, 45, 29, 28, 33, 40, 20,\n  26, 42, 10, 53, 32, 99, 37\n)\n\nbonds <- bonds |>\n  mutate(polluter = sic_code %in% polluting_industries)\ntrace_aggregated <- trace_enhanced |>\n  filter(rptd_pr > 25) |>\n  group_by(cusip_id, trd_exctn_dt) |>\n  summarize(\n    avg_yield = weighted.mean(yld_pt, entrd_vol_qt * rptd_pr),\n    trades = n(),\n    .groups = \"drop\"\n  ) |>\n  drop_na(avg_yield) |>\n  filter(trades >= 5) |>\n  mutate(month = floor_date(trd_exctn_dt, \"months\")) |>\n  group_by(cusip_id, month) |>\n  slice_max(trd_exctn_dt) |>\n  ungroup() |>\n  select(cusip_id, month, avg_yield)\nbonds_panel <- bonds |>\n  inner_join(trace_aggregated, by = \"cusip_id\") |>\n  drop_na()\nbonds_panel <- bonds_panel |>\n  mutate(post_period = month >= floor_date(treatment_date, \"months\"))\n\nbonds_panel <- bonds_panel |>\n  mutate(treated = polluter & post_period)\nbonds_panel |>\n  pivot_longer(\n    cols = c(avg_yield, time_to_maturity, log_offering_amt),\n    names_to = \"measure\"\n  ) |>\n  group_by(measure) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )# A tibble: 3 × 9\n  measure           mean    sd    min   q05   q50   q95   max      n\n  <chr>            <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>  <int>\n1 avg_yield         4.08 4.21  0.0595  1.27  3.37  8.07 128.  127428\n2 log_offering_amt 13.3  0.823 4.64   12.2  13.2  14.5   16.5 127428\n3 time_to_maturity  8.54 8.41  1.01    1.50  5.81 27.4  101.  127428"},{"path":"difference-in-differences.html","id":"panel-regressions","chapter":"13 Difference in differences","heading":"13.2 Panel regressions","text":"Paris Agreement legally binding international treaty climate change. adopted 196 Parties COP 21 Paris 12 December 2015 entered force 4 November 2016. Paris Agreement obliges developed countries support efforts build clean, climate-resilient futures. One may thus hypothesize adopting climate-related policies may affect financial markets. measure magnitude effect, first run OLS regression without fixed effects include treated, post_period, polluter dummies, well bond-specific characteristics log_offering_amt time_to_maturity. simple model assumes essentially two periods (PA) two groups (polluters non-polluters). Nonetheless, indicate whether polluters higher yields following Paris Agreement compared non-polluters.second model follows typical DD regression approach including individual (cusip_id) time (month) fixed effects. model, include variables simple model fixed effects subsume , observe coefficient main variable interest: treated.models indicate polluters significantly higher yields Paris Agreement non-polluting firms. Note magnitude treated coefficient varies considerably across models.","code":"\nmodel_without_fe <- feols(\n  fml = avg_yield ~ treated + post_period + polluter +\n    log_offering_amt + time_to_maturity,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\nmodel_with_fe <- feols(\n  fml = avg_yield ~ treated | cusip_id + month,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\netable(model_without_fe, model_with_fe, coefstat = \"tstat\")                    model_without_fe     model_with_fe\nDependent Var.:            avg_yield         avg_yield\n                                                      \n(Intercept)         10.66*** (56.60)                  \ntreatedTRUE        0.4610*** (9.284) 0.9807*** (29.43)\npost_periodTRUE  -0.1747*** (-5.940)                  \npolluterTRUE       0.4745*** (15.05)                  \nlog_offering_amt -0.5451*** (-38.55)                  \ntime_to_maturity   0.0573*** (41.29)                  \nFixed-Effects:   ------------------- -----------------\ncusip_id                          No               Yes\nmonth                             No               Yes\n________________ ___________________ _________________\nVCOV type                        IID               IID\nObservations                 127,428           127,428\nR2                           0.03151           0.64689\nWithin R2                         --           0.00713\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"difference-in-differences.html","id":"visualizing-parallel-trends","chapter":"13 Difference in differences","heading":"13.3 Visualizing parallel trends","text":"Even though regressions indicate impact Paris Agreement bond yields polluters, tables tell us anything dynamics treatment effect. particular, models provide indication whether crucial parallel trends assumption valid. assumption requires absence treatment, difference two groups constant time. Although well-defined statistical test assumption, visual inspection typically provides good indication.provide visual evidence, revisit simple OLS model replace treated post_period indicators month dummies group. approach estimates average yield change groups period provides corresponding confidence intervals. Plotting coefficient estimates groups around treatment date shows us dynamics panel data.\nFigure 13.1: figure shows coefficient estimates 95 percent confidence intervals OLS regressions estimating treatment effect Paris Climate Agreement bond yields (percent) polluters non-polluters. horizontal line represents benchmark yield polluters Paris Agreement. vertical line indicates date agreement (December 12, 2015).\ncan see throughout 2014, yields two groups changed unison. However, starting end 2014, yields start diverge, reaching highest difference around signing PA. Afterward, yields groups fall , polluters arrive level beginning 2014. non-polluters, hand, even experience significantly lower yields polluters signing agreement.Instead plotting groups using simple model approach, can also use fixed-effects model focus polluter’s yield response signing relative non-polluters. perform estimation, need replace treated indicator separate time dummies polluters, marking one-month period relative treatment date. regress monthly yields set time dummies cusip_id month fixed effects.\nFigure 13.2: figure shows coefficient estimates 95 percent confidence intervals OLS regressions estimating treatment effect Paris Climate Agreement bond yields (percent) polluters. horizontal line represents benchmark yield polluters Paris Agreement. vertical line indicates date agreement (December 12, 2015).\n\nresulting figure confirms main conclusion previous image: polluters’ yield patterns show considerable anticipation effect starting towards end 2014. Yields marginally increase signing agreement. However, opposed simple model, see complete reversal back pre-agreement level. Yields polluters stay significantly higher level even one year signing.Notice year Paris Agreement signed, 45th President United States elected November 8, 2016. campaign indications intentions withdraw US PA, ultimately happened November 4, 2020. Hence, reversal effects potentially driven actions.","code":"\nmodel_without_fe_time <- feols(\n  fml = avg_yield ~ polluter + month:polluter +\n    time_to_maturity + log_offering_amt,\n  vcov = \"iid\",\n  data = bonds_panel |>\n    mutate(month = factor(month))\n)\n\nmodel_without_fe_coefs <- tidy(model_without_fe_time) |>\n  filter(str_detect(term, \"month\")) |>\n  mutate(\n    month = ymd(substr(term, nchar(term) - 9, nchar(term))),\n    treatment = str_detect(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  )\n\nmodel_without_fe_coefs |>\n  ggplot(aes(month, color = treatment)) +\n  geom_vline(aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    color = \"Polluter?\",\n    title = \"Polluters respond stronger to Paris Agreement than green firms\"\n  )\nbonds_panel_alt <- bonds_panel |>\n  mutate(\n    diff_to_treatment = interval(\n      floor_date(treatment_date, \"month\"), month\n    ) %/% months(1)\n  )\n\nvariables <- bonds_panel_alt |>\n  distinct(diff_to_treatment, month) |>\n  arrange(month) |>\n  mutate(variable_name = as.character(NA))\n\nformula <- \"avg_yield ~ \"\n\nfor (j in 1:nrow(variables)) {\n  if (variables$diff_to_treatment[j] != 0) {\n    old_names <- names(bonds_panel_alt)\n    bonds_panel_alt <- bonds_panel_alt |>\n      mutate(new_var = diff_to_treatment == variables$diff_to_treatment[j] & \n               polluter)\n    new_var_name <- ifelse(variables$diff_to_treatment[j] < 0,\n      str_c(\"lag\", abs(variables$diff_to_treatment[j])),\n      str_c(\"lead\", variables$diff_to_treatment[j])\n    )\n    variables$variable_name[j] <- new_var_name\n    names(bonds_panel_alt) <- c(old_names, new_var_name)\n    formula <- str_c(\n      formula,\n      ifelse(j == 1,\n        new_var_name,\n        str_c(\"+\", new_var_name)\n      )\n    )\n  }\n}\nformula <- str_c(formula, \"| cusip_id + month\")\n\nmodel_with_fe_time <- feols(\n  fml = as.formula(formula),\n  vcov = \"iid\",\n  data = bonds_panel_alt\n)\n\nmodel_with_fe_time_coefs <- tidy(model_with_fe_time) |>\n  mutate(\n    term = str_remove(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  ) |>\n  left_join(\n    variables,\n    by = c(\"term\" = \"variable_name\")\n  ) |>\n  bind_rows(tibble(\n    term = \"lag0\",\n    estimate = 0,\n    ci_up = 0,\n    ci_low = 0,\n    month = floor_date(treatment_date, \"month\")\n  ))\n\nmodel_with_fe_time_coefs |>\n  ggplot(aes(x = month, y = estimate)) +\n  geom_vline(aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    title = \"Polluters' yield patterns around Paris Agreement signing\"\n  )"},{"path":"difference-in-differences.html","id":"exercises-12","chapter":"13 Difference in differences","heading":"13.4 Exercises","text":"46th president US rejoined Paris Agreement February 2021. Repeat difference differences analysis day election victory. Note also download new TRACE data. polluters’ yields react action?Based exercise ratings Chapter 4, include ratings control variable analysis . results change?","code":""},{"path":"factor-selection-via-machine-learning.html","id":"factor-selection-via-machine-learning","chapter":"14 Factor selection via machine learning","heading":"14 Factor selection via machine learning","text":"aim chapter twofold. data science perspective, introduce tidymodels, collection packages modeling machine learning (ML) using tidyverse principles. tidymodels comes handy workflow sorts typical prediction tasks. finance perspective, address notion factor zoo (Cochrane 2011) using ML methods. introduce Lasso Ridge regression special case penalized regression models. , explain concept cross-validation model tuning Elastic Net regularization popular example. implement showcase entire cycle model specification, training, forecast evaluation within tidymodels universe. tools can generally applied abundance interesting asset pricing problems, apply penalized regressions identifying macroeconomic variables asset pricing factors help explain cross-section industry portfolios.previous chapters, illustrate stock characteristics size provide valuable pricing information addition market beta.\nfindings question usefulness Capital Asset Pricing Model.\nfact, last decades, financial economists discovered plethora additional factors may correlated marginal utility consumption (thus deserve prominent role pricing applications). search factors explain cross section expected stock returns produced hundreds potential candidates, noted recently Harvey, Liu, Zhu (2016), Mclean Pontiff (2016), Hou, Xue, Zhang (2020).\nTherefore, given multitude proposed risk factors, challenge days rather : believe relevance 300+ risk factors? recent years, promising methods field ML got applied common finance applications. refer Mullainathan Spiess (2017) treatment ML perspective econometrician, Nagel (2021) excellent review ML practices asset pricing, Easley et al. (2020) ML applications (high-frequency) market microstructure, Dixon, Halperin, Bilokon (2020) detailed treatment methodological aspects.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"brief-theoretical-background","chapter":"14 Factor selection via machine learning","heading":"14.1 Brief theoretical background","text":"book empirical work tidy manner, refer many excellent textbook treatments ML methods especially penalized regressions deeper discussion. Excellent material provided, instance, Hastie, Tibshirani, Friedman (2009), Gareth et al. (2013), De Prado (2018). Instead, briefly summarize idea Lasso Ridge regressions well general Elastic Net. , turn fascinating question implement, tune, use models tidymodels workflow.set stage, start definition linear model: suppose data \\((y_t, x_t), t = 1,\\ldots, T\\) \\(x_t\\) \\((K \\times 1)\\) vector regressors \\(y_t\\) response observation \\(t\\).\nlinear model takes form \\(y_t = \\beta' x_t + \\varepsilon_t\\) error term \\(\\varepsilon_t\\) studied abundance. well-known ordinary-least square (OLS) estimator \\((K \\times 1)\\) vector \\(\\beta\\) minimizes sum squared residuals \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\]\noften interested estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML predictive performance time. new observation \\(\\tilde{x}_t\\), linear model generates predictions \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\]\nbest can ?\nreally: instead minimizing sum squared residuals, penalized linear models can improve predictive performance choosing estimators \\(\\hat{\\beta}\\) lower variance estimator \\(\\hat\\beta^\\text{ols}\\).\ntime, seems appealing restrict set regressors meaningful ones possible. words, \\(K\\) large (number proposed factors asset pricing literature), may desirable feature select reasonable factors set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) redundant factors.clear promised benefits penalized regressions, .e., reducing mean squared error (MSE), come cost. cases, reducing variance estimator introduces bias \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). effect bias-variance trade-? understand implications, assume following data-generating process \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] want recover \\(f(x)\\), denotes unknown functional maps relationship \\(x\\) \\(y\\). properties \\(\\hat\\beta^\\text{ols}\\) unbiased estimator may desirable circumstances, certainly consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) desirable: instance, MSE depends model choice follows: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(x))^2)=E((f(x)+\\epsilon-\\hat{f}(x))^2)\\\\\n&= \\underbrace{E((f(x)-\\hat{f}(x))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance model}}+ \\underbrace{E\\left((f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] model can reduce \\(\\sigma_\\varepsilon^2\\), biased estimator small variance may lower MSE unbiased estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"ridge-regression","chapter":"14 Factor selection via machine learning","heading":"14.1.1 Ridge regression","text":"One biased estimator known Ridge regression. Hoerl Kennard (1970) propose minimize sum squared errors simultaneously imposing penalty \\(L_2\\) norm parameters \\(\\hat\\beta\\). Formally, means penalty factor \\(\\lambda\\geq 0\\) minimization problem takes form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). \\(c\\geq 0\\) constant depends choice \\(\\lambda\\). larger \\(\\lambda\\), smaller \\(c\\) (technically speaking, one--one relationship \\(\\lambda\\), corresponds Lagrangian minimization problem \\(c\\)). , \\(X = \\left(x_1 \\ldots x_T\\right)'\\) \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). closed-form solution resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda \\right)^{-1}X'y.\\] couple observations worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) \\(\\lambda = 0\\) \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) \\(\\lambda\\rightarrow \\infty\\). Also \\(\\lambda > 0\\), \\(\\left(X'X + \\lambda \\right)\\) non-singular even \\(X'X\\) means \\(\\hat\\beta^\\text{ridge}\\) exists even \\(\\hat\\beta\\) defined. However, note also Ridge estimator requires careful choice hyperparameter \\(\\lambda\\) controls amount regularization: larger value \\(\\lambda\\) implies shrinkage regression coefficient towards 0, smaller value \\(\\lambda\\) reduces bias resulting estimator.Note, \\(X\\) usually contains intercept column ones. general rule, associated intercept coefficient penalized. practice, often implies \\(y\\) simply demeaned computing \\(\\hat\\beta^\\text{ridge}\\).statistical properties Ridge estimator? First, bad news \\(\\hat\\beta^\\text{ridge}\\) biased estimator \\(\\beta\\). However, good news (homoscedastic error terms) variance Ridge estimator guaranteed smaller variance ordinary least square estimator. encourage verify two statements exercises. result, face trade-: Ridge regression sacrifices unbiasedness achieve smaller variance OLS estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"lasso","chapter":"14 Factor selection via machine learning","heading":"14.1.2 Lasso","text":"alternative Ridge regression Lasso (least absolute shrinkage selection operator). Similar Ridge regression, Lasso (Tibshirani 1996) penalized biased estimator.\nmain difference Ridge regression Lasso shrink coefficients effectively selects variables setting coefficients irrelevant variables zero. Lasso implements \\(L_1\\) penalization parameters : \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| < c(\\lambda).\\] closed form solution \\(\\hat\\beta^\\text{Lasso}\\) maximization problem efficient algorithms exist (e.g., R package glmnet). Like Ridge regression, hyperparameter \\(\\lambda\\) specified beforehand.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"elastic-net","chapter":"14 Factor selection via machine learning","heading":"14.1.3 Elastic Net","text":"Elastic Net (Zou Hastie 2005) combines \\(L_1\\) \\(L_2\\) penalization encourages grouping effect strongly correlated predictors tend model together. general framework considers following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, chose two hyperparameters: shrinkage factor \\(\\lambda\\) weighting parameter \\(\\rho\\). Elastic Net resembles Lasso \\(\\rho = 1\\) Ridge regression \\(\\rho = 0\\). R package glmnet provides efficient algorithms compute coefficients penalized regressions, good exercise implement Ridge Lasso estimation use glmnet package tidymodels back-end.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"data-preparation-7","chapter":"14 Factor selection via machine learning","heading":"14.2 Data preparation","text":"get started, load required packages data. main focus workflow behind tidymodels package collection (Kuhn Wickham 2020).\nKuhn Silge (2018) provide thorough introduction tidymodels components. glmnet (Simon et al. 2011) developed released sync Tibshirani (1996) provides R implementation Elastic Net estimation. package timetk (Dancho Vaughan 2022b) provides useful tools time series data wrangling.analysis, use four different data sources load SQLite-database introduced Chapters 2-4. start two different sets factor portfolio returns suggested representing practical risk factor exposure thus relevant comes asset pricing applications.standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, high-minus-low book--market valuation sorts) defined Fama French (1992) Fama French (1993)Monthly q-factor returns Hou, Xue, Zhang (2014). factors contain size factor, investment factor, return--equity factor, expected growth factorNext, include macroeconomic predictors may predict general stock market economy. Macroeconomic variables effectively serve conditioning information inclusion hints relevance conditional models instead unconditional asset pricing. refer interested reader Cochrane (2009) role conditioning information.set macroeconomic predictors comes Welch Goyal (2008). data updated authors 2021 contains monthly variables suggested good predictors equity premium. variables dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, inflationFinally, need set test assets. aim understand plenty factors macroeconomic variable combinations prove helpful explaining test assets’ cross-section returns.\nline many existing papers, use monthly portfolio returns 10 different industries according definition Kenneth French’s homepage test assets.combine monthly observations one data frame.data contains 22 columns regressors 13 macro variables 8 factor returns month.\nfigure provides summary statistics 10 monthly industry excess returns percent.\nFigure 14.1: box plots show monthly dispersion returns 10 different industries\n","code":"\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(furrr)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(timetk)\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect() |>\n  rename_with(~ str_c(\"factor_ff_\", .), -month)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\") |>\n  collect() |>\n  rename_with(~ str_c(\"factor_q_\", .), -month)\n\nmacro_predictors <- tbl(tidy_finance, \"macro_predictors\") |>\n  collect() |>\n  rename_with(~ str_c(\"macro_\", .), -month) |>\n  select(-macro_rp_div)\n\nindustries_ff_monthly <- tbl(tidy_finance, \"industries_ff_monthly\") |>\n  collect() |>\n  pivot_longer(-month,\n    names_to = \"industry\", values_to = \"ret\"\n  ) |>\n  mutate(industry = as_factor(industry))\ndata <- industries_ff_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  left_join(factors_q_monthly, by = \"month\") |>\n  left_join(macro_predictors, by = \"month\") |>\n  mutate(\n    ret = ret - factor_ff_rf\n  ) |>\n  select(month, industry, ret, everything()) |>\n  drop_na()\ndata |>\n  group_by(industry) |>\n  mutate(ret = ret) |>\n  ggplot(aes(x = industry, y = ret)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Excess return distributions by industry in percent\"\n  ) +\n  scale_y_continuous(\n    labels = percent\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"the-tidymodels-workflow","chapter":"14 Factor selection via machine learning","heading":"14.3 The tidymodels workflow","text":"illustrate penalized linear regressions, employ tidymodels collection packages modeling ML using tidyverse principles. can simply use install.packages(\"tidymodels\") get access related packages. recommend checking work Kuhn Silge (2018): continuously write great book ‘Tidy Modeling R’ using tidy principles.tidymodels workflow encompasses main stages modeling process: pre-processing data, model fitting, post-processing results. demonstrate , tidymodels provides efficient workflows can update low effort.Using ideas Ridge Lasso regressions, following example guides () pre-processing data (data split variable mutation), (ii) building models, (iii) fitting models, (iv) tuning models create “best” possible predictions.start, restrict analysis just one industry: Manufacturing. first split sample training test set.\npurpose, tidymodels provides function initial_time_split() rsample package (Silge et al. 2022).\nsplit takes last 20% data test set, used model tuning.\nuse test set evaluate predictive accuracy --sample scenario.object split simply keeps track observations training test set.\ncan call training set training(split), can extract test set testing(split).","code":"\nsplit <- initial_time_split(\n  data |>\n    filter(industry == \"Manuf\") |>\n    select(-industry),\n  prop = 4 / 5\n)\nsplit<Training/Testing/Total>\n<527/132/659>"},{"path":"factor-selection-via-machine-learning.html","id":"pre-process-data","chapter":"14 Factor selection via machine learning","heading":"14.3.1 Pre-process data","text":"Recipes help pre-process data training model. Recipes series pre-processing steps variable selection, transformation, conversion qualitative predictors indicator variables. recipe starts formula defines general structure dataset role variable (regressor dependent variable). dataset, recipe contains following steps fit model:formula defines want explain excess returns available predictors. regression equation thus takes form\n\\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t \\] \\(r_t\\) vector industry excess returns time \\(t\\) \\(\\tilde f_t\\) \\(\\tilde z_t\\) (standardized) vectors factor portfolio returns macroeconomic variablesWe exclude column month analysisWe include interaction terms factors macroeconomic predictorsWe demean scale regressor standard deviation oneA table available recipe steps can found tidymodels documentation. 2022, 150 different processing steps available! One important point: definition recipe trigger calculations yet rather provides description tasks applied. result, easy reuse recipes different models thus make sure outcomes comparable based input.\nexample , make difference whether use input data = training(split) data = testing(split).\nmatters early stage column names types.can apply recipe data suitable structure. code combines two different functions: prep() estimates required parameters training set can applied data sets later. bake() applies processed computations new data.object data_prep contains information related different preprocessing steps applied training data: E.g., necessary compute sample means standard deviations center scale variables.Note resulting data contains 132 observations test set 126 columns. many? Recall recipe states compute every possible interaction term factors predictors, increases dimension data matrix substantially.may ask stage: use recipe instead simply using data wrangling commands mutate() select()? tidymodels beauty lot happening hood. Recall, simple scaling step, actually compute standard deviation column, store value, apply identical transformation different dataset, e.g., testing(split). prepped recipe stores values hands bake() novel dataset. Easy pie tidymodels, isn’t ?","code":"\nrec <- recipe(ret ~ ., data = training(split)) |>\n  step_rm(month) |>\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) |>\n  step_normalize(all_predictors()) |>\n  step_center(ret, skip = TRUE)\ndata_prep <- prep(rec, training(split))\ndata_bake <- bake(data_prep,\n  new_data = testing(split)\n)\ndata_bake# A tibble: 132 × 126\n  factor_ff…¹ facto…² facto…³ facto…⁴ facto…⁵ facto…⁶ facto…⁷ facto…⁸\n        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1       -1.80 1.37      0.139   1.14   0.0289   1.33  -1.63    -1.66 \n2       -1.80 0.333    -0.833   0.121 -0.745    0.450 -0.0117  -1.19 \n3       -1.80 0.656     0.399   0.297  0.327    0.510 -0.860   -0.905\n4       -1.80 0.00426   0.717  -0.756  0.470   -0.296 -0.590   -0.324\n5       -1.84 0.529    -0.177  -0.972 -0.390   -0.733  0.383    0.102\n# … with 127 more rows, 118 more variables: macro_dp <dbl>,\n#   macro_dy <dbl>, macro_ep <dbl>, macro_de <dbl>,\n#   macro_svar <dbl>, macro_bm <dbl>, macro_ntis <dbl>,\n#   macro_tbl <dbl>, macro_lty <dbl>, macro_ltr <dbl>,\n#   macro_tms <dbl>, macro_dfy <dbl>, macro_infl <dbl>, ret <dbl>,\n#   factor_ff_rf_x_macro_dp <dbl>, factor_ff_rf_x_macro_dy <dbl>,\n#   factor_ff_rf_x_macro_ep <dbl>, factor_ff_rf_x_macro_de <dbl>, …"},{"path":"factor-selection-via-machine-learning.html","id":"build-a-model","chapter":"14 Factor selection via machine learning","heading":"14.3.2 Build a model","text":"\nNext, can build actual model based pre-processed data. line definition , estimate regression coefficients Lasso regression get\n\\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned}\\] want emphasize tidymodels workflow model similar, irrespective specific model. see , straightforward fit Ridge regression coefficients - later - Neural networks Random forests basically code. structure always follows: create -called workflow() use fit() function. table available model APIs available .\nnow, start linear regression model given value penalty factor \\(\\lambda\\). setup , mixture denotes value \\(\\rho\\), hence setting mixture = 1 implies Lasso.’s - done! object lm_model contains definition model required information. Note set_engine(\"glmnet\") indicates API character tidymodels workflow: hood, package glmnet heavy lifting, linear_reg() provides unified framework collect inputs. workflow ends combining everything necessary serious data science workflow, namely, recipe model.","code":"\nlm_model <- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) |>\n  set_engine(\"glmnet\", intercept = FALSE)\nlm_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(lm_model)\nlm_fit══ Workflow ═════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ─────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_interact()\n• step_normalize()\n• step_center()\n\n── Model ────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-04\n  mixture = 1\n\nEngine-Specific Arguments:\n  intercept = FALSE\n\nComputational engine: glmnet "},{"path":"factor-selection-via-machine-learning.html","id":"fit-a-model","chapter":"14 Factor selection via machine learning","heading":"14.3.3 Fit a model","text":"workflow , ready use fit(). Typically, use training data fit model.\ntraining data pre-processed according recipe steps, Lasso regression coefficients computed.\nFirst, focus predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) figure illustrates projections entire time series Manufacturing industry portfolio returns. grey area indicates --sample period, use fit model.\nFigure 14.2: grey area corresponds sample period.\nestimated coefficients look like? analyze values illustrate difference tidymodels workflow underlying glmnet package, worth computing coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. code estimates coefficients Lasso Ridge regression processed training data sample. Note glmnet actually takes vector y matrix regressors \\(X\\) input. Moreover, glmnet requires choosing penalty parameter \\(\\alpha\\), corresponds \\(\\rho\\) notation . using tidymodels model API, details need consideration.objects fit_lasso fit_ridge contain entire sequence estimated coefficients multiple values penalty factor \\(\\lambda\\). figure illustrates trajectories regression coefficients function penalty factor. Lasso Ridge coefficients converge zero penalty factor increases.\nFigure 14.3: penalty parameters choosen iteratively resemble path penalization model excludes variables.\nOne word caution: package glmnet computes estimates coefficients \\(\\hat\\beta\\) based numerical optimization procedures.\nresult, estimated coefficients special case regularization (\\(\\lambda = 0\\)) can deviate standard OLS estimates.","code":"\npredicted_values <- lm_fit |>\n  fit(data = training(split)) |>\n  predict(data |> filter(industry == \"Manuf\")) |>\n  bind_cols(data |> filter(industry == \"Manuf\")) |>\n  select(month,\n    \"Fitted value\" = .pred,\n    \"Realization\" = ret\n  ) |>\n  pivot_longer(-month, names_to = \"Variable\")\npredicted_values |>\n  ggplot(aes(x = month, y = value, color = Variable)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Monthly realized and fitted manufacturing industry risk premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"5 years\"\n      )\n    },\n    minor_breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"1 years\"\n      )\n    },\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  annotate(\"rect\",\n    xmin = testing(split) |> pull(month) |> min(),\n    xmax = testing(split) |> pull(month) |> max(),\n    ymin = -Inf, ymax = Inf,\n    alpha = 0.5, fill = \"grey70\"\n  )\nx <- data_bake |>\n  select(-ret) |>\n  as.matrix()\ny <- data_bake |> pull(ret)\n\nfit_lasso <- glmnet(\n  x = x,\n  y = y,\n  alpha = 1,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge <- glmnet(\n  x = x,\n  y = y,\n  alpha = 0,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\nbind_rows(\n  tidy(fit_lasso) |> mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) |> mutate(Model = \"Ridge\")\n) |>\n  rename(\"Variable\" = term) |>\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Penalty factor (lambda)\", y = NULL,\n    title = \"Estimated coefficient paths for different penalty factors\"\n  ) +\n  theme(legend.position = \"none\")"},{"path":"factor-selection-via-machine-learning.html","id":"tune-a-model","chapter":"14 Factor selection via machine learning","heading":"14.3.4 Tune a model","text":"compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , simply imposed value penalty hyperparameter \\(\\lambda\\). Model tuning process optimally selecting hyperparameters. tidymodels provides extensive tuning options based -called cross-validation. , refer treatment cross-validation get detailed discussion statistical underpinnings. focus general idea implementation tidymodels.goal choosing \\(\\lambda\\) (hyperparameter, e.g., \\(\\rho\\) Elastic Net) find way produce predictors \\(\\hat{Y}\\) outcome \\(Y\\) minimizes mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, MSPE directly observable. can compute estimate data random observe entire population.Obviously, train algorithm data use compute error, estimate \\(\\hat{\\text{MSPE}}\\) indicate way better predictive accuracy can expect real --sample data. result called overfitting.Cross-validation technique allows us alleviate problem. approximate true MSPE average many MSPE obtained creating predictions \\(K\\) new random samples data, none used train algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). practice, done carving piece data pretending independent sample. divide data training set test set. MSPE test set measure actual predictive ability, use training set fit models aim find optimal hyperparameter values. , divide training sample (several) subsets, fit model grid potential hyperparameter values (e.g., \\(\\lambda\\)), evaluate predictive accuracy independent sample. works follows:Specify grid hyperparametersObtain predictors \\(\\hat{y}_i(\\lambda)\\) denote predictors used parameters \\(\\lambda\\)Compute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] K-fold cross-validation, computation \\(K\\) times. Simply pick validation set \\(M=T/K\\) observations random think random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), \\(k=1\\)pick \\(K\\)? Large values \\(K\\) preferable training data better imitates original data. However, larger values \\(K\\) much higher computation time.\ntidymodels provides required tools conduct \\(K\\)-fold cross-validation. just update model specification let tidymodels know parameters tune. case, specify penalty factor \\(\\lambda\\) well mixing factor \\(\\rho\\) free parameters. Note simple change existing workflow update_model().sample, consider time-series cross-validation sample. means tune models 20 random samples length five years validation period four years. grid possible hyperparameters, fit model fold evaluate \\(\\hat{\\text{MSPE}}\\) corresponding validation set. Finally, select model specification lowest MSPE validation set. First, define cross-validation folds based training data ., evaluate performance grid different penalty values. tidymodels provides functionalities construct suitable grid hyperparameters grid_regular. code chunk creates \\(10 \\times 3\\) hyperparameters grid. , function tune_grid() evaluates models fold.tuning process, collect evaluation metrics (root mean-squared error example) identify optimal model. figure illustrates average validation set’s root mean-squared error value \\(\\lambda\\) \\(\\rho\\).\nFigure 14.4: Evaluation Manufactoring excess returns different penalty factors (lambda) proportions Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net 0.0 indicates Ridge.\nfigure shows cross-validated MSPE drops Lasso Elastic Net spikes afterward. Ridge regression, MSPE increases certain threshold. Recall larger regularization, restricted model becomes. Thus, choose model lowest MSPE.","code":"\nlm_model <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit |>\n  update_model(lm_model)\ndata_folds <- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\n\ndata_folds# Time Series Cross Validation Plan \n# A tibble: 20 × 2\n  splits          id     \n  <list>          <chr>  \n1 <split [60/48]> Slice01\n2 <split [60/48]> Slice02\n3 <split [60/48]> Slice03\n4 <split [60/48]> Slice04\n5 <split [60/48]> Slice05\n# … with 15 more rows\nlm_tune <- lm_fit |>\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),\n    metrics = metric_set(rmse)\n  )\nautoplot(lm_tune) +\n  labs(\n    x = \"Penalty factor (lambda)\",\n    y = \"Root MSPE\",\n    title = \"Root MSPE for different penalty factors\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"parallelized-workflow","chapter":"14 Factor selection via machine learning","heading":"14.3.5 Parallelized workflow","text":"starting point question: factors determine industry returns? Avramov et al. (2022) provide Bayesian analysis related research question , choose simplified approach: illustrate entire workflow, now run penalized regressions ten industries.\nwant identify relevant variables fitting Lasso models industry returns time series. specifically, perform cross-validation industry identify optimal penalty factor \\(\\lambda\\).\n, use set finalize_*()-functions take list tibble tuning parameter values update objects values. determining best model, compute final fit entire training set analyze estimated coefficients.First, define Lasso model one tuning parameter.following task can easily parallelized reduce computing time substantially. use parallelization capabilities furrr. Note can also just recycle steps collect function.just happened? principle, exactly instead computing Lasso coefficients one industry, ten parallel. final option seed = TRUE required make cross-validation process reproducible.\nNow, just housekeeping keep variables Lasso set zero. illustrate results heat map.\nFigure 14.5: Grey areas indicate estimated Lasso regression coefficient set zero. White fields show variables get assigned value exactly zero.\nheat map conveys two main insights.\nFirst, see lot white, means many factors, macroeconomic variables, interaction terms relevant explaining cross-section returns across industry portfolios. fact, market factor return--equity factor play role several industries. Second, seems quite heterogeneity across different industries. barely variable selected Lasso Utilities, many factors selected , e.g., High-Tech Durable, coincide .\nwords, seems clear picture need many factors, Lasso provide factor consistently provides pricing abilities across industries.","code":"\nlasso_model <- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit |>\n  update_model(lasso_model)\nselect_variables <- function(input) {\n  # Split into training and testing data\n  split <- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds <- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune <- lm_fit |>\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Identify the best model and fit with the training data\n  lasso_lowest_rmse <- lm_tune |> select_by_one_std_err(\"rmse\")\n  lasso_final <- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit <- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  estimated_coefficients <- lasso_final_fit |>\n    extract_fit_parsnip() |>\n    tidy() |>\n    mutate(\n      term = str_remove_all(term, \"factor_|macro_|industry_\")\n    )\n\n  return(estimated_coefficients)\n}\n\n# Parallelization\nplan(multisession, workers = availableCores())\n\n# Computation by industry\nselected_factors <- data |>\n  nest(data = -industry) |>\n  mutate(selected_variables = future_map(\n    data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\nselected_factors |>\n  unnest(selected_variables) |>\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) |>\n  add_count(term) |>\n  mutate(\n    term = str_remove_all(term, \"NA|ff_|q_\"),\n    term = str_replace_all(term, \"_x_\", \" \"),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) |>\n  filter(term != \"Other\") |>\n  mutate(term = fct_drop(term)) |>\n  complete(industry, term, fill = list(selected = 0)) |>\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"grey30\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"exercises-13","chapter":"14 Factor selection via machine learning","heading":"14.4 Exercises","text":"Write function requires three inputs, namely, y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), lambda returns Ridge estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function indicate whether \\(X\\) contains vector ones first column, exempt \\(L_2\\) penalty.Compute \\(L_2\\) norm (\\(\\beta'\\beta\\)) regression coefficients based predictive regression previous exercise range \\(\\lambda\\)’s illustrate effect penalization suitable figure.Now, write function requires three inputs, namely,y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), ’lambda` returns Lasso estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function indicate whether \\(X\\) contains vector ones first column, exempt \\(L_1\\) penalty.understand Ridge Lasso regressions , familiarize glmnet() package’s documentation. thoroughly tested well-established package provides efficient code compute penalized regression coefficients Ridge Lasso combinations, commonly called Elastic Nets.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"option-pricing-via-machine-learning","chapter":"15 Option pricing via machine learning","heading":"15 Option pricing via machine learning","text":"chapter covers machine learning methods option pricing.\nFirst, briefly introduce regression trees, random forests, neural networks - methods advocated highly flexible universal approximators, capable recovering highly nonlinear structures data. focus implementation, leave thorough treatment statistical underpinnings textbooks authors real comparative advantage issues.\nshow implement random forests deep neural networks tidy principles using tidymodels TensorFlow complicated network structures. Machine learning (ML) seen part artificial intelligence.\nML algorithms build model based training data order make predictions decisions without explicitly programmed .\nML can specified along vast array different branches, chapter focuses -called supervised learning regressions. basic idea supervised learning algorithms build mathematical model data contains inputs desired outputs. chapter, apply well-known methods random forests neural networks simple application option pricing. specifically, create artificial dataset option prices different values based Black-Scholes pricing equation call options. , train different models learn price call options without prior knowledge theoretical underpinnings famous option pricing equation Black Scholes (1973).order replicate analysis regarding neural networks chapter, install TensorFlow system, requires administrator rights machine. Parts can done within R. Just follow quick-start instructions.Throughout chapter, need following packages.package keras (Allaire Chollet 2022) high-level neural networks API developed focus enabling fast experimentation Tensorflow.\npackage ranger (Wright Ziegler 2017) provides fast implementation random forests hardhat (Vaughan Kuhn 2022) helper function robust data preprocessing fit time prediction time.","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(keras)\nlibrary(hardhat)\nlibrary(ranger)"},{"path":"option-pricing-via-machine-learning.html","id":"regression-trees-and-random-forests","chapter":"15 Option pricing via machine learning","heading":"15.1 Regression trees and random forests","text":"Regression trees popular ML approach incorporating multiway predictor interactions. Finance, regression trees gaining popularity, also context asset pricing (see, e.g. Bryzgalova, Pelger, Zhu 2022).\nTrees possess logic departs markedly traditional regressions. Trees designed find groups observations behave similarly . tree grows sequence steps. step, new branch sorts data leftover preceding step bins based one predictor variables. sequential branching slices space predictors partitions approximates unknown function \\(f(x)\\) yields relation predictors \\(x\\) outcome variable \\(y\\) average value outcome variable within partition. thorough treatment regression trees, refer Coqueret Guida (2020).Formally, partition predictor space \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). predictor \\(x\\) falls within region \\(R_j\\), estimate \\(f(x)\\) average training observations, \\(\\hat y_i\\), associated predictor \\(x_i\\) also \\(R_j\\). select partition \\(x\\) split order create new partitions, find predictor \\(j\\) value \\(s\\) define two new partitions, called \\(R_1(j,s)\\) \\(R_2(j,s)\\), split observations current partition asking \\(x_j\\) bigger \\(s\\):\n\\[R_1(j,s) = \\{x \\mid x_j < s\\} \\mbox{   } R_2(j,s) = \\{x \\mid x_j \\geq s\\}.\\]\npick \\(j\\) \\(s\\), find pair minimizes residual sum square (RSS):\n\\[\\sum_{:\\, x_i \\R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{:\\, x_i \\R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\\]\nChapter 14 context penalized regressions, first relevant question : hyperparameter decisions? Instead regularization parameter, trees fully determined number branches used generate partition (sometimes one specifies minimum number observations final branch instead maximum number branches).Models single tree may suffer high predictive variance. Random forests address shortcomings decision trees. goal improve predictive performance reduce instability averaging multiple decision trees. forest basically implies creating many regression trees averaging predictions. assure individual trees , use bootstrap induce randomness. specifically, build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using training sample. purpose, randomly select features included building tree. observation test set form prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{=1}^B\\hat{y}_{T_i}\\).","code":""},{"path":"option-pricing-via-machine-learning.html","id":"neural-networks","chapter":"15 Option pricing via machine learning","heading":"15.2 Neural networks","text":"Roughly speaking, neural networks propagate information input layer, one multiple hidden layers, output layer. number units (neurons) input layer equal dimension predictors, output layer usually consists one neuron (regression) multiple neurons classification. output layer predicts future data, similar fitted value regression analysis. Neural networks theoretical underpinnings universal approximators smooth predictive association (Hornik 1991). complexity, however, ranks neural networks among least transparent, least interpretable, highly parameterized ML tools.\nfinance, applications neural networks can found context many different contexts, e.g. Avramov, Cheng, Metzker (2022), L. Chen, Pelger, Zhu (2019), Gu, Kelly, Xiu (2020).neuron applies nonlinear activation function \\(f\\) aggregated signal \nsending output next layer\n\\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\]\n, \\(\\theta\\) parameters fit, \\(N^l\\) denotes number units (hyperparameter tune), \\(z_j\\) input variables can either raw data , case multiple chained layers, outcome previous layers \\(z_j = x_k-1\\).\neasiest case \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions sigmoid (.e., \\(f(x) = (1+e^{-x})^{-1}\\)) ReLu (.e., \\(f(x) = max(x, 0)\\)).Neural networks gain flexibility chaining multiple layers together. Naturally, imposes many degrees freedom network architecture clear theoretical guidance exists. specification neural network requires, minimum, stance depth (number hidden layers), activation function, number neurons, connection structure units (dense sparse), application regularization techniques avoid overfitting. Finally, learning means choose optimal parameters relying numerical optimization, often requires specifying appropriate learning rate. Despite computational challenges, implementation R tedious can use API TensorFlow.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"option-pricing","chapter":"15 Option pricing via machine learning","heading":"15.3 Option pricing","text":"apply ML methods relevant field finance, focus option pricing. application core taken Hull (2020). basic form, call options give owner right obligation buy specific stock (underlying) specific price (strike price \\(K\\)) specific date (exercise date \\(T\\)). Black–Scholes price (Black Scholes 1973) call option non-dividend-paying underlying stock given \n\\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right)\n\\end{aligned}\n\\]\n\\(C(S, T)\\) price option function today’s stock price underlying, \\(S\\), time maturity \\(T\\), \\(r_f\\) risk-free interest rate, \\(\\sigma\\) volatility underlying stock return. \\(\\Phi\\) cumulative distribution function standard normal random variable.Black-Scholes equation provides way compute arbitrage-free price call option parameters \\(S, K, r_f, T\\), \\(\\sigma\\) specified (arguably, realistic context, parameters easy specify except \\(\\sigma\\) estimated). simple R function allows computing price .","code":"\nblack_scholes_price <- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  d1 <- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  value <- S * pnorm(d1) - K * exp(-r * T) * pnorm(d1 - sigma * sqrt(T))\n\n  return(value)\n}"},{"path":"option-pricing-via-machine-learning.html","id":"learning-black-scholes","chapter":"15 Option pricing via machine learning","heading":"15.4 Learning Black-Scholes","text":"illustrate concept ML showing ML methods learn Black-Scholes equation observing different specifications corresponding prices without us revealing exact pricing equation.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"data-simulation","chapter":"15 Option pricing via machine learning","heading":"15.4.1 Data simulation","text":"end, start simulated data. compute option prices call options grid different combinations times maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), current stock prices (S). code , add idiosyncratic error term observation prices considered exactly reflect values implied Black-Scholes equation.order keep analysis reproducible, use set.seed(). random seed specifies start point computer generates random number sequence ensures simulated data across different machines.code generates 1.5 million random parameter constellations. values, two observed prices reflecting Black-Scholes prices given random innovation term pollutes observed prices. intuition application simple: simulated data provides many observations option prices - using Black-Scholes equation can evaluate actual predictive performance ML method, hard realistic context actual arbitrage-free price unknown.Next, split data training set (contains 1% observed option prices) test set used final evaluation. Note entire grid possible combinations contains 3148992 different specifications. Thus, sample learn Black-Scholes price contains 31,489 observations therefore relatively small.process training dataset fit different ML models. define recipe() defines processing steps purpose. specific case, want explain observed price five variables enter Black-Scholes equation. true price (stored column black_scholes) obviously used fit model. recipe also reflects standardize predictors ensure variable exhibits sample average zero sample standard deviation one.","code":"\nset.seed(420)\n\noption_prices <- expand_grid(\n  S = 40:60,\n  K = 20:90,\n  r = seq(from = 0, to = 0.05, by = 0.01),\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1)\n) |>\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map(\n      black_scholes,\n      function(x) x + rnorm(2, sd = 0.15)\n    )\n  ) |>\n  unnest(observed_price)\nsplit <- initial_split(option_prices, prop = 1 / 100)\nrec <- recipe(observed_price ~ .,\n  data = option_prices\n) |>\n  step_rm(black_scholes) |>\n  step_normalize(all_predictors())"},{"path":"option-pricing-via-machine-learning.html","id":"single-layer-networks-and-random-forests","chapter":"15 Option pricing via machine learning","heading":"15.4.2 Single layer networks and random forests","text":"Next, show fit neural network data. Note requires keras installed local machine. function mlp() package parsnip provides functionality initialize single layer, feed-forward neural network. specification defines single layer feed-forward neural network 10 hidden units. set number training iterations epochs = 500. option set_mode(\"regression\") specifies linear activation function output layer.verbose = FALSE argument prevents logging results console. can follow straightforward tidymodel workflow chapter : define workflow, equip recipe, specify associated model. Finally, fit model training data.familiar tidymodels workflow, piece cake fit models parsnip family.\ninstance, model initializes random forest 50 trees contained ensemble require least 2000 observations node.\nrandom forests trained using package ranger, required installed order run code .Fitting model follows exactly convention neural network .","code":"\nnnet_model <- mlp(\n  epochs = 500,\n  hidden_units = 10,\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"keras\", verbose = FALSE)\nnn_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(nnet_model) |>\n  fit(data = training(split))\nrf_model <- rand_forest(\n  trees = 50,\n  min_n = 2000\n) |>\n  set_engine(\"ranger\") |>\n  set_mode(\"regression\")\nrf_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(rf_model) |>\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning.html","id":"deep-neural-networks","chapter":"15 Option pricing via machine learning","heading":"15.4.3 Deep neural networks","text":"deep neural network neural network multiple layers input output layers. chaining multiple layers together, complex structures can represented fewer parameters simple shallow (one-layer) networks one implemented . instance, image text recognition typical tasks deep neural networks used (applications deep neural networks finance, see, instance, Jiang, Kelly, Xiu 2022; Jensen et al. 2022).Note tidymodels workflow extremely convenient, sophisticated multi-layer (-called deep) neural networks supported tidymodels yet (September 2022). Instead, implementation deep neural network R requires additional computational tools. reason, code snippet illustrates initialize sequential model three hidden layers 10 units per layer. keras package provides convenient interface flexible enough handle different activation functions. compile() command defines loss function model predictions evaluated.train neural network, provide inputs (x) variable predict (y) fit parameters. Note slightly tedious use method extract_mold(nn_fit). Instead simply using raw data, fit neural network processed data used single-layer feed-forward network. difference simply calling x = training(data) |> select(-observed_price, -black_scholes)? Recall recipe standardizes variables columns unit standard deviation zero mean. , adds consistency ensure models trained using recipe change recipe reflected performance model. final note potentially irritating observation: fit() alters keras model - one instances function R alters input function call object model anymore!","code":"\nmodel <- keras_model_sequential() |>\n  layer_dense(\n    input_shape = 5,\n    units = 10,\n    activation = \"sigmoid\"\n  ) |>\n  layer_dense(units = 10, activation = \"sigmoid\") |>\n  layer_dense(units = 10, activation = \"sigmoid\") |>\n  layer_dense(units = 1, activation = \"linear\") |>\n  compile(\n    loss = \"mean_squared_error\"\n  )\nmodelModel: \"sequential_1\"\n_____________________________________________________________________\nLayer (type)                   Output Shape               Param #    \n=====================================================================\ndense_5 (Dense)                (None, 10)                 60         \n_____________________________________________________________________\ndense_4 (Dense)                (None, 10)                 110        \n_____________________________________________________________________\ndense_3 (Dense)                (None, 10)                 110        \n_____________________________________________________________________\ndense_2 (Dense)                (None, 1)                  11         \n=====================================================================\nTotal params: 291\nTrainable params: 291\nNon-trainable params: 0\n_____________________________________________________________________\nmodel |>\n  fit(\n    x = extract_mold(nn_fit)$predictors |> as.matrix(),\n    y = extract_mold(nn_fit)$outcomes |> pull(observed_price),\n    epochs = 500, verbose = FALSE\n  )"},{"path":"option-pricing-via-machine-learning.html","id":"universal-approximation","chapter":"15 Option pricing via machine learning","heading":"15.4.4 Universal approximation","text":"evaluate results, implement one model. principle, non-linear function can also approximated linear model containing input variables’ polynomial expansions. illustrate , first define new recipe, rec_linear, processes training data even . include polynomials fifth degree predictor add possible pairwise interaction terms. final recipe step, step_lincomb(), removes potentially redundant variables (instance, interaction \\(r^2\\) \\(r^3\\) term \\(r^5\\)). fit Lasso regression model pre-specified penalty term (consult Chapter 14 tune model hyperparameters).","code":"\nrec_linear <- rec |>\n  step_poly(all_predictors(),\n    degree = 5,\n    options = list(raw = T)\n  ) |>\n  step_interact(terms = ~ all_predictors():all_predictors()) |>\n  step_lincomb(all_predictors())\n\nlm_model <- linear_reg(penalty = 0.01) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- workflow() |>\n  add_recipe(rec_linear) |>\n  add_model(lm_model) |>\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning.html","id":"prediction-evaluation","chapter":"15 Option pricing via machine learning","heading":"15.5 Prediction evaluation","text":"Finally, collect predictions compare --sample prediction error evaluated ten thousand new data points. Note evaluation, use call extract_mold() ensure use pre-processing steps testing data across model. also use somewhat advanced functionality forge(), provides easy, consistent, robust pre-processor prediction time.lines , use fitted models generate predictions entire test data set option prices. evaluate absolute pricing error one possible measure pricing accuracy, defined absolute value difference predicted option price theoretical correct option price Black-Scholes model.\nFigure 15.1: Absolut prediction error USD different fitted methods. prediction error evaluated sample call options used training.\nresults can summarized follows:ML methods seem able price call options observing training test set.average prediction errors increase far -money options.Random forest Lasso seem perform consistently worse prediction option prices neural networks.complexity deep neural network relative single layer neural network result better --sample predictions.","code":"\nout_of_sample_data <- testing(split) |>\n  slice_sample(n = 10000)\n\npredictive_performance <- model |>\n  predict(forge(\n    out_of_sample_data,\n    extract_mold(nn_fit)$blueprint\n  )$predictors |> as.matrix()) |>\n  as.vector() |>\n  tibble(\"Deep NN\" = _) |>\n  bind_cols(nn_fit |>\n    predict(out_of_sample_data)) |>\n  rename(\"Single layer\" = .pred) |>\n  bind_cols(lm_fit |> predict(out_of_sample_data)) |>\n  rename(\"Lasso\" = .pred) |>\n  bind_cols(rf_fit |> predict(out_of_sample_data)) |>\n  rename(\"Random forest\" = .pred) |>\n  bind_cols(out_of_sample_data) |>\n  pivot_longer(\"Deep NN\":\"Random forest\", names_to = \"Model\") |>\n  mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\npredictive_performance |>\n  ggplot(aes(x = moneyness, y = pricing_error, color = Model)) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE, method = \"gam\") +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Absolut prediction error (USD)\",\n    title = \"Prediction errors of call option prices for different models\"\n  )"},{"path":"option-pricing-via-machine-learning.html","id":"exercises-14","chapter":"15 Option pricing via machine learning","heading":"15.6 Exercises","text":"Write function takes y matrix predictors X inputs returns characterization relevant parameters regression tree 1 branch.Create function creates predictions new matrix predictors newX based estimated regression tree.Use package rpart grow tree based training data use illustration tools rpart understand characteristics tree deems relevant option pricing.Make use training test set choose optimal depth (number sample splits) tree.Use keras initialize sequential neural network can take predictors training data set input, contains least one hidden layer, generates continuous predictions. sounds harder : see simple regression example . many parameters neural network aim fit ?Compile object previous exercise. important specify loss function. Illustrate difference predictive accuracy different architecture choices.","code":""},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies","chapter":"16 Parametric portfolio policies","heading":"16 Parametric portfolio policies","text":"chapter, apply different portfolio performance measures evaluate compare portfolio allocation strategies.\npurpose, introduce direct way estimate optimal portfolio weights large-scale cross-sectional applications. precisely, approach Brandt, Santa-Clara, Valkanov (2009) proposes parametrize optimal portfolio weights function stock characteristics instead estimating stock’s expected return, variance, covariances stocks prior step.\nchoose weights function characteristics, maximize expected utility investor. approach feasible large portfolio dimensions (entire CRSP universe) proposed Brandt, Santa-Clara, Valkanov (2009). See review paper Brandt (2010) excellent treatment related portfolio choice methods.current chapter relies following set packages:","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)"},{"path":"parametric-portfolio-policies.html","id":"data-preparation-8","chapter":"16 Parametric portfolio policies","heading":"16.1 Data preparation","text":"get started, load monthly CRSP file, forms investment universe. load data SQLite-database introduced Chapters 2-4.evaluate performance portfolios, use monthly market returns benchmark compute CAPM alphas.Next, retrieve stock characteristics shown effect expected returns expected variances (even higher moments) return distribution. particular, record lagged one-year return momentum (momentum_lag), defined compounded return months \\(t − 12\\) \\(t − 2\\) firm. finance, momentum empirically observed tendency rising asset prices rise , falling prices keep falling (Jegadeesh Titman 1993). second characteristic firm’s market equity (size_lag), defined log price per share times number shares outstanding (Banz 1981).\nconstruct correct lagged values, use approach introduced Chapter 3.","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\ncrsp_monthly_lags <- crsp_monthly |>\n  transmute(permno,\n    month_12 = month %m+% months(12),\n    mktcap\n  )\n\ncrsp_monthly <- crsp_monthly |>\n  inner_join(crsp_monthly_lags,\n    by = c(\"permno\", \"month\" = \"month_12\"),\n    suffix = c(\"\", \"_12\")\n  )\n\ndata_portfolios <- crsp_monthly |>\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_12,\n    size_lag = log(mktcap_lag)\n  ) |>\n  drop_na(contains(\"lag\"))"},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies-1","chapter":"16 Parametric portfolio policies","heading":"16.2 Parametric portfolio policies","text":"basic idea parametric portfolio weights follows. Suppose date \\(t\\) \\(N_t\\) stocks investment universe, stock \\(\\) return \\(r_{, t+1}\\) associated vector firm characteristics \\(x_{, t}\\) time-series momentum market capitalization. investor’s problem choose portfolio weights \\(w_{,t}\\) maximize expected utility portfolio return:\n\\[\\begin{aligned}\n\\max_{w} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{=1}^{N_t}w_{,t}r_{,t+1}\\right)\\right]\n\\end{aligned}\\]\n\\(u(\\cdot)\\) denotes utility function.stock characteristics show ? parameterize optimal portfolio weights function stock characteristic \\(x_{,t}\\) following linear specification portfolio weights:\n\\[w_{,t} = \\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t},\\]\n\\(\\bar{w}_{,t}\\) stock’s weight benchmark portfolio (use value-weighted naive portfolio application ), \\(\\theta\\) vector coefficients going estimate, \\(\\hat{x}_{,t}\\) characteristics stock \\(\\), cross-sectionally standardized zero mean unit standard deviation.Intuitively, portfolio strategy form active portfolio management relative performance benchmark. Deviations benchmark portfolio derived individual stock characteristics. Note construction weights sum one \\(\\sum_{=1}^{N_t}\\hat{x}_{,t} = 0\\) due standardization. Moreover, coefficients constant across assets time. implicit assumption characteristics fully capture aspects joint distribution returns relevant forming optimal portfolios.first implement cross-sectional standardization entire CRSP universe. also keep track (lagged) relative market capitalization relative_mktcap, represent value-weighted benchmark portfolio, n denotes number traded assets \\(N_t\\), use construct naive portfolio benchmark.","code":"\ndata_portfolios <- data_portfolios |>\n  group_by(month) |>\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) |>\n  ungroup() |>\n  select(-mktcap_lag, -altprc)"},{"path":"parametric-portfolio-policies.html","id":"computing-portfolio-weights","chapter":"16 Parametric portfolio policies","heading":"16.3 Computing portfolio weights","text":"Next, move identify optimal choices \\(\\theta\\). rewrite optimization problem together weight parametrization can estimate \\(\\theta\\) maximize objective function based sample\n\\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{=1}^{N_t}\\left(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\right)r_{,t+1}\\right).\n\\end{aligned}\\]\nallocation strategy straightforward number parameters estimate small. Instead tedious specification \\(N_t\\) dimensional vector expected returns \\(N_t(N_t+1)/2\\) free elements covariance matrix, need focus application vector \\(\\theta\\). \\(\\theta\\) contains two elements application - relative deviation benchmark due size momentum.get feeling performance allocation strategy, start arbitrary initial vector \\(\\theta_0\\). next step choose \\(\\theta\\) optimally maximize objective function. automatically detect number parameters counting number columns lagged values.function compute_portfolio_weights() computes portfolio weights \\(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\) according parametrization given value \\(\\theta_0\\). Everything happens within single pipeline, hence provide short walk .first compute characteristic_tilt, tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{, t}\\) resemble deviation benchmark portfolio. Next, compute benchmark portfolio weight_benchmark, can reasonable set portfolio weights. case, choose either value equal-weighted allocation.\nweight_tilt completes picture contains final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt deviate benchmark portfolio depending stock characteristics.final lines go bit implement simple version -short sale constraint. generally straightforward ensure portfolio weight constraints via parameterization, simply normalize portfolio weights enforced positive. Finally, make sure normalized weights sum one :\n\\[w_{,t}^+ = \\frac{\\max(0, w_{,t})}{\\sum_{j=1}^{N_t}\\max(0, w_{,t})}.\\]following function computes optimal portfolio weights way just described.next step, compute portfolio weights arbitrary vector \\(\\theta_0\\). example , use value-weighted portfolio benchmark allow negative portfolio weights.","code":"\nn_parameters <- sum(str_detect(\n  colnames(data_portfolios), \"lag\"\n))\n\ntheta <- rep(1.5, n_parameters)\n\nnames(theta) <- colnames(data_portfolios)[str_detect(\n  colnames(data_portfolios), \"lag\"\n)]\ncompute_portfolio_weights <- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data |>\n    group_by(month) |>\n    bind_cols(\n      characteristic_tilt = data |>\n        transmute(across(contains(\"lag\"), ~ . / n)) |>\n        as.matrix() %*% theta |> as.numeric()\n    ) |>\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) |>\n    ungroup()\n}\nweights_crsp <- compute_portfolio_weights(theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)"},{"path":"parametric-portfolio-policies.html","id":"portfolio-performance","chapter":"16 Parametric portfolio policies","heading":"16.4 Portfolio performance","text":"\ncomputed weights optimal way? likely , picked \\(\\theta_0\\) arbitrarily. evaluate performance allocation strategy, one can think many different approaches. original paper, Brandt, Santa-Clara, Valkanov (2009) focus simple evaluation hypothetical utility agent equipped power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^\\gamma}{1-\\gamma}\\), \\(\\gamma\\) risk aversion factor.want note Gehrig, Sögner, Westerkamp (2020) warn , leading case constant relative risk aversion (CRRA), strong assumptions properties returns, variables used implement parametric portfolio policy, \nparameter space necessary obtain well defined optimization problem.doubt, many ways evaluate portfolio. function provides summary kinds interesting measures can considered relevant. need evaluation measures? depends: original paper Brandt, Santa-Clara, Valkanov (2009) cares expected utility choose \\(\\theta\\). However, want choose optimal values achieve highest performance putting constraints portfolio weights, helpful everything one function.\nLet us take look different portfolio strategies evaluation measures.value-weighted portfolio delivers annualized return 6 percent clearly outperforms tilted portfolio, irrespective whether evaluate expected utility, Sharpe ratio CAPM alpha. can conclude market beta close one strategies (naturally almost identically 1 value-weighted benchmark portfolio). comes distribution portfolio weights, see benchmark portfolio weight takes less extreme positions (lower average absolute weights lower maximum weight). definition, value-weighted benchmark take negative positions, tilted portfolio also takes short positions.","code":"\npower_utility <- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\nevaluate_portfolio <- function(weights_crsp,\n                               full_evaluation = TRUE) {\n  evaluation <- weights_crsp |>\n    group_by(month) |>\n    summarize(\n      return_tilt = weighted.mean(ret_excess, weight_tilt),\n      return_benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) |>\n    pivot_longer(-month,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) |>\n    group_by(model) |>\n    left_join(factors_ff_monthly, by = \"month\") |>\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(12 * portfolio_return),\n      \"SD return\" = 100 * sqrt(12) * sd(portfolio_return),\n      \"Sharpe ratio\" = mean(portfolio_return) / sd(portfolio_return),\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n    )) |>\n    mutate(model = str_remove(model, \"return_\")) |>\n    pivot_longer(-model, names_to = \"measure\") |>\n    pivot_wider(names_from = model, values_from = value)\n\n  if (full_evaluation) {\n    weight_evaluation <- weights_crsp |>\n      select(month, contains(\"weight\")) |>\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") |>\n      group_by(model, month) |>\n      transmute(tibble(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight < 0]),\n        \"Avg. fraction of negative weights\" = sum(weight < 0) / n()\n      )) |>\n      group_by(model) |>\n      summarize(across(-month, ~ 100 * mean(.))) |>\n      mutate(model = str_remove(model, \"weight_\")) |>\n      pivot_longer(-model, names_to = \"measure\") |>\n      pivot_wider(names_from = model, values_from = value)\n    evaluation <- bind_rows(evaluation, weight_evaluation)\n  }\n  return(evaluation)\n}\nevaluate_portfolio(weights_crsp) |>\n  print(n = Inf)# A tibble: 11 × 3\n   measure                            benchmark     tilt\n   <chr>                                  <dbl>    <dbl>\n 1 Expected utility                  -0.249     -0.262  \n 2 Average return                     7.12      -0.445  \n 3 SD return                         15.3       21.0    \n 4 Sharpe ratio                       0.135     -0.00613\n 5 CAPM alpha                         0.000123  -0.00582\n 6 Market beta                        0.993      0.930  \n 7 Absolute weight                    0.0247     0.0632 \n 8 Max. weight                        3.54       3.67   \n 9 Min. weight                        0.0000277 -0.145  \n10 Avg. sum of negative weights       0         77.9    \n11 Avg. fraction of negative weights  0         49.4    "},{"path":"parametric-portfolio-policies.html","id":"optimal-parameter-choice","chapter":"16 Parametric portfolio policies","heading":"16.5 Optimal parameter choice","text":"Next, move choice \\(\\theta\\) actually aims improve () performance measures. first define helper function compute_objective_function(), pass optimizer.may wonder return negative value objective function. simply due common convention optimization procedures search minima default. minimizing negative value objective function, get maximum value result.\nbasic form, R optimization relies function optim(). main inputs, function requires initial guess parameters objective function minimize. Now, fully equipped compute optimal values \\(\\hat\\theta\\), maximize hypothetical expected utility investor.resulting values \\(\\hat\\theta\\) easy interpret: intuitively, expected utility increases tilting weights value-weighted portfolio towards smaller stocks (negative coefficient size) towards past winners (positive value momentum). findings line well-documented size effect (Banz 1981) momentum anomaly (Jegadeesh Titman 1993).","code":"\ncompute_objective_function <- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data <- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function <- evaluate_portfolio(processed_data,\n    full_evaluation = FALSE\n  ) |>\n    filter(measure == objective_measure) |>\n    pull(tilt)\n\n  return(-objective_function)\n}\noptimal_theta <- optim(\n  par = theta,\n  compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)\n\noptimal_theta$parmomentum_lag     size_lag \n      0.0713      -1.9487 "},{"path":"parametric-portfolio-policies.html","id":"more-model-specifications","chapter":"16 Parametric portfolio policies","heading":"16.6 More model specifications","text":"portfolio perform different model specifications? purpose, compute performance number different modeling choices based entire CRSP sample. next code chunk performs heavy lifting.Finally, can compare results. table shows summary statistics possible combinations: equal- value-weighted benchmark portfolio, without short-selling constraints, tilted towards maximizing expected utility.results indicate average annualized Sharpe ratio equal-weighted portfolio exceeds Sharpe ratio value-weighted benchmark portfolio. Nevertheless, starting weighted value portfolio benchmark tilting optimally respect momentum small stocks yields highest Sharpe ratio across specifications. Finally, imposing short-sale constraints improve performance portfolios application.","code":"\nfull_model_grid <- expand_grid(\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE),\n  data = list(data_portfolios)\n) |>\n  mutate(optimal_theta = pmap(\n    .l = list(\n      data,\n      value_weighting,\n      allow_short_selling\n    ),\n    .f = ~ optim(\n      par = theta,\n      compute_objective_function,\n      data = ..1,\n      objective_measure = \"Expected utility\",\n      value_weighting = ..2,\n      allow_short_selling = ..3\n    )$par\n  ))\nperformance_table <- full_model_grid |>\n  mutate(\n    processed_data = pmap(\n      .l = list(\n        optimal_theta,\n        data,\n        value_weighting,\n        allow_short_selling\n      ),\n      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)\n    ),\n    portfolio_evaluation = map(processed_data,\n      evaluate_portfolio,\n      full_evaluation = TRUE\n    )\n  ) |>\n  select(\n    value_weighting,\n    allow_short_selling,\n    portfolio_evaluation\n  ) |>\n  unnest(portfolio_evaluation)\n\nperformance_table |>\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) |>\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) |>\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) |>\n  select(\n    measure,\n    `EW    `,\n    `VW    `,\n    sort(contains(\"Optimal\"))\n  ) |>\n  print(n = 11)# A tibble: 11 × 7\n   measure      `EW    ` `VW    ` VW  Op…¹ VW (no…² EW  Op…³ EW (no…⁴\n   <chr>           <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 Expected ut… -0.250   -2.49e-1 -0.246   -0.247   -0.250   -2.50e-1\n 2 Average ret… 10.7      7.12e+0 14.9     13.6     13.1      8.14e+0\n 3 SD return    20.3      1.53e+1 20.5     19.5     22.6      1.70e+1\n 4 Sharpe ratio  0.152    1.35e-1  0.209    0.202    0.168    1.38e-1\n 5 CAPM alpha    0.00226  1.23e-4  0.00646  0.00526  0.00428  4.69e-4\n 6 Market beta   1.13     9.93e-1  1.01     1.04     1.14     1.08e+0\n 7 Absolute we…  0.0247   2.47e-2  0.0373   0.0247   0.0255   2.47e-2\n 8 Max. weight   0.0247   3.54e+0  3.37     2.69     0.0672   2.20e-1\n 9 Min. weight   0.0247   2.77e-5 -0.0283   0       -0.0305   0      \n10 Avg. sum of…  0        0       26.4      0        1.73     0      \n11 Avg. fracti…  0        0       38.7      0        6.27     0      \n# … with abbreviated variable names ¹​`VW  Optimal `,\n#   ²​`VW (no s.) Optimal `, ³​`EW  Optimal `, ⁴​`EW (no s.) Optimal `"},{"path":"parametric-portfolio-policies.html","id":"exercises-15","chapter":"16 Parametric portfolio policies","heading":"16.7 Exercises","text":"estimated parameters \\(\\hat\\theta\\) portfolio performance change objective maximize Sharpe ratio instead hypothetical expected utility?code flexible sense can easily add new firm characteristics. Construct new characteristic choice evaluate corresponding coefficient \\(\\hat\\theta_i\\).Tweak function optimal_theta() can impose additional performance constraints order determine \\(\\hat\\theta\\) maximizes expected utility constraint market beta 1.portfolio performance resemble realistic --sample backtesting procedure? Verify robustness results first estimating \\(\\hat\\theta\\) based past data . , use recent periods evaluate actual portfolio performance.formulating portfolio problem statistical estimation problem, can easily obtain standard errors coefficients weight function. Brandt, Santa-Clara, Valkanov (2009) provide relevant derivations paper Equation (10). Implement small function computes standard errors \\(\\hat\\theta\\).","code":""},{"path":"constrained-optimization-and-backtesting.html","id":"constrained-optimization-and-backtesting","chapter":"17 Constrained optimization and backtesting","heading":"17 Constrained optimization and backtesting","text":" chapter, conduct portfolio backtesting realistic setting including transaction costs investment constraints -short-selling rules.\nstart standard mean-variance efficient portfolios introduce constraints step--step manner. , rely numerical optimization procedures R. conclude chapter providing --sample backtesting procedure different strategies introduce chapter.Throughout chapter, use following packages:Compared previous chapters, introduce quadprog package (Turlach, Weingessel, Moler 2019) perform numerical constrained optimization quadratic objective functions alabama (Varadhan 2022) general non-linear objective functions constraints. ","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(quadprog)\nlibrary(alabama)"},{"path":"constrained-optimization-and-backtesting.html","id":"data-preparation-9","chapter":"17 Constrained optimization and backtesting","heading":"17.1 Data preparation","text":"start loading required data SQLite-database introduced Chapters 2-4. simplicity, restrict investment universe monthly Fama-French industry portfolio returns following application. ","code":"\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nindustry_returns <- tbl(tidy_finance, \"industries_ff_monthly\") |>\n  collect()\n\nindustry_returns <- industry_returns |>\n  select(-month)"},{"path":"constrained-optimization-and-backtesting.html","id":"recap-of-portfolio-choice","chapter":"17 Constrained optimization and backtesting","heading":"17.2 Recap of portfolio choice","text":"common objective portfolio optimization find mean-variance efficient portfolio weights, .e., allocation delivers lowest possible return variance given minimum level expected returns.\nextreme case, investor concerned portfolio variance, may choose implement minimum variance portfolio (MVP) weights given solution \n\\[w_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1\\]\n\\(\\Sigma\\) \\((N \\times N)\\) covariance matrix returns. optimal weights \\(\\omega_\\text{mvp}\\) can found analytically \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). terms code, math equivalent following chunk. Next, consider investor aims achieve minimum variance given required expected portfolio return \\(\\bar{\\mu}\\) chooses\n\\[w_\\text{eff}({\\bar{\\mu}}) =\\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\nleave exercise show portfolio choice problem can equivalently formulated investor mean-variance preferences risk aversion factor \\(\\gamma\\). means investor aims choose portfolio weights solution \n\\[ w^*_\\gamma = \\arg\\max w' \\mu - \\frac{\\gamma}{2}w'\\Sigma w\\quad \\text{ s.t. } w'\\iota = 1.\\]\nsolution optimal portfolio choice problem :\n\\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\]\nEmpirically, classical solution imposes many problems.\nparticular, estimates \\(\\mu\\) noisy short horizons, (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements thus, estimation error huge.\nSeminal papers effect ignoring estimation uncertainty, among others, Brown (1976), Jobson Korkie (1980), Jorion (1986), Chopra Ziemba (1993).Even worse, asset universe contains assets available time periods \\((N > T)\\), sample covariance matrix longer positive definite inverse \\(\\Sigma^{-1}\\) exist anymore.\naddress estimation issues vast-dimensional covariance matrices, regularization techniques popular tool (see, e.g., Ledoit Wolf 2003, 2004, 2012; Fan, Fan, Lv 2008).uncertainty associated estimated parameters challenging, data generating process also unknown investor. words, model uncertainty reflects ex-ante even clear parameters require estimation (instance, returns driven factor model, selecting universe relevant factors imposes model uncertainty). Wang (2005) Garlappi, Uppal, Wang (2007) provide theoretical analysis optimal portfolio choice model estimation uncertainty. extreme case, Pflug, Pichler, Wozabal (2012) shows naive portfolio allocates equal wealth assets optimal choice investor averse model uncertainty.top estimation uncertainty, transaction costs major concern.\nRebalancing portfolios costly, , therefore, optimal choice depend investor’s current holdings. presence transaction costs, benefits reallocating wealth may smaller costs associated turnover. aspect investigated theoretically, among others, one risky asset Magill Constantinides (1976) Davis Norman (1990). Subsequent extensions case multiple assets proposed Balduzzi Lynch (1999) Balduzzi Lynch (2000). recent papers empirical approaches explicitly account transaction costs include Gârleanu Pedersen (2013), DeMiguel, Nogales, Uppal (2014), DeMiguel, Martín-Utrera, Nogales (2015).","code":"\nSigma <- cov(industry_returns)\nw_mvp <- solve(Sigma) %*% rep(1, ncol(Sigma))\nw_mvp <- as.vector(w_mvp / sum(w_mvp))"},{"path":"constrained-optimization-and-backtesting.html","id":"estimation-uncertainty-and-transaction-costs","chapter":"17 Constrained optimization and backtesting","heading":"17.3 Estimation uncertainty and transaction costs","text":"empirical evidence regarding performance mean-variance optimization procedure simply plug sample estimates \\(\\hat \\mu\\) \\(\\hat \\Sigma\\) can summarized rather briefly: mean-variance optimization performs poorly! literature discusses many proposals overcome empirical issues. instance, one may impose form regularization \\(\\Sigma\\), rely Bayesian priors inspired theoretical asset pricing models (Kan Zhou 2007) use high-frequency data improve forecasting (Hautsch, Kyj, Malec 2015).\nOne unifying framework works easily, effectively (even large dimensions), purely inspired economic arguments ex-ante adjustment transaction costs (Hautsch Voigt 2019).Assume returns multivariate normal distribution mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, assume quadratic transaction costs penalize rebalancing \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) := \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\]\ncost parameter \\(\\beta>0\\) \\(\\omega_{t^+} := {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). \\(\\omega_{t^+}\\) denotes portfolio weights just rebalancing. Note \\(\\omega_{t^+}\\) differs mechanically \\(\\omega_t\\) due returns past period.\nIntuitively, transaction costs penalize portfolio performance portfolio shifted current holdings \\(\\omega_{t^+}\\) new allocation \\(\\omega_{t+1}\\).\nsetup, transaction costs increase linearly. Instead, larger rebalancing penalized heavily small adjustments.\n, optimal portfolio choice investor mean variance preferences \n\\[\\begin{aligned}\\omega_{t+1} ^* &:=  \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned}\\]\n\n\\[\\mu^*:=\\mu+\\beta \\omega_{t^+} \\quad  \\text{} \\quad \\Sigma^*:=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\\]\nresult, adjusting transaction costs implies standard mean-variance optimal portfolio choice adjusted return parameters \\(\\Sigma^*\\) \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]alternative formulation optimal portfolio can derived follows:\n\\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1.\\]\noptimal weights correspond mean-variance portfolio vector expected returns assets currently exhibit higher weight considered delivering higher expected return.","code":""},{"path":"constrained-optimization-and-backtesting.html","id":"optimal-portfolio-choice","chapter":"17 Constrained optimization and backtesting","heading":"17.4 Optimal portfolio choice","text":"function implements efficient portfolio weight general form, allowing transaction costs (conditional holdings reallocation).\n\\(\\beta=0\\), computation resembles standard mean-variance efficient framework. gamma denotes coefficient risk aversion \\(\\gamma\\), beta transaction cost parameter \\(\\beta\\) w_prev weights rebalancing \\(\\omega_{t^+}\\).portfolio weights indicate efficient portfolio investor risk aversion coefficient \\(\\gamma=2\\) absence transaction costs. positions negative implies short-selling, positions rather extreme. instance, position \\(-1\\) implies investor takes short position worth entire wealth lever long positions assets.\neffect transaction costs different levels risk aversion optimal portfolio choice? following lines code analyze distance minimum variance portfolio portfolio implemented investor different values transaction cost parameter \\(\\beta\\) risk aversion \\(\\gamma\\).code chunk computes optimal weight presence transaction cost different values \\(\\beta\\) \\(\\gamma\\) initial allocation, theoretical optimal minimum variance portfolio.\nStarting initial allocation, investor chooses optimal allocation along efficient frontier reflect risk preferences.\ntransaction costs absent, investor simply implement mean-variance efficient allocation. transaction costs make costly rebalance, optimal portfolio choice reflects shift towards efficient portfolio, whereas current portfolio anchors investment.\nFigure 17.1: horizontal axis indicates distance empirical minimum variance portfolio weight, measured sum absolut deviations chosen portfolio benchmark.\nfigure shows rebalancing initial portfolio (always set minimum variance portfolio weights example).\nhigher transaction costs parameter \\(\\beta\\), smaller rebalancing initial portfolio. addition, risk aversion \\(\\gamma\\) increases, efficient portfolio closer minimum variance portfolio weights investor desires less rebalancing initial holdings.","code":"\ncompute_efficient_weight <- function(Sigma,\n                                     mu,\n                                     gamma = 2,\n                                     beta = 0, # transaction costs\n                                     w_prev = rep(\n                                       1 / ncol(Sigma),\n                                       ncol(Sigma)\n                                     )) {\n  iota <- rep(1, ncol(Sigma))\n  Sigma_processed <- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed <- mu + beta * w_prev\n\n  Sigma_inverse <- solve(Sigma_processed)\n\n  w_mvp <- Sigma_inverse %*% iota\n  w_mvp <- as.vector(w_mvp / sum(w_mvp))\n  w_opt <- w_mvp + 1 / gamma *\n    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%\n      mu_processed\n  return(as.vector(w_opt))\n}\n\nmu <- colMeans(industry_returns)\ncompute_efficient_weight(Sigma, mu) [1]  1.395  0.293 -1.390  0.477  0.363 -0.320  0.545  0.446 -0.132\n[10] -0.675\ntransaction_costs <- expand_grid(\n  gamma = c(2, 4, 8, 20),\n  beta = 20 * qexp((1:99) / 100)\n) |>\n  mutate(\n    weights = map2(\n      .x = gamma,\n      .y = beta,\n      ~ compute_efficient_weight(Sigma,\n        mu,\n        gamma = .x,\n        beta = .y / 10000,\n        w_prev = w_mvp\n      )\n    ),\n    concentration = map_dbl(weights, ~ sum(abs(. - w_mvp)))\n  )\ntransaction_costs |>\n  mutate(risk_aversion = as_factor(gamma)) |>\n  ggplot(aes(\n    x = beta,\n    y = concentration,\n    color = risk_aversion\n  )) +\n  geom_line() +\n  labs(\n    x = \"Transaction cost parameter\",\n    y = \"Distance from MVP\",\n    color = \"Risk aversion\",\n    title = \"Portfolio weights for different risk aversion and transaction cost\"\n  )"},{"path":"constrained-optimization-and-backtesting.html","id":"constrained-optimization","chapter":"17 Constrained optimization and backtesting","heading":"17.5 Constrained optimization","text":"Next, introduce constraints optimization procedure.\noften, typical constraints short-selling restrictions prevent analytical solutions optimal portfolio weights (short-selling restrictions simply imply negative weights allowed require \\(w_i \\geq 0 \\forall \\)).\nHowever, numerical optimization allows computing solutions constrained problems. purpose mean-variance optimization, rely solve.QP() function package quadprog.function solve.QP() delivers numerical solutions quadratic programming problems form\n\\[\\min(-\\mu \\omega + 1/2 \\omega' \\Sigma \\omega) \\text{ s.t. } ' \\omega >= b_0.\\]\nfunction takes one argument (meq) number equality constraints. Therefore, matrix \\(\\) simply vector ones ensure weights sum one. case short-selling constraints, matrix \\(\\) form\n\\[' = \\begin{pmatrix}1 & 1& \\ldots&1 \\\\1 & 0 &\\ldots&0\\\\0 & 1 &\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}'\\qquad b_0 = \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}.\\]dive constrained optimization, revisit unconstrained problem replicate analytical solutions minimum variance efficient portfolio weights . verify output equal solution.\nNote near() safe way compare two vectors pairwise equality. alternative == sensitive small differences may occur due representation floating points computer near() built tolerance. just discussed, set Amat matrix column ones bvec 1 enforce constraint weights must sum one. meq=1 means one (one) constraints must satisfied equality.result shows indeed numerical procedure recovered optimal weights scenario already know analytic solution.\ncomplex optimization routines, R’s optimization task view provides overview vast optimization landscape. Next, approach problems analytical solutions exist. First, additionally impose short-sale constraints, implies \\(N\\) inequality constraints form \\(w_i >=0\\).expected, resulting portfolio weights positive (numerical precision). Typically, holdings presence short-sale constraints concentrated among way fewer assets unrestricted case.\ncan verify sum(w_no_short_sale$solution) returns 1. words: solve.QP() provides numerical solution portfolio choice problem mean-variance investor risk aversion gamma = 2 negative holdings forbidden.solve.QP() fast benefits clear problem structure quadratic objective linear constraints. However, optimization often requires flexibility. example, show compute optimal weights, subject -called Regulation T-constraint, requires sum absolute portfolio weights smaller 1.5, \\(\\sum_{=1}^N |w_i| \\leq 1.5\\).\nconstraint enforces maximum 50 percent allocated wealth can allocated short positions thus implies initial margin requirement 50 percent. Imposing margin requirement reduces portfolio risks extreme portfolio weights attainable anymore. implementation Regulation-T rules numerically interesting, margin constraints imply non-linear constraint portfolio weights.\nThus, can longer rely solve.QP() defined solve quadratic programming problems linear constraints.\nInstead, rely package alabama, requires separate definition objective constraint functions.Note function constrOptim.nl() requires starting vector parameter values, .e., initial portfolio. hood, alamaba performs numerical optimization searching local minimum function objective() (subject equality constraints equality_constraints() inequality constraints inequality_constraints()).\nNote starting point matter algorithm identifies global minimum.figure shows optimal allocation weights across 10 industries four different strategies considered far: minimum variance, efficient portfolio \\(\\gamma\\) = 2, efficient portfolio short-sale constraints, Regulation-T constrained portfolio.\nFigure 17.2: Optimal allocation weights 10 industry portfolios 4 different allocation strategies.\nresults clearly indicate effect imposing additional constraints: extreme holdings investor implements follows (theoretically optimal) efficient portfolio vanish , e.g., Regulation-T constraint.\nmay wonder investor deviate theoretically optimal portfolio imposing potentially arbitrary constraints.\nshort answer : efficient portfolio efficient true parameters data generating process correspond estimated parameters \\(\\hat\\Sigma\\) \\(\\hat\\mu\\).\nEstimation uncertainty may thus lead inefficient allocations. imposing restrictions, implicitly shrink set possible weights prevent extreme allocations result error-maximization due estimation uncertainty (Jagannathan Ma 2003).move , want propose final allocation strategy, reflects somewhat realistic structure transaction costs instead quadratic specification used . function computes efficient portfolio weights adjusting transaction costs form \\(\\beta\\sum_{=1}^N |(w_{, t+1} - w_{, t^+})|\\). closed-form solution exists, rely non-linear optimization procedures.","code":"\nn_industries <- ncol(industry_returns)\n\nw_mvp_numerical <- solve.QP(\n  Dmat = Sigma,\n  dvec = rep(0, n_industries),\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(w_mvp, w_mvp_numerical$solution))[1] TRUE\nw_efficient_numerical <- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(compute_efficient_weight(Sigma, mu), w_efficient_numerical$solution))[1] TRUE\nw_no_short_sale <- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(1, diag(n_industries)),\n  bvec = c(1, rep(0, n_industries)),\n  meq = 1\n)\nw_no_short_sale$solution [1] 5.17e-01 3.06e-18 4.27e-16 7.90e-02 3.47e-18 2.65e-17 1.48e-01\n [8] 2.56e-01 8.74e-18 0.00e+00\ninitial_weights <- rep(\n  1 / n_industries,\n  n_industries\n)\n\nobjective <- function(w, gamma = 2) {\n  -t(w) %*% (1 + mu) +\n    gamma / 2 * t(w) %*% Sigma %*% w\n}\n\ninequality_constraints <- function(w, reg_t = 1.5) {\n  reg_t - sum(abs(w))\n}\n\nequality_constraints <- function(w) {\n  sum(w) - 1\n}\n\nw_reg_t <- constrOptim.nl(\n  par = initial_weights,\n  hin = inequality_constraints,\n  fn = objective,\n  heq = equality_constraints,\n  control.outer = list(trace = FALSE)\n)\nw_reg_t$par [1]  3.41e-01 -1.81e-06 -1.08e-01  1.40e-01  7.30e-02 -1.08e-02\n [7]  2.46e-01  3.14e-01  1.35e-01 -1.30e-01\ntibble(\n  `No short-sale` = w_no_short_sale$solution,\n  `Minimum Variance` = w_mvp,\n  `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n  `Regulation-T` = w_reg_t$par,\n  Industry = colnames(industry_returns)\n) |>\n  pivot_longer(-Industry,\n    names_to = \"Strategy\",\n    values_to = \"weights\"\n  ) |>\n  ggplot(aes(\n    fill = Strategy,\n    y = weights,\n    x = Industry\n  )) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() +\n  labs(\n    y = \"Allocation weight\", fill = NULL,\n    title = \"Optimal allocations for different strategies\"\n  ) +\n  scale_y_continuous(labels = percent)\ncompute_efficient_weight_L1_TC <- function(mu,\n                                           Sigma,\n                                           gamma = 2,\n                                           beta = 0,\n                                           initial_weights = rep(\n                                             1 / ncol(Sigma),\n                                             ncol(Sigma)\n                                           )) {\n  objective <- function(w) {\n    -t(w) %*% mu +\n      gamma / 2 * t(w) %*% Sigma %*% w +\n      (beta / 10000) / 2 * sum(abs(w - initial_weights))\n  }\n\n  w_optimal <- constrOptim.nl(\n    par = initial_weights,\n    fn = objective,\n    heq = function(w) {\n      sum(w) - 1\n    },\n    control.outer = list(trace = FALSE)\n  )\n\n  return(w_optimal$par)\n}"},{"path":"constrained-optimization-and-backtesting.html","id":"out-of-sample-backtesting","chapter":"17 Constrained optimization and backtesting","heading":"17.6 Out-of-sample backtesting","text":"sake simplicity, committed one fundamental error computing portfolio weights : used full sample data determine optimal allocation (Arnott, Harvey, Markowitz 2019). implement strategy beginning 2000s, need know returns evolve 2021.\ninteresting methodological point view, evaluate performance portfolios reasonable --sample fashion. next backtesting application three strategies. backtest, recompute optimal weights just based past available data.lines define general setup. consider 120 periods past update parameter estimates recomputing portfolio weights. , update portfolio weights costly affects performance. portfolio weights determine portfolio return. period later, current portfolio weights changed form foundation transaction costs incurred next period. consider three different competing strategies: mean-variance efficient portfolio, mean-variance efficient portfolio ex-ante adjustment transaction costs, naive portfolio, allocates wealth equally across different assets.also define two helper functions: one adjust weights due returns one performance evaluation, compute realized returns net transaction costs.following code chunk performs rolling-window estimation, implement loop. period, estimation window contains returns available current period.\nNote use sample variance covariance matrix ignore estimation \\(\\hat\\mu\\) entirely, might use advanced estimators practice.Finally, get evaluation portfolio strategies net--transaction costs. Note compute annualized returns standard deviations. results clearly speak mean-variance optimization. Turnover huge investor considers portfolio’s expected return variance. Effectively, mean-variance portfolio generates negative annualized return adjusting transaction costs. time, naive portfolio turns perform well. fact, performance gains transaction-cost adjusted mean-variance portfolio small. --sample Sharpe ratio slightly higher naive portfolio. Note extreme effect turnover penalization turnover: MV (TC) effectively resembles buy--hold strategy updates portfolio estimated parameters \\(\\hat\\mu_t\\) \\(\\hat\\Sigma_t\\)indicate current allocation far away optimal theoretical portfolio.","code":"\nwindow_length <- 120\nperiods <- nrow(industry_returns) - window_length\n\nbeta <- 50\ngamma <- 2\n\nperformance_values <- matrix(NA,\n  nrow = periods,\n  ncol = 3\n)\ncolnames(performance_values) <- c(\"raw_return\", \"turnover\", \"net_return\")\n\nperformance_values <- list(\n  \"MV (TC)\" = performance_values,\n  \"Naive\" = performance_values,\n  \"MV\" = performance_values\n)\n\nw_prev_1 <- w_prev_2 <- w_prev_3 <- rep(\n  1 / n_industries,\n  n_industries\n)\nadjust_weights <- function(w, next_return) {\n  w_prev <- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance <- function(w, w_previous, next_return, beta = 50) {\n  raw_return <- as.matrix(next_return) %*% w\n  turnover <- sum(abs(w - w_previous))\n  net_return <- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\nfor (p in 1:periods) {\n  returns_window <- industry_returns[p:(p + window_length - 1), ]\n  next_return <- industry_returns[p + window_length, ] |> as.matrix()\n\n  Sigma <- cov(returns_window)\n  mu <- 0 * colMeans(returns_window)\n\n  # Transaction-cost adjusted portfolio\n  w_1 <- compute_efficient_weight_L1_TC(\n    mu = mu,\n    Sigma = Sigma,\n    beta = beta,\n    gamma = gamma,\n    initial_weights = w_prev_1\n  )\n\n  performance_values[[1]][p, ] <- evaluate_performance(w_1,\n    w_prev_1,\n    next_return,\n    beta = beta\n  )\n\n  w_prev_1 <- adjust_weights(w_1, next_return)\n\n  # Naive portfolio\n  w_2 <- rep(1 / n_industries, n_industries)\n\n  performance_values[[2]][p, ] <- evaluate_performance(\n    w_2,\n    w_prev_2,\n    next_return\n  )\n\n  w_prev_2 <- adjust_weights(w_2, next_return)\n\n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 <- compute_efficient_weight(\n    Sigma = Sigma,\n    mu = mu,\n    gamma = gamma\n  )\n\n  performance_values[[3]][p, ] <- evaluate_performance(\n    w_3,\n    w_prev_3,\n    next_return\n  )\n\n  w_prev_3 <- adjust_weights(w_3, next_return)\n}\nperformance <- lapply(\n  performance_values,\n  as_tibble\n) |>\n  bind_rows(.id = \"strategy\")\n\nperformance |>\n  group_by(strategy) |>\n  summarize(\n    Mean = 12 * mean(100 * net_return),\n    SD = sqrt(12) * sd(100 * net_return),\n    `Sharpe ratio` = if_else(Mean > 0,\n      Mean / SD,\n      NA_real_\n    ),\n    Turnover = 100 * mean(turnover)\n  )# A tibble: 3 × 5\n  strategy   Mean    SD `Sharpe ratio` Turnover\n  <chr>     <dbl> <dbl>          <dbl>    <dbl>\n1 MV       -0.635  12.5         NA     213.    \n2 MV (TC)  12.3    15.0          0.820   0.0298\n3 Naive    12.3    15.0          0.818   0.230 "},{"path":"constrained-optimization-and-backtesting.html","id":"exercises-16","chapter":"17 Constrained optimization and backtesting","heading":"17.7 Exercises","text":"argue investor quadratic utility function certainty equivalent \\[\\max_w CE(w) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\]\nfaces equivalent optimization problem framework portfolio weights chosen aim minimize volatility given pre-specified level expected returns\n\\[\\min_w \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ } \\iota'\\omega = 1. \\] Proof equivalence optimal portfolio weights cases.Consider portfolio choice problem transaction-cost adjusted certainty equivalent maximization risk aversion parameter \\(\\gamma\\)\n\\[\\omega_{t+1} ^* :=  \\arg\\max_{\\omega \\\\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\]\n\\(\\Sigma\\) \\(\\mu\\) (estimators ) variance-covariance matrix returns vector expected returns. Assume now transaction costs quadratic rebalancing proportional stock illiquidity \n\\[\\nu_t\\left(\\omega, B\\right) := \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) diagonal matrix \\(ill_1, \\ldots, ill_N\\). Derive closed-form solution mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based transaction cost specification . Discuss effect illiquidity \\(ill_i\\) individual portfolio weights relative investor myopically ignores transaction costs decision.Use solution previous exercise update function compute_efficient_weight() can compute optimal weights conditional matrix \\(B\\) illiquidity measures.Illustrate evolution optimal weights naive portfolio efficient portfolio mean-standard deviation diagram.always optimal choose \\(\\beta\\) optimization problem value used evaluating portfolio performance? words: can optimal choose theoretically sub-optimal portfolios based transaction cost considerations reflect actual incurred costs? Evaluate --sample Sharpe ratio transaction costs range different values imposed \\(\\beta\\) values.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"cover-design.html","id":"cover-design","chapter":"A Cover design","heading":"A Cover design","text":"cover book inspired fast growing generative art community R.\nGenerative art refers art whole part created use autonomous system.\nInstead creating random dynamics rely core book: evolution financial markets.\ncircle cover figure corresponds daily market return within one year sample. Deviations circle line indicate positive negative returns.\ncolors determined standard deviation market returns particular year.\nlines code replicate entire figure.\nuse Wes Andersen color palette (also throughout entire book), provided package ´wesanderson´ (Ram Wickham 2018)","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_daily <- tbl(\n  tidy_finance,\n  \"factors_ff_daily\"\n) |>\n  collect()\n\ndata_plot <- factors_ff_daily |>\n  select(date, mkt_excess) |>\n  group_by(year = floor_date(date, \"year\")) |>\n  mutate(group_id = cur_group_id())\n\ndata_plot <- data_plot |>\n  group_by(group_id) |>\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |>\n  filter(year >= \"1962-01-01\")\n\nlevels <- data_plot |>\n  distinct(group_id, vola) |>\n  arrange(vola) |>\n  pull(vola)\n\ncp <- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free <- function() TRUE\n\ncolors <- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\nplot <- data_plot |>\n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = plot,\n width = 10,\n height = 6,\n filename = \"cover.jpg\",\n bg = \"white\"\n)"},{"path":"clean-enhanced-trace-with-r.html","id":"clean-enhanced-trace-with-r","chapter":"B Clean enhanced TRACE with R","heading":"B Clean enhanced TRACE with R","text":"appendix contains code clean enhanced TRACE R. also available via following Github gist. Hence, also source function devtools::source_gist(\"3a05b3ab281563b2e94858451c2eb3a4\"). need function Chapter 4 download clean enhanced TRACE trade messages following Dick-Nielsen (2009) Dick-Nielsen (2014) enhanced TRACE specifically. WRDS provides SAS code clean enhanced TRACE data.function takes vector CUSIPs (cusips), connection WRDS (connection) explained Chapter 3, start end date (start_date end_date, respectively). Specifying many CUSIPs result slow downloads potential failure due size request WRDS. dates within coverage TRACE , .e., starting 2002, dates supplied using class date. output function contains valid trade messages selected CUSIPs specified period.","code":"\nclean_enhanced_trace <- function(cusips,\n                                 connection,\n                                 start_date = as.Date(\"2002-01-01\"),\n                                 end_date = today()) {\n\n  # Packages (required)\n  library(tidyverse)\n  library(lubridate)\n  library(dbplyr)\n  library(RPostgres)\n\n  # Function checks ---------------------------------------------------------\n  # Input parameters\n  ## Cusips\n  if (length(cusips) == 0 | any(is.na(cusips))) stop(\"Check cusips.\")\n\n  ## Dates\n  if (!is.Date(start_date) | !is.Date(end_date)) stop(\"Dates needed\")\n  if (start_date < as.Date(\"2002-01-01\")) stop(\"TRACE starts later.\")\n  if (end_date > today()) stop(\"TRACE does not predict the future.\")\n  if (start_date >= end_date) stop(\"Date conflict.\")\n\n  ## Connection\n  if (!dbIsValid(connection)) stop(\"Connection issue.\")\n\n  # Enhanced Trace ----------------------------------------------------------\n  # Main file\n  trace_all <- tbl(\n    connection,\n    in_schema(\"trace\", \"trace_enhanced\")\n  ) |>\n    filter(cusip_id %in% cusips) |>\n    filter(trd_exctn_dt >= start_date & trd_exctn_dt <= end_date) |>\n    select(\n      cusip_id, msg_seq_nb, orig_msg_seq_nb,\n      entrd_vol_qt, rptd_pr, yld_pt, rpt_side_cd, cntra_mp_id,\n      trd_exctn_dt, trd_exctn_tm, trd_rpt_dt, trd_rpt_tm,\n      pr_trd_dt, trc_st, asof_cd, wis_fl,\n      days_to_sttl_ct, stlmnt_dt, spcl_trd_fl\n    ) |>\n    collect()\n\n  # Enhanced Trace: Post 06-02-2012 -----------------------------------------\n  # Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR <- trace_all |>\n    filter(\n      (trc_st == \"T\" | trc_st == \"R\"),\n      trd_rpt_dt >= as.Date(\"2012-02-06\")\n    )\n\n  # Cancelations (trc_st = X) and correction cancelations (trc_st = C)\n  trace_post_XC <- trace_all |>\n    filter(\n      (trc_st == \"X\" | trc_st == \"C\"),\n      trd_rpt_dt >= as.Date(\"2012-02-06\")\n    )\n\n  # Cleaning corrected and cancelled trades\n  trace_post_TR <- trace_post_TR |>\n    anti_join(trace_post_XC,\n      by = c(\n        \"cusip_id\", \"msg_seq_nb\", \"entrd_vol_qt\",\n        \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n        \"trd_exctn_dt\", \"trd_exctn_tm\"\n      )\n    )\n\n  # Reversals (trc_st = Y)\n  trace_post_Y <- trace_all |>\n    filter(\n      trc_st == \"Y\",\n      trd_rpt_dt >= as.Date(\"2012-02-06\")\n    )\n\n  # Clean reversals\n  ## match the orig_msg_seq_nb of the Y-message to\n  ## the msg_seq_nb of the main message\n  trace_post <- trace_post_TR |>\n    anti_join(trace_post_Y,\n      by = c(\"cusip_id\",\n        \"msg_seq_nb\" = \"orig_msg_seq_nb\",\n        \"entrd_vol_qt\", \"rptd_pr\", \"rpt_side_cd\",\n        \"cntra_mp_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"\n      )\n    )\n\n\n  # Enhanced TRACE: Pre 06-02-2012 ------------------------------------------\n  # Cancelations (trc_st = C)\n  trace_pre_C <- trace_all |>\n    filter(\n      trc_st == \"C\",\n      trd_rpt_dt < as.Date(\"2012-02-06\")\n    )\n\n  # Trades w/o cancelations\n  ## match the orig_msg_seq_nb of the C-message\n  ## to the msg_seq_nb of the main message\n  trace_pre_T <- trace_all |>\n    filter(\n      trc_st == \"T\",\n      trd_rpt_dt < as.Date(\"2012-02-06\")\n    ) |>\n    anti_join(trace_pre_C,\n      by = c(\"cusip_id\",\n        \"msg_seq_nb\" = \"orig_msg_seq_nb\",\n        \"entrd_vol_qt\", \"rptd_pr\", \"rpt_side_cd\",\n        \"cntra_mp_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"\n      )\n    )\n\n  # Corrections (trc_st = W) - W can also correct a previous W\n  trace_pre_W <- trace_all |>\n    filter(\n      trc_st == \"W\",\n      trd_rpt_dt < as.Date(\"2012-02-06\")\n    )\n\n  # Implement corrections in a loop\n  ## Correction control\n  correction_control <- nrow(trace_pre_W)\n  correction_control_last <- nrow(trace_pre_W)\n\n  ## Correction loop\n  while (correction_control > 0) {\n    # Corrections that correct some msg\n    trace_pre_W_correcting <- trace_pre_W |>\n      semi_join(trace_pre_T,\n        by = c(\"cusip_id\", \"trd_exctn_dt\",\n          \"orig_msg_seq_nb\" = \"msg_seq_nb\"\n        )\n      )\n\n    # Corrections that do not correct some msg\n    trace_pre_W <- trace_pre_W |>\n      anti_join(trace_pre_T,\n        by = c(\"cusip_id\", \"trd_exctn_dt\",\n          \"orig_msg_seq_nb\" = \"msg_seq_nb\"\n        )\n      )\n\n    # Delete msgs that are corrected and add correction msgs\n    trace_pre_T <- trace_pre_T |>\n      anti_join(trace_pre_W_correcting,\n        by = c(\"cusip_id\", \"trd_exctn_dt\",\n          \"msg_seq_nb\" = \"orig_msg_seq_nb\"\n        )\n      ) |>\n      union_all(trace_pre_W_correcting)\n\n    # Escape if no corrections remain or they cannot be matched\n    correction_control <- nrow(trace_pre_W)\n    if (correction_control == correction_control_last) {\n      correction_control <- 0\n    }\n    correction_control_last <- nrow(trace_pre_W)\n  }\n\n\n  # Clean reversals\n  ## Record reversals\n  trace_pre_R <- trace_pre_T |>\n    filter(asof_cd == \"R\") |>\n    group_by(\n      cusip_id, trd_exctn_dt, entrd_vol_qt,\n      rptd_pr, rpt_side_cd, cntra_mp_id\n    ) |>\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |>\n    mutate(seq = row_number()) |>\n    ungroup()\n\n  ## Remove reversals and the reversed trade\n  trace_pre <- trace_pre_T |>\n    filter(is.na(asof_cd) | !(asof_cd %in% c(\"R\", \"X\", \"D\"))) |>\n    group_by(\n      cusip_id, trd_exctn_dt, entrd_vol_qt,\n      rptd_pr, rpt_side_cd, cntra_mp_id\n    ) |>\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |>\n    mutate(seq = row_number()) |>\n    ungroup() |>\n    anti_join(trace_pre_R,\n      by = c(\n        \"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n        \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\", \"seq\"\n      )\n    ) |>\n    select(-seq)\n\n\n  # Agency trades -----------------------------------------------------------\n  # Combine pre and post trades\n  trace_clean <- trace_post |>\n    union_all(trace_pre)\n\n  # Keep angency sells and unmatched agency buys\n  ## Agency sells\n  trace_agency_sells <- trace_clean |>\n    filter(\n      cntra_mp_id == \"D\",\n      rpt_side_cd == \"S\"\n    )\n\n  # Agency buys that are unmatched\n  trace_agency_buys_filtered <- trace_clean |>\n    filter(\n      cntra_mp_id == \"D\",\n      rpt_side_cd == \"B\"\n    ) |>\n    anti_join(trace_agency_sells,\n      by = c(\n        \"cusip_id\", \"trd_exctn_dt\",\n        \"entrd_vol_qt\", \"rptd_pr\"\n      )\n    )\n\n  # Agency clean\n  trace_clean <- trace_clean |>\n    filter(cntra_mp_id == \"C\") |>\n    union_all(trace_agency_sells) |>\n    union_all(trace_agency_buys_filtered)\n\n\n  # Additional Filters ------------------------------------------------------\n  trace_add_filters <- trace_clean |>\n    mutate(days_to_sttl_ct2 = stlmnt_dt - trd_exctn_dt) |>\n    filter(\n      is.na(days_to_sttl_ct) | as.numeric(days_to_sttl_ct) <= 7,\n      is.na(days_to_sttl_ct2) | as.numeric(days_to_sttl_ct2) <= 7,\n      wis_fl == \"N\",\n      is.na(spcl_trd_fl) | spcl_trd_fl == \"\",\n      is.na(asof_cd) | asof_cd == \"\"\n    )\n\n\n  # Output ------------------------------------------------------------------\n  # Only keep necessary columns\n  trace_final <- trace_add_filters |>\n    arrange(cusip_id, trd_exctn_dt, trd_exctn_tm) |>\n    select(\n      cusip_id, trd_exctn_dt, trd_exctn_tm,\n      rptd_pr, entrd_vol_qt, yld_pt, rpt_side_cd, cntra_mp_id\n    ) |>\n    mutate(trd_exctn_tm = format(as_datetime(trd_exctn_tm), \"%H:%M:%S\"))\n\n  # Return\n  return(trace_final)\n}"}]
