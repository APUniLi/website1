[{"path":"index.html","id":"welcome","chapter":"1 Welcome","heading":"1 Welcome","text":"online version Tidy Finance R, book currently development intended eventual print release. grateful kind feedback every aspect book. please get touch us contact@tidy-finance.com spot typos, discover issues deserve attention, even suggestions additional chapters sections.","code":""},{"path":"index.html","id":"motivation","chapter":"1 Welcome","heading":"1.1 Motivation","text":"Finance exciting area economic research broad range applied academic empirical applications. undergrad student, typically exposed different types financial data, ranging asset prices, accounting data, trading decisions kinds financial data. Despite vast number empirical studies financial phenomenons, students quickly learn actual implementation studies rather opaque. graduate students, also surprised lack public code seminal papers even textbooks key insights financial economics. lack transparent codes leads numerous replication efforts (failures), also waste resources problems already solved others.book aims lift curtain code finance providing fully transparent code base many common financial applications. target group comprises students, data analysts programmers want gain basic tools required financial research apply challenges face. materials book can hence used courses empirical finance, well study empirical finance .","code":""},{"path":"index.html","id":"why-r","chapter":"1 Welcome","heading":"1.2 Why R?","text":"believe R among best choices programming language area finance. favorite features include:R free open source, can use academic professional contextsA diverse active online community working broad range toolsA massive set actively maintained packages kinds applications, e.g. data manipulation, visualization, machine learning, etc.Powerful tools communication, e.g. Rmarkdown, shinyRStudio one best development environments interactive data analysisStrong foundation functional programmingSmooth integration programming languages, e.g., SQL, Python, C, C++, Fortran, etc.information, refer (Wickham2019?)(https://adv-r.hadley.nz/introduction.html).","code":""},{"path":"index.html","id":"why-tidy","chapter":"1 Welcome","heading":"1.3 Why tidy?","text":"start working data, quickly realize spend lot time reading, cleaning transforming data. fact, often said 80% data analysis spent preparing data. tidying data, want structure datasets facilitate analyses. (Wickham2014?) puts ,[T]idy datasets alike every messy dataset messy way. Tidy datasets provide standardized way link structure dataset (physical layout) semantics (meaning).essence, tidy data follows three principles:Every column variable.Every row observation.Every cell single value.Throughout book, try best follow principles. want learn tidy data principles informal manner, refer vignette.addition data layer, also tidy coding principles outlined INSERT CITATION try follow:Reuse existing data structures.Compose simple functions pipe.Embrace functional programming.Design humans.particular, heavily draw set packages called tidyverse. tidyverse consistent set packages data analysis tasks, ranging importing, wrangling visualizing modeling data grammar. addition explicit tidy principles, tidyverse benefits: () master one package, easier master others (ii) core packages developed maintained Public Benefit Company RStudio, Inc. ","code":""},{"path":"index.html","id":"license","chapter":"1 Welcome","heading":"1.4 License","text":"book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC -NC-SA 4.0.code samples book licensed Creative Commons CC0 1.0 Universal (CC0 1.0), .e. public domain.","code":""},{"path":"index.html","id":"prerequisites","chapter":"1 Welcome","heading":"1.5 Prerequisites","text":"work chapters, recommend perform following couple steps:Install R RStudio. get walk-installation (every major operating system), follow steps outlined summary. whole process done clicks. wonder difference: R open-source language environment statistical computing graphics, free download use. R runs computations, RStudio integrated development environment (IDE) provides interface adding many convenient features tools. hence suggest coding RStudio.Open RStudio install tidyverse. sure works? find helpful information install packages brief summary.new R, recommend start following sources:\n- gentle good introduction workings R can found . done setting R machine, try follow “weighted dice project.”\n- main book tidyverse available online free: R Data Science Hadley Wickham Garrett Grolemund explains majority tools use book.","code":""},{"path":"introduction-to-tidy-finance.html","id":"introduction-to-tidy-finance","chapter":"2 Introduction to Tidy Finance","heading":"2 Introduction to Tidy Finance","text":"main aim chapter familiarize tidyverse. start downloading visualizing stock data move simple portfolio choice problem. examples introduce approach tidy finance.","code":""},{"path":"introduction-to-tidy-finance.html","id":"stock_market_data","chapter":"2 Introduction to Tidy Finance","heading":"2.1 Download and work with stock market data","text":"start session, load required packages. can use convenient tidyquant package download price data.\ntrouble using tidyquant, check documentation. load packages tidyverse tidyquant, also show code install packages case yet.first download daily prices one stock market ticker, e.g. AAPL, directly data provider Yahoo!Finance. download data, can use command tq_get. know use , make sure read help file calling ?tq_get. especially recommend taking look documentation’s examples section.tq_get downloads stock market data Yahoo!Finance specify another data source. function returns tibble eight quite self-explanatory columns: symbol, date, market prices open, high, low close, daily volume (number shares), adjusted price USD. Notice adjusted prices corrected anything might affect stock price market closes, e.g., stock splits dividends. actions affect quoted prices, direct impact investors hold stock.Next, use ggplot visualize time series adjusted prices.Next, compute daily returns defined \\((p_t - p_{t-1}) / p_{t-1}\\) \\(p_t\\) adjusted day \\(t\\) price. function lag works well , can trickier applied multiple assets .resulting tibble contains three columns last contains daily returns. Note first entry naturally contains NA previous price. Additionally, computations require time series ordered date - otherwise, lag meaningless. also cautious working one ticker since lag account multiple stocks automatically.upcoming examples, remove missing values require separate treatment computing, e.g., sample averages. general, however, make sure understand NA values occur carefully examine can simply get rid observations.Next, visualize distribution daily returns histogram. also multiply returns 100 get returns percent, just visualizations. percentage return arguably nice plots tables, less appealing computations. Additionally, also add dashed red line indicates 5% quantile daily returns histogram, (crude) proxy worst return stock probability least 5%., bins = 100 determines number bins hence implicitly width bins. proceeding, make sure understand use geom geom_vline() add dotted red line indicates 5% quantile daily returns.typical task proceeding data compute summary statistics main variables interest.can also compute summary statistics year imposing group_by(year = year(date)), also computes year call year(date).case wonder: additional argument .names = \"{.fn}\" across() determines name output columns. specification rather flexible allows next arbitrary column names can useful reporting.","code":"\n# install.packages(\"tidyverse\")\n# install.packages(\"tidyquant\")\nlibrary(tidyverse)\nlibrary(tidyquant)\nprices <- tq_get(\"AAPL\", get = \"stock.prices\")\nprices %>% head() # Take a glimpse at the data## # A tibble: 6 x 8\n##   symbol date        open  high   low close    volume adjusted\n##   <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n## 1 AAPL   2012-01-03  14.6  14.7  14.6  14.7 302220800     12.6\n## 2 AAPL   2012-01-04  14.6  14.8  14.6  14.8 260022000     12.7\n## 3 AAPL   2012-01-05  14.8  14.9  14.7  14.9 271269600     12.8\n## 4 AAPL   2012-01-06  15.0  15.1  15.0  15.1 318292800     12.9\n## 5 AAPL   2012-01-09  15.2  15.3  15.0  15.1 394024400     12.9\n## 6 AAPL   2012-01-10  15.2  15.2  15.1  15.1 258196400     13.0\nprices %>% # Simple visualization of the downloaded price time series\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"AAPL stock prices\",\n    subtitle = \"Prices in USD, adjusted for dividend payments and stock splits\"\n  ) +\n  theme_bw()\nreturns <- prices %>%\n  arrange(date) %>%\n  mutate(ret = (adjusted / lag(adjusted) - 1)) %>%\n  select(symbol, date, ret)\nreturns %>% head()## # A tibble: 6 x 3\n##   symbol date            ret\n##   <chr>  <date>        <dbl>\n## 1 AAPL   2012-01-03 NA      \n## 2 AAPL   2012-01-04  0.00537\n## 3 AAPL   2012-01-05  0.0111 \n## 4 AAPL   2012-01-06  0.0105 \n## 5 AAPL   2012-01-09 -0.00159\n## 6 AAPL   2012-01-10  0.00358\nreturns <- returns %>%\n  drop_na(ret)\nquantile_05 <- quantile(returns %>% pull(ret) * 100, 0.05) # Compute the 5 % quantile of the returns\n\nreturns %>% # create a histogram for daily returns\n  ggplot(aes(x = ret * 100)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of daily AAPL returns (in percent)\",\n    subtitle = \"The dotted vertical line indicates the historical 5% quantile\"\n  ) +\n  theme_bw()\nreturns %>%\n  mutate(ret = ret * 100) %>%\n  summarise(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  )) %>%\n  kableExtra::kable(digits = 3)\n# Alternatively: compute summary statistics for each year\nreturns %>%\n  mutate(ret = ret * 100) %>%\n  group_by(year = year(date)) %>%\n  summarise(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) %>%\n  kableExtra::kable(digits = 3)"},{"path":"introduction-to-tidy-finance.html","id":"scale-the-analysis-up-tidyverse-magic","chapter":"2 Introduction to Tidy Finance","heading":"2.2 Scale the analysis up: tidyverse-magic","text":"next step, generalize code computations can handle arbitrary vector tickers (e.g., index constituents). Following tidy principles, quite easy download data, plot price time series, tabulate summary statistics arbitrary number assets.tidyverse magic starts: tidy data makes extremely easy generalize computations many assets like. following code takes vector tickers, e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\"), automates download well plot price time series. end, create table summary statistics arbitrary number assets.\nFigure (ref?)(fig:prices) illustrates time series downloaded adjusted prices 30 constituents Dow Jones Index. Make sure understand every single line code! (purpose %>%? arguments aes()? alternative geoms use visualize time series? Hint: know answers try change code see difference intervention causes).\nFigure 2.1: DOW index stock prices.\nnotice small differences relative code used ? tq_get(ticker) returns tibble several symbols well. need illustrate tickers simultaneously include color = symbol ggplot2 aesthetics. way, can generate separate line ticker. course, simply many lines graph properly identify individual stocks, illustrates point well.holds returns well. computing returns, use group_by(symbol) mutate command performed symbol individually. logic applies computation summary statistics: group_by(symbol) key aggregating time series ticker-specific variables interest.Note now also equipped tools download price data ticker listed S&P 500 index number lines code. Just use ticker <- tq_index(\"SP500\"), provides tibble contains symbol (currently) part S&P 500. However, don’t try prepared wait couple minutes - many tickers.","code":"\nticker <- tq_index(\"DOW\") # tidyquant delivers all constituents of the Dow Jones index\nindex_prices <- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\"\n) %>% # Exactly the same code as in the first part\n  filter(symbol != \"DOW\") # Exclude the index itself\n\nindex_prices <- index_prices %>% # Remove assets that did not trade since January 1st 2000\n  group_by(symbol) %>%\n  mutate(n = n()) %>%\n  ungroup() %>%\n  filter(n == max(n)) %>%\n  select(-n)\n\nindex_prices %>%\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"DOW index stock prices\",\n    subtitle = \"Prices in USD, adjusted for dividend payments and stock splits\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\")\nall_returns <- index_prices %>%\n  group_by(symbol) %>% # we perform the computations per symbol\n  mutate(ret = adjusted / lag(adjusted) - 1) %>%\n  select(symbol, date, ret) %>%\n  drop_na(ret)\n\nall_returns %>%\n  mutate(ret = ret * 100) %>%\n  group_by(symbol) %>%\n  summarise(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  )) %>%\n  kableExtra::kable(digits = 3)"},{"path":"introduction-to-tidy-finance.html","id":"other-forms-of-data-aggregation","chapter":"2 Introduction to Tidy Finance","heading":"2.3 Other forms of data aggregation","text":"Sometimes, aggregation across variables symbol makes sense well. instance, suppose interested answering question: days high aggregate trading volume followed high aggregate trading volume days? provide initial analysis question, take downloaded tibble prices compute aggregate daily trading volume Dow Jones constituents USD. Recall column volume denoted number traded shares. Thus, multiply trading volume daily closing price get proxy aggregate trading volume USD. Scaling 1e9 denotes daily trading volume billion USD.One way illustrate persistence trading volume plot volume day \\(t\\) volume day \\(t-1\\) example . add 45°-line indicate hypothetical one--one relation geom_abline, addressing potential differences axes’ scales.understand warning ## Warning: Removed 1 rows containing missing values (geom_point). comes means? Purely eye-balling (.e., just looking something), figure reveals days high trading volume often followed similarly high trading volume days.","code":"\nvolume <- index_prices %>%\n  mutate(volume_usd = volume * close / 1e9) %>%\n  group_by(date) %>%\n  summarise(volume = sum(volume_usd))\n\nvolume %>% # Plot the time series of aggregate trading volume\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume (billion USD)\"\n  ) +\n  theme_bw()\nvolume %>%\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1), \n              color = \"red\",\n              linetype = \"dotted\") +\n  labs(\n    x = \"Previous day aggregate trading volume (billion USD)\",\n    y = \"Aggregate trading volume (billion USD)\",\n    title = \"Persistence of trading volume\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"None\")## Warning: Removed 1 rows containing missing values (geom_point)."},{"path":"introduction-to-tidy-finance.html","id":"portfolio-choice-problems","chapter":"2 Introduction to Tidy Finance","heading":"2.4 Portfolio choice problems","text":"previous part, show download stock market data inspect graphs summary statistics. Now, move typical question Finance, namely, optimally allocate wealth across different assets. standard framework optimal portfolio selection considers investors like higher returns dislike return volatility (forward-looking measures, .e., expected return volatility): mean-variance investor.essential tool evaluate portfolios mean-variance context efficient frontier, set portfolios satisfy condition portfolio exists higher expected return standard deviation return (.e., risk). Let us compute visualize efficient frontier several stocks. First, use dataset compute asset’s monthly returns.Next, transform returns tidy tibble \\((T \\times N)\\) matrix one column \\(N\\) tickers compute covariance matrix \\(\\Sigma\\) also expected return vector \\(\\mu\\). achieve using pivot_wider() new column names column symbol set values ret.\ncompute vector sample average returns sample variance-covariance matrix, consider proxies expected return distribution., compute minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) well expected return \\(\\omega_\\text{mvp}'\\mu\\) volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) portfolio. Recall minimum variance portfolio vector portfolio weights solution \n\\[\\arg\\min w'\\Sigma w \\text{ s.t. } \\sum\\limits_{=1}^Nw_i = 1.\\]\neasy show analytically, \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\) \\(\\iota\\) vector ones.Note monthly volatility minimum variance portfolio order magnitude daily standard deviation individual components. Thus, diversification benefits terms risk reduction tremendous!Next, set find weights portfolio achieves three times expected return minimum variance portfolio. However, interested portfolio achieves return, efficient portfolio mean-variance setting, .e., portfolio lowest standard deviation. wonder solution \\(\\omega_\\text{eff}\\) comes : efficient portfolio chosen investor aims achieve minimum variance given minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, objective function choose \\(\\omega_\\text{eff}\\) solution \n\\[\\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\ncode implements analytic solution optimization problem, encourage verify correct.","code":"\nreturns <- index_prices %>%\n  mutate(month = floor_date(date, \"month\")) %>%\n  group_by(symbol, month) %>%\n  summarise(price = last(adjusted), .groups = \"drop_last\") %>%\n  mutate(ret = price / lag(price) - 1) %>%\n  drop_na(ret) %>%\n  select(-price)\nreturns_matrix <- returns %>%\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) %>%\n  select(-month)\n\nsigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\n\nc(t(mvp_weights) %*% mu, sqrt(t(mvp_weights) %*% sigma %*% mvp_weights)) # Expected return and volatility## [1] 0.008434436 0.031410466\n# Compute efficient portfolio weights for given level of expected return\nmu_bar <- 3 * t(mvp_weights) %*% mu # some benchmark return: 3 times the minimum variance portfolio expected return\n\nC <- as.numeric(t(iota) %*% solve(sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(sigma) %*% mu)\n\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights + lambda_tilde / 2 * (solve(sigma) %*% mu - D / C * solve(sigma) %*% iota)"},{"path":"introduction-to-tidy-finance.html","id":"the-efficient-frontier","chapter":"2 Introduction to Tidy Finance","heading":"2.5 The efficient frontier","text":"two mutual fund separation theorem states soon two efficient portfolios (minimum variance portfolio efficient portfolio another required level expected returns like ), can characterize entire efficient frontier combining two portfolios. code implements construction efficient frontier, characterizes highest expected return achievable level risk. understand code better, make sure familiarize inner workings loop.Finally, simple visualize efficient frontier alongside two efficient portfolios within one, powerful figure using ggplot2. show easy setup , also add individual stocks call.\nblack line indicates efficient frontier: set portfolios mean-variance efficient investor choose . Compare performance relative individual assets (blue dots) - become clear diversifying yields massive performance gains (least long take parameters \\(\\Sigma\\) \\(\\mu\\) given).","code":"\nc <- seq(from = -0.4, to = 1.9, by = 0.01) # Some values for a linear combination of two efficient portfolio weights\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\n\nfor (i in seq_along(c)) { # A for loop\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights # A portfolio of minimum variance and efficient portfolio\n  res$mu[i] <- 12 * 100 * t(w) %*% mu # Portfolio expected return (annualized, in percent)\n  res$sd[i] <- 12 * 10 * sqrt(t(w) %*% sigma %*% w) # Portfolio volatility (annualized, in percent)\n}\n# Visualize the efficient frontier\nres %>%\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() + # Plot all sd/mu portfolio combinations\n  geom_point(\n    data = res %>% filter(c %in% c(0, 1)),\n    color = \"red\",\n    size = 4\n  ) + # locate the minimum variance and efficient portfolio\n  geom_point(\n    data = tibble(mu = 12 * 100 * mu, sd = 12 * 10 * sqrt(diag(sigma))),\n    aes(y = mu, x = sd), color = \"blue\", size = 1\n  ) + # locate the individual assets\n  theme_bw() + # make the plot a bit nicer\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Dow Jones asset returns and efficient frontier\",\n    subtitle = \"Red dots indicate the location of the minimum variance and efficient tangency portfolio\"\n  )"},{"path":"introduction-to-tidy-finance.html","id":"exercises","chapter":"2 Introduction to Tidy Finance","heading":"2.6 Exercises","text":"Download daily prices another stock market ticker choice Yahoo!Finance tq_get tidyquant package. Plot two time series ticker’s un-adjusted adjusted closing prices. Explain differences.Compute daily net returns asset visualize distribution daily returns histogram. Also, use geom_vline() add dashed red line indicates 5% quantile daily returns within histogram. Compute summary statistics (mean, standard deviation, minimum maximum) daily returnsTake code generalize can perform computations arbitrary vector tickers (e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\")). Automate download, plot price time series, create table return summary statistics arbitrary number assets.Consider research question: days high aggregate trading volume often also days large absolute price changes? Find appropriate visualization analyze question.Compute monthly returns downloaded stock market prices. Compute vector historical average returns sample variance-covariance matrix. compute minimum variance portfolio weights portfolio volatility average returns, visualize mean-variance efficient frontier. Choose one assets identify portfolio yields historical volatility achieves highest possible average return.efficient frontier characterizes portfolios highest expected return different levels risk, .e., standard deviation. Identify portfolio highest expected return per standard deviation. Hint: ratio expected return standard deviation important concept Finance.","code":""},{"path":"accessing-managing-financial-data.html","id":"accessing-managing-financial-data","chapter":"3 Accessing & Managing Financial Data","heading":"3 Accessing & Managing Financial Data","text":"chapter, propose way organize financial data. Everybody, experience data, familiar storing data form various data formats like CSV, XLS, XLSX delimited value stores. Reading saving data can become cumbersome case using different data formats, across different projects, well across different programming languages. Moreover, storing data delimited files often leads problems respect column type consistency. instance, date-type columns frequently lead inconsistencies across different data formats programming languages.chapter shows import different data sets (data comes application programming interface (API) Yahoo!Finance, downloaded standard .csv files, .xlsx file stored public Google drive repositories, SQL database connection). store data one database makes easy retrieve share data later .First, load global packages use throughout chapter. load packages section needed later .Moreover, initially define date range fetch store financial data. case need another time frame, simply need adjust dates. data starts 1960 since asset pricing studies use data 1962 .","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nstart_date <- as.Date(\"1960-01-01\")\nend_date <- as.Date(\"2020-12-31\")"},{"path":"accessing-managing-financial-data.html","id":"downloading-fama-french-data","chapter":"3 Accessing & Managing Financial Data","heading":"3.1 Downloading Fama-French Data","text":"start downloading famous Fama-French factors portfolios commonly used empirical asset pricing. Fortunately, neat package Nelson Areal allows us easily access data: frenchdata package provides functions download read data sets Prof. Kenneth French finance data library.can use main function package download monthly Fama-French factors. Note manual work correctly parse columns scale appropriately raw Fama-French data comes unpractical data format. precise descriptions variables suggest consult Prof. Kenneth French finance data library directly.function, straight-forward also download corresponding daily Fama-French factors.subsequent chapter, also use 10 monthly industry portfolios, let us fetch data, .worth take look available portfolio return time series Kenneth French homepage. package makes easy calling frenchdata::get_french_data_list().","code":"\nlibrary(frenchdata)\nfactors_ff_monthly <- download_french_data(\"Fama/French 3 Factors\")$subsets$data[[1]] %>%\n  transmute(\n    month = floor_date(ymd(paste0(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) %>%\n  filter(month >= start_date & month <= end_date)\nfactors_ff_daily <- download_french_data(\"Fama/French 3 Factors [Daily]\")$subsets$data[[1]] %>%\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) %>%\n  filter(date >= start_date & date <= end_date)\nindustries_ff_monthly <- download_french_data(\"10 Industry Portfolios\")$subsets$data[[1]] %>%\n  mutate(month = floor_date(ymd(paste0(date, \"01\")), \"month\")) %>%\n  mutate(across(where(is.numeric), ~ . / 100)) %>%\n  select(month, everything(), -date) %>%\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"the-q-factors-download-and-read-in-.csv-files","chapter":"3 Accessing & Managing Financial Data","heading":"3.2 The q-factors: download and read-in .csv files","text":"recent years, academic discourse experienced rise alternative factor portfolios, e.g. form Hou, Xue, Zhang (2015) q-factor model. refer extended background information provided original authors information. q factor returns can downloaded directly authors homepage within read_csv().","code":"\nfactors_q_monthly <- read_csv(\"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2020.csv\") %>%\n  mutate(month = as.Date(paste(year, month, \"01\", sep = \"-\"))) %>%\n  select(-R_F, -R_MKT, -year) %>%\n  rename_with(~ gsub(\"R_\", \"\", .)) %>%\n  rename_with(~ str_to_lower(.)) %>%\n  mutate(across(-month, ~ . / 100)) %>%\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"macroeconomic-predictors-goyal-welchs-.xlsx-files","chapter":"3 Accessing & Managing Financial Data","heading":"3.3 Macroeconomic predictors: Goyal-Welchs .xlsx files","text":"next data source set macro variables often used predictors equity premium. Goyal & Welch (2007) comprehensively reexamine performance variables suggested academic literature good predictors equity premium. authors host data updated 2020 Amit Goyal’s website. Since data XLSX file stored public Google drive location, need additional packages access data directly R session.Usually, need authenticate interact Google drive directly R. Since data stored via public link, can proceed without authentication.drive_download() function googledrive package allows us download data store locally.Next, read data transform columns variables later use.Finally, reading macro predictors memory, remove raw data file temporary storage.","code":"\nlibrary(readxl)\nlibrary(googledrive)\ndrive_deauth()\ndrive_download(\"https://drive.google.com/file/d/1ACbhdnIy0VbCWgsnXkjcddiV8HF4feWv/view\",\n  path = \"data/macro_predictors.xlsx\",\n  overwrite = TRUE\n)\nmacro_predictors <- read_xlsx(\"data/macro_predictors.xlsx\", sheet = \"Monthly\") %>%\n  mutate(month = ym(yyyymm)) %>%\n  filter(month >= start_date & month <= end_date) %>%\n  mutate(across(where(is.character), as.numeric)) %>%\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend Yield\n    ep = log(E12) - log(Index), # Earnings Price ratio\n    de = log(D12) - log(E12), # Dividend Payout Ratio\n    tms = lty - tbl, # Term Spread\n    dfy = BAA - AAA\n  ) %>% # Default yield spread\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) %>%\n  drop_na()\nfile.remove(\"data/macro_predictors.xlsx\")## [1] TRUE"},{"path":"accessing-managing-financial-data.html","id":"setting-up-a-database","chapter":"3 Accessing & Managing Financial Data","heading":"3.4 Setting-Up a Database","text":"Now downloaded data web memory R session, let us set database store information future use. many ways set-organize database, depending use case. purpose, efficient ways use SQLite database C-language library implements small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note SQL (Structured Query Language) standard language accessing manipulating databases heavily inspired dplyr functions. refer tutorial information SQL.two packages make working SQLite R simple: RSQLite embeds SQLite database engine R dbplyr database back-end dplyr. packages allow set database remotely store tables use remote database tables -memory data frames automatically converting dplyr SQL. Check RSQLite dbplyr vignettes information.SQLite database super simple create. code really . Note use extended_types option enable date types storing fetching data, otherwise date columns stored integer values.Next, create remote table monthly Fama-French factor data.can use remote table -memory data frame building connection via tbl().dplyr calls evaluated lazily, data memory R session actually database work. see noticing output show number rows. fact, following code chunk fetches top 10 rows database printing.want whole table memory, need collect() .last couple code chunks really organize simple database! can also easily share SQLite database across devices programming languages.move next data source, let us also store four tables new SQLite database.now , need access data stored database follow 3 steps: ) Establish connection SQLite file, ii) call file want extract iii) collect . convenience, following steps show need compact fashion","code":"\nlibrary(RSQLite)\nlibrary(dbplyr)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\nfactors_ff_monthly %>%\n  dbWriteTable(tidy_finance, \"factors_ff_monthly\", ., overwrite = TRUE)\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\nfactors_ff_monthly_db %>%\n  select(month, rf)## # Source:   lazy query [?? x 2]\n## # Database: sqlite 3.37.0\n## #   [C:\\Users\\ncj140\\Dropbox\\Projects\\tidy_finance\\data\\tidy_finance.sqlite]\n##    month          rf\n##    <date>      <dbl>\n##  1 1960-01-01 0.0033\n##  2 1960-02-01 0.0029\n##  3 1960-03-01 0.0035\n##  4 1960-04-01 0.0019\n##  5 1960-05-01 0.0027\n##  6 1960-06-01 0.0024\n##  7 1960-07-01 0.0013\n##  8 1960-08-01 0.0017\n##  9 1960-09-01 0.0016\n## 10 1960-10-01 0.0022\n## # ... with more rows\nfactors_ff_monthly_db %>%\n  select(month, rf) %>%\n  collect()## # A tibble: 732 x 2\n##    month          rf\n##    <date>      <dbl>\n##  1 1960-01-01 0.0033\n##  2 1960-02-01 0.0029\n##  3 1960-03-01 0.0035\n##  4 1960-04-01 0.0019\n##  5 1960-05-01 0.0027\n##  6 1960-06-01 0.0024\n##  7 1960-07-01 0.0013\n##  8 1960-08-01 0.0017\n##  9 1960-09-01 0.0016\n## 10 1960-10-01 0.0022\n## # ... with 722 more rows\nfactors_ff_daily %>%\n  dbWriteTable(tidy_finance, \"factors_ff_daily\", ., overwrite = TRUE)\n\nindustries_ff_monthly %>%\n  dbWriteTable(tidy_finance, \"industries_ff_monthly\", ., overwrite = TRUE)\n\nfactors_q_monthly %>%\n  dbWriteTable(tidy_finance, \"factors_q_monthly\", ., overwrite = TRUE)\n\nmacro_predictors %>%\n  dbWriteTable(tidy_finance, \"macro_predictors\", ., overwrite = TRUE)\n# Minimal setup to load data into your R session memory from a fresh session\nlibrary(tidyverse)\nlibrary(RSQLite)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE) # Connection sqlite file\ndbListTables(tidy_finance) # Check in case you do not know the the tables names within the SQlite database\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\") # Call the desired file\nfactors_q_monthly <- factors_q_monthly %>% collect() # Collect the files"},{"path":"accessing-managing-financial-data.html","id":"accessing-wrds","chapter":"3 Accessing & Managing Financial Data","heading":"3.5 Accessing WRDS","text":"Wharton Research Data Services (WRDS) widely used source asset firm-specific financial data used academic context. WRDS data platform provides data validation, flexible delivery options access many different data sources. data WRDS also organized SQL database, although use PostgreSQL engine. database engine just easy handle R SQLite. use RPostgres package establish connection WRDS database. Note also use odbc package connect PostgreSQL database, need install appropriate drivers . RPostgres already contains suitable driver.establish connection, use following function. Note need replace user password fields credentials. defined system variables purpose book obviously want share credentials rest world.","code":"\nlibrary(RPostgres)\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)"},{"path":"accessing-managing-financial-data.html","id":"downloading-and-preparing-crsp","chapter":"3 Accessing & Managing Financial Data","heading":"3.6 Downloading and Preparing CRSP","text":"Center Research Security Prices (CRSP) provides widely used data US stocks. use wrds connection object just created first access monthly CRSP return data. Actually, need 3 tables get desired data: () CRSP monthly security file,identifying information,(iii) delisting information.use three remote tables fetch data want put local database. Just , idea let WRDS database work just download data actually need. apply common filters data selection criteria narrow data interest. can read great textbook Bali, Engel & Murray (2016) (BEM) extensive discussion filters apply code .Now, relevant monthly return data memory proceed preparing data future analyses. perform preparation step current stage since want avoid executing mutations every time use data subsequent chapters.first additional variable create market capitalization (mktcap). Note keep market cap millions US dollars just convenience (want print huge numbers figures tables).next variable frequently use one-month lagged market capitalization. Lagged market capitalization typically used compute value-weighted portfolios, demonstrate later chapter. simple consistent way add column lagged market cap values add one month observation join information monthly CRSP data.wonder simple use lag() function, e.g. via crsp_monthly %>% group_by(permno) %>% mutate(mktcap_lag = lag(mktcap)) take look exercises.\nNext, follow BEM transforming listing exchange codes explicit exchange names.Similarly, transform industry codes industry descriptions following BEM.also construct returns adjusted delistings described BEM. transformation, can drop delisting returns codes.Next, compute excess returns subtracting monthly risk-free rate provided Fama-French data. base analyses excess returns, can drop adjusted returns risk-free rate tibble.Since excess returns market capitalization crucial analyses, can safely exclude observations missing returns market capitalization.Finally, store monthly CRSP file database.","code":"\nmsf_db <- tbl(wrds, in_schema(\"crsp\", \"msf\"))\nmsf_db## # Source:   table<\"crsp\".\"msf\"> [?? x 21]\n## # Database: postgres [svoigt@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n##    cusip   permno permco issuno hexcd hsiccd date        bidlo askhi   prc   vol\n##    <chr>    <dbl>  <dbl>  <dbl> <dbl>  <dbl> <date>      <dbl> <dbl> <dbl> <dbl>\n##  1 683916~  10000   7952  10396     3   3990 1985-12-31 NA     NA    NA       NA\n##  2 683916~  10000   7952  10396     3   3990 1986-01-31 -2.5   -4.44 -4.38  1771\n##  3 683916~  10000   7952  10396     3   3990 1986-02-28 -3.25  -4.38 -3.25   828\n##  4 683916~  10000   7952  10396     3   3990 1986-03-31 -3.25  -4.44 -4.44  1078\n##  5 683916~  10000   7952  10396     3   3990 1986-04-30 -4     -4.31 -4      957\n##  6 683916~  10000   7952  10396     3   3990 1986-05-30 -3.06  -4.22 -3.11  1074\n##  7 683916~  10000   7952  10396     3   3990 1986-06-30 -2.91  -3.30 -3.09  1069\n##  8 683916~  10000   7952  10396     3   3990 1986-07-31 -2.59  -3.44 -2.84  1163\n##  9 683916~  10000   7952  10396     3   3990 1986-08-29 -1.03  -2.62 -1.09  3049\n## 10 683916~  10000   7952  10396     3   3990 1986-09-30 -0.969 -1.28 -1.03  3551\n## # ... with more rows, and 10 more variables: ret <dbl>, bid <dbl>, ask <dbl>,\n## #   shrout <dbl>, cfacpr <dbl>, cfacshr <dbl>, altprc <dbl>, spread <dbl>,\n## #   altprcdt <date>, retx <dbl>\nmsenames_db <- tbl(wrds, in_schema(\"crsp\", \"msenames\"))\nmsenames_db## # Source:   table<\"crsp\".\"msenames\"> [?? x 21]\n## # Database: postgres [svoigt@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n##    permno namedt     nameendt   shrcd exchcd siccd ncusip   ticker comnam shrcls\n##     <dbl> <date>     <date>     <dbl>  <dbl> <dbl> <chr>    <chr>  <chr>  <chr> \n##  1  10000 1986-01-07 1986-12-03    10      3  3990 68391610 OMFGA  OPTIM~ A     \n##  2  10000 1986-12-04 1987-03-09    10      3  3990 68391610 OMFGA  OPTIM~ A     \n##  3  10000 1987-03-10 1987-06-11    10      3  3990 68391610 OMFGA  OPTIM~ A     \n##  4  10001 1986-01-09 1993-11-21    11      3  4920 39040610 GFGC   GREAT~ <NA>  \n##  5  10001 1993-11-22 2004-06-09    11      3  4920 29274A10 EWST   ENERG~ <NA>  \n##  6  10001 2004-06-10 2004-10-18    11      3  4920 29274A10 EWST   ENERG~ <NA>  \n##  7  10001 2004-10-19 2004-12-26    11      3  4920 29274A10 EWST   ENERG~ <NA>  \n##  8  10001 2004-12-27 2008-02-04    11      3  4920 29274A10 EWST   ENERG~ <NA>  \n##  9  10001 2008-02-05 2008-03-04    11      3  4920 29274A20 EWST   ENERG~ <NA>  \n## 10  10001 2008-03-05 2009-08-03    11      3  4920 29274A20 EWST   ENERG~ <NA>  \n## # ... with more rows, and 11 more variables: tsymbol <chr>, naics <chr>,\n## #   primexch <chr>, trdstat <chr>, secstat <chr>, permco <dbl>, compno <dbl>,\n## #   issuno <dbl>, hexcd <dbl>, hsiccd <dbl>, cusip <chr>\nmsedelist_db <- tbl(wrds, in_schema(\"crsp\", \"msedelist\"))\nmsedelist_db## # Source:   table<\"crsp\".\"msedelist\"> [?? x 19]\n## # Database: postgres [svoigt@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n##    permno dlstdt     dlstcd nwperm nwcomp nextdt      dlamt   dlretx   dlprc\n##     <dbl> <date>      <dbl>  <dbl>  <dbl> <date>      <dbl>    <dbl>   <dbl>\n##  1  10000 1987-06-11    560      0      0 1987-06-12  0.219  0        -0.219\n##  2  10001 2017-08-03    233      0      0 NA         13.1    0.0116    0    \n##  3  10002 2013-02-15    231  35263   1658 NA          3.01   0.0460    0    \n##  4  10003 1995-12-15    231  10569   8477 NA          5.45   0.0137    0    \n##  5  10005 1991-07-11    560      0      0 1991-07-12  0.141  0.125    -0.141\n##  6  10006 1984-06-28    233      0      0 NA         54.5    0.0356    0    \n##  7  10007 1990-10-16    560      0      0 1990-10-17  0.406 -0.133    -0.406\n##  8  10008 1988-11-21    551      0      0 1988-11-22 28      0.00901 -28    \n##  9  10009 2000-11-03    233      0  16060 NA         33.2    0.00567   0    \n## 10  10010 1995-08-28    231  23588   5398 NA          7.26  -0.113     0    \n## # ... with more rows, and 10 more variables: dlpdt <date>, dlret <dbl>,\n## #   permco <dbl>, compno <dbl>, issuno <dbl>, hexcd <dbl>, hsiccd <dbl>,\n## #   cusip <chr>, acperm <dbl>, accomp <dbl>\ncrsp_monthly <- msf_db %>%\n  # Keep only data in time window of interest\n  filter(date >= start_date & date <= end_date) %>%\n  # Keep only relevant share codes\n  inner_join(msenames_db %>%\n    filter(shrcd %in% c(10, 11)) %>% # US listed stocks\n    select(permno, exchcd, siccd, namedt, nameendt), by = c(\"permno\")) %>%\n  # Check that the information is valid\n  filter(date >= namedt & date <= nameendt) %>%\n  # Add delisting information (i.e. delisting reason and return) by month\n  mutate(month = floor_date(date, \"month\")) %>%\n  left_join(msedelist_db %>%\n    select(permno, dlstdt, dlret, dlstcd) %>%\n    mutate(month = floor_date(dlstdt, \"month\")), by = c(\"permno\", \"month\")) %>%\n  # Keep only variables of interest\n  select(\n    permno, # Security identifier\n    date, # Date of the observation\n    month, # Month of the observation\n    ret, # Return\n    shrout, # Shares outstanding (in thousands)\n    altprc, # Last traded price in a month\n    exchcd, # Exchange code\n    siccd, # Industry code\n    dlret, # Delisting return\n    dlstcd # Delisting code\n  ) %>%\n  mutate(\n    month = as.Date(month),\n    shrout = shrout * 1000\n  ) %>%\n  collect()\ncrsp_monthly <- crsp_monthly %>%\n  mutate(\n    mktcap = abs(shrout * altprc) / 1000000, # market cap in millions of dollars\n    mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap)\n  ) # 0 market cap makes conceptually no sense, so we set it to missing\nmktcap_lag <- crsp_monthly %>%\n  mutate(month = month %m+% months(1)) %>% # Add one month (%m+% takes care of date subtleties)\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(mktcap_lag, by = c(\"permno\", \"month\"))\ncrsp_monthly <- crsp_monthly %>%\n  mutate(exchange = case_when(\n    exchcd %in% c(1, 31) ~ \"NYSE\",\n    exchcd %in% c(2, 32) ~ \"AMEX\",\n    exchcd %in% c(3, 33) ~ \"NASDAQ\",\n    TRUE ~ \"Other\"\n  ))\ncrsp_monthly <- crsp_monthly %>%\n  mutate(industry = case_when(\n    siccd >= 1 & siccd <= 999 ~ \"Agriculture\",\n    siccd >= 1000 & siccd <= 1499 ~ \"Mining\",\n    siccd >= 1500 & siccd <= 1799 ~ \"Construction\",\n    siccd >= 2000 & siccd <= 3999 ~ \"Manufacturing\",\n    siccd >= 4000 & siccd <= 4899 ~ \"Transportation\",\n    siccd >= 4900 & siccd <= 4999 ~ \"Utilities\",\n    siccd >= 5000 & siccd <= 5199 ~ \"Wholesale\",\n    siccd >= 5200 & siccd <= 5999 ~ \"Retail\",\n    siccd >= 6000 & siccd <= 6799 ~ \"Finance\",\n    siccd >= 7000 & siccd <= 8999 ~ \"Services\",\n    siccd >= 9000 & siccd <= 9999 ~ \"Public\",\n    TRUE ~ \"Missing\"\n  ))\ncrsp_monthly <- crsp_monthly %>%\n  mutate(ret_adj = case_when(\n    is.na(dlstcd) ~ ret,\n    !is.na(dlstcd) & !is.na(dlret) ~ dlret,\n    dlstcd %in% c(500, 520, 580, 584) |\n      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,\n    dlstcd == 100 ~ ret,\n    TRUE ~ -1\n  )) %>%\n  select(-c(dlret, dlstcd))\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly %>% select(month, rf), by = \"month\") %>%\n  mutate(\n    ret_excess = ret_adj - rf,\n    # Ensure that excess returns are bounded by -1 from below\n    ret_excess = pmax(ret_excess, -1)\n  ) %>%\n  select(-ret_adj, -rf)\ncrsp_monthly <- crsp_monthly %>%\n  drop_na(ret_excess, mktcap, mktcap_lag)\ncrsp_monthly %>%\n  dbWriteTable(tidy_finance, \"crsp_monthly\", ., overwrite = TRUE)"},{"path":"accessing-managing-financial-data.html","id":"the-crsp-sample","chapter":"3 Accessing & Managing Financial Data","heading":"3.7 The CRSP sample","text":"move data sources, let us look descriptive statistics CRSP sample main source stock returns. next figure shows monthly number securities listing exchange time. NYSE longest history data, NASDAQ exhibits considerable large number stocks. number stocks AMEX decreasing steadily last couple decades. end 2020, 2300 stocks NASDAQ, 1247 NYSE, 148 AMEX 1 belong category.\nNext, look aggregate market capitalization respective listing exchanges. ensure look meaningful data comparable time, adjust nominal values inflation. use already familiar tidyquant package fetch consumer price index (CPI) data Federal Reserve Economic Data (FRED).CPI data might come handy point, also put local database.fact, can use tables database calculate aggregate market caps listing exchange plot just memory. values end year(end_date) dollars ensure inter-temporal comparability. NYSE listed stocks far largest market capitalization, followed NASDAQ listed stocks.Next, look descriptive statistics industry. figure plots number stocks sample SIC industry classifiers. sample period, largest share stocks apparently Manufacturing albeit number peaked somewhere 90s. number firms associated public administration seems category rise recent years, even surpassing Manufacturing end sample period.","code":"\ncrsp_monthly %>%\n  count(exchange, date) %>%\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma) +\n  theme_bw()\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date, to = end_date\n) %>%\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(crsp_monthly$month)]\n  )\ncpi_monthly %>%\n  dbWriteTable(tidy_finance, \"cpi_monthly\", ., overwrite = TRUE)\ntbl(tidy_finance, \"crsp_monthly\") %>%\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), by = \"month\") %>%\n  group_by(month, exchange) %>%\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi\n  ) %>%\n  collect() %>%\n  mutate(month = as.Date(month)) %>%\n  ggplot(aes(x = month, y = mktcap / 1000, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market value (billions of Dec 2020 Dollars) by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma) +\n  theme_bw()\ncrsp_monthly_industry <- crsp_monthly %>%\n  left_join(cpi_monthly, by = \"month\") %>%\n  group_by(month, industry) %>%\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  ) %>%\n  ungroup()\n\ncrsp_monthly_industry %>%\n  ggplot(aes(x = month, y = securities, color = industry, linetype = industry)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma) +\n  theme_bw()\ncrsp_monthly_industry %>%\n  ggplot(aes(x = month, y = mktcap / 1000, color = industry, linetype = industry)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market value (billions of Dec 2020 Dollars) by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma) +\n  theme_bw()"},{"path":"accessing-managing-financial-data.html","id":"accessing-daily-crsp-data","chapter":"3 Accessing & Managing Financial Data","heading":"3.8 Accessing daily CRSP data","text":"turn Compustat data, also want provide proposal downloading daily CRSP data. monthly data typically fits easily memory can downloaded meaningful amount time, usually true daily return data. daily CRSP data file substantially larger monthly data can easily exceed 20GB. two important implications: hold daily return data memory (hence possible copy entire dataset local database) experience download usually crashes (never stops) much data WRDS cloud prepare send R session.simple solution challenge. many ‘big data’ problems, can simply split big task many small tasks easy handle. , instead downloading data many stocks , download data small batches stock consecutively. operations can implemented loops download, prepare store data single stock iteration. operation might nonetheless take couple hours, patient either way. Eventually, end 68 million rows daily return data. Note store identifying information actually need, namely permno, date, month excess returns. thus ensure local database explode later can load full daily data memory.","code":"\ndsf_db <- tbl(wrds, in_schema(\"crsp\", \"dsf\"))\npermnos <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  distinct(permno) %>%\n  pull()\n\nprogress <- txtProgressBar(min = 0, max = length(permnos), initial = 0, style = 3)\nfor (j in 1:length(permnos)) {\n  permno_sub <- permnos[j]\n  crsp_daily_sub <- dsf_db %>%\n    filter(permno == permno_sub &\n      date >= start_date & date <= end_date) %>%\n    select(permno, date, ret) %>%\n    collect() %>%\n    drop_na()\n\n  if (nrow(crsp_daily_sub)) {\n    crsp_daily_sub <- crsp_daily_sub %>%\n      mutate(month = floor_date(date, \"month\")) %>%\n      left_join(factors_ff_daily %>%\n        select(date, rf), by = \"date\") %>%\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) %>%\n      select(permno, date, month, ret_excess)\n\n    if (j == 1) {\n      overwrite <- TRUE\n      append <- FALSE\n    } else {\n      overwrite <- FALSE\n      append <- TRUE\n    }\n\n    crsp_daily_sub %>%\n      dbWriteTable(tidy_finance, \"crsp_daily\", ., overwrite = overwrite, append = append)\n  }\n  setTxtProgressBar(progress, j)\n}\nclose(progress)\n\ncrsp_daily_db <- tbl(tidy_finance, \"crsp_daily\")\ncrsp_daily_db %>% count() # contains 68,895,667 rows"},{"path":"accessing-managing-financial-data.html","id":"merging-crsp-with-compustat","chapter":"3 Accessing & Managing Financial Data","heading":"3.9 Merging CRSP with Compustat","text":"seen , CRSP data contains stock-specific information, need tap another source firm financials. financial information important source information use portfolio analyses later . commonly used source firm financial information Compustat, global data provider provides financial, statistical market information active inactive companies throughout world.Unfortunately, CRSP Compustat use different keys identify stocks firms. CRSP uses permno stocks, Compustat uses gvkey identify firms. Fortunately, curated matching table WRDS allows us merger CRSP Compustat, create connection remote table.However, need make sure keep relevant links, following description outlined BEM.use link table create new table mapping stock identifier, firm identifier month. add links Compustat gvkey monthly stock data.last step, update prepared monthly CRSP file linking information local database.","code":"\nccmxpf_linktable_db <- tbl(wrds, \"ccmxpf_linktable\")\nccmxpf_linktable <- ccmxpf_linktable_db %>%\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) %>%\n  select(permno = lpermno, gvkey, linkdt, linkenddt) %>%\n  collect() %>%\n  # Currently active links have no end date\n  mutate(linkenddt = replace_na(linkenddt, Sys.Date()))\nccmxpf_linktable## # A tibble: 30,159 x 4\n##    permno gvkey  linkdt     linkenddt \n##     <dbl> <chr>  <date>     <date>    \n##  1  25881 001000 1970-11-13 1978-06-30\n##  2  10015 001001 1983-09-20 1986-07-31\n##  3  10023 001002 1972-12-14 1973-06-05\n##  4  10031 001003 1983-12-07 1989-08-16\n##  5  54594 001004 1972-04-24 2022-01-24\n##  6  61903 001005 1973-01-31 1983-01-31\n##  7  10058 001007 1973-10-01 1979-01-30\n##  8  10058 001007 1979-01-31 1984-09-28\n##  9  10066 001008 1983-08-25 1987-10-30\n## 10  10074 001009 1982-01-18 1996-03-13\n## # ... with 30,149 more rows\nccm_links <- crsp_monthly %>%\n  inner_join(ccmxpf_linktable, by = \"permno\") %>%\n  filter(!is.na(gvkey) & (date >= linkdt & date <= linkenddt)) %>%\n  select(permno, gvkey, date)\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(ccm_links, by = c(\"permno\", \"date\"))\ncrsp_monthly %>%\n  dbWriteTable(tidy_finance, \"crsp_monthly\", ., overwrite = TRUE)"},{"path":"accessing-managing-financial-data.html","id":"preparing-compustat-data","chapter":"3 Accessing & Managing Financial Data","heading":"3.10 Preparing Compustat Data","text":"mentioned , use firm fundamentals provided Compustat WRDS. Compustat North America database U.S. Canadian fundamental market information active inactive publicly held companies. companies, annual history available back 1950 quarterly well monthly history goes back 1962.applications, use annual information provided via funda table.follow typical filter conventions pull data actually need.Next, calculate book value preferred stock equity inspired variable definition Ken French’s data library.keep last available information firm-year group. Note datadate refers time corresponding financial refers , date made available public. wonder can multiple observations firm year, check exercises.already done preparing firm fundamentals, can store local database.move next data source, let us look interesting descriptive data. book value equity plays crucial role many asset pricing applications, actually interesting know many stocks even piece information. next figure hence plots share securities book equity values exchange. turns coverage pretty bad AMEX NYSE listed stocks 60s, hovers around 80% periods thereafter. can ignore erratic coverage securities belong category since handful anyway sample.","code":"\nfunda_db <- tbl(wrds, \"funda\")\ncompustat <- funda_db %>%\n  filter(\n    # Get only industrial fundamental data (i.e. ignore financial services)\n    indfmt == \"INDL\" &\n      # Get data in standard format (i.e. consolidated information in standard presentation)\n      datafmt == \"STD\" &\n      consol == \"C\" &\n      # Get only data in the desired time window\n      datadate >= start_date & datadate <= end_date\n  ) %>%\n  # Select only relevant columns\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk # Preferred stock par value\n  ) %>%\n  # Fetch data from server into memory\n  collect()\ncompustat <- compustat %>%\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    # Negative or zero equity makes no sense because the firm would be dead\n    be = if_else(be <= 0, as.numeric(NA), be)\n  )\ncompustat <- compustat %>%\n  mutate(year = year(datadate)) %>%\n  group_by(gvkey, year) %>%\n  filter(datadate == max(datadate)) %>%\n  ungroup()\ncompustat %>%\n  dbWriteTable(tidy_finance, \"compustat\", ., overwrite = TRUE)\ncrsp_monthly %>%\n  group_by(permno, year = year(month)) %>%\n  filter(date == max(date)) %>%\n  ungroup() %>%\n  left_join(compustat, by = c(\"gvkey\", \"year\")) %>%\n  group_by(exchange, year) %>%\n  summarize(share = n_distinct(permno[!is.na(be)]) / n_distinct(permno)) %>%\n  ggplot(aes(x = year, y = share, color = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"End-of-year share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 1))"},{"path":"accessing-managing-financial-data.html","id":"managing-sqlite-databases","chapter":"3 Accessing & Managing Financial Data","heading":"3.11 Managing SQLite Databases","text":"drop database objects tables delete data tables, database file size remains unchanged SQLite just marks deleted objects free reserves future uses. result, size database file always grows size.optimize database file, can run VACUUM command database rebuild database free unused space. can execute command database using dbSendQuery() function.VACUUM command actually couple clean steps, can read tutorial.","code":"\ndbSendQuery(tidy_finance, \"VACUUM\")## <SQLiteResult>\n##   SQL  VACUUM\n##   ROWS Fetched: 0 [complete]\n##        Changed: 0"},{"path":"beta-estimation.html","id":"beta-estimation","chapter":"4 Beta Estimation","heading":"4 Beta Estimation","text":"section, introduce important concept financial economics: exposure individual stock changes market portfolio. According Capital Asset Pricing Model (CAPM), cross-sectional variation expected asset returns function covariance return asset return market portfolio – beta. section proposes estimation procedure -called market betas. go details foundations market beta, simply refer treatment CAPM information. importantly, provide details functions use compute results.use following packages throughout section:","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(slider)\nlibrary(scales)\nlibrary(furrr)"},{"path":"beta-estimation.html","id":"estimate-beta-using-monthly-returns","chapter":"4 Beta Estimation","heading":"4.1 Estimate beta using monthly returns","text":"estimation procedure based rolling window estimation may use either monthly daily returns different window lengths. First, let us start monthly data prepared earlier chapter.estimate CAPM equation\n\\[\nr_{, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{, t}\n\\]\nregress excess stock returns ret_excess excess returns market portfolio mkt_excess.\nR provides simple solution estimate (linear) models function lm(). lm() requires formula input specific compact symbolic form. expression form y ~ model interpreted specification response y modeled linear predictor specified symbolically model. model consists series terms separated + operators. addition standard linear models, lm() provides lot flexibility. check documentation information. start, restrict data time series observations CRSP correspond Apple’s stock (permno Apple 14593) compute \\(\\alpha_i\\) well \\(\\beta_i\\).lm() returns object class lm contains information usually care linear models. summary() returns easy understand overview estimated parameters. coefficients(fit) return estimated coefficients. output indicates Apple moves excessively market estimated \\(\\beta_i\\) one (\\(\\hat\\beta_i\\) = 1.4).Next, scale estimation \\(\\beta_i\\) whole different level perform rolling window estimations entire CRSP sample. following function implements regression data frame contains least min_obs observations avoid huge fluctuations case time-series just short. condition violated, function returns missing value.Next, define function rolling estimation. perform rolling window estimation, use slider package Davis Vaughan, provides family sliding window functions similar purrr::map() already seen previous sections. importantly, slide_period function able handle months window input straight-forward manner. thus avoid using time-series package (e.g., zoo) converting data fit package functions, rather stay tidyverse.following function takes input data slides across month vector, considering total months months. demonstrate , can apply function daily returns data.attack whole CRSP sample, let us focus couple well known examples.want estimate rolling betas Apple, can simple use mutate().actually quite simple perform rolling window estimation arbitrary number fo stocks visualize following code chunk.\nEven though now just apply function using group_by() whole CRSP sample, advise computationally quite expensive. Remember perform rolling window estimations across stocks time periods. However, estimation problem ideal scenario employ power parallelization. Parallelization means split tasks perform rolling window estimations across different workers (cores local machine). turns quite easy implement small addition already learned using map() functions.First, nest data permno. Nested data means now list permnos corresponding time series data.First note simply use map() across permnos get results .instead want perform estimations rolling betas different stocks parallel. can use flexibility future package use define want perform parallel estimation. Windows machine, makes sense define multisession means separate R processes running background machine perform estimations. check documentation plan(), can see ways resolve parallelization.Using 8 cores, estimation sample around 25k stocks takes around 20 mins. course can speed things considerably cores available share workload. Note difference code ? need replace map future_map.look descriptive statistics beta estimates, implement estimation daily data well. Depending application, might either use longer horizon beta estimates based monthly data shorter horizon estimates based daily returns.","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  select(permno, month, industry, ret_excess, mkt_excess)\n\ncrsp_monthly## # A tibble: 3,225,253 x 5\n##    permno month      industry      ret_excess mkt_excess\n##     <dbl> <date>     <chr>              <dbl>      <dbl>\n##  1  10000 1986-02-01 Manufacturing    -0.262      0.0713\n##  2  10000 1986-03-01 Manufacturing     0.359      0.0488\n##  3  10000 1986-04-01 Manufacturing    -0.104     -0.0131\n##  4  10000 1986-05-01 Manufacturing    -0.228      0.0462\n##  5  10000 1986-06-01 Manufacturing    -0.0102     0.0103\n##  6  10000 1986-07-01 Manufacturing    -0.0860    -0.0645\n##  7  10000 1986-08-01 Manufacturing    -0.620      0.0607\n##  8  10000 1986-09-01 Manufacturing    -0.0616    -0.086 \n##  9  10000 1986-10-01 Manufacturing    -0.247      0.0466\n## 10  10000 1986-11-01 Manufacturing     0.0561     0.0117\n## # ... with 3,225,243 more rows\nfit <- lm(ret_excess ~ mkt_excess,\n  data = crsp_monthly %>%\n    filter(permno == \"14593\")\n)\n\nsummary(fit)## \n## Call:\n## lm(formula = ret_excess ~ mkt_excess, data = crsp_monthly %>% \n##     filter(permno == \"14593\"))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.51670 -0.06098  0.00090  0.06426  0.39402 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 0.010511   0.005316   1.977   0.0486 *  \n## mkt_excess  1.400852   0.117479  11.924   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1151 on 478 degrees of freedom\n## Multiple R-squared:  0.2293, Adjusted R-squared:  0.2277 \n## F-statistic: 142.2 on 1 and 478 DF,  p-value: < 2.2e-16\nestimate_capm <- function(data, min_obs = 1) {\n  if (nrow(data) < min_obs) {\n    beta <- as.numeric(NA)\n  } else {\n    fit <- lm(ret_excess ~ mkt_excess, data = data)\n    beta <- as.numeric(fit$coefficients[2])\n  }\n  return(beta)\n}\nroll_capm_estimation <- function(data, months, min_obs) {\n  # Combine all rows into a single data frame (useful for daily data)\n  data <- bind_rows(data) %>%\n    arrange(month)\n\n  # Compute betas by sliding across months\n  betas <- slide_period_vec(\n    .x = data,\n    .i = data$month,\n    .period = \"month\",\n    .f = ~ estimate_capm(., min_obs),\n    .before = months - 1,\n    .complete = FALSE\n  )\n\n  # Return a tibble with months and beta estimates (useful for daily data)\n  tibble(\n    month = unique(data$month),\n    beta = betas\n  )\n}\nexamples <- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n)\nbeta_example <- crsp_monthly %>%\n  filter(permno == examples$permno[1]) %>%\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) %>%\n  drop_na()\nbeta_example## # A tibble: 433 x 6\n##    permno month      industry      ret_excess mkt_excess  beta\n##     <dbl> <date>     <chr>              <dbl>      <dbl> <dbl>\n##  1  14593 1984-12-01 Manufacturing     0.170      0.0184  2.05\n##  2  14593 1985-01-01 Manufacturing    -0.0108     0.0799  1.90\n##  3  14593 1985-02-01 Manufacturing    -0.152      0.0122  1.88\n##  4  14593 1985-03-01 Manufacturing    -0.112     -0.0084  1.89\n##  5  14593 1985-04-01 Manufacturing    -0.0467    -0.0096  1.90\n##  6  14593 1985-05-01 Manufacturing    -0.189      0.0509  1.76\n##  7  14593 1985-06-01 Manufacturing     0.0305     0.0127  1.76\n##  8  14593 1985-07-01 Manufacturing    -0.124     -0.0074  1.77\n##  9  14593 1985-08-01 Manufacturing    -0.0606    -0.0102  1.78\n## 10  14593 1985-09-01 Manufacturing     0.0440    -0.0454  1.71\n## # ... with 423 more rows\nbeta_examples <- crsp_monthly %>%\n  inner_join(examples, by = \"permno\") %>%\n  group_by(permno) %>%\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) %>%\n  ungroup() %>%\n  select(permno, company, month, beta_monthly = beta) %>%\n  drop_na()\n\nbeta_examples %>%\n  ggplot(aes(x = month, y = beta_monthly, color = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of monthly data\"\n  ) +\n  theme_bw()\ncrsp_monthly_nested <- crsp_monthly %>%\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_monthly_nested## # A tibble: 29,223 x 3\n##    permno industry      data              \n##     <dbl> <chr>         <list>            \n##  1  10000 Manufacturing <tibble [16 x 3]> \n##  2  10001 Utilities     <tibble [378 x 3]>\n##  3  10002 Finance       <tibble [324 x 3]>\n##  4  10003 Finance       <tibble [118 x 3]>\n##  5  10005 Mining        <tibble [65 x 3]> \n##  6  10006 Manufacturing <tibble [292 x 3]>\n##  7  10007 Services      <tibble [41 x 3]> \n##  8  10008 Manufacturing <tibble [33 x 3]> \n##  9  10009 Finance       <tibble [177 x 3]>\n## 10  10010 Manufacturing <tibble [114 x 3]>\n## # ... with 29,213 more rows\ncrsp_monthly_nested %>%\n  inner_join(examples, by = \"permno\") %>%\n  mutate(beta = map(data, ~ roll_capm_estimation(., months = 60, min_obs = 48))) %>%\n  unnest(c(beta)) %>%\n  select(permno, month, beta_monthly = beta) %>%\n  drop_na()## # A tibble: 1,362 x 3\n##    permno month      beta_monthly\n##     <dbl> <date>            <dbl>\n##  1  10107 1990-03-01         1.39\n##  2  10107 1990-04-01         1.38\n##  3  10107 1990-05-01         1.43\n##  4  10107 1990-06-01         1.43\n##  5  10107 1990-07-01         1.45\n##  6  10107 1990-08-01         1.44\n##  7  10107 1990-09-01         1.41\n##  8  10107 1990-10-01         1.42\n##  9  10107 1990-11-01         1.41\n## 10  10107 1990-12-01         1.41\n## # ... with 1,352 more rows\nplan(multisession, workers = availableCores())\nbeta_monthly <- crsp_monthly_nested %>%\n  mutate(beta = future_map(data, ~ roll_capm_estimation(., months = 60, min_obs = 48))) %>%\n  unnest(c(beta)) %>%\n  select(permno, month, beta_monthly = beta) %>%\n  drop_na()\nbeta_monthly## # A tibble: 2,070,653 x 3\n##    permno month      beta_monthly\n##     <dbl> <date>            <dbl>\n##  1  10001 1990-01-01       0.0974\n##  2  10001 1990-02-01       0.0967\n##  3  10001 1990-03-01       0.0962\n##  4  10001 1990-04-01       0.100 \n##  5  10001 1990-05-01       0.0810\n##  6  10001 1990-06-01       0.0811\n##  7  10001 1990-07-01       0.0793\n##  8  10001 1990-08-01       0.117 \n##  9  10001 1990-09-01       0.104 \n## 10  10001 1990-10-01       0.108 \n## # ... with 2,070,643 more rows"},{"path":"estimating-beta-using-daily-returns.html","id":"estimating-beta-using-daily-returns","chapter":"5 Estimating beta using daily returns","heading":"5 Estimating beta using daily returns","text":"First let us load daily CRSP data. Note sample quite huge compared monthly data, make sure enough memory availableWe also need daily Fama-French market excess returns.make sure keep relevant data.Just like , nest data permno easy parallelization.estimation looks like couple examples using map().Just safety sake completeness, tell session use multiple sessions parallelization.code chunk beta estimation using daily returns now looks similar one monthly data. Note now used 3 months daily data require least 50 observations reduce likelihood whack estimates. whole estimation takes around 30 mins using 8 cores 32gb memory.","code":"\ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\") %>%\n  collect()\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\") %>%\n  collect()\ncrsp_daily <- crsp_daily %>%\n  inner_join(factors_ff_daily, by = \"date\") %>%\n  select(permno, month, ret_excess, mkt_excess)\ncrsp_daily## # A tibble: 68,895,667 x 4\n##    permno month      ret_excess mkt_excess\n##     <dbl> <date>          <dbl>      <dbl>\n##  1  10000 1986-01-01   -0.0246     -0.0216\n##  2  10000 1986-01-01   -0.00025    -0.0117\n##  3  10000 1986-01-01   -0.00025    -0.0002\n##  4  10000 1986-01-01    0.0498      0.0028\n##  5  10000 1986-01-01    0.0474      0.0001\n##  6  10000 1986-01-01    0.0452      0.0079\n##  7  10000 1986-01-01    0.0432      0.0046\n##  8  10000 1986-01-01   -0.00025    -0.0017\n##  9  10000 1986-01-01   -0.00025    -0.0039\n## 10  10000 1986-01-01   -0.00025    -0.0067\n## # ... with 68,895,657 more rows\ncrsp_daily_nested <- crsp_daily %>%\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_daily_nested %>%\n  inner_join(examples, by = \"permno\") %>%\n  mutate(beta_daily = map(data, ~ roll_capm_estimation(., months = 3, min_obs = 50))) %>%\n  unnest(c(beta_daily)) %>%\n  select(permno, month, beta_daily = beta) %>%\n  drop_na()## # A tibble: 1,543 x 3\n##    permno month      beta_daily\n##     <dbl> <date>          <dbl>\n##  1  10107 1986-05-01      0.898\n##  2  10107 1986-06-01      0.906\n##  3  10107 1986-07-01      0.822\n##  4  10107 1986-08-01      0.900\n##  5  10107 1986-09-01      1.01 \n##  6  10107 1986-10-01      1.03 \n##  7  10107 1986-11-01      1.58 \n##  8  10107 1986-12-01      1.64 \n##  9  10107 1987-01-01      2.38 \n## 10  10107 1987-02-01      1.51 \n## # ... with 1,533 more rows\nplan(multisession, workers = availableCores())\nbeta_daily <- crsp_daily_nested %>%\n  mutate(beta_daily = future_map(data, ~ roll_capm_estimation(., months = 3, min_obs = 50))) %>%\n  unnest(c(beta_daily)) %>%\n  select(permno, month, beta_daily = beta) %>%\n  drop_na()\nbeta_daily## # A tibble: 3,231,788 x 3\n##    permno month      beta_daily\n##     <dbl> <date>          <dbl>\n##  1  10000 1986-03-01    0.603  \n##  2  10000 1986-04-01    0.00754\n##  3  10000 1986-05-01   -0.0514 \n##  4  10000 1986-06-01    0.0242 \n##  5  10000 1986-07-01    0.543  \n##  6  10000 1986-08-01    0.557  \n##  7  10000 1986-09-01    0.614  \n##  8  10000 1986-10-01    0.891  \n##  9  10000 1986-11-01    0.970  \n## 10  10000 1986-12-01    0.749  \n## # ... with 3,231,778 more rows"},{"path":"analysis-and-comparison-of-beta-estimates.html","id":"analysis-and-comparison-of-beta-estimates","chapter":"6 Analysis and comparison of beta estimates","heading":"6 Analysis and comparison of beta estimates","text":"typical value stock betas? get feeling, illustrate dispersion estimated \\(\\hat\\beta_i\\) across different industries across time . first figure shows typical business models across industries imply different exposure general market economy. However, barely firms exhibit negative exposure market factor.Next illustrate time-variation cross-section estimated betas. figure shows monthly deciles estimated betas (based monthly data) indicates interesting pattern: First, betas seem vary time sense periods clear trend across deciles. Second, sample exhibits periods dispersion across stocks increases sense lower decile decreases upper decile increases indicates stocks correlation market increases others decreases. Note also : Stocks negative betas extremely rare exception.compare difference daily monthly data, combine beta estimates single table use table plot comparison beta estimates example stocks.Finally, write estimates database can use later sections.Whenever perform kind estimation, also makes sense rough plausibility tests. simple check plot share stocks beta estimates time. simple descriptive helps us discover potential errors data preparation estimation procedure. instance, suppose huge gap output betas. case, go back check previous steps.also encourage everyone always look distributional summary statistics variables. can easily spot outliers weird distributions look tables.Finally, since two different estimators theoretical object, estimators least positively correlated (although clearly perfectly estimators based different sample periods).","code":"\ncrsp_monthly %>%\n  left_join(beta_monthly, by = c(\"permno\", \"month\")) %>%\n  drop_na(beta_monthly) %>%\n  group_by(industry, permno) %>%\n  summarise(beta = mean(beta_monthly)) %>% # Mean beta for each company\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Average beta estimates by industry\"\n  )\n# Calculate beta quantile\nbeta_monthly %>%\n  drop_na(beta_monthly) %>%\n  group_by(month) %>%\n  summarise(\n    x = quantile(beta_monthly, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1),\n    .groups = \"drop\"\n  ) %>%\n  ggplot(aes(x = month, y = x, color = as_factor(quantile))) +\n  geom_line() +\n  theme_bw() +\n  labs(\n    x = NULL, y = \"Beta\", color = NULL,\n    title = \"Distribution of estimated betas\",\n    subtitle = \"Monthly deciles for CRSP cross-section\"\n  )\nbeta <- beta_monthly %>%\n  full_join(beta_daily, by = c(\"permno\", \"month\")) %>%\n  arrange(permno, month)\n\nbeta %>%\n  inner_join(examples, by = \"permno\") %>%\n  pivot_longer(cols = c(beta_monthly, beta_daily)) %>%\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Comparison of beta estimates using 5 years of monthly and 3 months of daily data\"\n  ) +\n  theme_bw()## Warning: Removed 46 row(s) containing missing values (geom_path).\nbeta %>%\n  dbWriteTable(tidy_finance, \"beta\", ., overwrite = TRUE)\nbeta_long <- crsp_monthly %>%\n  left_join(beta, by = c(\"permno\", \"month\")) %>%\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long %>%\n  group_by(month, name) %>%\n  summarize(share = sum(!is.na(value)) / n()) %>%\n  ggplot(aes(x = month, y = share, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 1))\nbeta_long %>%\n  select(name, value) %>%\n  drop_na() %>%\n  group_by(name) %>%\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q25 = quantile(value, 0.25),\n    q50 = quantile(value, 0.50),\n    q75 = quantile(value, 0.75),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  )## # A tibble: 2 x 11\n##   name          mean    sd   min    q05   q25   q50   q75   q95   max       n\n##   <chr>        <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <int>\n## 1 beta_daily   0.743 0.925 -43.7 -0.452 0.203 0.679  1.22  2.22  56.6 3186483\n## 2 beta_monthly 1.10  0.711 -13.0  0.123 0.631 1.03   1.47  2.31  10.3 2070653\nbeta %>%\n  select(beta_daily, beta_monthly) %>%\n  cor(., use = \"complete.obs\")##              beta_daily beta_monthly\n## beta_daily    1.0000000    0.3222342\n## beta_monthly  0.3222342    1.0000000"},{"path":"univariate-sorts.html","id":"univariate-sorts","chapter":"7 Univariate Sorts","heading":"7 Univariate Sorts","text":"section, dive portfolio sorts - one widely used statistical methodologies empirical asset pricing. key application portfolio sorts examine whether one variables can predict future excess returns. general, idea sort individual stocks portfolios stocks, stocks within portfolio exhibit similar properties characteristic, firm size. different portfolios represent well-diversified investments differ level sorting variable. Differences performance attributed sorting variable.\nstart introducing univariate portfolio sorts (means sorting based one characteristic). Later, tackle bivariate sorting.univariate portfolio sort considers one sort variable \\(x_{t-1,}\\). , \\(\\) denotes stock \\(t-1\\) indicates characteristic known point time \\(t\\).\nobjective assess cross-sectional relation \\(x_{t-1,}\\) , typically, stock excess returns \\(r_{t,}\\) time \\(t\\) outcome variable. illustrate portfolio sorts work, use estimates market betas previous section sorting variable.","code":""},{"path":"univariate-sorts.html","id":"data-preparation","chapter":"7 Univariate Sorts","heading":"7.1 Data preparation","text":"current section relies following set packages.start loading required data database. particular, use monthly CRSP database asset universe. use Fama-French factor returns compute risk-adjusted performance (alpha) portfolios create. beta tibble asset betas, computed section .keep relevant data CRSP sample.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(scales)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>% \n  collect() \n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>% \n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") %>% \n  collect()\ncrsp_monthly <- crsp_monthly %>% \n  left_join(factors_ff_monthly, by = \"month\") %>% \n  select(permno, month, ret_excess, mkt_excess, mktcap_lag)\ncrsp_monthly## # A tibble: 3,225,253 x 5\n##    permno month      ret_excess mkt_excess mktcap_lag\n##     <dbl> <date>          <dbl>      <dbl>      <dbl>\n##  1  10000 1986-02-01    -0.262      0.0713      16.1 \n##  2  10000 1986-03-01     0.359      0.0488      12.0 \n##  3  10000 1986-04-01    -0.104     -0.0131      16.3 \n##  4  10000 1986-05-01    -0.228      0.0462      15.2 \n##  5  10000 1986-06-01    -0.0102     0.0103      11.8 \n##  6  10000 1986-07-01    -0.0860    -0.0645      11.7 \n##  7  10000 1986-08-01    -0.620      0.0607      10.8 \n##  8  10000 1986-09-01    -0.0616    -0.086        4.15\n##  9  10000 1986-10-01    -0.247      0.0466       3.91\n## 10  10000 1986-11-01     0.0561     0.0117       3.00\n## # ... with 3,225,243 more rows"},{"path":"univariate-sorts.html","id":"simple-sorts-by-market-beta","chapter":"7 Univariate Sorts","heading":"7.2 Simple sorts by market beta","text":"Next, merge sorting variable return data. use one-month lagged betas sorting variable ensure sorts rely information available point time create portfolios. lag stock beta one month, add one month current date join resulting information return data. procedure ensures month \\(t\\) information available month \\(t+1\\). (may tempted simply use call crsp_monthly %>% group_by(permno) %>% mutate(beta_lag = lag(beta))) instead. procedure, however, work non-explicit missing values time series).first step portfolio analysis calculate periodic breakpoints used group stocks portfolios. simplicity, start median single breakpoint. compute value-weighted returns two resulting portfolios means lagged market capitalizations determines weight.following figure shows monthly excess returns two portfolios.test whether long-short portfolio yields average positive negative excess returns, compute average return corresponding standard error. asset pricing literature, one typically uses Newey-West (1987) \\(t\\)-statistics (using six lags) test null hypothesis average portfolio excess returns equal zero. implement test, compute average return via lm() employ coeftest function.results indicate reject null hypothesis average returns equal zero. portfolio strategy using median breakpoint hence yield abnormal returns. finding surprising reconsider CAPM? certainly . CAPM yields high beta stocks yield higher expected returns. portfolio sort implicitly mimics investment strategy finances high beta stocks shorting low beta stocks. One therefore expect average excess returns yield risk-adjusted return risk-free rate.","code":"\nbeta_lag <- beta %>%\n  mutate(month = month %m+% months(1)) %>% \n  select(permno, month, beta_lag = beta_daily) %>%\n  drop_na()\n\ndata_beta <- crsp_monthly %>%\n  inner_join(beta_lag, by = c(\"permno\", \"month\"))\nbeta_portfolios <- data_beta %>%\n  group_by(month) %>%\n  mutate(breakpoint = median(beta_lag),\n         portfolio = case_when(beta_lag <= breakpoint ~ \"low\",\n                               beta_lag > breakpoint ~ \"high\")) %>%\n  group_by(month, portfolio) %>%\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\")\nbeta_portfolios %>% \n  ggplot(aes(x = month, y = ret, fill = portfolio)) +\n  geom_col() +\n  facet_wrap(~portfolio, ncol = 1) +\n  scale_y_continuous(labels = percent) + \n  labs(x = NULL, y = NULL, \n       title = \"Monthly beta portfolio excess returns using median as breakpoint\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\nbeta_longshort <- beta_portfolios %>%\n  pivot_wider(month, names_from = portfolio, values_from = ret) %>%\n  mutate(long_short = high - low) %>% \n  left_join(factors_ff_monthly, by = \"month\") \nfit <- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(fit, vcov = NeweyWest, lag = 6)## \n## t test of coefficients:\n## \n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept) -0.0001684  0.0010050 -0.1676    0.867"},{"path":"univariate-sorts.html","id":"functional-programming-for-portfolio-sorts","chapter":"7 Univariate Sorts","heading":"7.3 Functional programming for portfolio sorts","text":"Now take portfolio sorts next level. want able sort stocks arbitrary number portfolios. case, functional programming becomes handy: employ curly-curly operator give us flexibility concerning variable use sorting, denoted var. use quantile() compute breakpoints n_portfolios. , assign portfolios stocks using findInterval() function. output following function hence new column contains number portfolio stock ends .can use function easily sort stocks 10 portfolios month using lagged betas compute value-weighted returns portfolio. Note transform portfolio column factor variable provides convenience figure construction .","code":"\nassign_portfolio <- function(data, var, n_portfolios) {\n  # Calculate breakpoints\n  breakpoints <- data %>%\n    summarize(breakpoint = quantile({{ var }}, probs = seq(0, 1, length.out = n_portfolios + 1),\n                                    na.rm = TRUE)) %>%\n    pull(breakpoint) %>% \n    as.numeric()\n  \n  # Sort stocks into portfolios based on breakpoints\n  data %>%\n    mutate(portfolio = findInterval({{ var }}, breakpoints, all.inside = TRUE)) %>% \n    pull(portfolio)\n}\nbeta_portfolios <- data_beta %>%\n  group_by(month) %>%\n  mutate(portfolio = assign_portfolio(data = cur_data(), \n                                      var = beta_lag, \n                                      n_portfolios = 10),\n         portfolio = as.factor(portfolio)) %>%\n  group_by(portfolio, month) %>% \n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\")"},{"path":"univariate-sorts.html","id":"performance-evaluation","chapter":"7 Univariate Sorts","heading":"7.4 Performance evaluation","text":"next step, compute summary statistics beta portfolio. Namely, compute CAPM-adjusted alphas, beta beta portfolio, average returns.next figure illustrates CAPM alphas beta-sorted portfolios. shows low beta portfolios tend exhibit positive alphas, high beta portfolios exhibit negative alphas.CAPM predicts portfolios lie security market line (SML). slope SML equal market risk premium reflects risk-return trade-given time., resulting long-short strategy exhibit statistically significant returns.However, long-short portfolio yields statistically significant negative CAPM-adjusted alpha although, controlling effect beta, average excess stock returns zero according CAPM. results thus provide evidence support CAPM. negative value documented -called betting beta factor ((Frazzini2014?)). Betting beta corresponds strategy shorts high beta stocks takes (levered) long position low beta stocks. borrowing constraints prevent investors taking positions security market line instead incentivized buy high beta stocks yields relatively higher price (therefore lower expected returns implied CAPM) high beta stocks. result, betting--beta strategy earns providing liquidity capital constraint investors lower risk aversion.last plot shows annual returns beta portfolios mainly interested . figure illustrates striking consistent patterns last years - portfolio exhibits periods positive well negative annual returns.","code":"\nbeta_portfolios_summary <- beta_portfolios %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  group_by(portfolio) %>%\n  summarise(alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),\n            beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),\n            ret = mean(ret))\nbeta_portfolios_summary %>% \n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  theme_bw() +\n  labs(title = \"Alphas of beta-sorted portfolios\",\n       x = \"Portfolio\",\n       y = \"CAPM Alpha\",\n       fill = \"Portfolio\") +\n  theme_bw()\nsml_capm <- lm(ret ~ 1 + beta, data = beta_portfolios_summary)$coefficients \n\nbeta_portfolios_summary %>% \n  ggplot(aes(x = beta, y = ret, color = portfolio)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = mean(factors_ff_monthly$mkt_excess)) +\n  geom_abline(intercept = sml_capm[1], slope = sml_capm[2], color = \"green\") +\n  scale_y_continuous(labels = percent, limit = c(0, mean(factors_ff_monthly$mkt_excess)*2)) +\n  scale_x_continuous(limits = c(0,2)) +\n  theme_bw() +\n  labs(x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n       title = \"Average portfolio excess returns and average beta estimates\")\nbeta_longshort <- beta_portfolios %>%\n  ungroup() %>%\n  mutate(portfolio = case_when(portfolio == max(as.numeric(portfolio)) ~ \"high\",\n                               portfolio == min(as.numeric(portfolio)) ~ \"low\")) %>% \n  filter(portfolio %in% c(\"low\", \"high\")) %>%\n  pivot_wider(month, names_from = portfolio, values_from = ret) %>%\n  mutate(long_short = high - low) %>% \n  left_join(factors_ff_monthly, by = \"month\") \ncoeftest(lm(long_short ~ 1, data = beta_longshort), vcov = NeweyWest)## \n## t test of coefficients:\n## \n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept) 0.00072514 0.00248314   0.292   0.7704\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort), vcov = NeweyWest)## \n## t test of coefficients:\n## \n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.0044182  0.0026187 -1.6872    0.092 .  \n## mkt_excess   0.8937951  0.1021655  8.7485   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nbeta_longshort %>% \n  group_by(year = year(month)) %>% \n  summarize(low = prod(1 + low),\n            high = prod(1 + high),\n            long_short = prod(1 + long_short)) %>% \n  pivot_longer(cols = -year) %>% \n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") + \n  facet_wrap(~name, ncol = 1) +\n  theme_bw() + theme(legend.position = \"none\") + \n  scale_y_continuous(labels = percent) + \n  labs(title = \"Annual returns of beta portfolios\",\n       x = NULL, y = NULL)"},{"path":"univariate-sorts-firm-size.html","id":"univariate-sorts-firm-size","chapter":"8 Univariate Sorts: Firm Size","heading":"8 Univariate Sorts: Firm Size","text":"section, continue portfolio sorts univariate setting. Yet, consider firm size sorting variable, gives rise well-known return factor - size premium. size premium arises buying small stocks selling large stocks. Prominently, (Fama French 1993) include factor three-factor model. Apart , asset managers commonly include size key firm characteristic making investment decisions.also introduce new choices formation portfolios. particular, discuss listing exchanges, industries, weighting regimes, periods. choices matter portfolio returns result different size premiums. Exploiting ideas generate favorable results called p-hacking. Hence, want emphasize alternative specifications supposed robustness tests.","code":""},{"path":"univariate-sorts-firm-size.html","id":"data-preparation-1","chapter":"8 Univariate Sorts: Firm Size","heading":"8.1 Data preparation","text":"section relies following set packages.First, retrieve relevant data database. Firm size defined market equity asset pricing applications retrieve CRSP.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(scales)\nlibrary(furrr)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()"},{"path":"univariate-sorts-firm-size.html","id":"size-distribution","chapter":"8 Univariate Sorts: Firm Size","heading":"8.2 Size distribution","text":"build size portfolios, investigate distribution variable firm size. Visualizing data valuable starting point understand input analysis. figure shows fraction total market capitalization concentrated largest firm. produce graph, create monthly indicators track whether stock belongs largest x% firms setting indicator’s value one. , aggregate firms within bucket compute buckets’ share total market capitalization.figure shows largest 1% firms cover 50% total market capitalization, holding just 25% largest firms CRSP universe essentially replicates market portfolio. size distribution means largest firms market dominate many small firms whenever use value-weighted benchmarks.Next, firm sizes also differ across listing exchanges. Stocks’ primary listings important past potentially still relevant today. graph shows New York Stock Exchange (NYSE) still largest listing exchange terms market capitalization. recently, Nasdaq gained relevance listing exchange. know small peak Nasdaq’s market cap around year 2000 ?Finally, consider distribution firm size across listing exchanges creating summary statistics. pre-build function summary() include statistics interested , create function create_summary() adds standard deviation number observations. , apply current month CRSP data exchange. also add row add_row() overall summary statistics.resulting table shows firms listed NYSE significantly larger average firms listed exchanges. Moreover, Nasdaq lists largest number firms. discrepancy firm sizes across listing exchanges motivated researchers form breakpoints exclusively NYSE sample apply breakpoints stocks. following, use update portfolio sort procedure.","code":"\ncrsp_monthly %>%\n  group_by(month) %>%\n  mutate(\n    top01 = if_else(mktcap >= quantile(mktcap, 0.99), 1L, 0L),\n    top05 = if_else(mktcap >= quantile(mktcap, 0.95), 1L, 0L),\n    top10 = if_else(mktcap >= quantile(mktcap, 0.90), 1L, 0L),\n    top25 = if_else(mktcap >= quantile(mktcap, 0.75), 1L, 0L),\n    total_market_cap = sum(mktcap)\n  ) %>%\n  summarize(\n    `Largest 1% of Stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of Stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of Stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of Stocks` = sum(mktcap[top25 == 1]) / total_market_cap\n  ) %>%\n  pivot_longer(cols = -month) %>%\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of Stocks\", \"Largest 5% of Stocks\",\n    \"Largest 10% of Stocks\", \"Largest 25% of Stocks\"\n  ))) %>%\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  ) +\n  theme_bw()\ncrsp_monthly %>%\n  group_by(month, exchange) %>%\n  summarize(mktcap = sum(mktcap)) %>%\n  mutate(share = mktcap / sum(mktcap)) %>%\n    ggplot(aes(x = month, y = share, fill = exchange, color = exchange)) +\n  geom_area(position=\"stack\", \n            stat=\"identity\",\n            alpha = 0.5) +\n  geom_line(position = \"stack\") + \n  theme_bw() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per exchange\"\n  )\ncreate_summary <- function(data, column_name) {\n  data %>%\n    select(value = {{ column_name }}) %>%\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q25 = quantile(value, 0.25),\n      q50 = quantile(value, 0.50),\n      q75 = quantile(value, 0.75),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly %>%\n  filter(month == max(month)) %>%\n  group_by(exchange) %>%\n  create_summary(mktcap) %>%\n  add_row(crsp_monthly %>%\n    filter(month == max(month)) %>%\n    create_summary(mktcap) %>%\n    mutate(exchange = \"Overall\"))## # A tibble: 5 x 11\n##   exchange   mean     sd     min    q05    q25    q50    q75    q95    max     n\n##   <chr>     <dbl>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <int>\n## 1 AMEX       281.  1294.  6.04e0 1.01e1 3.06e1 6.57e1   157.   530. 1.51e4   148\n## 2 NASDAQ    8036. 74663.  4.65e0 2.46e1 1.30e2 4.73e2  2092. 19065. 2.26e6  2300\n## 3 NYSE     16390. 43085.  5.35e0 1.54e2 9.10e2 3.33e3 11966. 74635. 4.14e5  1247\n## 4 Other    10061.    NA   1.01e4 1.01e4 1.01e4 1.01e4 10061. 10061. 1.01e4     1\n## 5 Overall  10545. 64142.  4.65e0 2.95e1 1.82e2 8.59e2  4168. 36811. 2.26e6  3696"},{"path":"univariate-sorts-firm-size.html","id":"univariate-size-portfolios-with-flexible-breakpoints","chapter":"8 Univariate Sorts: Firm Size","heading":"8.3 Univariate size portfolios with flexible breakpoints","text":"previous section, construct portfolios varying number portfolios different sorting variables. , extend framework compute breakpoints subset data, instance, based selected listing exchanges. published asset pricing articles, many scholars compute sorting breakpoints NYSE-listed stocks. NYSE-specific breakpoints applied entire universe stocks. replicate decision, introduce exchanges argument assign_portfolio() function. exchange-specific argument enters filter filter(grepl(exchanges, exchange)). function grepl() part family functions regular expressions, provide various functionalities work manipulate character strings. , replace character string stored column exchange binary variable indicates string matches pattern specified argument exchanges. example, exchanges = 'NYSE' specified, stocks NYSE used compute breakpoints. Alternatively, specify exchanges = 'NYSE|NASDAQ|AMEX', keeps stocks listed either exchanges. Overall, regular expressions powerful tool, touch specific case .","code":"\nassign_portfolio <- function(n_portfolios,\n                             exchanges,\n                             data) {\n  breakpoints <- data %>%\n    filter(grepl(exchanges, exchange)) %>%\n    summarize(breakpoint = quantile(\n      mktcap_lag,\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) %>%\n    pull(breakpoint) %>%\n    as.numeric()\n\n  data %>%\n    mutate(portfolio = findInterval(mktcap_lag, breakpoints, all.inside = TRUE)) %>%\n    pull(portfolio)\n}"},{"path":"univariate-sorts-firm-size.html","id":"value-and-equal-portfolio-weighting","chapter":"8 Univariate Sorts: Firm Size","heading":"8.4 Value and equal portfolio weighting","text":"Apart computing breakpoints different samples, researchers often use different portfolio weighting schemes. far, weighted portfolio constituent relative market equity previous period. protocol called value-weighting. alternative protocol equal-weighting, assigns stock’s return weight, .e., simple average constituents’ returns. Notice equal-weighting difficult practice portfolio manager needs rebalance portfolio monthly value-weighting truly passive investment.implement two weighting schemes function compute_portfolio_returns() takes logical argument weight returns firm value. statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns value_weighted = TRUE. Additionally, long-short portfolio long smallest firms short largest firms, consistent research showing small firms outperform larger counterparts. Apart two changes, function similar procedure previous section.see function compute_portfolio_returns() works, consider simple median breakpoint example value-weighted returns. interested effect restricting listing exchanges estimation size premium. first function call, compute returns based breakpoints listing exchanges. , computed returns based breakpoints NYSE-listed stocks.table shows size premium 60% larger consider stocks NYSE form breakpoint month. NYSE-specific breakpoints larger, 50% stocks entire universe resulting small portfolio NYSE firms larger average. impact choice negligible.","code":"\ncompute_portfolio_returns <- function(n_portfolios = 10,\n                                      exchanges = \"NYSE|NASDAQ|AMEX\",\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data %>%\n    group_by(month) %>%\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = cur_data()\n    )) %>%\n    group_by(month, portfolio) %>%\n    summarise(\n      ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n      .groups = \"drop_last\"\n    ) %>%\n    summarise(size_premium = ret[portfolio == min(portfolio)] - ret[portfolio == max(portfolio)]) %>%\n    summarise(size_premium = mean(size_premium))\n}\nret_all <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE|NASDAQ|AMEX\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(Exchanges = c(\"all\", \"NYSE\"), Premium = as.numeric(c(ret_all, ret_nyse)) * 100)## # A tibble: 2 x 2\n##   Exchanges Premium\n##   <chr>       <dbl>\n## 1 all         0.110\n## 2 NYSE        0.181"},{"path":"univariate-sorts-firm-size.html","id":"p-hacking-and-non-standard-errors","chapter":"8 Univariate Sorts: Firm Size","heading":"8.5 P-hacking and non-standard errors","text":"Since choice exchange significant impact, next step investigate effect data processing decisions researchers make along way. particular, portfolio sort analysis decide least number portfolios, listing exchanges form breakpoints, equal- value-weighting. , one may exclude firms active finance industry restrict analysis parts time series. variations choices discuss part scholarly articles published top finance journals.reason behind exercise show different ways form portfolios result different estimated size premia. Despite effects multitude choices, correct way. also noted none procedures wrong, aim simply illustrate changes can arise due variation evidence-generating process (Menkveld2022?).\nmalicious perspective, modeling choices give researcher multiple chances find statistically significant results. Yet considered p-hacking important highlight p-hacking considered scientific fraud. Moreover, statistical inference multiple testing invalid (Harvey2016?).Nevertheless, creates problem since single correct way sorting portfolios; researcher convince reader results come p-hacking exercise? circumvent dilemma, academics encouraged present evidence different sorting schemes robustness tests report multiple approaches show result depend single choice. Thus, robustness premiums key feature.conduct series robustness tests also interpreted p-hacking exercise. , examine size premium different specifications presented table p_hacking_setup. function expand_grid() produces table possible permutations arguments. Notice use argument data exclude financial firms truncate time series. speed computation parallelize (many) different sorting procedures. Finally, report resulting size premiums descending order.","code":"\np_hacking_setup <- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = c(\"NYSE\", \"NYSE|NASDAQ|AMEX\"),\n  value_weighted = c(TRUE, FALSE),\n  data = rlang::parse_exprs('crsp_monthly; crsp_monthly %>% filter(industry != \"Finance\");\n                             crsp_monthly %>% filter(month < \"1990-06-01\");\n                             crsp_monthly %>% filter(month >=\"1990-06-01\")')\n)\np_hacking_setup## # A tibble: 48 x 4\n##    n_portfolios exchanges        value_weighted data      \n##           <dbl> <chr>            <lgl>          <list>    \n##  1            2 NYSE             TRUE           <sym>     \n##  2            2 NYSE             TRUE           <language>\n##  3            2 NYSE             TRUE           <language>\n##  4            2 NYSE             TRUE           <language>\n##  5            2 NYSE             FALSE          <sym>     \n##  6            2 NYSE             FALSE          <language>\n##  7            2 NYSE             FALSE          <language>\n##  8            2 NYSE             FALSE          <language>\n##  9            2 NYSE|NASDAQ|AMEX TRUE           <sym>     \n## 10            2 NYSE|NASDAQ|AMEX TRUE           <language>\n## # ... with 38 more rows\nplan(multisession, workers = availableCores())\n\np_hacking_setup <- p_hacking_setup %>%\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = rlang::eval_tidy(..4)\n    )\n  ))\np_hacking_setup %>%\n  mutate(data = map_chr(data, deparse)) %>%\n  unnest(size_premium) %>%\n  arrange(desc(size_premium))## # A tibble: 48 x 5\n##    n_portfolios exchanges        value_weighted data                size_premium\n##           <dbl> <chr>            <lgl>          <chr>                      <dbl>\n##  1           10 NYSE|NASDAQ|AMEX FALSE          \"crsp_monthly %>% ~      0.0184 \n##  2           10 NYSE|NASDAQ|AMEX FALSE          \"crsp_monthly %>% ~      0.0180 \n##  3           10 NYSE|NASDAQ|AMEX FALSE          \"crsp_monthly\"           0.0162 \n##  4           10 NYSE|NASDAQ|AMEX FALSE          \"crsp_monthly %>% ~      0.0139 \n##  5           10 NYSE|NASDAQ|AMEX TRUE           \"crsp_monthly %>% ~      0.0114 \n##  6           10 NYSE|NASDAQ|AMEX TRUE           \"crsp_monthly %>% ~      0.0109 \n##  7           10 NYSE|NASDAQ|AMEX TRUE           \"crsp_monthly\"           0.0103 \n##  8           10 NYSE|NASDAQ|AMEX TRUE           \"crsp_monthly %>% ~      0.00964\n##  9            5 NYSE|NASDAQ|AMEX FALSE          \"crsp_monthly %>% ~      0.00914\n## 10            5 NYSE|NASDAQ|AMEX FALSE          \"crsp_monthly %>% ~      0.00883\n## # ... with 38 more rows"},{"path":"univariate-sorts-firm-size.html","id":"the-size-premium-variation","chapter":"8 Univariate Sorts: Firm Size","heading":"8.6 The size-premium variation","text":"Finally, provide graph shows different premiums. plot also shows relation Fama-French SMB (small minus big) premium used literature.","code":"\np_hacking_setup %>%\n  mutate(data = map_chr(data, deparse)) %>%\n  unnest(size_premium) %>%\n  ggplot(aes(x = size_premium * 100)) +\n  geom_histogram(bins = 20) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Size premium over different sorting choices\",\n    subtitle = \"The dotted vertical line indicates the average size premium (in %)\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb) * 100),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  theme_bw()"},{"path":"bivariate-sorts-value.html","id":"bivariate-sorts-value","chapter":"9 Bivariate Sorts: Value","heading":"9 Bivariate Sorts: Value","text":"section extends univariate portfolio sorts bivariate sorts simply means assign stocks portfolios based two characteristics. Bivariate sorts regularly used academic asset pricing literature. Yet, scholars also use sorts three grouping variables. Conceptually, portfolio sorts easily applicable higher dimensions.form portfolios firm size book--market ratio. calculate book--market ratios, accounting data required necessitates additional steps portfolio formation. end, demonstrate form portfolios two sorting variables using -called independent dependent portfolio sorts.","code":""},{"path":"bivariate-sorts-value.html","id":"data-preparation-2","chapter":"9 Bivariate Sorts: Value","heading":"9.1 Data preparation","text":"current section relies set packages.conduct portfolio sorts based CRSP sample keep necessary columns memory. use data sources firm size previous section., utilize accounting data. common source accounting data Compustat. need book equity data application, select database. Additionally, convert variable datadate monthly value, consider monthly returns need account exact date. achieve , use function floor_date().","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(scales)\nlibrary(furrr)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\n\n# Keep only relevant data\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  select(permno, gvkey, month, ret_excess, mkt_excess, mktcap, mktcap_lag, exchange) %>%\n  drop_na()\ncompustat <- tbl(tidy_finance, \"compustat\") %>%\n  collect()\n\n# Book-equity data\nbe <- compustat %>%\n  select(gvkey, datadate, be) %>%\n  drop_na() %>%\n  mutate(month = floor_date(ymd(datadate), \"month\"))"},{"path":"bivariate-sorts-value.html","id":"book-to-market-ratio","chapter":"9 Bivariate Sorts: Value","heading":"9.2 Book-to-market ratio","text":"fundamental problem handling accounting data look-ahead bias - must include data forming portfolio public knowledge time. course, researchers information looking past agents moment. However, abnormal excess returns trading strategy rely information advantage differential result informed agents’ trades. Hence, lag accounting information.continue lag market capitalization firm size one month. , compute book--market ratio, relates firm’s book equity market equity. Firms high (low) book--market called value (growth) firms. matching accounting market equity information month, lag book--market six months. sufficiently conservative approach accounting information usually released well six months pass. However, asset pricing literature, even longer lags used well.1Having variables, .e., firm size lagged one month book--market lagged six months, merge sorting variables returns using sorting_date-column created purpose. final step data preparation deals differences frequency variables. Returns firm size recorded monthly. Yet accounting information released annual basis. Hence, match book--market one month per year eleven empty observations. solve frequency issue, carry latest book--market ratio firm subsequent months, .e., fill missing observations current report. done via fill()-function sorting date firm (identify permno gvkey) firm basis (group_by() usual). last step, remove rows missing entries returns matched annual report.last step preparation portfolio sorts computation breakpoints. continue use function allowing specification exchanges use breakpoints. Additionally, reintroduce argument var function defining different sorting variables via curly-curly.data preparation steps, present bivariate portfolio sorts independent dependent basis.","code":"\nme <- crsp_monthly %>%\n  mutate(sorting_date = month %m+% months(1)) %>%\n  select(permno, sorting_date, me = mktcap)\n\n# Book-to-market\nbm <- be %>%\n  inner_join(crsp_monthly %>%\n    select(month, permno, gvkey, mktcap), by = c(\"gvkey\", \"month\")) %>%\n  mutate(\n    bm = be / mktcap,\n    sorting_date = month %m+% months(6)\n  ) %>%\n  select(permno, gvkey, sorting_date, bm) %>%\n  arrange(permno, gvkey, sorting_date)\n\n# Merged data\nccm <- crsp_monthly %>%\n  left_join(bm, by = c(\"permno\", \"gvkey\", \"month\" = \"sorting_date\")) %>%\n  left_join(me, by = c(\"permno\", \"month\" = \"sorting_date\")) %>%\n  select(permno, gvkey, month, ret_excess, mktcap_lag, me, bm, exchange)\n\n# Fill accounting data\nccm <- ccm %>%\n  arrange(permno, gvkey, month) %>%\n  group_by(permno, gvkey) %>%\n  fill(bm) %>%\n  drop_na()\nassign_portfolio <- function(data, var, n_portfolios, exchanges) {\n  breakpoints <- data %>%\n    filter(exchange %in% exchanges) %>%\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) %>%\n    pull(breakpoint) %>%\n    as.numeric()\n\n  data %>%\n    mutate(portfolio = findInterval({{ var }}, breakpoints, all.inside = TRUE)) %>%\n    pull(portfolio)\n}"},{"path":"bivariate-sorts-value.html","id":"independent-sorts","chapter":"9 Bivariate Sorts: Value","heading":"9.3 Independent sorts","text":"Bivariate sorts create portfolios within two-dimensional space spanned two sorting variables. possible assess return impact either sorting variable return differential trading strategy invests portfolios either end respective variables spectrum. create five--five matrix using book--market firm size sorting variables example . end 25 portfolios. Since interested value premium (.e., return differential high low book--market firms), go long five portfolios highest book--market firms short five portfolios lowest book--market firms. five portfolios end due size splits employed alongside book--market splits.implement independent bivariate portfolio sort, assign monthly portfolios sorting variables separately create variables portfolio_bm portfolio_bm, respectively. , separate portfolios combined final stored portfolio_combined. assigning portfolios, compute average return within portfolio month. Additionally, keep book--market portfolio makes computation value premium easier - alternative disaggregate combined portfolio separate step. Notice weigh stocks within portfolio market capitalization, .e., decide value-weight returns.Equipped monthly portfolio returns, ready compute value premium. However, still decide invest five high five low book--market portfolios. common approach weigh portfolios equally, yet another researcher’s choice. , compute return differential high low book--market portfolios show average value premium.","code":"\nccm_portfolios <- ccm %>%\n  group_by(month) %>%\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = paste0(portfolio_bm, portfolio_me)\n  ) %>%\n  group_by(month, portfolio_combined) %>%\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\n\n# TODO: Should we create a table with average returns?\nvalue_premium <- ccm_portfolios %>%\n  group_by(month, portfolio_bm) %>%\n  summarise(ret = mean(ret), .groups = \"drop_last\") %>%\n  summarise(value_premium = ret[portfolio_bm == max(portfolio_bm)] - ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)## [1] 0.3311771"},{"path":"bivariate-sorts-value.html","id":"dependent-sorts","chapter":"9 Bivariate Sorts: Value","heading":"9.4 Dependent sorts","text":"previous exercise, assigned portfolios without considering second variable assignment. protocol called independent portfolio sorts. alternative, .e., dependent sorts, creates portfolios second sorting variable within bucket first sorting variable. example , sort firms five size buckets, within buckets, assign firms five book--market portfolios. Hence, monthly breakpoints specific size group. decision independent dependent portfolio sorts another choice researcher. Notice dependent sorts ensure equal amount stocks within portfolio.implement dependent sorts, first create size portfolios calling assign_portfolio() var = . , group data month size portfolio assigning book--market portfolio. rest implementation . Finally, compute value premium.","code":"\nccm_portfolios <- ccm %>%\n  group_by(month) %>%\n  mutate(portfolio_me = assign_portfolio(\n    data = cur_data(),\n    var = me,\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) %>%\n  group_by(month, portfolio_me) %>%\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = paste0(portfolio_bm, portfolio_me)\n  ) %>%\n  group_by(month, portfolio_combined) %>%\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\n\nvalue_premium <- ccm_portfolios %>%\n  group_by(month, portfolio_bm) %>%\n  summarise(ret = mean(ret), .groups = \"drop_last\") %>%\n  summarise(value_premium = ret[portfolio_bm == max(portfolio_bm)] - ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)## [1] 0.2890729"},{"path":"bivariate-sorts-replication-of-fama-french-factors.html","id":"bivariate-sorts-replication-of-fama-french-factors","chapter":"10 Bivariate Sorts: Replication of Fama & French Factors","heading":"10 Bivariate Sorts: Replication of Fama & French Factors","text":"Fama French three-factor model (see CITATION) cornerstone asset pricing. top market factor represented traditional CAPM beta, model includes factors size value. introduce factors previous section, definition remains . Size SMB factor (small-minus-big) long small firms short large firms. value factor HML (high-minus-low) long high book--market firms short low book--market counterparts. demonstrated main idea portfolio sorts, also want show replicate significant factors. However, aim perfect replication want show main ideas.current section relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)"},{"path":"bivariate-sorts-replication-of-fama-french-factors.html","id":"databases","chapter":"10 Bivariate Sorts: Replication of Fama & French Factors","heading":"10.1 Databases","text":"use data sources CRSP compustat, need exactly variables compute size value factors way Fama French . Hence, nothing new .","code":"\n# Load data from database\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>% \n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>% \n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") %>% \n  collect()\n\n# Keep only relevant data\ndata_ff <- crsp_monthly %>% \n  left_join(factors_ff_monthly, by = \"month\") %>% \n  select(permno, gvkey, month, ret_excess, mkt_excess, mktcap, mktcap_lag, exchange) %>% \n  drop_na()\n\n# Compustat data\nbe <- compustat %>%\n  select(gvkey, datadate, be) %>% \n  drop_na()"},{"path":"bivariate-sorts-replication-of-fama-french-factors.html","id":"data-preparation-3","chapter":"10 Bivariate Sorts: Replication of Fama & French Factors","heading":"10.2 Data Preparation","text":"Yet start merging data set computing premiums, differences previous sections. First, Fama French form portfolios June year \\(t\\), whereby returns July first monthly return respective portfolio. firm size, consequently use market capitalization recorded June. held constant June year \\(t+1\\).Second, Fama French also different protocol computing book--market ratio. use market equity end year \\(t - 1\\) book equity reported year \\(t-1\\), .e., datadate within last year. Hence, book--market ratio can based accounting information 18 months old. Market equity also necessarily reflect time point book equity.implement time lags, employ temporary sorting_date-column. Notice combine information, want single observation per year stock since interested computing breakpoints held constant entire year. ensure call distinct() end chunk .","code":"\n# Market equity\nff_me <- data_ff %>%\n  filter(month(month) == 6) %>%\n  mutate(sorting_date = month %m+% months(1)) %>%\n  select(permno, sorting_date, ff_me = mktcap)\n\n# Book-to-market\n## December market equity\nff_me_dec <- data_ff %>%\n  filter(month(month) == 12) %>%\n  mutate(sorting_date = ymd(paste0(year(month) + 1, \"07-01)\"))) %>%\n  select(permno, gvkey, sorting_date, bm_me = mktcap)\n\n## Book to market\nff_bm <- be %>%\n  # be from year before\n  mutate(sorting_date = ymd(paste0(year(datadate) + 1, \"07-01\"))) %>%\n  select(gvkey, sorting_date, bm_be = be) %>%\n  drop_na() %>%\n  inner_join(ff_me_dec, by = c(\"gvkey\", \"sorting_date\")) %>%\n  mutate(ff_bm = bm_be/bm_me) %>%\n  select(permno, sorting_date, ff_bm)\n\n# Combine factors\nff_variables <- ff_me %>% \n  inner_join(ff_bm, by = c(\"permno\", \"sorting_date\")) %>%\n  drop_na() %>%\n  distinct(permno, sorting_date, .keep_all = TRUE)"},{"path":"bivariate-sorts-replication-of-fama-french-factors.html","id":"breakpoints","chapter":"10 Bivariate Sorts: Replication of Fama & French Factors","heading":"10.3 Breakpoints","text":"attach sorting variables returns, compute breakpoints separately. Matching everything computing breakpoints possible less efficient. Regarding choice variables, Fama French rely NYSE-specific breakpoints, form two portfolios size dimension median three portfolios dimension book--market 30%- 70%-percentiles, use independent sorts. Additionally, perform inner_join() return data ensure use traded stocks computing breakpoints first step.Next, merge June breakpoints return data. implement step, create new column sorting_date return data setting date sort July \\(t-1\\) month June (year \\(t\\)) earlier July year \\(t\\) month July later. ensures form portfolios based information assigned June year \\(t\\). principle, still form portfolios monthly basis, incorrect stocks drop year.","code":"\nff_breakpoints <- ff_variables %>%\n  inner_join(data_ff, by = c(\"permno\" = \"permno\", \"sorting_date\" = \"month\")) %>%\n  filter(exchange == \"NYSE\") %>%\n  group_by(sorting_date) %>%\n  summarise(ff_me_50 = median(ff_me),\n            ff_bm_30 = quantile(ff_bm, 0.3, names = FALSE),\n            ff_bm_70 = quantile(ff_bm, 0.7, names = FALSE))\nff_portfolios <- data_ff %>%\n  mutate(sorting_date = case_when(month(month) <= 6 ~ ymd(paste0(year(month) - 1, \"0701\")),\n                                  month(month) >= 7 ~ ymd(paste0(year(month), \"0701\")))) %>%\n  inner_join(ff_variables, by = c(\"permno\", \"sorting_date\")) %>%\n  left_join(ff_breakpoints, by = \"sorting_date\")"},{"path":"bivariate-sorts-replication-of-fama-french-factors.html","id":"fama-and-french-factor-returns","chapter":"10 Bivariate Sorts: Replication of Fama & French Factors","heading":"10.4 Fama and French Factor Returns","text":"can now use returns alongside breakpoints assign portfolios. slightly differently using case_when()-function. assign portfolios adding numbers result respective portfolio. base portfolio small growth stocks gets value 11. add 10 large firms 1 2 higher book--market ratios. end, add letter p indicate portfolio numbers. Overall, procedure potentially less intuitive suitable exercise given low number portfolios.forming monthly value-weighted return per portfolio, use function pivot_wider(). function useful various scenarios. case, use transform data containing portfolio-months row data frame one row month columns indicate portfolio returns. argument id_cols defines identify new observations uniquely, names_from indicates column old structure defines new set’s column names, values_from gives column contains data stored created columns.Finally, can compute equal-weighted monthly returns size value factors. size premium results going long three small portfolios (.e., portfolios 11, 12, 13) shorting large portfolios (.e., portfolios 21, 22, 23). value premium achieved investing two high book--market portfolios (.e., portfolios 13 23) shorting growth portfolios (.e., portfolios 11 21).","code":"\nff_factors <- ff_portfolios %>%\n  mutate(portfolio = 10,\n         portfolio = case_when(ff_me < ff_me_50 ~ portfolio,\n                               ff_me >= ff_me_50 ~ portfolio + 10),\n         portfolio = case_when(ff_bm < ff_bm_30 ~ portfolio + 1,\n                               ff_bm >= ff_bm_30 & ff_bm < ff_bm_70 ~ portfolio + 2,\n                               ff_bm >= ff_bm_70 ~ portfolio + 3),\n         portfolio = paste0(\"p\", as.character(portfolio))) %>%\n  group_by(portfolio, month) %>%\n  summarise(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\") %>%\n  pivot_wider(id_cols = month, names_from = portfolio, values_from = ret) %>%\n  mutate(ff_SMB = (p11 + p12 + p13) / 3 - (p21 + p22 + p23) / 3,\n         ff_HML = (p13 + p23) / 2 - (p11 + p21) / 2) %>%\n  select(month, ff_SMB, ff_HML)"},{"path":"bivariate-sorts-replication-of-fama-french-factors.html","id":"correlation-tests","chapter":"10 Bivariate Sorts: Replication of Fama & French Factors","heading":"10.5 Correlation Tests","text":"last step, replicated size value premiums following procedure outlined Fama French. However, follow procedure strictly. final question ; close get? answer question looking two time-series estimates correlation analysis using function cor.test() test means using function t.test().can see reject null hypothesis premiums correlation tests, .e., hypothesis time series uncorrelated rejected. Moreover, reject hypothesis means Fama French factors replication results different. Hence, can conclude relatively good job replicating premiums.","code":"\ntest <- factors_ff_monthly %>% \n  inner_join(ff_factors, by = \"month\") %>%\n  mutate(ff_SMB = round(ff_SMB, 4),\n         ff_HML = round(ff_HML, 4))\n\ncor.test(test$ff_SMB, test$smb) ## \n##  Pearson's product-moment correlation\n## \n## data:  test$ff_SMB and test$smb\n## t = 216.1, df = 712, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.9912746 0.9934900\n## sample estimates:\n##      cor \n## 0.992463\ncor.test(test$ff_HML, test$hml)## \n##  Pearson's product-moment correlation\n## \n## data:  test$ff_HML and test$hml\n## t = 129.46, df = 712, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.9761907 0.9822018\n## sample estimates:\n##       cor \n## 0.9794122\nt.test(test$ff_SMB, test$smb) ## \n##  Welch Two Sample t-test\n## \n## data:  test$ff_SMB and test$smb\n## t = 0.084818, df = 1426, p-value = 0.9324\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.002996805  0.003267673\n## sample estimates:\n##   mean of x   mean of y \n## 0.001945378 0.001809944\nt.test(test$ff_HML, test$hml)## \n##  Welch Two Sample t-test\n## \n## data:  test$ff_HML and test$hml\n## t = -0.11467, df = 1425.4, p-value = 0.9087\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.003147035  0.002799416\n## sample estimates:\n##   mean of x   mean of y \n## 0.002485714 0.002659524"},{"path":"factor-selection-via-machine-learning.html","id":"factor-selection-via-machine-learning","chapter":"11 Factor selection via machine learning","heading":"11 Factor selection via machine learning","text":"aim section twofold: data science perspective, introduce tidymodels, collection packages modeling machine learning using tidyverse principles. tidymodels comes handy workflow sorts typical prediction tasks. finance perspective, address factor zoo ((Cochrane 2011)). previous chapters, illustrate stock-characteristics size provide valuable pricing information addition stock beta. findings question usefulness Capital Asset Pricing Model. fact, last decades, financial economists “discovered” plethora additional factors may correlated marginal utility consumption (thus deserve prominent role pricing applications). Therefore, challenge days rather : really believe relevance 300+ risk factors?.introduce Lasso Ridge Regression special case penalized regression models. , explain concept cross-validation model tuning elastic net regularization popular example. implement showcase entire cycle model specification, training forecast evaluation within tidymodels universe. tools can general applied abundance interesting asset pricing problems, apply penalized regressions identify macro-economic variables asset pricing factors help explain cross-section industry portfolios.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"brief-theoretical-background","chapter":"11 Factor selection via machine learning","heading":"11.1 Brief theoretical background","text":"book empirical work tidy manner refer many excellent textbook treatments machine learning methods especially penalized regressions deeper discussion. Instead, just briefly summarize idea Lasso Ridge regressions well general elastic net turn fascinating question implement, tune use models tidymodels workflows.set stage, start definition linear model: suppose data \\((y_t, x_t), t = 1,\\ldots, T\\) \\(x_t\\) \\((K \\times 1)\\) vector regressors \\(y_t\\) response observation \\(t\\).\nlinear model takes form \\(y_t = \\beta' x_t + \\varepsilon_t\\) error term \\(\\varepsilon_t\\) studied abundance. well-known ordinary-least square (OLS) estimator \\((K \\times 1)\\) vector \\(\\beta\\) minimizes sum squared residuals \\(\\hat{\\beta} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t\\).\noften interested estimated coefficient vector \\(\\hat\\beta\\), machine learning time predictive performance. new observation \\(\\tilde{x}_t\\), linear model generates predictions \\(\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta' \\tilde x_t\\).\nbest can ?\nreally: Instead minimizing sum squared residuals, penalized linear models can improve predictive performance reducing variance estimator \\(\\hat\\beta\\). time, seems appealing restrict set regressors meaningful ones possible. words, \\(K\\) large number proposed factors asset pricing literature, may desirable feature select reasonable factors set \\(\\hat\\beta_k = 0\\) redundant factors.clear promised benefits penalized regressions come cost. cases, reducing variance estimator introduces bias \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). effect bias-variance trade-? understand necessary considerations, assume following data-generating process \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] properties \\(\\hat\\beta\\) unbiased estimator may desirable circumstances, certainly consider predictive accuracy. instance, mean-squared error (MSE) depends model choice follow: \\[\\begin{aligned}\n&=E((y-\\hat{f}(\\textbf{x}))^2)=E((f(\\textbf{x})+\\epsilon-\\hat{f}(\\textbf{x}))^2)\\\\\n&= \\underbrace{E((f(\\textbf{x})-\\hat{f}(\\textbf{x}))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(\\textbf{x})^2\\right)+E\\left(f(\\textbf{x})^2\\right)-2E\\left(f(\\textbf{x})\\hat{f}(\\textbf{x})\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(\\textbf{x})^2\\right)+f(\\textbf{x})^2-2f(\\textbf{x})E\\left(\\hat{f}(\\textbf{x})\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(\\textbf{x})\\right)}_{\\text{variance model}}+ \\underbrace{E\\left((f(\\textbf{x})-\\hat{f}(\\textbf{x}))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2. \n\\end{aligned}\\] model can reduce \\(\\sigma_\\varepsilon^2\\), biased estimator small variance may lower mean squared error unbiased estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"ridge-regression","chapter":"11 Factor selection via machine learning","heading":"11.1.1 Ridge Regression","text":"One biased estimator known Ridge regression. (Hoerl Kennard 1970) propose minimize sum squared errors simultaneously imposing penalty \\(L_2\\) norm parameters \\(\\hat\\beta\\). Formally, means penalty factor \\(\\lambda\\geq 0\\) minimization problem takes form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq \\lambda\\). , \\(X = \\left(x_1 \\ldots x_T\\right)'\\) \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). closed-form solution resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda \\right)^{-1}X'y.\\] couple observations worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta\\) \\(\\lambda = 0\\) \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) \\(\\lambda\\rightarrow \\infty\\). Also \\(\\lambda > 0\\), \\(\\left(X'X + \\lambda \\right)\\) non-singular even \\(X'X\\) means \\(\\hat\\beta^\\text{ridge}\\) exists even \\(\\hat\\beta\\) defined. note also Ridge estimator requires careful choice hyperparameter \\(\\lambda\\) controls amount regularization.\nUsually, \\(X\\) contains intercept column ones. general rule, associated intercept coefficient penalized. practice, often implies \\(y\\) simply demeaned computing \\(\\hat\\beta^\\text{ridge}\\).statistical properties Ridge estimator? First, bad news \\(\\hat\\beta^\\text{ridge}\\) biased estimator \\(\\beta\\). However, good news (homoscedastic error terms) variance Ridge estimator smaller variance ordinary least square estimator. encourage verify two statements exercises. result, face trade-: Ridge regression sacrifice bias achieve smaller variance OLS estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"lasso","chapter":"11 Factor selection via machine learning","heading":"11.1.2 Lasso","text":"alternative Ridge regression Lasso (least absolute shrinkage selection operator). Similar Ridge regression, Lasso (Tibshirani, 1996) penalized hence biased estimator.\nmain difference Ridge regression Lasso shrink coefficients effectively selects variables setting coefficients irrelevant variables zero. Lasso implements \\(L_1\\) penalization parameters : \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\] closed form solution \\(\\hat\\beta^\\text{Lasso}\\) maximization problem efficient algorithms exist (e.g., R package glmnet). Like Ridge regression, hyperparameter \\(\\lambda\\) specified beforehand.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"elastic-net","chapter":"11 Factor selection via machine learning","heading":"11.1.3 Elastic Net","text":"elastic net (Zou Hastie 2005) combines \\(L_1\\) \\(L_2\\) penalization encourages grouping effect strongly correlated predictors tend model together. general framework considers following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, chose 2 hyperparameters: shrinkage factor \\(\\lambda\\) weighting parameter \\(\\rho\\). elastic net resembles Lasso \\(\\rho = 1\\) Ridge regression \\(\\rho = 0\\).\nR package glmnet provides efficient algorithms compute coefficients penalized regressions, good exercise implement Ridge Lasso estimation use glmnet package tidymodels back-end.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"data-preparation-4","chapter":"11 Factor selection via machine learning","heading":"11.2 Data preparation","text":"get started, load required packages data. main focus workflows behind amazing tidymodels package collection.analysis use 4 different data sources. start two different set factor portfolio returns suggested representing useful risk factor exposure thus relevant comes asset pricing applications.standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big high-minus-low book--market valuation sorts) Fama French (1993)Monthly q-factor returns (Hou, Xue, Zhang 2014). factors contain size factor, investment factor, return--equity factor expected growth factorNext, include macroeconomic predictors may predict general stock market economy. Macroeconomic variables effectively serve conditioning information inclusion hints relevance conditional models instead unconditional asset pricing. refer interested reader (Cochrane2005?) role conditioning information.set macroeconomic predictors comes paper “Comprehensive Look Empirical Performance Equity Premium Prediction” (Welch Goyal 2008). data updated authors 2020 comprises monthly variables suggested good predictors equity premium. variables Dividend Price Ratio, Earnings Price Ratio, Stock Variance, Net Equity Expansion, Treasury Bill rate inflationFinally, need set test assets. aim understand plenty factors macroeconomic variable combinations proof useful explain cross-section returns test assets.line many existing papers use monthly portfolio returns 10 different industries according definition Kenneth French’s homepage test assets.combine observations one data frame.data contains 22 columns regressors 13 macro variables 8 factor returns month. panel ranges januar 1967 november 2020. Table (ref?(tab:industryreturns)) provides summary statistics 10 industries sample standard deviation minimum maximum monthly excess returns.\nTable 11.1: Summary statistics: Industry excess returns percent.\n","code":"\nlibrary(RSQLite) # To gather the data\nlibrary(tidyverse)\nlibrary(tidymodels) # For ML applications\nlibrary(furrr) # For parallelization\nlibrary(glmnet) # For penalized regressions\nlibrary(broom)\nlibrary(timetk)\nlibrary(kableExtra)\nlibrary(scales)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\n# Load factor returns\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect() %>%\n  rename_with(~ paste0(\"factor_ff_\", .), -month)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\") %>%\n  collect() %>%\n  rename_with(~ paste0(\"factor_q_\", .), -month)\n\n# Load macroeconomic variables\nmacro_predictors <- tbl(tidy_finance, \"macro_predictors\") %>%\n  collect() %>%\n  rename_with(~ paste0(\"macro_\", .), -month) %>%\n  select(-macro_rp_div)\n\n# Load test assets\nindustries_ff_monthly <- tbl(tidy_finance, \"industries_ff_monthly\") %>%\n  collect() %>%\n  pivot_longer(-month, \n               names_to = \"industry\", values_to = \"ret\") %>%\n  mutate(industry = as_factor(industry))\ndata <- industries_ff_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  left_join(factors_q_monthly, by = \"month\") %>%\n  left_join(macro_predictors, by = \"month\") %>%\n  mutate(\n    ret = ret - factor_ff_rf\n  ) %>% # Compute excess returns\n  select(month, industry, ret, everything()) %>%\n  drop_na()\ndata %>%\n  group_by(industry) %>%\n  mutate(ret = 100 * ret) %>%\n  summarise(\n    mean = mean(ret),\n    sd = max(ret),\n    min = min(ret),\n    median = median(ret),\n    max = max(ret)\n  ) %>%\n  kable(caption = \"Summary statistics: Industry excess returns in percent.\",\n        digits = 3)"},{"path":"factor-selection-via-machine-learning.html","id":"the-tidymodels-workflow","chapter":"11 Factor selection via machine learning","heading":"11.3 The tidymodels workflow","text":"illustrate penalized linear regressions, employ tidymodels collection packages modeling machine learning using tidyverse principles. can simply use install.packages(\"tidymodels\") get access related packages. recommend check work Max Kuhn Julia Silge: continuously write great book ‘Tidy Modeling R’ using tidy principles.tidymodels workflow encompasses main stages modeling process: pre-processing data, model fitting, post-processing results. demonstrate , tidymodels provides efficient workflows can updated low effort.Using ideas Ridge Lasso regression, following example guides () preprocessing data (data split variable mutation), (ii) building models, (iii) fitting models, (iv) tuning models create “best” possible predictions.start, restrict analysis just one industry: Manufacturing. first split sample training test set. purpose, tidymodels provides function initial_time_split rsample package. split takes last 20% data test set used model tuning. use test set evaluate predictive accuracy --sample scenario.object split simply takes track observation index belongs training test set. can call training set training(split), can extract test set testing(split).","code":"\nsplit <- initial_time_split(\n  data %>%\n    filter(industry == \"Manuf\") %>%\n    select(-industry),\n  prop = 4 / 5\n)\nsplit## <Analysis/Assess/Total>\n## <517/130/647>"},{"path":"factor-selection-via-machine-learning.html","id":"preprocess-data","chapter":"11 Factor selection via machine learning","heading":"11.3.1 Preprocess Data","text":"Recipes help preprocess data training model. Recipes series preprocessing steps variable selection, transformation converting qualitative predictors indicator variables. recipe starts formula defines general structure dataset role variable (regressor dependent variable). dataset, recipe contains following steps fit model:formula defines want explain excess returns available predictorsWe exclude column month analysisWe include interaction terms factors macro economic predictorsWe demean scale regressor standard deviation oneA table available recipe steps can found . format(Sys.Date(), \"%Y\"), 100 different processing steps available! One important point: definition recipe imply calculations yet rather provides description tasks applied later. result, easy reuse recipes different models thus make sure outcomes comparable based input.\nexample , make difference whether input data = training(split) data = testing(split) used.\nmatters early stage column names types.can apply recipe data suitable structure. code combines two different functions: prep estimates required parameters training set can later applied data sets. bake applies processed computations new data.Note resulting data contains 130 observations test set 126 columns. many? Recall recipe states compute every possible interaction term factors predictors increases dimension data matrix substantially.may ask stage: use recipe instead simply using data wrangling commands mutate select? tidymodels beauty lot happening hood. Recall, simple scaling step actually compute standard deviation column, store value apply identical transformation new dataset, e.g. testing(split). prepped recipe stores values hands bake novel dataset. Easy pie tidymodels, isn’t ?","code":"\nrec <- recipe(ret ~ ., data = training(split)) %>%\n  step_rm(month) %>% # remove date variable\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) %>% \n  step_normalize(all_predictors()) %>%\n  step_center(ret, skip = TRUE)\ntmp_data <- bake(prep(rec, training(split)), new_data = testing(split))\ntmp_data## # A tibble: 130 x 126\n##    factor_ff_rf factor_ff_mkt_excess factor_ff_smb factor_ff_hml factor_q_me\n##           <dbl>                <dbl>         <dbl>         <dbl>       <dbl>\n##  1        -1.92               0.644        0.298           0.947      0.371 \n##  2        -1.88               1.27         0.387           0.607      0.527 \n##  3        -1.88               0.341        1.43            0.836      1.12  \n##  4        -1.88              -1.80        -0.0411         -0.963     -0.0921\n##  5        -1.88              -1.29        -0.627          -1.73      -0.850 \n##  6        -1.88               1.41         0.00517        -0.240      0.0440\n##  7        -1.88              -1.12        -0.978          -0.801     -1.25  \n##  8        -1.88               1.97         1.15           -1.21       1.13  \n##  9        -1.88               0.747        0.304          -0.979      0.109 \n## 10        -1.88               0.0387       1.09           -0.436      1.04  \n## # ... with 120 more rows, and 121 more variables: factor_q_ia <dbl>,\n## #   factor_q_roe <dbl>, factor_q_eg <dbl>, macro_dp <dbl>, macro_dy <dbl>,\n## #   macro_ep <dbl>, macro_de <dbl>, macro_svar <dbl>, macro_bm <dbl>,\n## #   macro_ntis <dbl>, macro_tbl <dbl>, macro_lty <dbl>, macro_ltr <dbl>,\n## #   macro_tms <dbl>, macro_dfy <dbl>, macro_infl <dbl>, ret <dbl>,\n## #   factor_ff_rf_x_macro_dp <dbl>, factor_ff_rf_x_macro_dy <dbl>,\n## #   factor_ff_rf_x_macro_ep <dbl>, factor_ff_rf_x_macro_de <dbl>, ..."},{"path":"factor-selection-via-machine-learning.html","id":"build-a-model","chapter":"11 Factor selection via machine learning","heading":"11.3.2 Build a Model","text":"Next can build actual model based pre-processed data. line definition , estimate regression coefficients Lasso regression get\\[\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\] want emphasize tidymodels workflow model similar, irrespective specific model. see , piece cake fit Ridge regression coefficients - later - Neural networks Random forests basically code. structure always follows: create -called workflow use fit function. table available model APIs available .\nnow, start linear regression model given value penalty factor \\(\\lambda\\). setup , mixture denotes value \\(\\rho\\), hence setting mixture = 1 implies Lasso.’s - done! object lm_model contains definition model required information. Note set_engine(\"glmnet\") indicates API character tidymodels workflow: hood, package glmnet heavy lifting, linear_reg provides unified framework collect inputs. workflow ends combining everything necessary (serious) data science workflow: recipe model. Now ready use fit.","code":"\nlm_model <- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) %>%\n  set_engine(\"glmnet\", intercept = FALSE)\nlm_fit <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(lm_model)\nlm_fit## == Workflow ====================================================================\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## -- Preprocessor ----------------------------------------------------------------\n## 4 Recipe Steps\n## \n## * step_rm()\n## * step_interact()\n## * step_normalize()\n## * step_center()\n## \n## -- Model -----------------------------------------------------------------------\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = 1e-04\n##   mixture = 1\n## \n## Engine-Specific Arguments:\n##   intercept = FALSE\n## \n## Computational engine: glmnet"},{"path":"factor-selection-via-machine-learning.html","id":"fit-a-model","chapter":"11 Factor selection via machine learning","heading":"11.3.3 Fit a Model","text":"use training data fit model. training data pre-processed according recipe steps Lasso regression coefficients computed. First, focus predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure (ref?(fig:industrypremia)) illustrates projections entire time series Manufacturing industry portfolio returns. grey area indicates --sample period used fit model.estimated coefficients look like? analyse values illustrate difference tidymodels workflow underlying glmnet package, worth compute coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. code estimates coefficients Lasso Ridge regression processed training data sample. Note glmnet actually takes vector y matrix regressors \\(X\\) input. Moreover, glmnet requires choosing penalty parameter \\(\\alpha\\) corresponds \\(\\rho\\) notation . details need consideration using tidymodels model API.objects fit_lasso fit_ridge contain entire sequence estimated coefficients multiple values penalty factor \\(\\lambda\\). figure illustrates trajectory regression coefficients function penalty factor \\(\\lambda\\). penalty factor increases, Lasso Ridge coefficients converge zero.One word caution: package glmnet computes estimates coefficients \\(\\hat\\beta\\) based numerical optimization procedures. result estimated coefficients special case regularization (\\(\\lambda = 0\\)) can deviate standard OLS estimates.","code":"\npredicted_values <- lm_fit %>%\n  fit(data = training(split)) %>%\n  predict(data %>% filter(industry == \"Manuf\")) %>%\n  bind_cols(data %>% filter(industry == \"Manuf\")) %>%\n  select(month, .pred, ret) %>%\n  pivot_longer(-month, names_to = \"Variable\") %>%\n  mutate(Variable = case_when(\n    Variable == \".pred\" ~ \"Fitted Value\",\n    Variable == \"ret\" ~ \"Realization\"\n  )) \n\npredicted_values %>%\n  ggplot(aes(x = month, y = value, color = Variable)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Monthly Realized and Fitted Manufacturing Industry Risk Premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) seq.Date(from = min(x), to = max(x), by = \"5 years\"),\n    minor_breaks = function(x) seq.Date(from = min(x), to = max(x), by = \"1 years\"),\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  geom_rect(aes(\n    xmin = testing(split) %>% pull(month) %>% min(),\n    xmax = testing(split) %>% pull(month) %>% max(),\n    ymin = -Inf, ymax = Inf\n  ),\n  alpha = 0.005\n  ) +\n  theme(legend.position = \"bottom\")\nx <- tmp_data %>%\n  select(-ret) %>%\n  as.matrix()\ny <- tmp_data %>% pull(ret)\n\nfit_lasso <- glmnet(\n  x = x,\n  y = y,\n  alpha = 1, intercept = FALSE, standardize = FALSE,\n  lambda.min.ratio = 0\n) # Lasso\n\nfit_ridge <- glmnet(\n  x = x,\n  y = y,\n  alpha = 0, intercept = FALSE, standardize = FALSE,\n  lambda.min.ratio = 0\n) # Ridge\nbind_rows(\n  tidy(fit_lasso) %>% mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) %>% mutate(Model = \"Ridge\")\n) %>%\n  rename(\"Variable\" = term) %>%\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Lambda\", y = \"\",\n    title = \"Estimated Coefficients paths as a function of the penalty factor Lambda\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"},{"path":"factor-selection-via-machine-learning.html","id":"tune-a-model","chapter":"11 Factor selection via machine learning","heading":"11.3.4 Tune a Model","text":"compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , simply imposed value penalty hyperparameter \\(\\lambda\\). Model tuning process optimally selecting hyperparameters. tidymodels provides extensive tuning options based -called cross validation. , refer treatment cross-validation get detailed discussion statistical underpinnings, focus general idea implementation tidymodels.goal choosing \\(\\lambda\\) (hyperparameter, instance \\(\\rho\\)) find way produce predictors \\(\\hat{Y}\\) outcome \\(Y\\) minimizes mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, MSPE directly observable can compute estimate data random observe entire population.Obviously, train algorithm data use compute error, estimate \\(\\hat{\\text{MSPE}}\\) indicate way better predictive accuracy can expect real --sample data. result called overfitting.Cross validation technique allows us alleviate problem. approximate true MSPE average many mean squared prediction errors obtained creating predictions \\(K\\) new random samples data, none used train algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). practice, done carving piece data pretend independent sample. divide data training set test set. MSPE test set measure actual predictive ability, use training set fit models aim find optimal hyperparameter values. , divide training sample (several) subsets, fit model grid potential hyperparameter values (e.g. \\(\\lambda\\)) evaluate predictive accuracy independent sample. works follows:Specify grid hyperparametersObtain predictors \\(\\hat{y}_i(\\lambda)\\) denote predictors used parameters \\(\\lambda\\)Compute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2 \n\\] K-fold cross validation, computation \\(K\\) times. Simply pick validation set \\(M=T/K\\) observations random think random samples \\(y_1^k, \\dots, y_\\tilde{T}^k\\), \\(k=1\\).pick \\(K\\)? Large values \\(K\\) preferable training data better imitates original data. However, larger values \\(K\\) much higher computation time.\ntidymodels provides required tools conduct \\(K\\)-fold cross validation. just update model specification let tidymodels know parameters tune. case specify penalty factor \\(\\lambda\\) well mixing factor \\(\\rho\\) free parameters.sample, consider time-series cross validation sample. , tune models 20 random samples length 5 years validation period 4 years. grid possible hyperparameters, fit model fold evaluate \\(\\hat{\\text{MSPE}}\\) corresponding validation set. Finally, select model specification lowest MSPE validation set. First, define cross-validation folds based training data ., evaluate performance grid different penalty values. tidymodels provides functionalities construct suitable grid hyperparameters grid_regular. code chunk creates grid \\(10 \\times 3\\) hyperparameters function tune_grid tasks evaluating models fold.tuning process, collect evaluation metrics (RMSE example) identify optimal model. Figure ?? illustrates average validation set root mean-squared error value \\(\\lambda\\) \\(\\rho\\).","code":"\nlm_model <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n\n# Update the existing model\nlm_fit <- lm_fit %>%\n  update_model(lm_model)\ndata_folds <- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\nlm_tune <- lm_fit %>%\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),\n    metrics = metric_set(rmse)\n  )\nautoplot(lm_tune) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"Root mean-squared prediction error\",\n       title = \"MSPE for manufacturing excess returns\",\n       subtitle = \"Lasso (1.0), Ridge (0.0) and Elastic net (0.5) with different levels of regularization.\")"},{"path":"factor-selection-via-machine-learning.html","id":"parallelized-workflow","chapter":"11 Factor selection via machine learning","heading":"11.3.5 Parallelized Workflow","text":"starting point question: factors determine industry returns? illustrate entire workflow, now run penalized regressions 10 industries. want identify relevant variables fitting Lasso models time-series industry returns. specifically, industry, perform cross-validation identify optimal penalty factor \\(\\lambda\\). , use set finalize_* functions take list tibble tuning parameter values update objects values. determining best model, compute final fit entire training set analyse estimated coefficients.First define Lasso model one tuning parameter:following task can easily parallelized substantially reduce computing time. use parallelization capabilities furrr. Note can also just recycle steps collect function.just happened? principle, exactly instead computing Lasso coefficients one industry 10 parallel. Now just housekeeping keep variables Lasso set zero. illustrate results heat map.heat map conveys two main insights: First, see lot white simply means lot factors, macroeconomic variables also interaction terms relevant comes explain cross-section returns across industry portfolios. fact, market factor return--equity factor play role several industries. Second, seems quite heterogeneity across different industries. even market factor selected Lasso Utilities (means proposed model essentially just contains intercept), quite number factors selected , e.g., High-Tech Energy coincide . words, seems clear picture need lot factors, Lasso provide conses across industries comes pricing abilities.","code":"\nlasso_model <- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) %>%\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit %>%\n  update_model(lasso_model)\nselect_variables <- function(input) {\n  # Split into training and testing dataset\n  split <- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross validation\n  data_folds <- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune <- lm_fit %>%\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Finalizing: Identify the best model and fit with the training data\n  lasso_lowest_rmse <- lm_tune %>% select_by_one_std_err(\"rmse\")\n  lasso_final <- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit <- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  lasso_final_fit %>%\n    extract_fit_parsnip() %>%\n    tidy() %>%\n    mutate(\n      term = gsub(\"factor_|macro_|industry_\", \"\", term)\n    )\n}\n\nplan(multisession, workers = availableCores()) # Parallelization\n\nselected_factors <- data %>%\n  nest(data = -industry) %>% # Computation by industry\n  mutate(selected_variables = future_map(data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  )) # Seed is required to make the cross-validation process reproducible\nselected_factors %>%\n  unnest(selected_variables) %>%\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) %>%\n  add_count(term) %>%\n  mutate(\n    term = gsub(\"NA|ff_|q_\", \"\", term),\n    term = gsub(\"_x_\", \" \", term),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) %>%\n  filter(term != \"Other\") %>%\n  mutate(term = fct_drop(term)) %>%\n  complete(industry, term, fill = list(selected = 0)) %>%\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"cornflowerblue\")) +\n  theme_minimal() +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"exercises-1","chapter":"11 Factor selection via machine learning","heading":"11.4 Exercises","text":"Write function requires three inputs, y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), lambda returns Ridge estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function allow indicate whether \\(X\\) contains vector ones first column exempt \\(L_2\\) penalty.Compute \\(L_2\\) norm (\\(\\beta'\\beta\\)) regression coefficients based predictive regression previous exercise range \\(\\lambda\\)’s illustrate effect penalization suitable figure.Now, write function requires three inputs, y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), ’lambda` returns Lasso estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function allow indicate whether \\(X\\) contains vector ones first column exempt \\(L_1\\) penalty.really sure understand Ridge Lasso regression , familiarize documentation package glmnet(). thoroughly tested well-established package provides efficient code compute penalized regression coefficients Ridge Lasso also combinations therefore, commonly called elastic nets.","code":""},{"path":"option-pricing-via-machine-learning-methods.html","id":"option-pricing-via-machine-learning-methods","chapter":"12 Option Pricing via Machine learning methods","heading":"12 Option Pricing via Machine learning methods","text":"Machine learning seen part artificial intelligence.\nMachine learning algorithms build model based training data order make predictions decisions without explicitly programmed .\nMachine learning can specified along vast array different branches, chapter focuses -called supervised learning regressions. basic idea supervised learning algorithms build mathematical model data contains inputs desired outputs. chapter, apply well-known methods random forests neural networks simple application Option pricing. specifically, going create artificial dataset option prices different values based Black-Scholes pricing equation Call options. , train different models learn price Call options without prior knowledge theoretical underpinnings famous Option pricing equation.roadmap follows: first provide brief introduction regression trees, random forests neural networks. focus implementation, leave thorough treatment statistical underpinnings textbooks authors real comparative advantage issues.\nshow implement random forests deep neural networks tidy principles using tidymodels tensorflow complicated network structures.order replicate analysis regarding neural networks chapter, install TensorFlow system requires administrator rights machine. Parts can done within R, just follow quick start instructions.Throughout chapter need following packages.","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(keras)\nlibrary(hardhat)"},{"path":"option-pricing-via-machine-learning-methods.html","id":"regression-trees-and-random-forests","chapter":"12 Option Pricing via Machine learning methods","heading":"12.1 Regression trees and random forests","text":"Regression trees become popular machine learning approach incorporating multiway predictor interactions. Trees fully nonparametric possess logic departs markedly traditional regressions. Trees designed find groups observations behave similarly . tree “grows” sequence steps. step, new “branch” sorts data left preceding step bins based one predictor variables. sequential branching slices space predictors rectangular partitions, approximates unknown function \\(f(x)\\) average value outcome variable within partitionWe partition predictor space \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). predictor \\(x\\) falls within region \\(R_j\\) estimate \\(f(x)\\) average training observations, \\(\\hat y_i\\), associated predictor \\(x_i\\) also \\(R_j\\). select partition \\(\\mathbf{x}\\) split order create new partitions, find predictor \\(j\\) value \\(s\\) define two new partitions, call \\(R_1(j,s)\\) \\(R_2(j,s)\\), split observations current partition asking \\(x_j\\) bigger \\(s\\):\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j < s\\} \\mbox{   } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\npick \\(j\\) \\(s\\) find pair minimizes residual sum square (RSS):\n\\[\n\\sum_{:\\, x_i \\R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{:\\, x_i \\R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nNote: Unlike sample variance, don’t scale number elements \\(R_k(j, s)\\)! chapter penalized regressions, first relevant question ask : hyperparameters decisions? Instead regularization parameter, trees fully determined number branches used generate partition (sometimes one specifies minimum number observations final branch instead maximum number branches).Single tree models suffer high variance. Random forests address shortcomings decision trees. goal improve prediction performance reduce instability averaging multiple decision trees (forest trees constructed randomness). forest basically implies create many regression trees average predictions. assure individual trees , use bootstrap induce randomness. specifically, build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using training sample. purpose randomly select features included building tree. observation test set form prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{=1}^B\\hat{y}_{T_i}\\).","code":""},{"path":"option-pricing-via-machine-learning-methods.html","id":"neural-networks","chapter":"12 Option Pricing via Machine learning methods","heading":"12.2 Neural Networks","text":"Roughly speaking, neural networks propagate information input layer, one multiple hidden layers, output layer. number units (neurons) input layer equal dimension predictors, output layer usually consists one neuron (regression) multiple neurons classification. output layer predicts future data, similar fitted value regression analysis. Neural networks theoretical underpinnings “universal approximators” smooth predictive association (Hornik1991?). complexity, however, ranks neural networks among least transparent, least interpretable, highly parameterized machine learning toolsEach neuron applies nonlinear “activation function” \\(f\\) aggregated signal \nsending output next layer\n\\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\]\neasiest case \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions sigmoid (\\(f(x) = (1+e^{-x})^{-1}\\)) ReLu (\\(f(x) = max(x, 0)\\))Neural networks gain flexibility form chaining multiple layers together. Naturally, imposes large number degrees freedom network architecture clear theoretical guidance exist. specification neural network requires, minimum, stancen depth (number hidden layers), activation function, number neurons, \nconnection structure units (dense sparse), application regularization techniques avoid overfitting. Finally, learning means choose optimal parameters relys numerical optimization often requires specify appropriate learning\nrate.Despite computational challenges, implementation R tedious \ncan use API tensorflow.","code":""},{"path":"option-pricing-via-machine-learning-methods.html","id":"option-pricing","chapter":"12 Option Pricing via Machine learning methods","heading":"12.3 Option Pricing","text":"apply Machine Learning methods relevant field finance focus option pricing. basic form, Call options give owner right obligation buy specific stock (underlying) specific price (strike price \\(K\\)) specific date (exercise date \\(T\\)). Black–Scholes price ([Black1971]) call option non-dividend-paying underlying stock given \n\\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left[\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right]\n\\end{aligned}\n\\]\n\\(C(S, T)\\) price option function today’s stock price underlying, \\(S\\), time maturity\\(T\\), \\(r_f\\) risk-free interest rate, \\(\\sigma\\) volatility underlying stock return. \\(\\Phi\\) cumulative distribution function standard normal random variable.Black-Scholes equation provides easy way compute arbitrage-free price Call option parameters \\(S, K, r_f, T\\) \\(\\sigma\\) specified (arguably, parameters easily specify except \\(\\sigma\\) estimated). simple R function allows compute price .","code":"\nblack_scholes_price <- function(S = 50, K = 70, r = 0, T = 1, sigma = 0.2) {\n  # Arbitrage-free price of a Call option\n  d1 <- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  value <- S * pnorm(d1) - K * exp(-r * T) * pnorm(d1 - sigma * sqrt(T))\n  return(value)\n}"},{"path":"option-pricing-via-machine-learning-methods.html","id":"learning-black-scholes","chapter":"12 Option Pricing via Machine learning methods","heading":"12.4 Learning Black-Scholes","text":"illustrate concept machine learning showing machine learning methods learn Black-Scholes equation observing different specifications corresponding prices without us revealing exact pricing equation.","code":""},{"path":"option-pricing-via-machine-learning-methods.html","id":"data-simulation","chapter":"12 Option Pricing via Machine learning methods","heading":"12.4.1 Data simulation","text":"end start simulated data. compute option prices Call options grid different combinations times maturity (T), risk-free rate (r), volatility (sigma), strike prices (K) current stock prices (S). code add idiosyncratic error term observation prices considered observed exactly reflect values implied Black-Scholes equation.code generates 1.574496^{6} random parameter constellations values two observed prices reflect Black-Scholes prices random innovation term pollutes observed prices.Next, split data training set (contains 1% observed option prices) test set going used final evaluation. Note entire grid possible combinations contains 3148992 different specifications, thus sample learn Black-Scholes price contains 3.1489^{4} therefore relatively small.\norder keep analysis reproducible, use set.seed(). random seed specifies start point computer generates random number sequence ensures simulated data across different machines.process training dataset fit different Machine learning models. purpose define recipe defines processing steps. specific case want explain observed price 5 variables enter Black-Scholes equation. true price obviously used fit model. recipe also reflect standardize predictors ensure variable exhibits sample average zero sample standard deviation one.Next, propose two ways fit neural network data. Note require keras installed local machine. function mlp package parsnip provides functionality initialize single layer, feed-forward neural network. specification defines single layer feed-forward neural network 20 hidden units. set number training iterations epochs = 75. option set_mode(\"regression\") specifies linear activation function output layer.","code":"\noption_prices <- expand_grid(\n  S = 40:60, # stock price\n  K = 20:90, # strike price\n  r = seq(from = 0, to = 0.05, by = 0.01), # risk-free rate\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12), # Time to maturity\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1)\n) %>%   mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma), # Option price in theory\n    observed_price = map(black_scholes, function(x) x + rnorm(2, sd = 0.15))\n  ) %>% # Add some random deviations to each option price\n  unnest(observed_price)\nset.seed(42809) # Ensure the analysis can be reproduced\nsplit <- initial_split(option_prices, prop = 1 / 100)\nrec <- recipe(observed_price ~ .,\n  data = option_prices\n) %>%\n  step_rm(black_scholes) %>% # Exclude the true price\n  step_normalize(all_predictors())"},{"path":"option-pricing-via-machine-learning-methods.html","id":"random-forests-and-single-layer-networks","chapter":"12 Option Pricing via Machine learning methods","heading":"12.4.2 Random forests and single layer networks","text":"can follow straightforward tidymodel workflow chapter : Define workflow, equip recipe associated model. Finally, fit model training data.familiar tidymodel workflow, piece cake fit models parsnip family. instance, model initializes random forest 50 trees contained ensemble require least 20 observations node.Fitting model follows exactly convention neural network .","code":"\n# Single layer neural network\nnnet_model <- mlp(\n  epochs = 75,\n  hidden_units = 20\n) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"keras\", verbose = 0) # `verbose=0` argument prevents logging the results\nnn_fit <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(nnet_model) %>%\n  fit(data = training(split))\nrf_model <- rand_forest(\n  trees = 50,\n  min_n = 20\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\nrf_fit <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(rf_model) %>%\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning-methods.html","id":"deep-neural-networks","chapter":"12 Option Pricing via Machine learning methods","heading":"12.4.3 Deep neural networks","text":"Note tidymodels workflow extremely convenient, sophisticated deep neural networks supported yet (January 2022). reason, code snippet illustrates initialize sequential model 3 hidden layers 20 units per layer. keras package provides convenient interface flexible enough handle different activation functions. compile command defines loss function model predictions evaluated.train neural network, simply provide inputs (x) variable predict (y) fit parameters. Note slightly tedious use method extract_mold(nn_fit): instead simply using raw data, fit neural network processed data used single-layer feed-forward network. difference simply calling x = training(data) %>% select(-observed_price, -black_scholes)? Recall, recipe standardizes variables columns unit standard deviation zero mean. , adds consistency ensure models trained using recipe change recipe reflected performance model. final note potentially irritating observation: Note fit() alters keras model: one instances function R alters input calling function object model going anymore!","code":"\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 20, activation = \"sigmoid\", input_shape = 5) %>%\n  layer_dense(units = 20, activation = \"sigmoid\") %>%\n  layer_dense(units = 20, activation = \"sigmoid\") %>%\n  layer_dense(units = 1, activation = \"linear\") %>%\n  compile(\n    loss = \"mean_absolute_error\"\n  )\nmodel## Model\n## Model: \"sequential_1\"\n## ________________________________________________________________________________\n## Layer (type)                        Output Shape                    Param #     \n## ================================================================================\n## dense_5 (Dense)                     (None, 20)                      120         \n## ________________________________________________________________________________\n## dense_4 (Dense)                     (None, 20)                      420         \n## ________________________________________________________________________________\n## dense_3 (Dense)                     (None, 20)                      420         \n## ________________________________________________________________________________\n## dense_2 (Dense)                     (None, 1)                       21          \n## ================================================================================\n## Total params: 981\n## Trainable params: 981\n## Non-trainable params: 0\n## ________________________________________________________________________________\nmodel %>%\n  fit(\n    x = extract_mold(nn_fit)$predictors %>% as.matrix(),\n    y = extract_mold(nn_fit)$outcomes %>% pull(observed_price),\n    epochs = 75, verbose = 0\n  )"},{"path":"option-pricing-via-machine-learning-methods.html","id":"universal-approximation","chapter":"12 Option Pricing via Machine learning methods","heading":"12.4.4 Universal approximation","text":"comes evaluation implement one final model: principle, non-linear function can also approximated linear model contains polynomial expansions input variables. illustrate first define new recipe, rec_linear, processes training data even : include polynomials tenth degree predictor add possible pairwise interaction terms. final recipe step step_lincomb removes potentially redundant variables (instance, interaction \\(r^4\\) \\(r^5\\) term \\(r^9\\)). fit Lasso regression model pre-specified penalty term (consult chapter factor selection tune model hyperparameters).","code":"\nrec_linear <- rec %>%\n  step_poly(all_predictors(), degree = 10, options = list(raw = T)) %>%\n  step_interact(terms = ~ all_predictors():all_predictors()) %>%\n  step_lincomb(all_predictors())\n\nlm_model <- linear_reg(penalty = 0.01) %>%\n  set_engine(\"glmnet\")\n\nlm_fit <- workflow() %>%\n  add_recipe(rec_linear) %>%\n  add_model(lm_model) %>%\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning-methods.html","id":"evaluating-predictions","chapter":"12 Option Pricing via Machine learning methods","heading":"12.5 Evaluating predictions","text":"Finally, collect predictions compare --sample prediction error.\nNote, evaluation use, , call extract_mold ensure use pre-processing steps testing data across model. make also use somewhat advanced functionality hardhat::forge provides easy, consistent, robust pre-processor prediction time.lines use fitted models generate predictions entire test data set option prices. one possible measure pricing accuracy evaluate absolute pricing error, defined absolute value difference predicted option price theoretical correct option price Black-Scholes model.results can summarized follow: ) machine learning methods seem able price Call options observing training test set. ii) average prediction errors increase far --money options, especially Single Layer neural network. ii) Random forest seems perform consistently better prediction option prices Single Layer network. iii) deep neural network yields best --sample predictions.","code":"\nout_of_sample_data <- testing(split) %>% slice_sample(n = 10000) # We evaluate the predictions based on 100k new data points\n\npredictive_performance <- model %>%\n  predict(forge(out_of_sample_data, extract_mold(nn_fit)$blueprint)$predictors %>% as.matrix()) %>%\n  as.vector() %>%\n  tibble(\"Deep NN\" = .) %>%\n  bind_cols(nn_fit %>%\n    predict(out_of_sample_data)) %>%\n  rename(\"Single Layer\" = .pred) %>%\n  bind_cols(lm_fit %>% predict(out_of_sample_data)) %>%\n  rename(\"Lasso\" = .pred) %>%\n  bind_cols(rf_fit %>% predict(out_of_sample_data)) %>%\n  rename(\"Random Forest\" = .pred) %>%\n  bind_cols(out_of_sample_data) %>%\n  pivot_longer(\"Deep NN\":\"Random Forest\", names_to = \"Model\") %>%\n  mutate(\n    moneyness = (S - K),\n    pricing_error = sqrt((value - black_scholes)^2) # mean squared prediction error\n  ) \npredictive_performance %>%\n  ggplot(aes(x = moneyness, y = pricing_error, color = Model)) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Moneyness (S - K)\", color = NULL, \n       y = \"Mean Squared Prediction Error (USD)\", \n       title = \"Prediction Errors: Call option prices\") "},{"path":"option-pricing-via-machine-learning-methods.html","id":"exercises-2","chapter":"12 Option Pricing via Machine learning methods","heading":"12.6 Exercises","text":"Write function takes y, matrix predictors X inputs returns characterization relevant parameters regression tree 1 branch.Create function allows create predictions new matrix predictors `newX´ based estimated regression tree.Use package rpart grow tree based training data use illustration tools rpart understand characteristics tree deems relevant option pricing.Make use training test set choose optimal depth (number sample splits) treeUse ‘keras’ initialize sequential neural network can take predictors training dataset input, contains least one hidden layer generates continuous predictions. sounds harder : see simply regression example . many parameters neural network aim fit ?Next, compile object. important specify loss function. Illustrate difference predictive accuracy different architecture choices.","code":""},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies","chapter":"13 Parametric Portfolio Policies","heading":"13 Parametric Portfolio Policies","text":"section, introduce different portfolio performance measures evaluate compare different allocation strategies. purpose, introduce direct (probably simplest) way estimate optimal portfolio weights stock characteristics related stock’s expected return, variance, covariance stocks: parametrize weights function characteristics maximize expected utility. approach feasible large portfolio dimensions (entire CRSP universe) proposed (Brandt, Santa-Clara, Valkanov 2009) influential paper Parametric Portfolio Policies: Exploiting Characteristics Cross Section Equity Returns.","code":""},{"path":"parametric-portfolio-policies.html","id":"data-preparation-5","chapter":"13 Parametric Portfolio Policies","heading":"13.1 Data preparation","text":"get started, load monthly CRSP file forms investment universe.purpose performance evaluation, need monthly market returns order able compute CAPM alphas retrieve Fama French 3 factor model data.Next create characteristics proposed literature effect expected returns expected variances (even higher moments) return distribution: record firm lagged one-year return (mom) defined compounded return months \\(t − 13\\) \\(t − 2\\) firm’s market equity (), defined log price per share times number shares outstanding","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\n# Load data from database\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\ncrsp_monthly_lags <- crsp_monthly %>% \n  transmute(permno, \n            month_12 = month %m+% months(12), \n            month_1 = month %m+% months(1),\n            altprc = abs(altprc))\n\ncrsp_monthly <- crsp_monthly %>% \n  inner_join(crsp_monthly_lags %>% select(-month_1), \n             by = c(\"permno\", \"month\" = \"month_12\"), suffix = c(\"\", \"_12\")) %>% \n  inner_join(crsp_monthly_lags %>% select(-month_12), \n             by = c(\"permno\", \"month\" = \"month_1\"), suffix = c(\"\", \"_1\"))\n\n# Create characteristics for portfolio tilts\ncrsp_monthly <- crsp_monthly %>%\n  group_by(permno) %>%\n  mutate(\n    momentum_lag = altprc_1 / altprc_12, # Gross returns TODO for SV: Do we need slider?\n    size_lag = log(mktcap_lag)\n  ) %>%\n  drop_na(contains(\"lag\"))"},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies-1","chapter":"13 Parametric Portfolio Policies","heading":"13.2 Parametric Portfolio Policies","text":"basic idea parametric portfolio weights easy explain: Suppose date \\(t\\) \\(N_t\\) stocks investment universe, stock \\(\\) return \\(r_{, t+1}\\) associated vector firm characteristics \\(x_{, t}\\) time-series momentum market capitalization. investors problem choose portfolio weights \\(w_{,t}\\) maximize expected utility portfolio return:\n\\[\\begin{align}\n\\max_{w} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{=1}^{N_t}w_{,t}r_{,t+1}\\right)\\right]\n\\end{align}\\]\n\\(u(\\cdot)\\) denotes utility function.\nstock characteristics show ? parametrize optimal portfolio weights function stock characteristic \\(x_{,t}\\) following linear specification portfolio weights:\n\\[w_{,t} = \\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\] \\(\\bar{w}_{,t}\\) weight benchmark portfolio (use value-weighted naive portfolio application ), \\(\\theta\\) vector coefficients going estimate \\(\\hat{x}_{,t}\\) characteristics stick \\(\\), cross-sectionally standardized zero mean unit standard deviation. Think portfolio strategy form active portfolio management relative performance benchmark: Deviations benchmark portfolio derived individual stock characteristics. Note construction weights sum one \\(\\sum_{=1}^{N_t}\\hat{x}_{,t} = 0\\) due standardization. Note also coefficients constant across assets time. implicit assumption characteristics fully capture aspects joint distribution returns relevant forming optimal portfolios.first implement cross-sectional standardization entire CRSP universe. also keep track (lagged) relative market capitalization represent value-weighted benchmark portfolio.","code":"\ncrsp_monthly <- crsp_monthly %>%\n  group_by(month) %>%\n  mutate(\n    n = n(), # number of traded assets N_t (benchmark for naive portfolio)\n    relative_mktcap = mktcap_lag / sum(mktcap_lag), # Value weighting benchmark\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)), # standardization. Note: Code handles every column with \"lag\" as a characteristic)\n  ) %>%\n  ungroup() %>%\n  select(-mktcap_lag, -altprc)"},{"path":"parametric-portfolio-policies.html","id":"compute-portfolio-weights","chapter":"13 Parametric Portfolio Policies","heading":"13.3 Compute portfolio weights","text":"Next can move optimal choices \\(\\theta\\). rewrite optimization problem together weight parametrisation can estimate \\(\\theta\\) maximize objective function based sample\n\\[\\begin{align}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{=1}^{N_t}\\left(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\right)r_{,t+1}\\right).\n\\end{align}\\]\nallocation strategy simple number parameters estimate small. Instead tedious specification \\(N_t\\) dimensional vector expected returns \\(N_t(N_t+1)/2\\) free elements variance covariance, need focus application vector \\(\\theta\\) contains 2 elements application - relative deviation benchmark due size due past returns.get feeling performance allocation strategy start arbitrary vector \\(\\theta\\) - next step choose \\(\\theta\\) optimal fashion maximize objective function.following function computes portfolio weights \\(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\) according parametrization given value \\(\\theta\\). Everything happens within single pipeline, thus goes quick walk-:first compute characteristic_tilt, tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{, t}\\) resemble deviation benchmark portfolio. Next, compute benchmark portfolio weight_benchmark principle can reasonable set portfolio weights. case choose either value- equal-weighted allocation.\nweight_tilt completes picture contains final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt deviate benchmark portfolio depending stock characteristics.final lines go bit implement simple version -short sale constraint. generally straightforward ensure portfolio weight constraints via parametrization, simply normalize portfolio weights enforced positive. Finally, make sure normalized weights sum one . \\[w_{,t}^+ = \\frac{\\max(0, w_{,t})}{\\sum\\limits_{j=1}^{N_t}\\max(0, w_{,t})}.\\]\ngo optimal portfolio weights 20 lines.Done! Next step compute portfolio weights given vector \\(\\theta\\) convenience. example use value weighted portfolio benchmark allow negative portfolio weights.","code":"\n# Automatic detection of parameters and initialization of parameter vector theta\nnumber_of_param <- sum(grepl(\"lag\", crsp_monthly %>% colnames()))\ntheta <- rep(1.5, number_of_param) # We start with some arbitrary initial values for theta\nnames(theta) <- colnames(crsp_monthly)[grepl(\"lag\", crsp_monthly %>% colnames())]\ncompute_portfolio_weights <- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data %>%\n    group_by(month) %>%\n    bind_cols(\n      characteristic_tilt = data %>% # Computes theta'x / N_t\n        transmute(across(contains(\"lag\"), ~ . / n)) %>%\n        as.matrix() %*% theta %>% as.numeric()\n    ) %>%\n    mutate(\n      weight_benchmark = case_when( # Definition of the benchmark weight\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      weight_tilt = weight_benchmark + characteristic_tilt, # Parametric Portfolio Weights\n      weight_tilt = case_when( # Short-sell constraint\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      weight_tilt = weight_tilt / sum(weight_tilt) # Weights sum up to 1\n    ) %>%\n    ungroup()\n}\nweights_crsp <- compute_portfolio_weights(theta,\n  crsp_monthly,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)"},{"path":"parametric-portfolio-policies.html","id":"portfolio-perfomance","chapter":"13 Parametric Portfolio Policies","heading":"13.4 Portfolio perfomance","text":"weights optimal way? likely chose \\(\\theta\\) arbitrarily. order evaluate performance allocation strategy, one can think many different approaches. original paper, (Brandt, Santa-Clara, Valkanov 2009) focus simple evaluation hypothetical utility agent equipped power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^\\gamma}{1-\\gamma}\\), \\(\\gamma\\) risk aversion factor.doubt, many ways evaluate portfolio. function provides summary kinds interesting measures can considered relevant.\nneed evaluation measures? depends: original paper cares \nexpected utility order choose \\(\\theta\\). want choose optimal values achieve highest performance putting constraints portfolio weights helpful everything one function.Let’s take look different portfolio strategies evaluation measures.value weighted portfolio delivers annualized return 6 percent clearly outperforms tilted portfolio, irrespective whether evaluate expected utility, Sharpe ratio CAPM alpha. can conclude market beta close one strategies (naturally almost identically 1 value-weighted benchmark portfolio). comes distribution portfolio weights, see benchmark portfolio weight takes less extreme positions (lower average absolute weights lower maximum weight). definition, value-weighted benchmark take negative positions, tilted portfolio also takes short positions.","code":"\npower_utility <- function(r, gamma = 5) { \n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\nevaluate_portfolio <- function(weights_crsp,\n                               full_evaluation = TRUE) {\n\n  evaluation <- weights_crsp %>%\n    group_by(month) %>%\n    # Compute monthly portfolio returns\n    summarise(\n      return_tilt = weighted.mean(ret_excess, weight_tilt),\n      return_benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) %>%\n    pivot_longer(-month, values_to = \"portfolio_return\", names_to = \"model\") %>%\n    group_by(model) %>%\n    left_join(factors_ff_monthly, by = \"month\") %>% # FF data to compute alpha\n    summarise(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(12 * portfolio_return),\n      \"SD return\" = 100 * sqrt(12) * sd(portfolio_return),\n      \"Sharpe ratio\" = mean(portfolio_return) / sd(portfolio_return),\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n    )) %>%\n    mutate(model = gsub(\"return_\", \"\", model)) %>%\n    pivot_longer(-model, names_to = \"measure\") %>%\n    pivot_wider(names_from = model, values_from = value)\n\n  if (full_evaluation) { # additional values based on the portfolio weights\n    weight_evaluation <- weights_crsp %>%\n      select(month, contains(\"weight\")) %>%\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") %>%\n      group_by(model, month) %>%\n      transmute(tibble(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight < 0]),\n        \"Avg. fraction of negative weights\" = sum(weight < 0) / n()\n      )) %>%\n      group_by(model) %>%\n      summarise(across(-month, ~ 100 * mean(.))) %>%\n      mutate(model = gsub(\"weight_\", \"\", model)) %>%\n      pivot_longer(-model, names_to = \"measure\") %>%\n      pivot_wider(names_from = model, values_from = value)\n    evaluation <- bind_rows(evaluation, weight_evaluation)\n  }\n  return(evaluation)\n}\nevaluate_portfolio(weights_crsp) %>% \n  kableExtra::kable(digits = 3)"},{"path":"parametric-portfolio-policies.html","id":"optimal-parameter-choice","chapter":"13 Parametric Portfolio Policies","heading":"13.5 Optimal parameter choice","text":"Next move choice \\(\\theta\\) actually aims improve () performance measures. first define helper function compute_objective_function passed R’s optimization schemes.may wonder return negative value objective function. simply due common convention optimization procedures search minima default. minimizing negative value objective function get maximum value result.\nOptimization R basic form done optim. main inputs, function requires initial “guess” parameters, function minimize. fully equipped compute optimal values \\(\\hat\\theta\\) maximize hypothetical expected utility investor.chosen values \\(\\theta\\) easy interpret intuitive basis: Expected utility increases tilting weights value weighted portfolio towards smaller stocks (negative coefficient size) towards past winners (positive value momentum).","code":"\ncompute_objective_function <- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting,\n                                       allow_short_selling) {\n  processed_data <- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function <- evaluate_portfolio(processed_data, full_evaluation = FALSE) %>%\n    filter(measure == objective_measure) %>%\n    pull(tilt)\n\n  return(-objective_function)\n}\noptimal_theta <- optim(\n  par = theta, # Initial vector of thetas (can be any value)\n  compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = crsp_monthly,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)\n\noptimal_theta$par # Optimal values## momentum_lag     size_lag \n##    0.5889182   -2.0715581"},{"path":"parametric-portfolio-policies.html","id":"more-model-specifications","chapter":"13 Parametric Portfolio Policies","heading":"13.6 More model specifications","text":"final open question : portfolio perform different model specifications? purpose compute performance number different modelling choices based entire CRSP sample. lines perform heavy lifting.Finally, can compare results. table shows summary statistics possible combinations: Equal- Value-weighted benchmark portfolio, without short-selling constraints, tilted towards maximizing expected utility.results indicate first average annualized Sharpe ratio equal weighted portfolio exceeded Sharpe ratio value weighted benchmark portfolio. Nevertheless, starting value weighted portfolio benchmark tilting optimally respect momentum small stocks yields highest Sharpe ratio across specifications. Imposing short-sale constraints improve performance portfolios application.","code":"\nfull_model_grid <- expand_grid(\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE),\n  data = list(crsp_monthly)\n) %>%\n  mutate(optimal_theta = pmap(\n    .l = list(\n      data,\n      value_weighting,\n      allow_short_selling\n    ),\n    .f = ~ optim(\n      par = theta,\n      compute_objective_function,\n      data = ..1,\n      objective_measure = \"Expected utility\",\n      value_weighting = ..2,\n      allow_short_selling = ..3)$par\n  ))\nperformance_table <- full_model_grid %>%\n  mutate(\n    processed_data = pmap(\n      .l = list(optimal_theta, data, value_weighting, allow_short_selling),\n      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)\n    ),\n    portfolio_evaluation = map(processed_data, evaluate_portfolio, full_evaluation = TRUE)\n  ) %>%\n  select(value_weighting, allow_short_selling, portfolio_evaluation) %>%\n  unnest(portfolio_evaluation)\n\nperformance_table %>%\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) %>%\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) %>%\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) %>%\n  select(measure, `EW    `, `VW    `, sort(contains(\"Optimal\"))) %>%\n  kableExtra::kable(digits = 3)"},{"path":"parametric-portfolio-policies.html","id":"exercises-3","chapter":"13 Parametric Portfolio Policies","heading":"13.7 Exercises","text":"estimated parameters \\(\\hat\\theta\\) portfolio performance change objective maximize Sharpe ratio instead hypothetical expected utility?code flexible sense can easily add new firm characteristics. Construct new characteristic evaluate corresponding coefficient \\(\\hat\\theta_i\\).Tweak function optimal_thetasuch can impose additional performance constraints order determine \\(\\hat\\theta\\) maximizes expected utility constraint market beta 1.portfolio performance resemble realistic --sample backtesting procedure? Verify robustness results first estimating \\(\\hat\\theta\\) based past data use recent periods evaluate actual portfolio performance.formulating portfolio problem statistical estimation problem, can easily obtain standard errors coefficients weight function. (Brandt, Santa-Clara, Valkanov 2009) provide relevant derivations paper Equation (10). Implement small function computes standard errors \\(\\hat\\theta\\).","code":""},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"constraint-optimization-and-portfolio-backtesting","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14 Constraint Optimization and Portfolio Backtesting","text":"section conduct portfolio back testing realistic setting transaction costs investment constraints -short selling rules.start standard mean-variance efficient portfolios introduce constraints step--step. Numerical constrained optimization done packages quadprog (quadratic objective functions typical mean-variance framework) alabama (general, non-linear objectives constraints).","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(quadprog) # Optimization (mean-variance)\nlibrary(alabama)  # Advanced optimization (non-linear objective or constraints)\nlibrary(scales)"},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"data-preparation-6","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.1 Data preparation","text":"start loading required data. application restrict investment universe monthly Fama-French industry portfolio returns.","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\n# Load industry data\nindustry_returns <- tbl(tidy_finance, \"industries_ff_monthly\") %>% \n  collect() \n\nindustry_returns <- industry_returns %>% \n  select(-month)\n\nN <- ncol(industry_returns) # Number of assets"},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"recap-portfolio-choice","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.2 Recap: Portfolio choice","text":"First brief recap. common objective portfolio optimization choose mean-variance efficient portfolio weights, allocation delivers lowest possible return variance given minimum level expected returns. extreme case investor concerned portfolio variance, may choose implement minimum variance portfolio weights given solution \n\\[w_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1\\]\n\\(\\Sigma\\) \\((N \\times N)\\) variance covariance matrix returns. optimal weights \\(\\omega_\\text{mvp}\\) can found analytically \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). code, equivalent following:Next, consider investor aims achieve minimum variance given required expected portfolio return \\(\\bar{\\mu}\\) chooses\n\\[w_\\text{eff}({\\bar{\\mu}}) =\\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\ncan shown (see Exercises) portfolio choice problem can equivalently formulated investor mean-variance preferences risk aversion factor \\(\\gamma\\). investor aims choose portfolio weights \n\\[\\begin{aligned} w^*_\\gamma = \\arg\\max w' \\mu - \\frac{\\gamma}{2}w'\\Sigma w\\end{aligned}\\quad s.t. w'\\iota = 1.\\]\nsolution optimal portfolio choice problem :\n\\[\\begin{aligned}\n \\omega^*_{\\gamma} & = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\n \\end{aligned}\\]\nEmpirically classical solution imposes many problems: Especially estimates \\(\\mu_t\\) noisy short horizons, (\\(N \\times N\\)) matrix \\(\\Sigma_t\\) contains \\(N(N-1)/2\\) distinct elements thus, estimation error huge. Even worse, asset universe contains assets available time periods \\((N > T)\\), sample variance covariance matrix longer positive definite inverse \\(\\Sigma^{-1}\\) exist anymore. top estimation uncertainty, transaction costs major concern. Rebalancing portfolios costly therefore optimal choice depend current holdings investor.","code":"\nSigma <- cov(industry_returns)\nw_mvp <- solve(Sigma) %*% rep(1, ncol(Sigma))\nw_mvp <- as.vector(w_mvp / sum(w_mvp))"},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"estimation-uncertainty-and-transaction-costs","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.3 Estimation uncertainty and transaction costs","text":"empirical evidence regarding performance mean-variance optimization procedure simply plugin sample estimates \\(\\hat \\mu_t\\) \\(\\hat \\Sigma_t\\) can summarised rather easily: Mean-variance optimization performs badly! literature brought forward many proposals overcome empirical issues. instance, one may impose form regularization \\(\\Sigma\\), rely Bayesian priors inspired theoretical asset pricing models, use high-frequency data improve forecasting. One unifying framework works easily, effective (even large dimensions) purely inspired economic arguments ex-ante adjustment transaction costs (Hautsch2019?).Assume returns multivariate normal distributed : \\(p_t({r}_{t+1}|\\mathcal{M})=N(\\mu,\\Sigma)\\). Additionally assume quadratic transaction costs penalize rebalancing \\[\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) :=\\nu_t\\left(\\omega_{t+1}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\n\\end{aligned}\\]\ncost parameter \\(\\beta>0\\) \\(\\omega_{t^+} := {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). Note: \\(\\omega_{t^+}\\) differs mechanically \\(\\omega_t\\) due returns past period.\n, optimal portfolio choice \n\\[\\begin{aligned}\n\\omega_{t+1} ^* :=&  \\arg\\max_{\\omega \\\\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\\\\n=&\\arg\\max_{\\omega\\\\mathbb{R}^N,\\text{ }  \\iota'\\omega=1}\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega ,\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\n\\mu^*:=\\mu+\\beta \\omega_{t^+} \\quad  \\text{} \\quad \\Sigma^*:=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\n\\end{aligned}\\]\nresult, adjusting transaction costs implies standard mean-variance optimal portfolio choice adjusted return parameters \\(\\Sigma^*\\) \\(\\mu^*\\): \\[\\begin{aligned}\n \\omega^*_{t+1} & = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\n \\end{aligned}\\]alternative formulation optimal portfolio can derived follows:\n\\[\\begin{aligned}\n\\omega_{t+1} ^*=\\arg\\max_{\\omega\\\\mathbb{R}^N,\\text{ }  \\iota'\\omega=1}\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega .\n\\end{aligned}\\]\noptimal weights correspond mean-variance portfolio vector expected returns assets currently exhibit higher weight considered delivering higher expected return.","code":""},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"optimal-portfolio-choice-in-r","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.4 Optimal portfolio choice in R","text":"function implements efficient portfolio weight general form also allows reflect transaction costs (conditional holdings reallocation). \\(\\beta=0\\), computation resembles standard mean-variance efficient framework.effect transaction costs different levels risk aversion optimal portfolio choice? following lines code analyse distance minimum variance portfolio portfolio implemented investor different values transaction cost parameter \\(\\beta\\) risk aversion \\(\\gamma\\).\nfigure show higher transaction costs parameter \\(\\beta\\), smaller rebalancing initial portfolio (always set minimum variance portfolio weights example). , risk aversion \\(\\gamma\\) increases, efficient portfolio closer minimum variance portfolio weights investor desires less rebalancing initial holdings.","code":"\ncompute_efficient_weight <- function(Sigma,\n                                     mu,\n                                     gamma = 2, # risk-aversion\n                                     beta = 0, # transaction costs\n                                     w_prev = 1/ncol(Sigma) * rep(1, ncol(Sigma))){ # weights before rebalancing\n\n  iota <- rep(1, ncol(Sigma))\n  Sigma_processed <- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed <- mu + beta * w_prev\n  \n  Sigma_inverse <- solve(Sigma_processed)\n  \n  w_mvp <- Sigma_inverse %*% iota\n  w_mvp <- as.vector(w_mvp / sum(w_mvp))\n  w_opt <- w_mvp  + 1/gamma * (Sigma_inverse - 1 / sum(Sigma_inverse) * Sigma_inverse %*% iota %*% t(iota) %*% Sigma_inverse) %*% mu_processed\n  return(as.vector(w_opt))\n}\n\nmu <- colMeans(industry_returns)\ncompute_efficient_weight(Sigma, mu)##  [1]  1.4307987  0.2701363 -1.3024366  0.3742729  0.3093118 -0.1521964\n##  [7]  0.5378376  0.4712071 -0.1669907 -0.7719408\ntransaction_costs <- expand_grid(gamma = c(2, 4, 8, 20),\n                                 beta = 20 * qexp((1:99)/100)) %>% # transaction costs in basis points\n  mutate(weights = map2(.x = gamma, \n                        .y = beta,\n                        ~compute_efficient_weight(Sigma,\n                                                        mu,\n                                                        gamma = .x,\n                                                        beta = .y / 10000,\n                                                  w_prev = w_mvp)),\n         concentration = map_dbl(weights, ~sum(abs(. - w_mvp))))\n\ntransaction_costs %>% \n  mutate(`Risk aversion` = as_factor(gamma)) %>% \n  ggplot(aes(x = beta, y = concentration, color = `Risk aversion`)) + \n  geom_line() +\n  scale_x_sqrt() +\n  labs(x = \"Transaction cost parameter\", \n       y = \"Distance from MVP\",\n       title = \"Optimal portfolio weights for different risk aversion and transaction cost values\",\n       caption = \"Initial portfolio is always the (sample) minimum variance portfolio. Distance is measured as the sum of absolute deviations. \") + \n  theme_minimal() +\n  theme(legend.position = \"bottom\")"},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"constrained-optimization","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.5 Constrained optimization","text":"Next introduce constrained optimization. often, typical constraints -short selling rules prevent analytical solutions optimal portfolio weights. However, numerical optimization allows compute solutions constrained problems. purpose mean-variance optimization rely solve.QP package quadprog. First, start unconstrained problem replicate analytical solutions minimum variance efficient portfolio weights .function solve.QP package quadprog delivers numerical solution quadratic programming problem form\n\\[\\min(-\\mu \\omega + 1/2 \\omega' \\Sigma \\omega) \\text{ s.t. } ' \\omega >= b_0.\\]\nfunction takes one argument (meq) number equality constraints. Therefore, matrix \\(\\) simply vector ones ensure weights sum one. case -short selling, matrix \\(\\) form\n\\[\\begin{aligned}= \\begin{pmatrix}1 & 1& \\ldots&1 \\\\1 & 0 &\\ldots&0\\\\0 & 1 &\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}'\\qquad b_0 = \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}\\end{aligned}.\\]\ncomplex optimization routines, link (optimization task view) provides overview wast optimization landscape R. Next approach problems analytical solutions exist. First, additionally impose short-sale constraints implies \\(N\\) additional inequality constraints: \\(w_i >=0\\).solve.QP fast benefits clear structure quadratic objective linear constraints. Often, however, optimization requires flexibility. one example, show compute optimal weights subject -called regulation T-constraint requires sum absolute portfolio weights smaller 1.5. constraint implies initial margin requirement 50%. clearly non-linear objective function, thus longer rely solve.QP. Instead, rely package alabama requires us define objective constraint functions individually.figure shows optimal allocation weights across 10 industries four different strategies considered far: minimum variance, efficient portfolio \\(\\gamma\\) = 2, efficient portfolio -short sale constraint Reg-T constrained portfolio.moving , propose final allocation strategy reflects somewhat realistic structure transaction costs instead quadratic specification used . function computes efficient portfolio weights adjusting \\(L_1\\) transaction costs \\(\\beta\\sum\\limits_{=1}^N |(w_{, t+1} - w_{, t^+})|\\). closed-form solution exists, thus rely non-linear optimization procedures.","code":"\n# To get started: replicate minimum variance portfolio as numerical solution\nw_mvp_numerical <- solve.QP(Dmat = Sigma,\n                            dvec = rep(0, N), # no vector of expected returns for MVP \n                            Amat = cbind(rep(1, N)), # Matrix A has one column which is a vector of ones\n                            bvec = 1, # bvec is 1 and enforces the constraint that weights sum up to one\n                            meq = 1) # there is one (out of one) equality constraint\n\n# Check that w and w_numerical are the same (up to numerical instabilities)\ncbind(w_mvp, w_mvp_numerical$solution)##              w_mvp             \n##  [1,]  0.218217861  0.218217861\n##  [2,] -0.024598381 -0.024598381\n##  [3,]  0.132583584  0.132583584\n##  [4,]  0.061873595  0.061873595\n##  [5,]  0.009580795  0.009580795\n##  [6,]  0.247523928  0.247523928\n##  [7,]  0.090240088  0.090240088\n##  [8,]  0.145530839  0.145530839\n##  [9,]  0.493397247  0.493397247\n## [10,] -0.374349557 -0.374349557\nw_efficient_numerical <- solve.QP(Dmat = 2 * Sigma,\n                            dvec = mu, # no vector of expected returns for MVP \n                            Amat = cbind(rep(1, N)), # Matrix A has one column which is a vector of ones\n                            bvec = 1, # bvec is 1 and enforces the constraint that weights sum up to one\n                            meq = 1) # there is one (out of one) equality constraint\n\ncbind(compute_efficient_weight(Sigma, mu), w_efficient_numerical$solution)##             [,1]       [,2]\n##  [1,]  1.4307987  1.4307987\n##  [2,]  0.2701363  0.2701363\n##  [3,] -1.3024366 -1.3024366\n##  [4,]  0.3742729  0.3742729\n##  [5,]  0.3093118  0.3093118\n##  [6,] -0.1521964 -0.1521964\n##  [7,]  0.5378376  0.5378376\n##  [8,]  0.4712071  0.4712071\n##  [9,] -0.1669907 -0.1669907\n## [10,] -0.7719408 -0.7719408\nA <- cbind(1, diag(N)) # Matrix of constraints t(A) >= bvec = c(1, rep(0, N))\n\n# Introduce short-selling constraint: no element of w is allowed to be negative\nw_no_short_sale <- solve.QP(Dmat = 2 * Sigma, # Efficient portfolio with risk aversion gamma = 2\n                            dvec = mu, \n                            Amat = A, \n                            bvec = c(1, rep(0, N)), \n                            meq = 1)$solution\nfn <- function(w, gamma = 2) -t(w) %*% mu + gamma / 2 * t(w)%*%Sigma%*%w # Objective function which we minimize\nhin <- function(w, reg_t = 1.5) return(reg_t - sum(abs(w))) # inequality constraints such that reg_t > sum(abs(w))\nheq <- function(w) return(sum(w) - 1) # equality constraint\n\nw_reg_t <- constrOptim.nl( # from alabama package\n  par = 1/N * rep(1, N),# Initial set of weights\n  hin = hin,\n  fn = fn, \n  heq = heq,\n  control.outer = list(trace = FALSE))$par # omit optimization output\ntibble(`No short-sale` = w_no_short_sale, \n       `Minimum Variance` = w_mvp, \n       `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n       `Regulation-T` = w_reg_t,\n       Industry = colnames(industry_returns)) %>%\n  pivot_longer(-Industry, \n               names_to = \"Strategy\") %>% \nggplot(aes(fill = Strategy, \n                 y = value, \n                 x = Industry)) + \n  geom_bar(position=\"dodge\", stat=\"identity\") +\n  coord_flip() + \n  theme_minimal() +\n  labs(y = \"Allocation weight (in percent)\",\n       title =\"Optimal allocations for different investment rules\") +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels = percent)\ncompute_efficient_weight_L1_TC <- function(mu,\n                                          Sigma, \n                                          gamma = 2, \n                                          beta = 0, # in basis points\n                                          w_prev = 1 / ncol(sigma) * rep(1, ncol(sigma))) {\n  \n  # Define objective function \n  fn <- function(w) -t(w) %*% mu + gamma / 2* t(w) %*% Sigma %*% w + (beta / 10000) / 2 * sum(abs(w - w_prev))\n\n  w_optimal <- constrOptim.nl(\n    par = w_prev,# Initial set of weights\n    fn = fn, \n    heq = function(w){sum(w) - 1},\n    control.outer = list(trace = FALSE))$par # To omit optimization output\n  return(w_optimal)\n}"},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"out-of-sample-backtesting","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.6 Out-of-sample backtesting","text":"sake keeping things easy, committed one fundamental error computing portfolio weights : used full sample data determine optimal allocation. words, order implement strategy beginning 2000’s, need know advance returns evolve 2020. Instead, interesting methodological point view, evaluate performance portfolios reasonable --sample fashion. next backtesting exercise. backtest recompute optimal weights just based past available data.lines define general setup: consider 120 periods past update parameter estimates recomputing portfolio weights. , portfolio weights updated costly affects net performance. portfolio weights determine portfolio return. period later, current portfolio weights changed form foundation transaction costs incurred next period. consider three different competing strategies: mean-variance efficient portfolio, mean-variance efficient portfolio ex-ante adjustment transaction costs naive portfolio simply allocates wealth equally across different assets.Finally get evalation portfolio strategies net--transaction costs.results clearly speak mean-variance optimization. Turnover huge investor considers expected return variance portfolio. Effectively, mean-variance portfolio generated negative annualized return adjusting transaction costs. time, naive portfolio turns perform well. fact, performance gains transaction-cost adjusted mean-variance portfolio small. --sample Sharpe ratio slightly higher naive portfolio. Note extreme effect turnover penalization turnover: MV (TC) effectively resembles buy--hold strategy updates portfolio estimated parameters \\(\\hat\\mu_t\\) \\(\\hat\\Sigma_t\\)indicate current allocation far away theoretical optimal portfolio.","code":"\nwindow_length <- 120 # Estimation window (length of past available data)\nperiods <- nrow(industry_returns) - window_length # total number of out-of-sample periods\n\nbeta <- 50 # Transaction costs in basis points\ngamma <- 2 # Risk aversion\n\nperformance_values <- matrix(NA, \n                     nrow = periods, \n                     ncol = 3) # A matrix to collect all returns\ncolnames(performance_values) <- c(\"raw_return\", \"turnover\", \"net_return\") # we implement 3 strategies\n\nperformance_values <- list(\"MV (TC)\" = performance_values, \n                           \"Naive\" = performance_values, \n                           \"MV\" = performance_values)\n\nw_prev_1 <- w_prev_2 <- w_prev_3 <- rep(1/N ,N) # Every strategy starts with naive portfolio\n\n# Two small helper functions: Weight adjustments due to returns and evaluation\nadjust_weights <- function(w, next_return){\n  w_prev <- 1 + w * next_return\n  return(as.numeric(w_prev / sum(as.vector(w_prev))))\n}\n\nevaluate_performance <- function(w, w_previous, next_return, beta = 50){\n  raw_return <- as.matrix(next_return) %*% w\n  turnover <- sum(abs(w - w_previous))\n  # Realized returns net of TC\n  net_return <- raw_return - beta / 10000 * turnover\n  return(c(raw_return, turnover, net_return))\n}\nfor(i in 1:periods){ # Rolling window\n  \n  # Estimation window\n  returns_window <- industry_returns[i : (i + window_length - 1),] # the last X returns available up to date t\n  next_return <- industry_returns[i + window_length, ] # Out-of-sample return in the next period\n  \n  # Sample moments (in practice: replace mu or sigma with more advanced methods) \n  Sigma <- cov(returns_window) \n  mu <- 0 * colMeans(returns_window) # Note: We essentially perform MV optimization \n  \n  # TC robust portfolio\n  w_1 <- compute_efficient_weight_L1_TC(mu = mu, \n                                     Sigma = Sigma, \n                                     beta = beta, \n                                     gamma = gamma,\n                                     w_prev = w_prev_1)\n\n  # Evaluation\n  performance_values[[1]][i, ] <- evaluate_performance(w_1, \n                                               w_prev_1, \n                                               next_return, \n                                               beta = beta)\n  \n  #Computes adjusted weights based on the weights and next period returns\n  w_prev_1 <- adjust_weights(w_1, next_return)\n\n  # Naive Portfolio \n  w_2 <- rep(1/N, N)\n  \n  # Evaluation\n  performance_values[[2]][i, ] <- evaluate_performance(w_2, \n                                               w_prev_2, \n                                               next_return)\n  \n  #Computes adjusted weights based on the weights and next period returns\n  w_prev_2 <- adjust_weights(w_2, next_return)\n  \n  # Mean-variance efficient portfolio (ignoring transaction costs)\n  w_3 <- compute_efficient_weight(Sigma = Sigma,\n                                  mu = mu, \n                                  gamma = gamma)\n  # Evaluation\n  performance_values[[3]][i, ] <- evaluate_performance(w_3, \n                                               w_prev_3, \n                                               next_return)\n  \n  #Computes adjusted weights based on the weights and next period returns\n  w_prev_3 <- adjust_weights(w_3, next_return)\n}\nperformance <- lapply(performance_values, as_tibble) %>% \n  bind_rows(.id = \"strategy\")\n\nperformance %>%\n  group_by(strategy) %>%\n  summarise(Mean = 12 * mean(100 * net_return), # Annualized returns\n            SD = sqrt(12) * sd(100 * net_return), # Annualized standard deviation\n            Sharpe = if_else(Mean > 0, Mean/SD, NA_real_),\n            Turnover = 100 * mean(turnover)) %>% \n  knitr::kable(digits = 3)"},{"path":"constraint-optimization-and-portfolio-backtesting.html","id":"exercises-4","chapter":"14 Constraint Optimization and Portfolio Backtesting","heading":"14.7 Exercises","text":"argue investor quadratic utility function certainty equivalent \\[\\max_w CE(w) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\]\nfaces equivalent optimization problem framework portfolio weights chosen aim minimize volatility given pre-specified level expected returns\n\\[\\min_w \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ } \\iota'\\omega = 1. \\] Proof equivalence optimal portfolio weights cases.Consider portfolio choice problem transaction-cost adjusted certainty equivalent maximization risk aversion parameter \\(\\gamma\\)\n\\[\\begin{aligned}\n\\omega_{t+1} ^* :=&  \\arg\\max_{\\omega \\\\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\\\\n\\end{aligned}\\]\n\\(\\Sigma\\) \\(\\mu\\) (estimators ) variance-covariance matrix returns vector expected returns. Assume now transaction costs quadratic rebalancing proportional stock illiquidity \n\\[\\nu_t\\left(\\omega, \\mathbf{\\beta}\\right) := \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) diagonal matrix \\(ill_1, \\ldots, ill_N\\). Derive closed-form solution mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based transaction cost specification . Discuss effect illiquidity \\(ill_i\\) individual portfolio weights relative investor myopically ignores transaction costs decision.Use solution previous exercise update function compute_efficient_weight can compute optimal weights conditional matrix \\(B\\) illiquidity measures.Illustrate evolution optimal weights naive portfolio efficient portfolio mean-standard deviation diagram.always optimal choose \\(\\beta\\) optimization problem value used evaluating portfolio performance? words: Can optimal choose theoretically sub-optimal portfolios based transaction costs considerations reflect actual incurred costs? Evaluate --sample Sharpe ratio transaction costs range different values imposed \\(\\beta\\) values.","code":""}]
