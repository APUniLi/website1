[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"website online version Tidy Finance R, book currently development intended eventual print release via Chapman & Hall/CRC. book result joint effort Christoph Scheuch, Stefan Voigt, Patrick Weiss.grateful kind feedback every aspect book. please get touch us via contact@tidy-finance.org spot typos, discover issues deserve attention, suggestions additional chapters sections. Additionally, let us know found text helpful. look forward hearing !","code":""},{"path":"index.html","id":"why-does-this-book-exist","chapter":"Preface","heading":"Why does this book exist?","text":"Financial economics vibrant area research, central part businesses activities, least implicitly relevant everyday life. Despite relevance society vast number empirical studies financial phenomenons, one quickly learns actual implementation models solve problems area financial economics typically rather opaque.\ngraduate students, particularly surprised lack public code seminal papers even textbooks key concepts financial economics. lack transparent code leads numerous replication efforts (failures), also constitutes waste resources problems already solved countless others secrecy.book aims lift curtain reproducible finance providing fully transparent code base many common financial applications. hope inspire others share code publicly take part journey towards reproducible research future.","code":""},{"path":"index.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who should read this book?","text":"write book three audiences:Students want acquire basic tools required conduct financial research ranging undergrad graduate level. book’s structure simple enough material sufficient self-study purposes.Instructors look materials teach courses empirical finance financial economics. provide plenty examples focus intuitive explanations can easily adjusted expanded.Data analysts statisticians work issues dealing financial data need practical tools succeed.","code":""},{"path":"index.html","id":"what-will-you-learn","chapter":"Preface","heading":"What will you learn?","text":"book currently divided 5 parts:Chapter 1 introduces important concepts around approach Tidy Finance revolves.Chapters 2-3 provide tools organize data prepare common data sets used financial research. Although many important data behind paywalls, start describing different open source data download . move prepare two popular data financial research: CRSP Compustat. reuse data chapters following chapters.Chapters 4-9 deal key concepts empirical asset pricing beta estimation, portfolio sorts, performance analysis, asset pricing regressions.Chapters 10-12 apply linear models panel data machine learning methods problems factor selection option pricing.Chapters 13-14 provide approaches parametric, constrained portfolio optimization, backtesting procedures.chapter self-contained can read individually. Yet data chapter provides important background necessary data management subsequent chapters.","code":""},{"path":"index.html","id":"what-wont-you-learn","chapter":"Preface","heading":"What won’t you learn?","text":"book empirical work. assume basic knowledge statistics econometrics, provide detailed treatments underlying theoretical models methods applied book. Instead, find references seminal academic work journal articles textbooks detailed treatments.\nbelieve comparative advantage provide thorough implementation typical approaches portfolio sorts, backtesting procedures, regressions, machine learning methods, related topics empirical finance. enrich implementations discussions needy-greedy choices face conducting empirical analyses. hence refrain deriving theoretical models extensively discussing statistical properties well-established tools.book thus close spirit books provide fully reproducible code financial applications. view complementary work want highlight differences:Regenstein Jr (2018) provides excellent introduction discussion different tools standard applications finance (e.g., compute returns sample standard deviations time series stock returns). book, contrast, clear focus applications state---art academic research finance. thus fill niche allows aspiring researchers instructors rely well-designed code base.Coqueret Guida (2020) constitutes great compendium book respect applications related return prediction portfolio formation. book primarily targets practitioners hands-focus. book, contrast, relies typical databases used financial research focuses preparation datasets academic applications. addition, chapter machine learning focuses factor selection instead return prediction.Although emphasizes importance reproducible workflow principles, provide introductions core tools relied create maintain book:Version control systems Git vital managing programming project. Originally designed organize collaboration software developers, even solo data analysts benefit adopting version control. Git also makes simple publicly share code allow others reproduce findings. refer Bryan (2022) gentle introduction (sometimes painful) life Git.Good communication results key ingredient reproducible transparent research. compile book, heavily draw suite fantastic open source tools. First, Wickham (2016a) provides highly customizable, yet easy use system creating data visualizations. Wickham Grolemund (2016) provides intuitive introduction creating graphics using approach. Second, daily work compile book, used markdown-based authoring framework described Xie, Allaire, Grolemund (2018) Xie, Dervieux, Riederer (2020). Markdown documents fully reproducible support dozens static dynamic output formats. Lastly, Xie (2016) tremendously facilitated authoring markdown-based books. provide introductions tools, resources already provide easily accessible tutorials.Good writing also important presentation findings. neither claim experts domain, try sound particularly academic. contrary, deliberately use colloquial approach describe methods results presented book order allow readers connect easily mostly technical content. desire guidance respect proper academic writing financial economics, recommend Kiesling (2003), John H. Cochrane (2005), Jacobsen (2014) provide essential tips (condensed pages).","code":""},{"path":"index.html","id":"why-r","chapter":"Preface","heading":"Why R?","text":"believe R among best choices programming language area finance. favorite features include:R free open-source can use academic professional contexts.diverse active online community works broad range tools.massive set actively maintained packages kinds applications exists, e.g., data manipulation, visualization, machine learning, etc.Powerful tools communication, e.g., Rmarkdown shiny, readily available.RStudio one best development environments interactive data analysis.Strong foundations functional programming provided.Smooth integration programming languages, e.g., SQL, Python, C, C++, Fortran, etc.information R great, refer Wickham et al. (2019a).","code":""},{"path":"index.html","id":"why-tidy","chapter":"Preface","heading":"Why tidy?","text":"start working data, quickly realize spend lot time reading, cleaning, transforming data. fact, often said 80% data analysis spent preparing data. tidying data, want structure data sets facilitate analyses. Wickham (2014) puts :[T]idy datasets alike, every messy dataset messy way. Tidy datasets provide standardized way link structure dataset (physical layout) semantics (meaning).essence, tidy data follows three principles:Every column variable.Every row observation.Every cell single value.Throughout book, try follow principles best can. want learn tidy data principles informal manner, refer vignette part (Wickham Girlich 2022).addition data layer, also tidy coding principles outlined tidy tools manifesto try follow:Reuse existing data structures.Compose simple functions pipe.Embrace functional programming.Design humans.particular, heavily draw set packages called tidyverse (Wickham et al. 2019b). tidyverse consistent set packages data analysis tasks, ranging importing wrangling visualizing modeling data grammar. addition explicit tidy principles, tidyverse benefits: () master one package, easier master others, (ii) core packages developed maintained Public Benefit Company RStudio, Inc.\ncore packages contained tidyverse : ggplot2 (Wickham 2016b), dplyr (Wickham, François, et al. 2022), tidyr (Wickham Girlich 2022), readr (Wickham, Hester, Bryan 2022), purrr (Henry Wickham 2020), tibble (Müller Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021).Throughout book use native pipe |>, powerful tool clearly express sequence operations. Readers familiar tidyverse may used predecessor %>% part magrittr package. applications, native magrittr pipe behave identically, opt one simpler part base R. thorough discussion subtle differences two pipes, refer second edition Wickham Grolemund (2016).","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"continue, make sure software need book:Install R RStudio. get walk-installation every major operating system, follow steps outlined summary. whole process done clicks. wonder difference: R open-source language environment statistical computing graphics, free download use. R runs computations, RStudio integrated development environment provides interface adding many convenient features tools. suggest coding RStudio.Install R RStudio. get walk-installation every major operating system, follow steps outlined summary. whole process done clicks. wonder difference: R open-source language environment statistical computing graphics, free download use. R runs computations, RStudio integrated development environment provides interface adding many convenient features tools. suggest coding RStudio.Open RStudio install tidyverse. sure works? find helpful information install packages brief summary.\nnew R, recommend starting following sources:Open RStudio install tidyverse. sure works? find helpful information install packages brief summary.\nnew R, recommend starting following sources:gentle good introduction workings R can found form weighted dice project. done setting R machine, try follow instructions project.gentle good introduction workings R can found form weighted dice project. done setting R machine, try follow instructions project.main book tidyverse, Wickham Grolemund (2016) available online free: R Data Science explains majority tools use book.main book tidyverse, Wickham Grolemund (2016) available online free: R Data Science explains majority tools use book.instructor searching effectively teach R data science methods, recommend take look excellent data science toolbox Mine Cetinkaya-Rundel.instructor searching effectively teach R data science methods, recommend take look excellent data science toolbox Mine Cetinkaya-Rundel.RStudio provides range excellent cheat sheets extensive information use tidyverse packages.RStudio provides range excellent cheat sheets extensive information use tidyverse packages.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"Preface","heading":"About the authors","text":"met Vienna Graduate School Finance us graduated different focus shared passion: coding R. continue sharpen R skills part current occupations:Christoph Scheuch Director Product social trading platform wikifolio.com responsible product planning, execution, monitoring. also manages team data scientists analyze user behavior develop new products.Stefan Voigt Assistant Professor Finance Department Economics University Copenhagen research fellow Danish Finance Institute. research focuses blockchain technology, high-frequency trading, financial econometrics. Stefan teaches parts book courses empirical finance.Patrick Weiss Post-Doc Vienna University Economics Business. research centers around intersection asset pricing corporate finance.","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC -NC-SA 4.0.code samples book licensed Creative Commons CC0 1.0 Universal (CC0 1.0), .e., public domain.","code":""},{"path":"index.html","id":"colophon","chapter":"Preface","heading":"Colophon","text":"book written RStudio using bookdown. website hosted github pages automatically updated every commit. complete source available GitHub.\ngenerated plots book using ggplot2 classic dark--light theme (theme_bw()).version book built R version 4.2.1 (2022-06-23, Funny-Looking Kid) following packages:","code":""},{"path":"introduction-to-tidy-finance.html","id":"introduction-to-tidy-finance","chapter":"1 Introduction to Tidy Finance","heading":"1 Introduction to Tidy Finance","text":"main aim chapter familiarize tidyverse.\nstart downloading visualizing stock data moving simple portfolio choice problem.\nexamples introduce approach Tidy Finance.","code":""},{"path":"introduction-to-tidy-finance.html","id":"working-with-stock-market-data","chapter":"1 Introduction to Tidy Finance","heading":"1.1 Working with stock market data","text":"start session, load required packages.\nThroughout entire book, always use tidyverse.\nchapter, also load convenient tidyquant package download price data. tidyquant package (Dancho Vaughan 2022a) provides convenient wrapper various quantitative functions compatible ‘tidyverse’.typically install package can load .\ncase done yet, call install.packages(\"tidyquant\").\ntrouble using tidyquant, check documentation.first download daily prices one stock market ticker, e.g., Apple stock, AAPL, directly data provider Yahoo!Finance.\ndownload data, can use command tq_get.\nknow use , make sure read help file calling ?tq_get.\nespecially recommend taking look documentation’s examples section. request daily data period 20 year. tq_get downloads stock market data Yahoo!Finance specify another data source.\nfunction returns tibble eight quite self-explanatory columns: symbol, date, market prices open, high, low close, daily volume (number traded shares), adjusted price USD.\nadjusted prices corrected anything might affect stock price market closes, e.g., stock splits dividends.\nactions affect quoted prices, direct impact investors hold stock. Therefore, often rely adjusted prices comes analyzing returns investor earned holding stock continuously.Next, use ggplot2 visualize time series adjusted prices. package ggplot2 (Wickham 2016b) takes care visualization tasks based principles Grammar Graphics (Wilkinson 2012).\nFIGURE 1.1: Apple stock prices. Prices USD, adjusted divident payments stock splits. Source: Yahoo!Finance\n Instead analyzing prices, compute daily returns defined \\((p_t - p_{t-1}) / p_{t-1} = p_t / p_{t-1} - 1\\) \\(p_t\\) adjusted day \\(t\\) price.\ncontext, function lag() helpful, returns previous value vector.resulting tibble contains three columns last contains daily returns (ret).\nNote first entry naturally contains missing value (NA) previous price.\nObviously, use lag() meaningless time series ordered date.\ncommand arrange() provides neat way order observations.\nAdditionally, computations require time series ordered date.upcoming examples, remove missing values require separate treatment computing, e.g., sample averages. general, however, make sure understand NA values occur carefully examine can simply get rid observations.Next, visualize distribution daily returns histogram. convenience, multiply returns 100 get returns percent visualizations.\nAdditionally, add dashed red line indicates 5% quantile daily returns histogram, (crude) proxy worst return stock probability least 5%.\n5% quantile closely connected (historical) Value--risk, risk measure commonly monitored regulators. refer Tsay (2010) thorough introduction stylized facts returns.\nFIGURE 1.2: Distribution daily AAPL returns (percent). dotted vertical line indicates historical 5 percent quantile\n, bins = 100 determines number bins used illustration hence implicitly width bins.\nproceeding, make sure understand use geom geom_vline() add dotted red line indicates 5% quantile daily returns.\ntypical task proceeding data compute summary statistics main variables interest.see maximum daily return around 13.905 percent. surprisingly, perhaps, however, daily average return close slightly 0.\nline illustration , large losses day minimum returns indicate strong asymmetry distribution returns.\ncan also compute summary statistics year individually imposing group_by(year = year(date)), call year(date) returns year. specifically, lines code compute summary statistics individual groups data, defined year. summary statistics therefore allow eyeball analysis time-series dynamics return distribution.case wonder: additional argument .names = \"{.fn}\" across() determines name output columns. specification rather flexible allows almost arbitrary column names, can useful reporting. command print() simply controls output options R console.","code":"\nlibrary(tidyverse)\nlibrary(tidyquant)\nprices <- tq_get(\"AAPL\",\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2022-06-30\"\n)\nprices# A tibble: 5,659 × 8\n  symbol date        open  high   low close    volume adjusted\n  <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800    0.855\n2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600    0.782\n3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600    0.794\n4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800    0.725\n5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400    0.760\n# … with 5,654 more rows\nprices |>\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"AAPL stock prices\",\n    subtitle = \"Prices in USD, adjusted for dividend payments and stock splits\"\n  )\nreturns <- prices |>\n  arrange(date) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret)\nreturns# A tibble: 5,659 × 3\n  symbol date           ret\n  <chr>  <date>       <dbl>\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# … with 5,654 more rows\nreturns <- returns |>\n  drop_na(ret)\nquantile_05 <- quantile(returns |> pull(ret) * 100, 0.05)\n\nreturns |>\n  ggplot(aes(x = ret * 100)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily AAPL returns (in percent)\",\n    subtitle = \"The dotted vertical line indicates the historical 5 percent quantile\"\n  )\nreturns |>\n  mutate(ret = ret * 100) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           <dbl>        <dbl>         <dbl>         <dbl>\n1          0.123         2.52         -51.9          13.9\nreturns |>\n  mutate(ret = ret * 100) |>\n  group_by(year = year(date)) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)# A tibble: 23 × 5\n    year daily_mean daily_sd daily_min daily_max\n   <dbl>      <dbl>    <dbl>     <dbl>     <dbl>\n 1  2000   -0.346       5.49    -51.9      13.7 \n 2  2001    0.233       3.93    -17.2      12.9 \n 3  2002   -0.121       3.05    -15.0       8.46\n 4  2003    0.186       2.34     -8.14     11.3 \n 5  2004    0.470       2.55     -5.58     13.2 \n 6  2005    0.349       2.45     -9.21      9.12\n 7  2006    0.0949      2.43     -6.33     11.8 \n 8  2007    0.366       2.38     -7.02     10.5 \n 9  2008   -0.265       3.67    -17.9      13.9 \n10  2009    0.382       2.14     -5.02      6.76\n11  2010    0.183       1.69     -4.96      7.69\n12  2011    0.104       1.65     -5.59      5.89\n13  2012    0.130       1.86     -6.44      8.87\n14  2013    0.0472      1.80    -12.4       5.14\n15  2014    0.145       1.36     -7.99      8.20\n16  2015    0.00199     1.68     -6.12      5.74\n17  2016    0.0575      1.47     -6.57      6.50\n18  2017    0.164       1.11     -3.88      6.10\n19  2018   -0.00573     1.81     -6.63      7.04\n20  2019    0.266       1.65     -9.96      6.83\n21  2020    0.281       2.94    -12.9      12.0 \n22  2021    0.131       1.58     -4.17      5.39\n23  2022   -0.170       2.26     -5.64      6.98"},{"path":"introduction-to-tidy-finance.html","id":"scaling-up-the-analysis","chapter":"1 Introduction to Tidy Finance","heading":"1.2 Scaling up the analysis","text":"next step, generalize code computations can handle arbitrary vector tickers (e.g., constituents index). Following tidy principles, quite easy download data, plot price time series, tabulate summary statistics arbitrary number assets.tidyverse magic starts: tidy data makes extremely easy generalize computations many assets like. following code takes vector tickers, e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\"), automates download well plot price time series.\nend, create table summary statistics arbitrary number assets. perform analysis data current constituents Dow Jones Industrial Average index. Conveniently, tidyquant provides function get stocks stock index single call (similarly, tq_exchange(\"NASDAQ\") delivers stocks currently listed NASDAQ exchange).resulting file contains 161753 daily observations 30 different corporations.\nfigure illustrates time series downloaded adjusted prices constituents Dow Jones index. Make sure understand every single line code! (arguments aes()? alternative geoms use visualize time series? Hint: know answers try change code see difference intervention causes).\nFIGURE 1.3: DOW index stock prices. Prices USD, adjusted dividend payments stock splits.\nnotice small differences relative code used ? tq_get(ticker) returns tibble several symbols well. need illustrate tickers simultaneously include color = symbol ggplot2 aesthetics. way, generate separate line ticker. course, simply many lines graph properly identify individual stocks, illustrates point well.holds stock returns. computing returns, use group_by(symbol) mutate() command performed symbol individually. logic also applies computation summary statistics: group_by(symbol) key aggregating time series ticker-specific variables interest.Note now also equipped tools download price data ticker listed S&P 500 index number lines code. Just use ticker <- tq_index(\"SP500\"), provides tibble contains symbol (currently) part S&P 500. However, don’t try prepared wait couple minutes quite data download!","code":"\nticker <- tq_index(\"DOW\")\nticker# A tibble: 30 × 8\n  symbol company        identifier sedol weight sector shares_held local_currency\n  <chr>  <chr>          <chr>      <chr>  <dbl> <chr>        <dbl> <chr>         \n1 UNH    UnitedHealth … 91324P10   2917… 0.109  Healt…     5679861 USD           \n2 GS     Goldman Sachs… 38141G10   2407… 0.0668 Finan…     5679861 USD           \n3 HD     Home Depot In… 43707610   2434… 0.0631 Consu…     5679861 USD           \n4 MSFT   Microsoft Cor… 59491810   2588… 0.0533 Infor…     5679861 USD           \n5 MCD    McDonald's Co… 58013510   2550… 0.0516 Consu…     5679861 USD           \n# … with 25 more rows\nindex_prices <- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2022-06-30\"\n)\nindex_prices |>\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"DOW index stock prices\",\n    subtitle = \"Prices in USD, adjusted for dividend payments and stock splits\"\n  ) +\n  theme(legend.position = \"none\")\nall_returns <- index_prices |>\n  group_by(symbol) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret) |>\n  drop_na(ret)\n\nall_returns |>\n  mutate(ret = ret * 100) |>\n  group_by(symbol) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  ))# A tibble: 30 × 5\n  symbol daily_mean daily_sd daily_min daily_max\n  <chr>       <dbl>    <dbl>     <dbl>     <dbl>\n1 AAPL       0.123      2.52     -51.9      13.9\n2 AMGN       0.0483     1.98     -13.4      15.1\n3 AXP        0.0514     2.30     -17.6      21.9\n4 BA         0.0544     2.23     -23.8      24.3\n5 CAT        0.0670     2.04     -14.5      14.7\n# … with 25 more rows"},{"path":"introduction-to-tidy-finance.html","id":"other-forms-of-data-aggregation","chapter":"1 Introduction to Tidy Finance","heading":"1.3 Other forms of data aggregation","text":"course, aggregation across variables symbol can make sense well. instance, suppose interested answering question: days high aggregate trading volume likely followed days high aggregate trading volume? provide initial analysis question, take downloaded data compute aggregate daily trading volume Dow Jones constituents USD.\nRecall column volume denoted number traded shares.\nThus, multiply trading volume daily closing price get proxy aggregate trading volume USD. Scaling 1e9 denotes daily trading volume billion USD.\nFIGURE 1.4: Aggregate daily trading volume DOW constituents billion USD.\nfigure indicates clear upwards trend aggregated daily trading volume. particular since outbreak COVID-19 pandemic markets process huge trading volume, analyzed instance Goldstein, Koijen, Mueller (2021).\nOne way illustrate persistence trading volume plot volume day \\(t\\) volume day \\(t-1\\) example . add dotted 45°-line indicate hypothetical one--one relation geom_abline(), addressing potential differences axes’ scales.\nFIGURE 1.5: Trading volume Dow Index versus previous day volume\nunderstand warning ## Warning: Removed 1 rows containing missing values (geom_point). comes means? Purely eye-balling reveals days high trading volume often followed similarly high trading volume days.","code":"\nvolume <- index_prices |>\n  mutate(volume_usd = volume * close / 1e9) |>\n  group_by(date) |>\n  summarize(volume = sum(volume_usd))\n\nvolume |>\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume in billion USD\"\n  )\nvolume |>\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dotted\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume (billion USD)\",\n    y = \"Aggregate trading volume (billion USD)\",\n    title = \"Trading volume on Dow Index versus previous day volume\"\n  )Warning: Removed 1 rows containing missing values (geom_point)."},{"path":"introduction-to-tidy-finance.html","id":"portfolio-choice-problems","chapter":"1 Introduction to Tidy Finance","heading":"1.4 Portfolio choice problems","text":"previous part, show download stock market data inspect graphs summary statistics.\nNow, move typical question Finance, namely, optimally allocate wealth across different assets. standard framework optimal portfolio selection considers investors prefer higher future returns dislike future return volatility (defined square root return variance): mean-variance investor (Markowitz 1952). essential tool evaluate portfolios mean-variance context efficient frontier, set portfolios satisfy condition portfolio exists higher expected return volatility (square-root variance, .e., risk), see, e.g., Merton (1972).\ncompute visualize efficient frontier several stocks.\nFirst, extract asset’s monthly returns.\norder keep things simple, work balanced panel exclude tickers observe price every single trading day since 2000., floor_date() function lubridatepackage (Grolemund Wickham 2011) provides useful functions work dates time.Next, transform returns tidy tibble \\((T \\times N)\\) matrix one column \\(N\\) tickers compute sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] \\(r_t\\) \\(N\\) vector returns date \\(t\\) sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'\\].\nachieve using pivot_wider() new column names column symbol setting values ret.\ncompute vector sample average returns sample variance-covariance matrix, consider proxies parameters distribution future stock returns.\nThus, simplicity refer \\(\\Sigma\\) \\(\\mu\\) instead explictly highlighting sample moments estimates. later chapters, discuss issues arise take estimation uncertainty account., compute minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) well expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) portfolio.\nRecall minimum variance portfolio vector portfolio weights solution \n\\[\\omega_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } \\sum\\limits_{=1}^Nw_i = 1.\\]\nconstraint weights sum one simply implies funds distributed across available asset universe, possible retain cash.\neasy show analytically, \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\) \\(\\iota\\) vector ones \\(\\Sigma^{-1}\\) inverse \\(\\Sigma\\). Note, \\(\\iota'\\Sigma^{-1}\\iota = \\sum\\limits_{j=1}^N\\sum\\limits_{= 1}^N \\sigma_{ij}\\) \\(\\sigma_{ij}\\) \\((,j)\\)-th element \\(\\Sigma\\).command solve(, b) returns solution system equations \\(Ax = b\\). b provided, example , defaults identity matrix solve(Sigma) delivers \\(\\Sigma^{-1}\\) (unique solution exists).\nNote monthly volatility minimum variance portfolio order magnitude daily standard deviation individual components. Thus, diversification benefits terms risk reduction tremendous!Next, set find weights portfolio achieves three times expected return minimum variance portfolio.\nHowever, mean-variance investors interested portfolio achieves required return rather efficient portfolio, .e., portfolio lowest standard deviation.\nwonder solution \\(\\omega_\\text{eff}\\) comes : efficient portfolio chosen investor aims achieve minimum variance given minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, objective function choose \\(\\omega_\\text{eff}\\) solution \n\\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\ncode implements analytic solution optimization problem benchmark return \\(\\bar\\mu\\) set 3 times expected return minimum variance portfolio. encourage verify correct.","code":"\nindex_prices <- index_prices |>\n  group_by(symbol) |>\n  mutate(n = n()) |>\n  ungroup() |>\n  filter(n == max(n)) |>\n  select(-n)\n\nreturns <- index_prices |>\n  mutate(month = floor_date(date, \"month\")) |>\n  group_by(symbol, month) |>\n  summarize(price = last(adjusted), .groups = \"drop_last\") |>\n  mutate(ret = price / lag(price) - 1) |>\n  drop_na(ret) |>\n  select(-price)\nreturns_matrix <- returns |>\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |>\n  select(-month)\n\nSigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(Sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\n\ntibble(\n  expected_ret = t(mvp_weights) %*% mu,\n  volatility = sqrt(t(mvp_weights) %*% Sigma %*% mvp_weights)\n)# A tibble: 1 × 2\n  expected_ret[,1] volatility[,1]\n             <dbl>          <dbl>\n1          0.00801         0.0314\nmu_bar <- 3 * t(mvp_weights) %*% mu\n\nC <- as.numeric(t(iota) %*% solve(Sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(Sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(Sigma) %*% mu)\n\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights +\n  lambda_tilde / 2 * (solve(Sigma) %*% mu - D * mvp_weights)"},{"path":"introduction-to-tidy-finance.html","id":"the-efficient-frontier","chapter":"1 Introduction to Tidy Finance","heading":"1.5 The efficient frontier","text":" two mutual fund separation theorem states soon two efficient portfolios (minimum variance portfolio \\(w_{mvp}\\) efficient portfolio higher required level expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\) like ), can characterize entire efficient frontier combining two portfolios.\n, linear combination two portfolio weights represent efficient portfolio.\ncode implements construction efficient frontier, characterizes highest expected return achievable level risk. understand code better, make sure familiarize inner workings loop.code proceeds two steps: First, compute vector combination weights \\(c\\) evaluate resulting linear combination \\(c\\\\mathbb{R}_+\\):\\[w^* = cw_\\text{eff}(\\bar\\mu) + (1-c)w_{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] \\(\\lambda^* = 2\\frac{c\\bar\\mu + (1-c)\\tilde\\mu - D/C}{E-D^2/C}\\).\nFinally, simple visualize efficient frontier alongside two efficient portfolios within one, powerful figure using ggplot2. also add individual stocks call.\ncompute annualized returns based simple assumption monthly returns iid distributed. Thus, average annualized return just 12 times expected monthly return.\nFIGURE 1.6: Efficient frontier Dow Jones constituents.\nline indicates efficient frontier: set portfolios mean-variance efficient investor choose . Compare performance relative individual assets (blue dots) - become clear diversifying yields massive performance gains (least long take parameters \\(\\Sigma\\) \\(\\mu\\) given).","code":"\nc <- seq(from = -0.4, to = 1.9, by = 0.01)\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\n\nfor (i in seq_along(c)) {\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights\n  res$mu[i] <- 12 * 100 * t(w) %*% mu\n  res$sd[i] <- 12 * sqrt(100) * sqrt(t(w) %*% Sigma %*% w)\n}\nres |>\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point( # locate the minimum variance and efficient portfolio\n    data = res |> filter(c %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point( # locate the individual assets\n    data = tibble(\n      mu = 12 * 100 * mu,\n      sd = 12 * 10 * sqrt(diag(Sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Efficient frontier for Dow Jones constituents\",\n    subtitle = str_c(\n      \"Thick dots indicate the location of the minimum \",\n      \"variance and efficient tangency portfolio\"\n    )\n  )"},{"path":"introduction-to-tidy-finance.html","id":"exercises","chapter":"1 Introduction to Tidy Finance","heading":"1.6 Exercises","text":"Download daily prices another stock market ticker choice Yahoo!Finance tq_get() tidyquant package. Plot two time series ticker’s un-adjusted adjusted closing prices. Explain differences.Compute daily net returns asset visualize distribution daily returns histogram. Also, use geom_vline() add dashed red line indicates 5% quantile daily returns within histogram. Compute summary statistics (mean, standard deviation, minimum maximum) daily returnsTake code generalize can perform computations arbitrary vector tickers (e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\")). Automate download, plot price time series, create table return summary statistics arbitrary number assets.Consider research question: days high aggregate trading volume often also days large absolute price changes? Find appropriate visualization analyze question.Compute monthly returns downloaded stock market prices. Compute vector historical average returns sample variance-covariance matrix. Compute minimum variance portfolio weights portfolio volatility average returns, visualize mean-variance efficient frontier. Choose one assets identify portfolio yields historical volatility achieves highest possible average return.portfolio choice analysis, restricted sample assets trading every single day since 2000. decision problem want infer future expected portfolio performance results?efficient frontier characterizes portfolios highest expected return different levels risk, .e., standard deviation. Identify portfolio highest expected return per standard deviation. Hint: ratio expected return standard deviation important concept Finance.","code":""},{"path":"accessing-managing-financial-data.html","id":"accessing-managing-financial-data","chapter":"2 Accessing & managing financial data","heading":"2 Accessing & managing financial data","text":"chapter, propose way organize financial data. Everybody, experience data, also familiar storing data various formats like CSV, XLS, XLSX, delimited value stores. Reading saving data can become cumbersome case using different data formats, across different projects, well across different programming languages. Moreover, storing data delimited files often leads problems respect column type consistency. instance, date-type columns frequently lead inconsistencies across different data formats programming languages.chapter shows import different open source data sets. Specifically, data comes application programming interface (API) Yahoo!Finance, downloaded standard CSV files, XLSX file stored public Google drive repositories macroeconomic time series. store data single database, serves source data subsequent chapters. conclude chapter providing tips managing databases.First, load global packages use throughout chapter. Later , load packages sections need .Moreover, initially define date range fetch store financial data, making future data updates tractable. case need another time frame, need adjust dates. data starts 1960 since asset pricing studies use data 1962 .","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nstart_date <- ymd(\"1960-01-01\")\nend_date <- ymd(\"2020-12-31\")"},{"path":"accessing-managing-financial-data.html","id":"fama-french-data","chapter":"2 Accessing & managing financial data","heading":"2.1 Fama-French data","text":"start downloading famous Fama-French factors (e.g., Fama French 1993) portfolio returns commonly used empirical asset pricing. Fortunately, neat package Nelson Areal allows us easily access data: frenchdata package provides functions download read data sets Prof. Kenneth French finance data library.can use main function package download monthly Fama-French factors. set 3 Factors includes return time series market, size, value factors alongside risk-free rates. Note manual work correctly parse columns scale appropriately raw Fama-French data comes unpractical data format. precise descriptions variables, suggest consulting Prof. Kenneth French finance data library directly. site, check raw data files appreciate time saved frenchdata.straightforward download corresponding daily Fama-French factors function.subsequent chapter, also use 10 monthly industry portfolios, let us fetch data, .worth taking look available portfolio return time series Kenneth French’s homepage. check sets calling get_french_data_list().","code":"\nlibrary(frenchdata)\nfactors_ff_monthly_raw <- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff_monthly <- factors_ff_monthly_raw$subsets$data[[1]] |>\n  transmute(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(month >= start_date & month <= end_date)\nfactors_ff_daily_raw <- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff_daily <- factors_ff_daily_raw$subsets$data[[1]] |>\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(date >= start_date & date <= end_date)\nindustries_ff_monthly_raw <- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly <- industries_ff_monthly_raw$subsets$data[[1]] |>\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |>\n  mutate(across(where(is.numeric), ~ . / 100)) |>\n  select(month, everything(), -date) |>\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"q-factors","chapter":"2 Accessing & managing financial data","heading":"2.2 q-factors","text":"recent years, academic discourse experienced rise alternative factor models, e.g., form Hou, Xue, Zhang (2014) q-factor model. refer extended background information provided original authors information. q factors can downloaded directly authors’ homepage within read_csv().also need adjust data. First, discard information use . , rename columns “R_”-prescript using regular expressions write column names lower case. can try sticking consistent style naming objects, try illustrate - emphasis try. can check style guides available online, e.g., Hadley Wickham’s tidyverse style guide.","code":"\nfactors_q_monthly_link <- \n  \"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2020.csv\"\nfactors_q_monthly <- read_csv(factors_q_monthly_link) |>\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |>\n  select(-R_F, -R_MKT, -year) |>\n  rename_with(~ str_remove(., \"R_\")) |>\n  rename_with(~ str_to_lower(.)) |>\n  mutate(across(-month, ~ . / 100)) |>\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"macroeconomic-predictors","chapter":"2 Accessing & managing financial data","heading":"2.3 Macroeconomic predictors","text":"next data source set macroeconomic variables often used predictors equity premium. Welch Goyal (2008) comprehensively reexamine performance variables suggested academic literature good predictors equity premium. authors host data updated 2020 Amit Goyal’s website. Since data .xlsx-file stored public Google drive location, need additional packages access data directly R session. Therefore, load readxl read .xlsx-file googledrive Google drive connection.Usually, need authenticate interact Google drive directly R. Since data stored via public link, can proceed without authentication.drive_download() function googledrive package allows us download data store locally.Next, read new data transform columns variables later use. can consult material Amit Goyal’s website definitions variables transformations.Finally, reading macro predictors memory, remove raw data file temporary storage.","code":"\nlibrary(readxl)\nlibrary(googledrive)\ndrive_deauth()\nmacro_predictors_link <- \n  \"https://drive.google.com/file/d/1ACbhdnIy0VbCWgsnXkjcddiV8HF4feWv/view\"\ndrive_download(\n  macro_predictors_link, \n  path = \"data/macro_predictors.xlsx\"\n  )\nmacro_predictors <- read_xlsx(\n  \"data/macro_predictors.xlsx\", \n  sheet = \"Monthly\"\n) |>\n  mutate(month = ym(yyyymm)) |>\n  filter(month >= start_date & month <= end_date) |>\n  mutate(across(where(is.character), as.numeric)) |>\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |>\n  select(month, rp_div, dp, dy, ep, de, svar,\n         bm = `b/m`, ntis, tbl, lty, ltr,\n         tms, dfy, infl\n  ) |>\n  drop_na()\nfile.remove(\"data/macro_predictors.xlsx\")[1] TRUE"},{"path":"accessing-managing-financial-data.html","id":"other-macroeconomic-data","chapter":"2 Accessing & managing financial data","heading":"2.4 Other macroeconomic data","text":"Federal Reserve bank St. Louis provides Federal Reserve Economic Data (FRED), extensive database macroeconomic data. total, 817,000 US international time series 108 different sources. illustration, use already familiar tidyquant package fetch consumer price index (CPI) data can found CPIAUCNS key.download time series, just FRED website extract corresponding key address. instance, produce price index gold ores can found PCU2122212122210 key. tidyquant package provides access around 10,000 time series FRED database. desired time series included, recommend working fredr package (Boysel Vaughan 2021). Note need get API key use functionality, refer package documentation details.","code":"\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date, to = end_date\n) |>\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)]\n  )"},{"path":"accessing-managing-financial-data.html","id":"setting-up-a-database","chapter":"2 Accessing & managing financial data","heading":"2.5 Setting up a database","text":"Now downloaded data web memory R session, let us set database store information future use. use data stored database throughout following chapters, alternatively implement different strategy replace respective code.many ways set organize database, depending use case. purpose, efficient way use SQLite database, C-language library implements small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note SQL (Structured Query Language) standard language accessing manipulating databases, heavily inspired dplyr functions. refer tutorial information SQL.two packages make working SQLite R simple: RSQLite embeds SQLite database engine R dbplyr database back-end dplyr. packages allow set database remotely store tables use remote database tables -memory data frames automatically converting dplyr SQL. Check RSQLite dbplyr vignettes information.SQLite database easily created - code really . Note use extended_types=TRUE option enable date types storing fetching data, otherwise date columns stored integer values.Next, create remote table monthly Fama-French factor data. function dbWriteTable(), copies data SQLite-database. Notice use base R pipe placeholder _ named argument pipe factors_ff_monthly argument df.can use remote table -memory data frame building connection via tbl().dplyr calls evaluated lazily, .e., data memory R session, actually, database work. can see noticing output show number rows. fact, following code chunk fetches top 10 rows database printing.want whole table memory, need collect() . see regularly load data memory next chapters.last couple code chunks really organize simple database! can also share SQLite database across devices programming languages.move next data source, let us also store five tables new SQLite database.now , need access data stored database follow three steps: () Establish connection SQLite database, (ii) call table want extract, (iii) collect data. convenience, following steps show need compact fashion.","code":"\nlibrary(RSQLite)\nlibrary(dbplyr)\ntidy_finance <- dbConnect(\n  SQLite(), \n  \"data/tidy_finance.sqlite\", \n  extended_types = TRUE\n)\nfactors_ff_monthly |>\n  dbWriteTable(tidy_finance, \n               \"factors_ff_monthly\", \n               value = _,     \n               overwrite = TRUE)\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\nfactors_ff_monthly_db |>\n  select(month, rf)# Source:   SQL [?? x 2]\n# Database: sqlite 3.38.5 [F:\\Dropbox (SentinelConsulting)\\A.. Privat\\Git\\tidy_finance\\data\\tidy_finance.sqlite]\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with more rows\nfactors_ff_monthly_db |>\n  select(month, rf) |>\n  collect()# A tibble: 732 × 2\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with 727 more rows\nfactors_ff_daily |>\n  dbWriteTable(tidy_finance, \n               \"factors_ff_daily\", \n               value = _, \n               overwrite = TRUE)\n\nindustries_ff_monthly |>\n  dbWriteTable(tidy_finance, \n               \"industries_ff_monthly\", \n               value = _, \n               overwrite = TRUE)\n\nfactors_q_monthly |>\n  dbWriteTable(tidy_finance, \n               \"factors_q_monthly\", \n               value = _, \n               overwrite = TRUE)\n\nmacro_predictors |>\n  dbWriteTable(tidy_finance, \n               \"macro_predictors\",\n               value = _, \n               overwrite = TRUE)\n\ncpi_monthly |>\n  dbWriteTable(tidy_finance, \n               \"cpi_monthly\", \n               value = _, \n               overwrite = TRUE)\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance <- dbConnect(\n  SQLite(), \n  \"data/tidy_finance.sqlite\", \n  extended_types = TRUE\n)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly <- factors_q_monthly |> collect()"},{"path":"accessing-managing-financial-data.html","id":"managing-sqlite-databases","chapter":"2 Accessing & managing financial data","heading":"2.6 Managing SQLite databases","text":"Finally, end data chapter, revisit SQLite database . drop database objects tables delete data tables, database file size remains unchanged SQLite just marks deleted objects free reserves space future uses. result, database file always grows size.optimize database file, can run VACUUM command database, rebuilds database frees unused space. can execute command database using dbSendQuery() function.VACUUM command actually performs couple additional cleaning steps, can read tutorial.Apart cleaning , might interested listing tables currently database. can via dbListTables() function.function comes handy unsure correct naming tables database.","code":"\ndbSendQuery(tidy_finance, \"VACUUM\")<SQLiteResult>\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\ndbListTables(tidy_finance)Warning: Closing open result set, pending rows [1] \"beta\"                  \"compustat\"             \"cpi_monthly\"          \n [4] \"crsp_daily\"            \"crsp_monthly\"          \"factors_ff_daily\"     \n [7] \"factors_ff_monthly\"    \"factors_q_monthly\"     \"industries_ff_monthly\"\n[10] \"macro_predictors\"      \"mergent\"              "},{"path":"accessing-managing-financial-data.html","id":"exercises-1","chapter":"2 Accessing & managing financial data","heading":"2.7 Exercises","text":"Download monthly Fama-French factors manually Ken French’s data library read via read_csv(). Validate get data via frenchdata package.Download Fama-French 5 factors using package. , compare estimates three factors common Three-Factor model. Explain findings.","code":""},{"path":"wrds-crsp-and-compustat.html","id":"wrds-crsp-and-compustat","chapter":"3 WRDS, CRSP, and Compustat","heading":"3 WRDS, CRSP, and Compustat","text":"chapter shows connect Wharton Research Data Services (WRDS), popular provider financial economic data research applications. use connection download commonly used data stock firm characteristics, CRSP Compustat. Unfortunately, data freely available, students research typically access WRDS libraries. Assuming access WRDS, show prepare merge databases store SQLite-database introduced previous chapter. conclude chapter providing tips working WRDS database.First, load global packages use throughout chapter. Later , load packages sections need .use date range previous chapter ensure consistency.","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(RSQLite)\nlibrary(dbplyr)\nstart_date <- ymd(\"1960-01-01\")\nend_date <- ymd(\"2020-12-31\")"},{"path":"wrds-crsp-and-compustat.html","id":"accessing-wrds","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.1 Accessing WRDS","text":"WRDS widely used source asset firm-specific financial data used academic settings. WRDS data platform provides data validation, flexible delivery options, access many different data sources. data WRDS also organized SQL database, although use PostgreSQL engine. database engine just easy handle R SQLite. use RPostgres package establish connection WRDS database. Note also use odbc package connect PostgreSQL database, need install appropriate drivers . RPostgres already contains suitable driver.establish connection, use function dbConnect() following arguments. Note need replace user password fields credentials. defined system variables purpose book obviously want share credentials rest world.remote connection WRDS useful. Yet, database contains many different databases tables. can check WRDS homepage identify table’s name looking (go beyond exposition). Alternatively, can also query data structure function dbSendQuery(). interested, exercise based WRDS’ tutorial “Querying WRDS Data using R”. Furthermore, penultimate section chapter shows investigate structure databases.","code":"\nlibrary(RPostgres)\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)"},{"path":"wrds-crsp-and-compustat.html","id":"downloading-and-preparing-crsp","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.2 Downloading and preparing CRSP","text":"Center Research Security Prices (CRSP) provides widely used data US stocks. use wrds connection object just created first access monthly CRSP return data. Actually, need three tables get desired data: () CRSP monthly security file,identifying information,(iii) delisting information.use three remote tables fetch data want put local database. Just , idea let WRDS database work just download data actually need. apply common filters data selection criteria narrow data interest: () keep data time windows interest, (ii) keep US-listed stocks identified via share codes 10 11, (iii) keep months valid permno-specific information msenames. addition, add delisting reasons returns. can read great textbook Bali, Engle, Murray (2016) (BEM) extensive discussion filters apply code .Now, relevant monthly return data memory proceed preparing data future analyses. perform preparation step current stage since want avoid executing mutations every time use data subsequent chapters.first additional variable create market capitalization (mktcap). Note keep market cap millions USD just convenience (want print huge numbers figures tables). Moreover, set zero market cap missing makes conceptually little sense (.e., firm bankrupt).next variable frequently use one-month lagged market capitalization. Lagged market capitalization typically used compute value-weighted portfolio returns, demonstrate later chapter. simple consistent way add column lagged market cap values add one month observation join information monthly CRSP data.wonder use lag() function, e.g., via crsp_monthly |> group_by(permno) |> mutate(mktcap_lag = lag(mktcap)), take look exercises.Next, follow BEM transforming listing exchange codes explicit exchange names.Similarly, transform industry codes industry descriptions following BEM. Notice also categorizations industries (e.g., Eugene Fama Kenneth French) commonly used.also construct returns adjusted delistings described BEM. transformation, can drop delisting returns codes.Next, compute excess returns subtracting monthly risk-free rate provided Fama-French data. base analyses excess returns, can drop adjusted returns risk-free rate tibble. Note ensure excess returns bounded -1 less -100% return make conceptually sense. can adjust returns, connect database load tibble factors_ff_monthly.Since excess returns market capitalization crucial analyses, can safely exclude observations missing returns market capitalization.Finally, store monthly CRSP file database.","code":"\nmsf_db <- tbl(wrds, in_schema(\"crsp\", \"msf\"))\nmsf_db# Source:   table<\"crsp\".\"msf\"> [?? x 21]\n# Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n  cusip    permno permco issuno hexcd hsiccd date       bidlo askhi   prc   vol\n  <chr>     <dbl>  <dbl>  <dbl> <dbl>  <dbl> <date>     <dbl> <dbl> <dbl> <dbl>\n1 68391610  10000   7952  10396     3   3990 1985-12-31 NA    NA    NA       NA\n2 68391610  10000   7952  10396     3   3990 1986-01-31 -2.5  -4.44 -4.38  1771\n3 68391610  10000   7952  10396     3   3990 1986-02-28 -3.25 -4.38 -3.25   828\n4 68391610  10000   7952  10396     3   3990 1986-03-31 -3.25 -4.44 -4.44  1078\n5 68391610  10000   7952  10396     3   3990 1986-04-30 -4    -4.31 -4      957\n# … with more rows, and 10 more variables: ret <dbl>, bid <dbl>, ask <dbl>,\n#   shrout <dbl>, cfacpr <dbl>, cfacshr <dbl>, altprc <dbl>, spread <dbl>,\n#   altprcdt <date>, retx <dbl>\nmsenames_db <- tbl(wrds, in_schema(\"crsp\", \"msenames\"))\nmsenames_db# Source:   table<\"crsp\".\"msenames\"> [?? x 21]\n# Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n  permno namedt     nameendt   shrcd exchcd siccd ncusip   ticker comnam   shrcls\n   <dbl> <date>     <date>     <dbl>  <dbl> <dbl> <chr>    <chr>  <chr>    <chr> \n1  10000 1986-01-07 1986-12-03    10      3  3990 68391610 OMFGA  OPTIMUM… A     \n2  10000 1986-12-04 1987-03-09    10      3  3990 68391610 OMFGA  OPTIMUM… A     \n3  10000 1987-03-10 1987-06-11    10      3  3990 68391610 OMFGA  OPTIMUM… A     \n4  10001 1986-01-09 1993-11-21    11      3  4920 39040610 GFGC   GREAT F… <NA>  \n5  10001 1993-11-22 2004-06-09    11      3  4920 29274A10 EWST   ENERGY … <NA>  \n# … with more rows, and 11 more variables: tsymbol <chr>, naics <chr>,\n#   primexch <chr>, trdstat <chr>, secstat <chr>, permco <dbl>, compno <dbl>,\n#   issuno <dbl>, hexcd <dbl>, hsiccd <dbl>, cusip <chr>\nmsedelist_db <- tbl(wrds, in_schema(\"crsp\", \"msedelist\"))\nmsedelist_db# Source:   table<\"crsp\".\"msedelist\"> [?? x 19]\n# Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n  permno dlstdt     dlstcd nwperm nwcomp nextdt      dlamt dlretx  dlprc\n   <dbl> <date>      <dbl>  <dbl>  <dbl> <date>      <dbl>  <dbl>  <dbl>\n1  10000 1987-06-11    560      0      0 1987-06-12  0.219 0      -0.219\n2  10001 2017-08-03    233      0      0 NA         13.1   0.0116  0    \n3  10002 2013-02-15    231  35263   1658 NA          3.01  0.0460  0    \n4  10003 1995-12-15    231  10569   8477 NA          5.45  0.0137  0    \n5  10005 1991-07-11    560      0      0 1991-07-12  0.141 0.125  -0.141\n# … with more rows, and 10 more variables: dlpdt <date>, dlret <dbl>,\n#   permco <dbl>, compno <dbl>, issuno <dbl>, hexcd <dbl>, hsiccd <dbl>,\n#   cusip <chr>, acperm <dbl>, accomp <dbl>\ncrsp_monthly <- msf_db |>\n  filter(date >= start_date & date <= end_date) |>\n  inner_join(\n    msenames_db |>\n      filter(shrcd %in% c(10, 11)) |>\n      select(permno, exchcd, siccd, namedt, nameendt), \n    by = c(\"permno\")) |>\n  filter(date >= namedt & date <= nameendt) |>\n  mutate(month = floor_date(date, \"month\")) |>\n  left_join(\n    msedelist_db |>\n      select(permno, dlstdt, dlret, dlstcd) |>\n      mutate(month = floor_date(dlstdt, \"month\")), \n    by = c(\"permno\", \"month\")) |>\n  select(\n    permno, # Security identifier\n    date, # Date of the observation\n    month, # Month of the observation\n    ret, # Return\n    shrout, # Shares outstanding (in thousands)\n    altprc, # Last traded price in a month\n    exchcd, # Exchange code\n    siccd, # Industry code\n    dlret, # Delisting return\n    dlstcd # Delisting code\n  ) |>\n  collect() |>\n  mutate(\n    month = ymd(month),\n    shrout = shrout * 1000\n  )\ncrsp_monthly <- crsp_monthly |>\n  mutate(\n    mktcap = abs(shrout * altprc) / 1000000,\n    mktcap = if_else(mktcap == 0, \n                     as.numeric(NA), \n                     mktcap)\n  )\nmktcap_lag <- crsp_monthly |>\n  mutate(month = month %m+% months(1)) |>\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(mktcap_lag, by = c(\"permno\", \"month\"))\ncrsp_monthly <- crsp_monthly |>\n  mutate(exchange = case_when(\n    exchcd %in% c(1, 31) ~ \"NYSE\",\n    exchcd %in% c(2, 32) ~ \"AMEX\",\n    exchcd %in% c(3, 33) ~ \"NASDAQ\",\n    TRUE ~ \"Other\"\n  ))\ncrsp_monthly <- crsp_monthly |>\n  mutate(industry = case_when(\n    siccd >= 1 & siccd <= 999 ~ \"Agriculture\",\n    siccd >= 1000 & siccd <= 1499 ~ \"Mining\",\n    siccd >= 1500 & siccd <= 1799 ~ \"Construction\",\n    siccd >= 2000 & siccd <= 3999 ~ \"Manufacturing\",\n    siccd >= 4000 & siccd <= 4899 ~ \"Transportation\",\n    siccd >= 4900 & siccd <= 4999 ~ \"Utilities\",\n    siccd >= 5000 & siccd <= 5199 ~ \"Wholesale\",\n    siccd >= 5200 & siccd <= 5999 ~ \"Retail\",\n    siccd >= 6000 & siccd <= 6799 ~ \"Finance\",\n    siccd >= 7000 & siccd <= 8999 ~ \"Services\",\n    siccd >= 9000 & siccd <= 9999 ~ \"Public\",\n    TRUE ~ \"Missing\"\n  ))\ncrsp_monthly <- crsp_monthly |>\n  mutate(ret_adj = case_when(\n    is.na(dlstcd) ~ ret,\n    !is.na(dlstcd) & !is.na(dlret) ~ dlret,\n    dlstcd %in% c(500, 520, 580, 584) |\n      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,\n    dlstcd == 100 ~ ret,\n    TRUE ~ -1\n  )) |>\n  select(-c(dlret, dlstcd))\ntidy_finance <- dbConnect(\n  SQLite(), \n  \"data/tidy_finance.sqlite\", \n  extended_types = TRUE\n)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly |> select(month, rf), \n            by = \"month\") |>\n  mutate(\n    ret_excess = ret_adj - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) |>\n  select(-ret_adj, -rf)\ncrsp_monthly <- crsp_monthly |>\n  drop_na(ret_excess, mktcap, mktcap_lag)\ncrsp_monthly |>\n  dbWriteTable(tidy_finance, \n               \"crsp_monthly\", \n               value = _, \n               overwrite = TRUE)"},{"path":"wrds-crsp-and-compustat.html","id":"first-glimpse-of-the-crsp-sample","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.3 First glimpse of the CRSP sample","text":"move data sources, let us look descriptive statistics CRSP sample, main source stock returns.figure shows monthly number securities listing exchange time. NYSE longest history data, NASDAQ exhibits considerable large number stocks. number stocks AMEX decreasing steadily last couple decades. end 2020, 2300 stocks NASDAQ, 1244 NYSE, 147 AMEX 1 belongs category.\nFIGURE 3.1: Monthly number securities exchange.\nNext, look aggregate market capitalization respective listing exchanges. ensure look meaningful data comparable time, adjust nominal values inflation. fact, can use tables already database calculate aggregate market caps listing exchange plotting just memory. values end year(end_date) USD ensure inter-temporal comparability. NYSE-listed stocks far largest market capitalization, followed NASDAQ-listed stocks.\nFIGURE 3.2: Monthly market cap listing exchange billions Dec 2020 USD.\ncourse, performing computation database really meaningful can easily pull required data memory. code chunk slower performing steps tables already memory. However, just want illustrate can perform many things database loading data memory. proceed, load monthly CPI data.Next, look descriptive statistics industry. figure plots number stocks sample SIC industry classifiers. sample period, largest share stocks apparently Manufacturing, albeit number peaked somewhere 90s. number firms associated public administration seems category rise recent years, even surpassing Manufacturing end sample period.\nFIGURE 3.3: Monthly number securities industry.\nalso compute market cap stocks belonging respective industries. values terms billions end 2020 USD. points time, manufacturing firms comprise largest portion market capitalization. Towards end sample, however, financial firms services begin make substantial portion market cap.\nFIGURE 3.4: Monthly total market cap industry billions Dec 2020 USD.\n","code":"\ncrsp_monthly |>\n  count(exchange, date) |>\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ntbl(tidy_finance, \"crsp_monthly\") |>\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), by = \"month\") |>\n  group_by(month, exchange) |>\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi,\n    .groups = 'drop'\n  ) |>\n  collect() |>\n  mutate(month = ymd(month)) |>\n  ggplot(aes(x = month, y = mktcap / 1000, \n             color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2020 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ncpi_monthly <- tbl(tidy_finance, \"cpi_monthly\") %>% \n  collect()\ncrsp_monthly_industry <- crsp_monthly |>\n  left_join(cpi_monthly, by = \"month\") |>\n  group_by(month, industry) |>\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry |>\n  ggplot(aes(x = month, \n             y = securities, \n             color = industry, \n             linetype = industry)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ncrsp_monthly_industry |>\n  ggplot(aes(x = month, \n             y = mktcap / 1000, \n             color = industry, \n             linetype = industry)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market cap by industry in billions of Dec 2020 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)"},{"path":"wrds-crsp-and-compustat.html","id":"daily-crsp-data","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.4 Daily CRSP data","text":"turn accounting data, also want provide proposal downloading daily CRSP data. monthly data typically fit memory can downloaded meaningful amount time, usually true daily return data. daily CRSP data file substantially larger monthly data can exceed 20GB. two important implications: hold daily return data memory (hence possible copy entire data set local database), experience, download usually crashes (never stops) much data WRDS cloud prepare send R session.solution challenge. many ‘big data’ problems, can split big task several smaller tasks easy handle. , instead downloading data many stocks , download data small batches stock consecutively. operations can implemented ()-loops, download, prepare, store data single stock iteration. operation might nonetheless take couple hours, patient either way (often run code overnight). keep track progress, can use txtProgressBar(). Eventually, end 68 million rows daily return data. Note store identifying information actually need, namely permno, date, month alongside excess returns. thus ensure local database contains data actually use can load full daily data memory later. Notice also use function dbWriteTable() option append new data existing table, process second following batches.","code":"\ndsf_db <- tbl(wrds, in_schema(\"crsp\", \"dsf\"))\npermnos <- tbl(tidy_finance, \"crsp_monthly\") |>\n  distinct(permno) |>\n  pull()\n\nprogress <- txtProgressBar(min = 0, \n                           max = length(permnos), \n                           initial = 0, \n                           style = 3)\nfor (j in 1:length(permnos)) {\n  permno_sub <- permnos[j]\n  crsp_daily_sub <- dsf_db |>\n    filter(permno == permno_sub &\n      date >= start_date & date <= end_date) |>\n    select(permno, date, ret) |>\n    collect() |>\n    drop_na()\n\n  if (nrow(crsp_daily_sub)) {\n    crsp_daily_sub <- crsp_daily_sub |>\n      mutate(month = floor_date(date, \"month\")) |>\n      left_join(factors_ff_daily |>\n        select(date, rf), by = \"date\") |>\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) |>\n      select(permno, date, month, ret_excess)\n\n    if (j == 1) {\n      overwrite <- TRUE\n      append <- FALSE\n    } else {\n      overwrite <- FALSE\n      append <- TRUE\n    }\n\n    crsp_daily_sub |>\n      dbWriteTable(tidy_finance, \n                   \"crsp_daily\", \n                   value = _, \n                   overwrite = overwrite, \n                   append = append)\n  }\n  setTxtProgressBar(progress, j)\n}\nclose(progress)\n\ncrsp_daily_db <- tbl(tidy_finance, \"crsp_daily\")"},{"path":"wrds-crsp-and-compustat.html","id":"preparing-compustat-data","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.5 Preparing Compustat data","text":"Firm accounting data important source information use portfolio analyses subsequent chapters. commonly used source firm financial information Compustat provided S&P Global Market Intelligence, global data vendor provides financial, statistical, market information active inactive companies throughout world. US Canadian companies, annual history available back 1950 quarterly well monthly histories date back 1962.access Compustat data, can tap WRDS, hosts funda table contains annual firm-level information North American companies.follow typical filter conventions pull data actually need: () get industrial fundamental data (.e., ignore financial services) (ii) standard format (.e., consolidated information standard presentation), (iii) data desired time window.Next, calculate book value preferred stock equity inspired variable definition Ken French’s data library. Note set negative zero equity missing makes conceptually little sense (.e., firm bankrupt).keep last available information firm-year group. Note datadate defines time corresponding financial data refers (e.g., annual report December 31, 2020). Therefore, datadate date data made available public. Check exercises insights peculiarities datadate.last step, already done preparing firm fundamentals. Thus, can store local database.","code":"\nfunda_db <- tbl(wrds, in_schema(\"comp\", \"funda\"))\ncompustat <- funda_db |>\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" &\n      consol == \"C\" &\n      datadate >= start_date & datadate <= end_date\n  ) |>\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk, # Preferred stock par value\n    capx, # Capital investment\n    oancf # Operating cash flow\n  ) |>\n  collect()\ncompustat <- compustat |>\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be <= 0, as.numeric(NA), be)\n  )\ncompustat <- compustat |>\n  mutate(year = year(datadate)) |>\n  group_by(gvkey, year) |>\n  filter(datadate == max(datadate)) |>\n  ungroup()\ncompustat |>\n  dbWriteTable(tidy_finance, \n               \"compustat\", \n               value = _, \n               overwrite = TRUE)"},{"path":"wrds-crsp-and-compustat.html","id":"merging-crsp-with-compustat","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.6 Merging CRSP with Compustat","text":"Unfortunately, CRSP Compustat use different keys identify stocks firms. CRSP uses permno stocks, Compustat uses gvkey identify firms. Fortunately, curated matching table WRDS allows us merge CRSP Compustat, create connection CRSP-Compustat Merged table (provided CRSP).linking table contains links CRSP Compustat identifiers various approaches. However, need make sure keep relevant correct links, following description outlined BEM. Note also currently active links end date, just enter current date via Sys.Date().use links create new table mapping stock identifier, firm identifier, month. add links Compustat gvkey monthly stock data.last step, update previously prepared monthly CRSP file linking information local database.close chapter, let us look interesting descriptive statistic data. book value equity plays crucial role many asset pricing applications, interesting know many stocks information available. Hence, figure plots share securities book equity values exchange. turns coverage pretty bad AMEX- NYSE-listed stocks 60s hovers around 80% periods thereafter. can ignore erratic coverage securities belong category since handful anyway sample.\nFIGURE 3.5: End--year share securities book equity values exchange.\n","code":"\nccmxpf_linktable_db <- tbl(wrds, \n                           in_schema(\"crsp\", \"ccmxpf_linktable\"))\nccmxpf_linktable <- ccmxpf_linktable_db |>\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) |>\n  select(permno = lpermno, gvkey, linkdt, linkenddt) |>\n  collect() |>\n  mutate(linkenddt = replace_na(linkenddt, Sys.Date()))\nccmxpf_linktable# A tibble: 31,770 × 4\n  permno gvkey  linkdt     linkenddt \n   <dbl> <chr>  <date>     <date>    \n1  25881 001000 1970-11-13 1978-06-30\n2  10015 001001 1983-09-20 1986-07-31\n3  10023 001002 1972-12-14 1973-06-05\n4  10031 001003 1983-12-07 1989-08-16\n5  54594 001004 1972-04-24 2022-07-26\n# … with 31,765 more rows\nccm_links <- crsp_monthly |>\n  inner_join(ccmxpf_linktable, by = \"permno\") |>\n  filter(!is.na(gvkey) & (date >= linkdt & date <= linkenddt)) |>\n  select(permno, gvkey, date)\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(ccm_links, by = c(\"permno\", \"date\"))\ncrsp_monthly |>\n  dbWriteTable(tidy_finance, \n                 \"crsp_monthly\", \n                 value = _, \n                 overwrite = TRUE)\ncrsp_monthly |>\n  group_by(permno, year = year(month)) |>\n  filter(date == max(date)) |>\n  ungroup() |>\n  left_join(compustat, by = c(\"gvkey\", \"year\")) |>\n  group_by(exchange, year) |>\n  summarize(share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),\n            .groups = 'drop') |>\n  ggplot(aes(x = year, y = share, color = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"End-of-year share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) + \n  coord_cartesian(ylim = c(0, 1))"},{"path":"wrds-crsp-and-compustat.html","id":"some-tricks-for-postgresql-databases","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.7 Some tricks for PostgreSQL databases","text":"mentioned , WRDS database runs PostgreSQL rather SQLite. Finding right tables data needs can tricky WRDS PostgreSQL instance, tables organized schemas. wonder purpose schemas , check documetation. instance, want find tables live crsp schema, runThis operation returns list tables belong crsp family WRSD, e.g. <Id> schema = crsp, table = msenames. Similarly, can fetch list tables belong comp family viaIf want get schemas, run","code":"\ndbListObjects(wrds, Id(schema = \"crsp\"))\ndbListObjects(wrds, Id(schema = \"comp\"))\ndbListObjects(wrds)"},{"path":"wrds-crsp-and-compustat.html","id":"exercises-2","chapter":"3 WRDS, CRSP, and Compustat","heading":"3.8 Exercises","text":"Check structure WRDS database sending queries spirit “Querying WRDS Data using R” verify output dbListObjects(). many tables associated CRSP? Can identify stored within msp500?Compute mkt_cap_lag using lag(mktcap) rather joins . Filter rows lag-based market capitalization measure different one computed . different?main part, look distribution market capitalization across exchanges industries. Now, plot average market capitalization firms exchange industry. find?datadate refers date fiscal year corresponding firm refers . Count number observations Compustat month date variable. find? finding suggest pooling observations fiscal year?Go back original Compustat data funda_db extract rows firm multiple rows fiscal year. reason observations?Repeat analysis market capitalization book equity, computed Compustat data. , used matched sample plot book equity market capitalization. two variables related?","code":""},{"path":"trace-and-fisd.html","id":"trace-and-fisd","chapter":"4 TRACE and FISD","heading":"4 TRACE and FISD","text":"Bond markets far diverse stock markets, feature broad set issuers, ranging corporations governments municipalities. Moreover, issuers multiple bonds outstanding simultaneously potentially different indentures. chapter, dive US corporate bond market. market segment exciting due size (roughly $10 trillion outstanding), heterogeneity issuers (opposed government bonds), market structure (mostly --counter trades), data availability. introduce use bond characteristics Mergent FISD trade reports TRACE provide code download clean TRACE R.Many researchers study liquidity US corporate bond market O’Hara Zhou (2021). cover bond returns , compute TRACE data. Instead, refer studies topic Bessembinder et al. (2008), Bai, Bali, Wen (2019), Kelly, Palhares, Pruitt (2021) survey Huang, Shi, et al. (2021). Moreover, WRDS includes bond returns computed TRACE data monthly frequency.current chapter relies set packages.Compared previous chapters, load devtools package (Wickham, Hester, et al. 2022) source code provided public via gist.","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(dbplyr)\nlibrary(RSQLite)\nlibrary(RPostgres)\nlibrary(devtools)"},{"path":"trace-and-fisd.html","id":"sqlite-database-and-wrds","chapter":"4 TRACE and FISD","heading":"4.1 SQLite-Database and WRDS","text":"bond databases need available (WRDS) establish RPostgres-connection described previous chapter. Additionally, connect local SQLite-database, also introduced previous chapters.","code":"\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)\n\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)"},{"path":"trace-and-fisd.html","id":"mergent-fisd","chapter":"4 TRACE and FISD","heading":"4.2 Mergent FISD","text":"research US corporate bonds, Mergent Fixed Income Securities Database (FISD) primary resource bond characteristics. detailed manual WRDS, cover necessary subjects . FISD data comes two main variants, namely, centered issuers issues. either case, useful identifiers CUSIPs. 9-digit CUSIPs identify securities issued issuers. issuers can identified first six digits security CUSIP, also called 6-digit CUSIP. principle, stocks bonds CUSIPs. connection , principle, allow matching easily, due changing issuer details, approach yields small coverage.use issue-centered version Mergent FISD identify subset US corporate bonds meet standard criteria. WRDS table fisd_mergedissue contains information need 9-digit CUSIP level. mentioned introduction chapter, corporate bonds diverse, details indenture vary significantly. focus common bonds make majority trading volume market without diverging much indentures.following chunk connects data selects bond sample remove certain bond types less commonly O’Hara Zhou (2021).also pull issuer information fisd_mergedissuer regarding industry country firm issued particular bond. , filter include US-domiciled firms’ bonds. match data issuer_id.Finally, save bond characteristics local database. selection bonds also constitutes sample collect trade reports TRACE .Mergent FISD database also contains data. issue-based file contains information covenants, .e., restrictions included bond indentures limit specific actions firms (e.g., Handler, Jankowitsch, Weiss 2021). Moreover, FISD also provides information bond ratings. need either .","code":"\nmergent <- tbl(wrds, in_schema(\"fisd\", \"fisd_mergedissue\")) |>\n  filter(\n    security_level == \"SEN\",   # senior bonds\n    slob == \"N\",               # secured lease obligation\n    is.na(security_pledge),    # unsecured bonds\n    asset_backed == \"N\",       # not asset backed\n    defeased == \"N\",           # not defeased\n    bond_type %in% c(\"CDEB\",   # US Corporate Debentures\n                     \"CMTN\",   # US Corporate MTN (Medium Term Note)\n                     \"CMTZ\",   # US Corporate MTN Zero\n                     \"CZ\",     # US Corporate Zero,\n                     \"USBN\"),  # US Corporate Bank Note\n    pay_in_kind != \"Y\",        # not payable in kind\n    yankee == \"N\",             # no foreign issuer\n    canadian == \"N\",           # not Canadian \n    foreign_currency == \"N\",   # USD\n    coupon_type %in% c(\"F\",    # fixed coupon\n                       \"Z\"),   # zero coupon\n    is.na(fix_frequency), \n    coupon_change_indicator == \"N\", \n    interest_frequency %in% c(\"0\", # per year\n                              \"1\", \n                              \"2\", \n                              \"4\", \n                              \"12\"),\n    rule_144a == \"N\",          # publicly traded\n    private_placement == \"N\", \n    defaulted == \"N\",          # not defaulted\n    is.na(filing_date),\n    is.na(settlement),\n    convertible == \"N\",        # not convertible\n    is.na(exchange),\n    putable == \"N\",            # not putable\n    unit_deal == \"N\",          # not issued with another security\n    exchangeable == \"N\",       # not exchangeable\n    perpetual == \"N\",          # not perpetual\n    preferred_security == \"N\")  |> # not preferred\n  select(\n    complete_cusip, maturity, \n    offering_amt, offering_date, \n    dated_date, first_interest_date, \n    interest_frequency, coupon, \n    last_interest_date, day_count_basis, \n    issue_id, issuer_id\n    ) |> \n  collect()\nmergent_issuer <- tbl(wrds, in_schema(\"fisd\", \"fisd_mergedissuer\")) |>  \n  select(issuer_id, sic_code, country_domicile) |> \n  collect()\n\nmergent <- mergent |> \n  inner_join(mergent_issuer, by = \"issuer_id\")  |> \n  filter(country_domicile == \"USA\") |> \n  select(-country_domicile)\nmergent |> \n  dbWriteTable(conn = tidy_finance,\n               name = \"mergent\",\n               value = _,\n               overwrite = TRUE)"},{"path":"trace-and-fisd.html","id":"trace","chapter":"4 TRACE and FISD","heading":"4.3 TRACE","text":"Financial Industry Regulatory Authority (FINRA) provides Trade Reporting Compliance Engine (TRACE). TRACE, dealers trade corporate bonds must report trades individually. Hence, observe trade messages TRACE contain information bond traded, trade time, price, volume. TRACE comes two variants; standard enhanced TRACE. show download clean enhanced TRACE contains uncapped volume, crucial quantity missing standard distribution. Moreover, enhanced TRACE also provides information respective parties’ roles direction trade report. items become essential cleaning messages.repeatedly talk cleaning TRACE? Trade messages submitted within short time window trade executed (less 15 minutes). messages can contain errors, reporters subsequently correct cancel trade altogether. cleaning needs described Dick-Nielsen (2009) detail, Dick-Nielsen (2014) shows clean enhanced TRACE data using SAS. go cleaning steps . code lengthy serves educational purpose. However, downloading cleaning enhanced TRACE data straightforward setup.store code cleaning enhanced TRACE R following Github gist. (https://docs.github.com/en/get-started/writing--github/editing--sharing-content--gists/creating-gists) function. appendix also contains code reference. need source code gist, can source_gist(). Alternatively, can also go gist, download , source() respective R-file. clean_enhanced_trace() function takes vector CUSIPs, connection WRDS explained chapter 3, start end date respectively.TRACE database considerably large. Therefore, download subsets data . Specifying many CUSIPs long time horizon result long download times potential failure due size request WRDS. size limit depends many parameters, give guideline . working complete TRACE data CUSIPs , splitting data 100 parts takes roughly two hours using setup. applications book, need data one year December 2015, .e., two years, download data ten sets, defined.Finally, run loop style Chapter 2 download daily returns CRSP. CUSIP sets defined , call cleaning function save resulting output. add new data existing dataframe batch two following batches.","code":"\nsource_gist(\"3a05b3ab281563b2e94858451c2eb3a4\")\nmergent_cusips <- mergent |>\n  pull(complete_cusip)\n\nset.seed(123)\nmergent_parts <- split(mergent_cusips, \n                       sample(1:10, \n                              length(mergent_cusips), \n                              replace = T))\nfor(j in 1:length(mergent_parts)) {\n    trace_enhanced <- clean_enhanced_trace(\n    cusips = mergent_parts[[j]],\n    connection = wrds,\n    start_date = ymd(\"2014-12-01\"),\n    end_date = ymd(\"2016-12-31\"))\n    \n    if (j == 1) {\n      overwrite <- TRUE\n      append <- FALSE\n    } else {\n      overwrite <- FALSE\n      append <- TRUE\n    }\n  \n  trace_enhanced |> \n    dbWriteTable(conn = tidy_finance,\n                 name = \"trace_enhanced\",\n                 value = _,\n                 overwrite = overwrite, \n                 append = append)\n}"},{"path":"beta-estimation.html","id":"beta-estimation","chapter":"5 Beta estimation","heading":"5 Beta estimation","text":"chapter, introduce important concept financial economics: exposure individual stock changes market portfolio. According Capital Asset Pricing Model (CAPM) Sharpe (1964), Lintner (1965), Mossin (1966), cross-sectional variation expected asset returns function covariance excess return asset excess return market portfolio. regression coefficient market returns excess returns usually called market beta. show estimation procedure market betas. go details foundations market beta simply refer treatment CAPM information. Instead, provide details functions use compute results. particular, leverage useful computational concepts: rolling-window estimation parallelization.use following packages throughout chapter:Compared previous chapters, introduce slider (Vaughan 2021) sliding window functions, furrr (Vaughan Dancho 2022) apply mapping functions parallel.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(slider)\nlibrary(furrr)"},{"path":"beta-estimation.html","id":"estimating-beta-using-monthly-returns","chapter":"5 Beta estimation","heading":"5.1 Estimating beta using monthly returns","text":"estimation procedure based rolling-window estimation may use either monthly daily returns different window lengths. First, let us start loading monthly data SQLite-database introduced previous chapters 2-3.estimate CAPM equation\n\\[\nr_{, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{, t}\n\\]\nregress excess stock returns ret_excess excess returns market portfolio mkt_excess.\nR provides simple solution estimate (linear) models function lm(). lm() requires formula input specified compact symbolic form. expression form y ~ model interpreted specification response y modeled linear predictor specified symbolically model. model consists series terms separated + operators. addition standard linear models, lm() provides lot flexibility. check documentation information. start, restrict data time series observations CRSP correspond Apple’s stock (.e., permno 14593 Apple) compute \\(\\alpha_i\\) well \\(\\beta_i\\).lm() returns object class lm contains information usually care linear models. summary() returns overview estimated parameters. coefficients(fit) return estimated coefficients. output indicates Apple moves excessively market estimated \\(\\beta_i\\) one (\\(\\hat\\beta_i\\) = 1.4).","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  select(permno, month, industry, ret_excess, mkt_excess)\nfit <- lm(ret_excess ~ mkt_excess,\n  data = crsp_monthly |>\n    filter(permno == \"14593\")\n)\n\nsummary(fit)\nCall:\nlm(formula = ret_excess ~ mkt_excess, data = filter(crsp_monthly, \n    permno == \"14593\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5167 -0.0610  0.0009  0.0643  0.3940 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.01051    0.00532    1.98    0.049 *  \nmkt_excess   1.40081    0.11748   11.92   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.115 on 478 degrees of freedom\nMultiple R-squared:  0.229, Adjusted R-squared:  0.228 \nF-statistic:  142 on 1 and 478 DF,  p-value: <2e-16"},{"path":"beta-estimation.html","id":"rolling-window-estimation","chapter":"5 Beta estimation","heading":"5.2 Rolling-window estimation","text":"estimated regression coefficients example, scale estimation \\(\\beta_i\\) whole different level perform rolling-window estimations entire CRSP sample. following function implements CAPM regression data frame (part thereof) containing least min_obs observations avoid huge fluctuations time series short. condition violated, function returns missing value.Next, define function rolling estimation. slide_period function able handle months window input straightforward manner. thus avoid using time-series package (e.g., zoo) converting data fit package functions, rather stay world tibbles.following function takes input data slides across month vector, considering total months months. function essentially performs three steps: () combine rows single data frame (comes handy case daily data), (ii) compute betas sliding across months, (iii) return tibble months corresponding beta estimates (particularly useful case daily data).\ndemonstrate , can also apply function daily returns data.attack whole CRSP sample, let us focus couple examples well-known firms.want estimate rolling betas Apple, can use mutate().\ntake total 5 years data require least 48 months return data compute betas.\nCheck exercises want ot compute beta different time periods.actually quite simple perform rolling-window estimation arbitrary number stocks, visualize following code chunk.\nFIGURE 5.1: Monthly beta estimates example stocks using 5 years data.\n","code":"\nestimate_capm <- function(data, min_obs = 1) {\n  if (nrow(data) < min_obs) {\n    beta <- as.numeric(NA)\n  } else {\n    fit <- lm(ret_excess ~ mkt_excess, data = data)\n    beta <- as.numeric(fit$coefficients[2])\n  }\n  return(beta)\n}\nroll_capm_estimation <- function(data, months, min_obs) {\n  data <- bind_rows(data) |>\n    arrange(month)\n\n  betas <- slide_period_vec(\n    .x = data,\n    .i = data$month,\n    .period = \"month\",\n    .f = ~estimate_capm(., min_obs),\n    .before = months - 1,\n    .complete = FALSE\n  )\n\n  tibble(\n    month = unique(data$month),\n    beta = betas\n  )\n}\nexamples <- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n)\nbeta_example <- crsp_monthly |>\n  filter(permno == examples$permno[1]) |>\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) |>\n  drop_na()\nbeta_example# A tibble: 433 × 6\n  permno month      industry      ret_excess mkt_excess  beta\n   <dbl> <date>     <chr>              <dbl>      <dbl> <dbl>\n1  14593 1984-12-01 Manufacturing     0.170      0.0184  2.05\n2  14593 1985-01-01 Manufacturing    -0.0108     0.0799  1.90\n3  14593 1985-02-01 Manufacturing    -0.152      0.0122  1.88\n4  14593 1985-03-01 Manufacturing    -0.112     -0.0084  1.89\n5  14593 1985-04-01 Manufacturing    -0.0467    -0.0096  1.90\n# … with 428 more rows\nbeta_examples <- crsp_monthly |>\n  inner_join(examples, by = \"permno\") |>\n  group_by(permno) |>\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) |>\n  ungroup() |>\n  select(permno, company, month, beta_monthly = beta) |>\n  drop_na()\n\nbeta_examples |>\n  ggplot(aes(x = month, y = beta_monthly, color = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of data\"\n  )"},{"path":"beta-estimation.html","id":"parallelized-rolling-window-estimation","chapter":"5 Beta estimation","heading":"5.3 Parallelized rolling-window estimation","text":"Even though now just apply function using group_by() whole CRSP sample, advise computationally quite expensive.\nRemember perform rolling-window estimations across stocks time periods.\nHowever, estimation problem ideal scenario employ power parallelization.\nParallelization means split tasks perform rolling-window estimations across different workers (cores local machine).First, nest() data permno. Nested data means now list permno corresponding time series data industry label. get one row output unique combination non-nested variables permno industry.Alternatively, created nested data excluding variables want nest, following code chunk. However, applications desirable explicitly state variables nested data list-column, reader can track ends .Next, ant apply roll_capm_estimation() function stock. situation ideal use case map(), takes list vector input returns object length input. case, map() returns single data frame time series beta estimates stock. Therefore, use unnest() transform list outputs tidy data frame.However, instead, want perform estimations rolling betas different stocks parallel. Windows machine, makes sense define multisession, means separate R processes running background machine perform individual jobs. check documentation plan(), can also see ways resolve parallelization different environments.Using eight cores, estimation sample around 25k stocks takes around 20 minutes. course, can speed things considerably cores available share workload powerful cores. Notice difference code ? need replace map() future_map().","code":"\ncrsp_monthly_nested <- crsp_monthly |>\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_monthly_nested# A tibble: 29,203 × 3\n  permno industry      data              \n   <dbl> <chr>         <list>            \n1  10000 Manufacturing <tibble [16 × 3]> \n2  10001 Utilities     <tibble [378 × 3]>\n3  10002 Finance       <tibble [324 × 3]>\n4  10003 Finance       <tibble [118 × 3]>\n5  10005 Mining        <tibble [65 × 3]> \n# … with 29,198 more rows\ncrsp_monthly_nested <- crsp_monthly |>\n  nest(data = -c(permno, industry))\ncrsp_monthly_nested |>\n  inner_join(examples, by = \"permno\") |>\n  mutate(beta = map(\n    data, \n    ~roll_capm_estimation(., months = 60, min_obs = 48))\n    ) |>\n  unnest(c(beta)) |>\n  select(permno, month, beta_monthly = beta) |>\n  drop_na()# A tibble: 1,362 × 3\n  permno month      beta_monthly\n   <dbl> <date>            <dbl>\n1  10107 1990-03-01         1.39\n2  10107 1990-04-01         1.38\n3  10107 1990-05-01         1.43\n4  10107 1990-06-01         1.43\n5  10107 1990-07-01         1.45\n# … with 1,357 more rows\nplan(multisession, workers = availableCores())\nbeta_monthly <- crsp_monthly_nested |>\n  mutate(beta = future_map(\n    data, ~roll_capm_estimation(., months = 60, min_obs = 48))\n    ) |>\n  unnest(c(beta)) |>\n  select(permno, month, beta_monthly = beta) |>\n  drop_na()"},{"path":"beta-estimation.html","id":"estimating-beta-using-daily-returns","chapter":"5 Beta estimation","heading":"5.4 Estimating beta using daily returns","text":"provide descriptive statistics beta estimates, implement estimation daily CRSP sample well.\nDepending application, might either use longer horizon beta estimates based monthly data shorter horizon estimates based daily returns.First, load daily CRSP data.\nNote sample large compared monthly data, make sure enough memory available.also need daily Fama-French market excess returns.make sure keep relevant data save memory space.\nHowever, note machine might enough memory read whole daily CRSP sample. case, refer exercises try working loops chapter 2.Just like , nest data permno parallelization.estimation looks like couple examples using map().\ndaily data, use function take 3 months data require least 50 daily return observations months.\nrestrictions help us retrieve somehow smooth coefficient estimates.sake completeness, tell session use multiple workers parallelization.code chunk beta estimation using daily returns now looks similar one monthly data. whole estimation takes around 30 minutes using eight cores 32gb memory.","code":"\ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\") |>\n  collect()\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\") |>\n  collect()\ncrsp_daily <- crsp_daily |>\n  inner_join(factors_ff_daily, by = \"date\") |>\n  select(permno, month, ret_excess, mkt_excess)\ncrsp_daily_nested <- crsp_daily |>\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_daily_nested |>\n  inner_join(examples, by = \"permno\") |>\n  mutate(beta_daily = map(\n    data, \n    ~roll_capm_estimation(., months = 3, min_obs = 50))\n    ) |>\n  unnest(c(beta_daily)) |>\n  select(permno, month, beta_daily = beta) |>\n  drop_na()# A tibble: 1,543 × 3\n  permno month      beta_daily\n   <dbl> <date>          <dbl>\n1  10107 1986-05-01      0.898\n2  10107 1986-06-01      0.906\n3  10107 1986-07-01      0.822\n4  10107 1986-08-01      0.900\n5  10107 1986-09-01      1.01 \n# … with 1,538 more rows\nplan(multisession, workers = availableCores())\nbeta_daily <- crsp_daily_nested |>\n  mutate(beta_daily = future_map(\n    data, ~roll_capm_estimation(., months = 3, min_obs = 50))\n    ) |>\n  unnest(c(beta_daily)) |>\n  select(permno, month, beta_daily = beta) |>\n  drop_na()"},{"path":"beta-estimation.html","id":"comparing-beta-estimates","chapter":"5 Beta estimation","heading":"5.5 Comparing beta estimates","text":"typical value stock betas? get feeling, illustrate dispersion estimated \\(\\hat\\beta_i\\) across different industries across time . first figure shows typical business models across industries imply different exposure general market economy. However, barely firms exhibit negative exposure market factor.\nFIGURE 5.2: Box plots average firm-specific beta estimates industry.\nNext, illustrate time-variation cross-section estimated betas. figure shows monthly deciles estimated betas (based monthly data) indicates interesting pattern: First, betas seem vary time sense periods, clear trend across deciles. Second, sample exhibits periods dispersion across stocks increases sense lower decile decreases upper decile increases, indicates stocks correlation market increases others decreases. Note also : stocks negative betas extremely rare exception.\nFIGURE 5.3: Monthly deciles estimated betas.\ncompare difference daily monthly data, combine beta estimates single table. , use table plot comparison beta estimates example stocks.\nFIGURE 5.4: Comparison beta estimates using 5 years monthly 3 months daily data.\nestimates look expected. can see, really depends estimation window data frequency beta estimates turn .Finally, write estimates database can use later chapters.Whenever perform kind estimation, also makes sense rough plausibility tests. possible check plot share stocks beta estimates time.\ndescriptive helps us discover potential errors data preparation estimation procedure.\ninstance, suppose gap output betas.\ncase, go back check previous steps.\nFIGURE 5.5: End--year share securities book equity values exchange.\nfigure indicate troubles, let us move next check.also encourage everyone always look distributional summary statistics variables. can easily spot outliers weird distributions looking tables.Finally, since two different estimators theoretical object, expect estimators least positively correlated (although perfectly estimators based different sample periods).Indeed, find positive correlation beta estimates. subsequent chapters, mainly use estimates based monthly data readers able replicate due potential memory limitations might arise daily data.","code":"\ncrsp_monthly |>\n  left_join(beta_monthly, by = c(\"permno\", \"month\")) |>\n  drop_na(beta_monthly) |>\n  group_by(industry, permno) |>\n  summarize(beta = mean(beta_monthly)) |>\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Box plots of average firm-specific beta estimates by industry\"\n  )\nbeta_monthly |>\n  drop_na(beta_monthly) |>\n  group_by(month) |>\n  summarize(\n    x = quantile(beta_monthly, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1),\n    .groups = \"drop\"\n  ) |>\n  ggplot(aes(x = month, y = x, color = as_factor(quantile))) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Monthly deciles of estimated betas\",\n  )\nbeta <- beta_monthly |>\n  full_join(beta_daily, by = c(\"permno\", \"month\")) |>\n  arrange(permno, month)\n\nbeta |>\n  inner_join(examples, by = \"permno\") |>\n  pivot_longer(cols = c(beta_monthly, beta_daily)) |>\n  drop_na() |>\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Comparison of beta estimates using 5 years of monthly and 3 months of daily data\"\n  )\nbeta |>\n  dbWriteTable(tidy_finance, \n               \"beta\", \n               value = _, \n               overwrite = TRUE)\nbeta_long <- crsp_monthly |>\n  left_join(beta, by = c(\"permno\", \"month\")) |>\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long |>\n  group_by(month, name) |>\n  summarize(share = sum(!is.na(value)) / n()) |>\n  ggplot(aes(x = month, y = share, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\nbeta_long |>\n  select(name, value) |>\n  drop_na() |>\n  group_by(name) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q25 = quantile(value, 0.25),\n    q50 = quantile(value, 0.50),\n    q75 = quantile(value, 0.75),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  )# A tibble: 2 × 11\n  name          mean    sd   min    q05   q25   q50   q75   q95   max       n\n  <chr>        <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <int>\n1 beta_daily   0.743 0.925 -43.7 -0.452 0.203 0.679  1.22  2.22  56.6 3186174\n2 beta_monthly 1.10  0.711 -13.0  0.123 0.631 1.03   1.47  2.32  10.3 2071080\nbeta |>\n  select(beta_daily, beta_monthly) |>\n  cor(use = \"complete.obs\")             beta_daily beta_monthly\nbeta_daily        1.000        0.322\nbeta_monthly      0.322        1.000"},{"path":"beta-estimation.html","id":"exercises-3","chapter":"5 Beta estimation","heading":"5.6 Exercises","text":"Compute beta estimates based monthly data using 1, 3, 5 years data impose minimum number observations 10, 28, 48 months return data, respectively. strongly correlated estimated betas?Compute beta estimates based monthly data using 5 years data impose different numbers minimum observations. share permno-month observations successful beta estimates vary across different requirements? find high correlation across estimated betas?Instead using future_map(), perform beta estimation loop (using either monthly daily data) subset 100 permnos choice. Verify get results parallelized code .Filter stocks negative betas. stocks frequently exhibit negative betas, resemble estimation errors?","code":""},{"path":"univariate-portfolio-sorts.html","id":"univariate-portfolio-sorts","chapter":"6 Univariate portfolio sorts","heading":"6 Univariate portfolio sorts","text":"chapter, dive portfolio sorts, one widely used statistical methodologies empirical asset pricing (e.g., Bali, Engle, Murray 2016). key application portfolio sorts examine whether one variables can predict future excess returns. general, idea sort individual stocks portfolios, stocks within portfolio similar respect sorting variable, firm size. different portfolios represent well-diversified investments differ level sorting variable. can attribute differences return distribution impact sorting variable.\nstart introducing univariate portfolio sorts (sort based one characteristic). later chapter, tackle bivariate sorting.univariate portfolio sort considers one sorting variable \\(x_{t-1,}\\).\n, \\(\\) denotes stock \\(t-1\\) indicates characteristic observable investors time \\(t\\).\nobjective assess cross-sectional relation \\(x_{t-1,}\\) , typically, stock excess returns \\(r_{t,}\\) time \\(t\\) outcome variable.\nillustrate portfolio sorts work, use estimates market betas previous chapter sorting variable.current chapter relies following set packages.Compared previous chapters, introduce lmtest (Zeileis Hothorn 2002) inference estimated coefficients, sandwich (Zeileis 2006) different covariance matrix estimators.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(sandwich)"},{"path":"univariate-portfolio-sorts.html","id":"data-preparation","chapter":"6 Univariate portfolio sorts","heading":"6.1 Data preparation","text":"start loading required data SQLite-database introduced chapters 2-3. particular, use monthly CRSP sample asset universe.\nform portfolios, use Fama-French factor returns compute risk-adjusted performance (.e., alpha).\nbeta tibble market betas computed previous chapter.keep relevant data CRSP sample.","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") |>\n  collect()\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  select(permno, month, ret_excess, mkt_excess, mktcap_lag)\ncrsp_monthly# A tibble: 3,225,079 × 5\n  permno month      ret_excess mkt_excess mktcap_lag\n   <dbl> <date>          <dbl>      <dbl>      <dbl>\n1  10000 1986-02-01    -0.262      0.0713       16.1\n2  10000 1986-03-01     0.359      0.0488       12.0\n3  10000 1986-04-01    -0.104     -0.0131       16.3\n4  10000 1986-05-01    -0.228      0.0462       15.2\n5  10000 1986-06-01    -0.0102     0.0103       11.8\n# … with 3,225,074 more rows"},{"path":"univariate-portfolio-sorts.html","id":"sorting-by-market-beta","chapter":"6 Univariate portfolio sorts","heading":"6.2 Sorting by market beta","text":"Next, merge sorting variable return data. use one-month lagged betas sorting variable ensure sorts rely information available create portfolios.\nlag stock beta one month, add one month current date join resulting information return data.\nprocedure ensures month \\(t\\) information available month \\(t+1\\).\nmay tempted simply use call crsp_monthly |> group_by(permno) |> mutate(beta_lag = lag(beta))) instead.\nprocedure, however, work non-explicit missing values time series.first step conduct portfolio sorts calculate periodic breakpoints can use group stocks portfolios.\nsimplicity, start median single breakpoint.\ncompute value-weighted returns two resulting portfolios, means lagged market capitalization determines weight weighted.mean().","code":"\nbeta_lag <- beta |>\n  mutate(month = month %m+% months(1)) |>\n  select(permno, month, beta_lag = beta_monthly) |>\n  drop_na()\n\ndata_for_sorts <- crsp_monthly |>\n  inner_join(beta_lag, by = c(\"permno\", \"month\"))\nbeta_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(\n    breakpoint = median(beta_lag),\n    portfolio = case_when(\n      beta_lag <= breakpoint ~ \"low\",\n      beta_lag > breakpoint ~ \"high\"\n    )\n  ) |>\n  group_by(month, portfolio) |>\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\")"},{"path":"univariate-portfolio-sorts.html","id":"performance-evaluation","chapter":"6 Univariate portfolio sorts","heading":"6.3 Performance evaluation","text":"following figure shows monthly excess returns two portfolios.\nFIGURE 6.1: Monthly beta portfolio excess returns using median breakpoint.\ncan construct long-short strategy based two portfolios: buy high-beta portfolio , time, short low-beta portfolio. Thereby, overall position market net-zero, .e., need invest money realize strategy absence frictions.compute average return corresponding standard error test whether long-short portfolio yields average positive negative excess returns. asset pricing literature, one typically adjust autocorrelation using Newey West (1987) \\(t\\)-statistics test null hypothesis average portfolio excess returns equal zero. One necessary input Newey-West standard errors chosen bandwidth based number lags employed estimation. seems researchers often default choosing pre-specified lag length 6 months, instead recommend data-driven approach. automatic selection advocated Newey West (1994) available sandwich package. implement test, compute average return via lm() employ coeftest() function. want implement typical 6-lag default setting, can enforce passing arguments lag = 6, prewhite = FALSE coeftest() function code passes NeweyWest().results indicate reject null hypothesis average returns equal zero. portfolio strategy using median breakpoint hence yield abnormal returns. finding surprising reconsider CAPM? certainly . CAPM yields high beta stocks yield higher expected returns. portfolio sort implicitly mimics investment strategy finances high beta stocks shorting low beta stocks. Therefore, one expect average excess returns yield return risk-free rate.","code":"\nbeta_portfolios |>\n  ggplot(aes(x = month, y = ret, fill = portfolio)) +\n  geom_col() +\n  facet_wrap(~portfolio, ncol = 1) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Monthly beta portfolio excess returns using median as breakpoint\"\n  ) +\n  theme(legend.position = \"none\")\nbeta_longshort <- beta_portfolios |>\n  pivot_wider(month, names_from = portfolio, values_from = ret) |>\n  mutate(long_short = high - low) |>\n  left_join(factors_ff_monthly, by = \"month\")\nmodel_fit <- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest)\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 0.000164   0.001319    0.12      0.9"},{"path":"univariate-portfolio-sorts.html","id":"functional-programming-for-portfolio-sorts","chapter":"6 Univariate portfolio sorts","heading":"6.4 Functional programming for portfolio sorts","text":"Now take portfolio sorts next level. want able sort stocks arbitrary number portfolios. case, functional programming handy: employ curly-curly-operator give us flexibility concerning variable use sorting, denoted var. use quantile() compute breakpoints n_portfolios. , assign portfolios stocks using findInterval() function. output following function new column contains number portfolio stock belongs.can use function sort stocks ten portfolios month using lagged betas compute value-weighted returns portfolio. Note transform portfolio column factor variable provides convenience figure construction .","code":"\nassign_portfolio <- function(data, var, n_portfolios) {\n  breakpoints <- data |>\n    summarize(breakpoint = quantile({{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  data |>\n    mutate(portfolio = findInterval({{ var }},\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n}\nbeta_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(\n    portfolio = assign_portfolio(\n      data = cur_data(),\n      var = beta_lag,\n      n_portfolios = 10\n    ),\n    portfolio = as.factor(portfolio)\n  ) |>\n  group_by(portfolio, month) |>\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), \n            .groups = \"drop\")"},{"path":"univariate-portfolio-sorts.html","id":"more-performance-evaluation","chapter":"6 Univariate portfolio sorts","heading":"6.5 More performance evaluation","text":"next step, compute summary statistics beta portfolio. Namely, compute CAPM-adjusted alphas, beta beta portfolio, average returns.figure illustrates CAPM alphas beta-sorted portfolios. shows low beta portfolios tend exhibit positive alphas, high beta portfolios exhibit negative alphas.\nFIGURE 6.2: Alphas beta-sorted portfolios.\nresults suggest negative relation beta future stock returns, contradicts predictions CAPM. According CAPM, returns increase beta across portfolios risk-adjusted returns statistically indistinguishable zero.","code":"\nbeta_portfolios_summary <- beta_portfolios |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  group_by(portfolio) |>\n  summarize(\n    alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),\n    beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),\n    ret = mean(ret)\n  )\nbeta_portfolios_summary |>\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")"},{"path":"univariate-portfolio-sorts.html","id":"the-security-market-line-and-beta-portfolios","chapter":"6 Univariate portfolio sorts","heading":"6.6 The security market line and beta portfolios","text":"CAPM predicts portfolios lie security market line (SML). slope SML equal market risk premium reflects risk-return trade-given time.\nFIGURE 6.3: Average portfolio excess returns average beta estimates.\nprovide evidence CAPM predictions, form long-short strategy buys high-beta portfolio shorts low-beta portfolio., resulting long-short strategy exhibit statistically significant returns.However, long-short portfolio yields statistically significant negative CAPM-adjusted alpha, although, controlling effect beta, average excess stock returns zero according CAPM. results thus provide evidence support CAPM. negative value documented -called betting beta factor (Frazzini Pedersen 2014). Betting beta corresponds strategy shorts high beta stocks takes (levered) long position low beta stocks. borrowing constraints prevent investors taking positions SML instead incentivized buy high beta stocks, leads relatively higher price (therefore lower expected returns implied CAPM) high beta stocks. result, betting--beta strategy earns providing liquidity capital constraint investors lower risk aversion.plot shows annual returns extreme beta portfolios mainly interested . figure illustrates consistent striking patterns last years - portfolio exhibits periods positive negative annual returns.\nFIGURE 6.4: Annual returns beta portfolios.\nOverall, chapter shows functional programming can leveraged form arbitrary number portfolios using sorting variable evaluate performance resulting portfolios. next chapter, dive deeper many degrees freedom arise context portfolio analysis.","code":"\nsml_capm <- lm(ret ~ 1 + beta, data = beta_portfolios_summary)$coefficients\n\nbeta_portfolios_summary |>\n  ggplot(aes(x = beta, y = ret, color = portfolio)) +\n  geom_point() +\n  geom_abline(intercept = 0, \n              slope = mean(factors_ff_monthly$mkt_excess), \n              linetype = \"solid\") +\n  geom_abline(intercept = sml_capm[1], \n              slope = sml_capm[2], \n              linetype = \"dashed\") +\n  scale_y_continuous(labels = percent, \n                     limit = c(0, mean(factors_ff_monthly$mkt_excess) * 2)) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\nbeta_longshort <- beta_portfolios |>\n  ungroup() |>\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) |>\n  filter(portfolio %in% c(\"low\", \"high\")) |>\n  pivot_wider(month, names_from = portfolio, values_from = ret) |>\n  mutate(long_short = high - low) |>\n  left_join(factors_ff_monthly, by = \"month\")\ncoeftest(lm(long_short ~ 1, data = beta_longshort), \n         vcov = NeweyWest)\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.00212    0.00329    0.64     0.52\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort), \n         vcov = NeweyWest)\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.00448    0.00256   -1.75    0.081 .  \nmkt_excess   1.17706    0.09600   12.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nbeta_longshort |>\n  group_by(year = year(month)) |>\n  summarize(\n    low = prod(1 + low),\n    high = prod(1 + high),\n    long_short = prod(1 + long_short)\n  ) |>\n  pivot_longer(cols = -year) |>\n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )"},{"path":"univariate-portfolio-sorts.html","id":"exercises-4","chapter":"6 Univariate portfolio sorts","heading":"6.7 Exercises","text":"Take two long-short beta strategies based different numbers portfolios compare returns. significant difference returns? Sharpe ratios compare strategies? Find one additional portfolio evaluation statistic compute .plotted alphas ten beta portfolios . Write function tests estimates significance. portfolios significant alphas?analysis based betas daily returns. However, also computed betas monthly returns. Re-run analysis point differences results.Given results chapter, can define long-short strategy yields positive abnormal returns (.e., alphas)? Plot cumulative excess return strategy market excess return comparison.","code":""},{"path":"size-sorts-and-p-hacking.html","id":"size-sorts-and-p-hacking","chapter":"7 Size sorts and p-hacking","heading":"7 Size sorts and p-hacking","text":"chapter, continue portfolio sorts univariate setting. Yet, consider firm size sorting variable, gives rise well-known return factor: size premium. size premium arises buying small stocks selling large stocks. Prominently, Fama French (1993) include factor three-factor model. Apart , asset managers commonly include size key firm characteristic making investment decisions.also introduce new choices formation portfolios. particular, discuss listing exchanges, industries, weighting regimes, periods. choices matter portfolio returns result different size premiums (see Walter, Weber, Weiss 2022 insights decision nodes effect premiums). Exploiting ideas generate favorable results called p-hacking.\narguably thin line p-hacking conducting robustness tests, purpose simply illustrate substantial variation can arise along evidence generating process.chapter relies following set packages:Compared previous chapters, introduce rlang package (Henry Wickham 2022) advanced parsing functional expressions.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(furrr)\nlibrary(rlang)"},{"path":"size-sorts-and-p-hacking.html","id":"data-preparation-1","chapter":"7 Size sorts and p-hacking","heading":"7.1 Data preparation","text":"First, retrieve relevant data SQLite-database introduced chapter “Accessing & managing financial data”. Firm size defined market equity asset pricing applications retrieve CRSP. use Fama-French factor returns performance evaluation.","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()"},{"path":"size-sorts-and-p-hacking.html","id":"size-distribution","chapter":"7 Size sorts and p-hacking","heading":"7.2 Size distribution","text":"build size portfolios, investigate distribution variable firm size. Visualizing data valuable starting point understand input analysis. figure shows fraction total market capitalization concentrated largest firm. produce graph, create monthly indicators track whether stock belongs largest x% firms.\n, aggregate firms within bucket compute buckets’ share total market capitalization.figure shows largest 1% firms cover 50% total market capitalization, holding just 25% largest firms CRSP universe essentially replicates market portfolio. distribution firm size thus implies largest firms market dominate many small firms whenever use value-weighted benchmarks.\nFIGURE 7.1: Percentage total market capitalization largest stocks.\nNext, firm sizes also differ across listing exchanges. Stocks’ primary listings important past potentially still relevant today. graph shows New York Stock Exchange (NYSE) still largest listing exchange terms market capitalization. recently, NASDAQ gained relevance listing exchange. know small peak NASDAQ’s market cap around year 2000 ?\nFIGURE 7.2: Share total market capitalization per listing exchange.\nFinally, consider distribution firm size across listing exchanges create summary statistics. function summary() include statistics interested , create function create_summary() adds standard deviation number observations. , apply current month CRSP data listing exchange. also add row add_row() overall summary statistics.resulting table shows firms listed NYSE significantly larger average firms listed exchanges. Moreover, NASDAQ lists largest number firms. discrepancy firm sizes across listing exchanges motivated researchers form breakpoints exclusively NYSE sample apply breakpoints stocks. following, use distinction update portfolio sort procedure.","code":"\ncrsp_monthly |>\n  group_by(month) |>\n  mutate(\n    top01 = if_else(mktcap >= quantile(mktcap, 0.99), 1L, 0L),\n    top05 = if_else(mktcap >= quantile(mktcap, 0.95), 1L, 0L),\n    top10 = if_else(mktcap >= quantile(mktcap, 0.90), 1L, 0L),\n    top25 = if_else(mktcap >= quantile(mktcap, 0.75), 1L, 0L),\n    total_market_cap = sum(mktcap)\n  ) |>\n  summarize(\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap\n  ) |>\n  pivot_longer(cols = -month) |>\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |>\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\ncrsp_monthly |>\n  group_by(month, exchange) |>\n  summarize(mktcap = sum(mktcap)) |>\n  mutate(share = mktcap / sum(mktcap)) |>\n  ggplot(aes(x = month, y = share, fill = exchange, color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\ncreate_summary <- function(data, column_name) {\n  data |>\n    select(value = {{ column_name }}) |>\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q25 = quantile(value, 0.25),\n      q50 = quantile(value, 0.50),\n      q75 = quantile(value, 0.75),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |>\n  filter(month == max(month)) |>\n  group_by(exchange) |>\n  create_summary(mktcap) |>\n  add_row(crsp_monthly |>\n    filter(month == max(month)) |>\n    create_summary(mktcap) |>\n    mutate(exchange = \"Overall\"))# A tibble: 5 × 11\n  exchange   mean     sd      min    q05    q25    q50    q75    q95    max     n\n  <chr>     <dbl>  <dbl>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <int>\n1 AMEX       283.  1298.     6.04 1.01e1 3.07e1 6.59e1   158.   535. 1.51e4   147\n2 NASDAQ    8041. 74386.     4.65 2.73e1 1.34e2 4.85e2  2108. 19107. 2.23e6  2300\n3 NYSE     16427. 43130.     5.35 1.54e2 9.16e2 3.34e3 12024. 74922. 4.14e5  1244\n4 Other    10061.    NA  10061.   1.01e4 1.01e4 1.01e4 10061. 10061. 1.01e4     1\n5 Overall  10558. 63975.     4.65 3.10e1 1.85e2 8.72e2  4196. 37064. 2.23e6  3692"},{"path":"size-sorts-and-p-hacking.html","id":"univariate-size-portfolios-with-flexible-breakpoints","chapter":"7 Size sorts and p-hacking","heading":"7.3 Univariate size portfolios with flexible breakpoints","text":"previous chapter, construct portfolios varying number portfolios different sorting variables. , extend framework compute breakpoints subset data, instance, based selected listing exchanges. published asset pricing articles, many scholars compute sorting breakpoints NYSE-listed stocks. NYSE-specific breakpoints applied entire universe stocks.replicate NYSE-centered sorting procedure, introduce exchanges argument assign_portfolio() function. exchange-specific argument enters filter filter(exchange %% exchanges). example, exchanges = 'NYSE' specified, stocks NYSE used compute breakpoints. Alternatively, specify exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"), keeps stocks listed either exchanges. Overall, regular expressions powerful tool, touch specific case .","code":"\nassign_portfolio <- function(n_portfolios,\n                             exchanges,\n                             data) {\n  breakpoints <- data |>\n    filter(exchange %in% exchanges) |>\n    summarize(breakpoint = quantile(\n      mktcap_lag,\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  data |>\n    mutate(portfolio = findInterval(mktcap_lag, \n                                    breakpoints, all.inside = TRUE)) |>\n    pull(portfolio)\n}"},{"path":"size-sorts-and-p-hacking.html","id":"weighting-schemes-for-portfolios","chapter":"7 Size sorts and p-hacking","heading":"7.4 Weighting schemes for portfolios","text":"Apart computing breakpoints different samples, researchers often use different portfolio weighting schemes. far, weighted portfolio constituent relative market equity previous period. protocol called value-weighting. alternative protocol equal-weighting, assigns stock’s return weight, .e., simple average constituents’ returns. Notice equal-weighting difficult practice portfolio manager needs rebalance portfolio monthly value-weighting truly passive investment.implement two weighting schemes function compute_portfolio_returns() takes logical argument weight returns firm value. statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns value_weighted = TRUE. Additionally, long-short portfolio long smallest firms short largest firms, consistent research showing small firms outperform larger counterparts. Apart two changes, function similar procedure previous chapter.see function compute_portfolio_returns() works, consider simple median breakpoint example value-weighted returns. interested effect restricting listing exchanges estimation size premium. first function call, compute returns based breakpoints listing exchanges. , computed returns based breakpoints NYSE-listed stocks.table shows size premium 60% larger consider stocks NYSE form breakpoint month. NYSE-specific breakpoints larger, 50% stocks entire universe resulting small portfolio NYSE firms larger average. impact choice negligible.","code":"\ncompute_portfolio_returns <- function(n_portfolios = 10,\n                                      exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data |>\n    group_by(month) |>\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = cur_data()\n    )) |>\n    group_by(month, portfolio) |>\n    summarize(\n      ret = if_else(value_weighted, \n                    weighted.mean(ret_excess, mktcap_lag), \n                    mean(ret_excess)),\n      .groups = \"drop_last\"\n    ) |>\n    summarize(size_premium = ret[portfolio == min(portfolio)] - \n                ret[portfolio == max(portfolio)]) |>\n    summarize(size_premium = mean(size_premium))\n}\nret_all <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(Exchanges = c(\"NYSE, NASDAQ & AMEX\", \"NYSE\"), \n       Premium = as.numeric(c(ret_all, ret_nyse)) * 100)# A tibble: 2 × 2\n  Exchanges           Premium\n  <chr>                 <dbl>\n1 NYSE, NASDAQ & AMEX   0.110\n2 NYSE                  0.181"},{"path":"size-sorts-and-p-hacking.html","id":"p-hacking-and-non-standard-errors","chapter":"7 Size sorts and p-hacking","heading":"7.5 P-hacking and non-standard errors","text":"Since choice exchange significant impact, next step investigate effect data processing decisions researchers make along way. particular, portfolio sort analysis decide least number portfolios, listing exchanges form breakpoints, equal- value-weighting. , one may exclude firms active finance industry restrict analysis parts time series. variations choices discuss part scholarly articles published top finance journals. refer Walter, Weber, Weiss (2022) extensive set decision nodes discretion researchers.intention application show different ways form portfolios result different estimated size premiums. Despite effects multitude choices, correct way. also noted none procedures wrong, aim simply illustrate changes can arise due variation evidence-generating process (Menkveld et al. 2021).\nmalicious perspective, modeling choices give researcher multiple chances find statistically significant results. Yet considered p-hacking, renders statistical inference due multiple testing invalid (Harvey, Liu, Zhu 2016).Nevertheless, multitude options creates problem since single correct way sorting portfolios. researcher convince reader results come p-hacking exercise? circumvent dilemma, academics encouraged present evidence different sorting schemes robustness tests report multiple approaches show result depend single choice. Thus, robustness premiums key feature.conduct series robustness tests also interpreted p-hacking exercise. , examine size premium different specifications presented table p_hacking_setup. function expand_grid() produces table possible permutations arguments. Note use argument data exclude financial firms truncate time series.speed computation parallelize (many) different sorting procedures, Chapter 3. Finally, report resulting size premiums descending order. indeed substantial size premiums possible data, particular use equal-weighted portfolios.","code":"\np_hacking_setup <- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n  value_weighted = c(TRUE, FALSE),\n  data = parse_exprs(\n  'crsp_monthly; crsp_monthly |> filter(industry != \"Finance\");\n   crsp_monthly |> filter(month < \"1990-06-01\");\n   crsp_monthly |> filter(month >=\"1990-06-01\")')\n)\np_hacking_setup# A tibble: 48 × 4\n  n_portfolios exchanges value_weighted data      \n         <dbl> <list>    <lgl>          <list>    \n1            2 <chr [1]> TRUE           <sym>     \n2            2 <chr [1]> TRUE           <language>\n3            2 <chr [1]> TRUE           <language>\n4            2 <chr [1]> TRUE           <language>\n5            2 <chr [1]> FALSE          <sym>     \n# … with 43 more rows\nplan(multisession, workers = availableCores())\n\np_hacking_setup <- p_hacking_setup |>\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = eval_tidy(..4)\n    )\n  ))\n\np_hacking_results <- p_hacking_setup |>\n  mutate(data = map_chr(data, deparse)) |>\n  unnest(size_premium) |>\n  arrange(desc(size_premium))\np_hacking_results# A tibble: 48 × 5\n  n_portfolios exchanges value_weighted data                         size_premium\n         <dbl> <list>    <lgl>          <chr>                               <dbl>\n1           10 <chr [3]> FALSE          \"filter(crsp_monthly, month…       0.0184\n2           10 <chr [3]> FALSE          \"filter(crsp_monthly, indus…       0.0180\n3           10 <chr [3]> FALSE          \"crsp_monthly\"                     0.0162\n4           10 <chr [3]> FALSE          \"filter(crsp_monthly, month…       0.0139\n5           10 <chr [3]> TRUE           \"filter(crsp_monthly, indus…       0.0114\n# … with 43 more rows"},{"path":"size-sorts-and-p-hacking.html","id":"the-size-premium-variation","chapter":"7 Size sorts and p-hacking","heading":"7.6 The size-premium variation","text":"provide graph shows different premiums. plot also shows relation average Fama-French SMB (small minus big) premium used literature include dotted vertical line.\nFIGURE 7.3: Size premium different sorting choices. dotted vertical line indicates average Fama-French SMB permium\n","code":"\np_hacking_results |>\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Size premium over different sorting choices\",\n    subtitle = \"The dotted vertical line indicates the average Fama-French SMB permium\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb)),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)"},{"path":"size-sorts-and-p-hacking.html","id":"exercises-5","chapter":"7 Size sorts and p-hacking","heading":"7.7 Exercises","text":"gained several insights size distribution . However, analyze average size across exchanges industries. exchanges/industries largest firms? Plot average firm size three exchanges time. see?compute breakpoints take look exposition . might cover potential data errors. Plot breakpoints ten size portfolios time. , take difference two extreme portfolios plot . Describe results.returns analyse account differences exposure market risk, .e., CAPM beta. Change function compute_portfolio_returns() output CAPM alpha beta instead average excess return.saw spread returns p-hacking exercise, show choices led largest effects. Find way investigate choice variable largest impact estimated size premium.computed several size premiums, follow definition Fama French (1993). approaches comes closest SMB premium?","code":""},{"path":"value-and-bivariate-sorts.html","id":"value-and-bivariate-sorts","chapter":"8 Value and bivariate sorts","heading":"8 Value and bivariate sorts","text":"chapter extends univariate portfolio analysis bivariate sorts means assign stocks portfolios based two characteristics. Bivariate sorts regularly used academic asset pricing literature. Yet, scholars also use sorts three grouping variables. Conceptually, portfolio sorts easily applicable higher dimensions.form portfolios firm size book--market ratio. calculate book--market ratios, accounting data required necessitates additional steps portfolio formation. end, demonstrate form portfolios two sorting variables using -called independent dependent portfolio sorts.current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(sandwich)"},{"path":"value-and-bivariate-sorts.html","id":"data-preparation-2","chapter":"8 Value and bivariate sorts","heading":"8.1 Data preparation","text":"First, load necessary data SQLite-database introduced chapter “Accessing & managing financial data”. conduct portfolio sorts based CRSP sample keep necessary columns memory. use data sources firm size previous chapter., utilize accounting data. common source accounting data Compustat. need book equity data application, select database. Additionally, convert variable datadate monthly value, consider monthly returns need account exact date. achieve , use function floor_date().","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  select(permno, gvkey, month, ret_excess, mkt_excess, \n         mktcap, mktcap_lag, exchange) |>\n  drop_na()\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\n\nbe <- compustat |>\n  select(gvkey, datadate, be) |>\n  drop_na() |>\n  mutate(month = floor_date(ymd(datadate), \"month\"))"},{"path":"value-and-bivariate-sorts.html","id":"book-to-market-ratio","chapter":"8 Value and bivariate sorts","heading":"8.2 Book-to-market ratio","text":"fundamental problem handling accounting data look-ahead bias - must include data forming portfolio public knowledge time. course, researchers information looking past agents moment. However, abnormal excess returns trading strategy rely information advantage differential result informed agents’ trades. Hence, lag accounting information.continue lag market capitalization firm size one month. , compute book--market ratio, relates firm’s book equity market equity. Firms high (low) book--market called value (growth) firms. matching accounting market equity information month, lag book--market six months. sufficiently conservative approach accounting information usually released well six months pass. However, asset pricing literature, even longer lags used well.1Having variables, .e., firm size lagged one month book--market lagged six months, merge sorting variables returns using sorting_date-column created purpose. final step data preparation deals differences frequency variables. Returns firm size recorded monthly. Yet accounting information released annual basis. Hence, match book--market one month per year eleven empty observations. solve frequency issue, carry latest book--market ratio firm subsequent months, .e., fill missing observations current report. done via fill()-function sorting date firm (identify permno gvkey) firm basis (group_by() usual). last step, remove rows missing entries returns matched annual report.last step preparation portfolio sorts computation breakpoints. continue use function allowing specification exchanges use breakpoints. Additionally, reintroduce argument var function defining different sorting variables via curly-curly.data preparation steps, present bivariate portfolio sorts independent dependent basis.","code":"\nme <- crsp_monthly |>\n  mutate(sorting_date = month %m+% months(1)) |>\n  select(permno, sorting_date, me = mktcap)\n\nbm <- be |>\n  inner_join(crsp_monthly |>\n    select(month, permno, gvkey, mktcap), by = c(\"gvkey\", \"month\")) |>\n  mutate(\n    bm = be / mktcap,\n    sorting_date = month %m+% months(6)\n  ) |>\n  select(permno, gvkey, sorting_date, bm) |>\n  arrange(permno, gvkey, sorting_date)\n\ndata_for_sorts <- crsp_monthly |>\n  left_join(bm, by = c(\"permno\", \"gvkey\", \"month\" = \"sorting_date\")) |>\n  left_join(me, by = c(\"permno\", \"month\" = \"sorting_date\")) |>\n  select(permno, gvkey, month, ret_excess, mktcap_lag, me, bm, exchange)\n\ndata_for_sorts <- data_for_sorts |>\n  arrange(permno, gvkey, month) |>\n  group_by(permno, gvkey) |>\n  fill(bm) |>\n  drop_na()\nassign_portfolio <- function(data, var, n_portfolios, exchanges) {\n  breakpoints <- data |>\n    filter(exchange %in% exchanges) |>\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  data |>\n    mutate(portfolio = findInterval({{ var }}, \n                                    breakpoints, all.inside = TRUE)) |>\n    pull(portfolio)\n}"},{"path":"value-and-bivariate-sorts.html","id":"independent-sorts","chapter":"8 Value and bivariate sorts","heading":"8.3 Independent sorts","text":"Bivariate sorts create portfolios within two-dimensional space spanned two sorting variables. possible assess return impact either sorting variable return differential trading strategy invests portfolios either end respective variables spectrum. create five--five matrix using book--market firm size sorting variables example . end 25 portfolios. Since interested value premium (.e., return differential high low book--market firms), go long five portfolios highest book--market firms short five portfolios lowest book--market firms. five portfolios end due size splits employed alongside book--market splits.implement independent bivariate portfolio sort, assign monthly portfolios sorting variables separately create variables portfolio_bm portfolio_bm, respectively. , separate portfolios combined final sort stored portfolio_combined. assigning portfolios, compute average return within portfolio month. Additionally, keep book--market portfolio makes computation value premium easier. alternative disaggregate combined portfolio separate step. Notice weigh stocks within portfolio market capitalization, .e., decide value-weight returns.Equipped monthly portfolio returns, ready compute value premium. However, still decide invest five high five low book--market portfolios. common approach weigh portfolios equally, yet another researcher’s choice. , compute return differential high low book--market portfolios show average value premium.resulting annualized value premium 3.936 percent.","code":"\nvalue_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = str_c(portfolio_bm, portfolio_me)\n  ) |>\n  group_by(month, portfolio_combined) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\nvalue_premium <- value_portfolios |>\n  group_by(month, portfolio_bm) |>\n  summarize(ret = mean(ret), .groups = \"drop_last\") |>\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] - \n              ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)[1] 0.329"},{"path":"value-and-bivariate-sorts.html","id":"dependent-sorts","chapter":"8 Value and bivariate sorts","heading":"8.4 Dependent sorts","text":"previous exercise, assigned portfolios without considering second variable assignment. protocol called independent portfolio sorts. alternative, .e., dependent sorts, creates portfolios second sorting variable within bucket first sorting variable. example , sort firms five size buckets, within buckets, assign firms five book--market portfolios. Hence, monthly breakpoints specific size group. decision independent dependent portfolio sorts another choice researcher. Notice dependent sorts ensure equal amount stocks within portfolio.implement dependent sorts, first create size portfolios calling assign_portfolio() var = . , group data month size portfolio assigning book--market portfolio. rest implementation . Finally, compute value premium.value premium dependent sorts 3.18 percent per year.Overall, show conduct bivariate portfolio sorts chapter. one case, sort portfolios independently . Yet also discuss create dependent portfolio sorts. Along line previous chapter, see many choices researcher make implement portfolio sorts, bivariate sorts increase number choices.","code":"\nvalue_portfolios <- data_for_sorts |>\n  group_by(month) |>\n  mutate(portfolio_me = assign_portfolio(\n    data = cur_data(),\n    var = me,\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) |>\n  group_by(month, portfolio_me) |>\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = str_c(portfolio_bm, portfolio_me)\n  ) |>\n  group_by(month, portfolio_combined) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\n\nvalue_premium <- value_portfolios |>\n  group_by(month, portfolio_bm) |>\n  summarize(ret = mean(ret), .groups = \"drop_last\") |>\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] - \n              ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)[1] 0.265"},{"path":"value-and-bivariate-sorts.html","id":"exercises-6","chapter":"8 Value and bivariate sorts","heading":"8.5 Exercises","text":"previous chapter, examined distribution market equity. Repeat analysis book equity book--market ratio (alongside plot breakpoints, .e., deciles).investigate portfolios, focus returns exclusively. However, also interest understand characteristics portfolios. Write function compute average characteristics size book--market across 25 independently dependently sorted portfolios.size premium, also value premium constructed follow Fama French (1993). Implement p-hacking setup previous chapter find premium comes closest HML premium.","code":""},{"path":"replicating-fama-french-factors.html","id":"replicating-fama-french-factors","chapter":"9 Replicating Fama & French factors","heading":"9 Replicating Fama & French factors","text":"Fama French three-factor model (see Fama French 1993) cornerstone asset pricing. top market factor represented traditional CAPM beta, model includes size value factors. introduce factors previous chapter, definition remains . Size SMB factor (small-minus-big) long small firms short large firms. value factor HML (high-minus-low) long high book--market firms short low book--market counterparts. chapter, also want show main idea replicate significant factors.current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)"},{"path":"replicating-fama-french-factors.html","id":"databases","chapter":"9 Replicating Fama & French factors","heading":"9.1 Databases","text":"use CRSP Compustat data sources, need exactly variables compute size value factors way Fama French . Hence, nothing new load data SQLite-database introduced chapter “Accessing & managing financial data”.","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\n\ndata_ff <- crsp_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  select(\n    permno, gvkey, month, ret_excess, mkt_excess,\n    mktcap, mktcap_lag, exchange\n  ) |>\n  drop_na()\n\nbe <- compustat |>\n  select(gvkey, datadate, be) |>\n  drop_na()"},{"path":"replicating-fama-french-factors.html","id":"data-preparation-3","chapter":"9 Replicating Fama & French factors","heading":"9.2 Data preparation","text":"Yet start merging data set computing premiums, differences previous chapter. First, Fama French form portfolios June year \\(t\\), whereby returns July first monthly return respective portfolio. firm size, consequently use market capitalization recorded June. held constant June year \\(t+1\\).Second, Fama French also different protocol computing book--market ratio. use market equity end year \\(t - 1\\) book equity reported year \\(t-1\\), .e., datadate within last year. Hence, book--market ratio can based accounting information 18 months old. Market equity also necessarily reflect time point book equity.implement time lags, employ temporary sorting_date-column. Notice combine information, want single observation per year stock since interested computing breakpoints held constant entire year. ensure call distinct() end chunk .","code":"\nme_ff <- data_ff |>\n  filter(month(month) == 6) |>\n  mutate(sorting_date = month %m+% months(1)) |>\n  select(permno, sorting_date, me_ff = mktcap)\n\nme_ff_dec <- data_ff |>\n  filter(month(month) == 12) |>\n  mutate(sorting_date = ymd(str_c(year(month) + 1, \"0701)\"))) |>\n  select(permno, gvkey, sorting_date, bm_me = mktcap)\n\nbm_ff <- be |>\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |>\n  select(gvkey, sorting_date, bm_be = be) |>\n  drop_na() |>\n  inner_join(me_ff_dec, by = c(\"gvkey\", \"sorting_date\")) |>\n  mutate(bm_ff = bm_be / bm_me) |>\n  select(permno, sorting_date, bm_ff)\n\nvariables_ff <- me_ff |>\n  inner_join(bm_ff, by = c(\"permno\", \"sorting_date\")) |>\n  drop_na() |>\n  distinct(permno, sorting_date, .keep_all = TRUE)"},{"path":"replicating-fama-french-factors.html","id":"portfolio-sorts","chapter":"9 Replicating Fama & French factors","heading":"9.3 Portfolio sorts","text":"Next, construct portfolios adjusted assign_portfolio() function. Fama French rely NYSE-specific breakpoints, form two portfolios size dimension median three portfolios dimension book--market 30%- 70%-percentiles, use independent sorts. sorts book--market require adjustment previous function seq() produce produce right breakpoints. Instead n_portfolios, now specify percentiles, take breakpoint-sequence object specified function’s call. Specifically, give percentiles = c(0, 0.3, 0.7, 1) function. Additionally, perform inner_join() return data ensure use traded stocks computing breakpoints first step.Next, merge portfolios return data rest year. implement step, create new column sorting_date return data setting date sort July \\(t-1\\) month June (year \\(t\\)) earlier July year \\(t\\) month July later.","code":"\nassign_portfolio <- function(data, var, percentiles) {\n  breakpoints <- data |>\n    filter(exchange == \"NYSE\") |>\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = {{ percentiles }},\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  data |>\n    mutate(portfolio = findInterval({{ var }},\n                                    breakpoints, all.inside = TRUE)) |>\n    pull(portfolio)\n}\n\nportfolios_ff <- variables_ff |>\n  inner_join(data_ff, by = c(\"permno\" = \"permno\", \"sorting_date\" = \"month\")) |>\n  group_by(sorting_date) |>\n  mutate(\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me_ff,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm_ff,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) |>\n  select(permno, sorting_date, portfolio_me, portfolio_bm)\nportfolios_ff <- data_ff |>\n  mutate(sorting_date = case_when(\n    month(month) <= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) >= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |>\n  inner_join(portfolios_ff, by = c(\"permno\", \"sorting_date\"))"},{"path":"replicating-fama-french-factors.html","id":"fama-and-french-factor-returns","chapter":"9 Replicating Fama & French factors","heading":"9.4 Fama and French factor returns","text":"Equipped return data assigned portfolios, can now compute value-weighted average return six portfolios. , form Fama French factors. size factor (.e., SMB), go long three small portfolios short three large portfolios taking average across either group. value factor (.e., HML), go long two high book--market portfolios short two low book--market portfolios, weighting equally.","code":"\nfactors_ff_monthly_replicated <- portfolios_ff |>\n  mutate(portfolio = str_c(portfolio_me, portfolio_bm)) |>\n  group_by(portfolio, month) |>\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\",\n    portfolio_me = unique(portfolio_me),\n    portfolio_bm = unique(portfolio_bm)\n  ) |>\n  group_by(month) |>\n  summarize(\n    smb_replicated = mean(ret[portfolio_me == 1]) - \n      mean(ret[portfolio_me == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) - \n      mean(ret[portfolio_bm == 1])\n  )"},{"path":"replicating-fama-french-factors.html","id":"replication-evaluation","chapter":"9 Replicating Fama & French factors","heading":"9.5 Replication evaluation","text":"previous section, replicated size value premiums following procedure outlined Fama French. However, follow procedure strictly. final question : close get? answer question looking two time-series estimates regression analysis using lm(). good job, see non-significant intercept (rejecting notion systematic error), coefficient close 1 (indicating high correlation), adjusted R-squared close 1 (indicating high proportion explained variance).results SMB factor quite convincing three criteria outlined met coefficient R-squared 99%.replication HML factor also success, although slightly lower level coefficient R-squared around 95%.evidence hence allows us conclude relatively good job replicating original Fama-French premiums, although see underlying code.\nperspective, perfect match possible additional information maintainers original data.","code":"\ntest <- factors_ff_monthly |>\n  inner_join(factors_ff_monthly_replicated, by = \"month\") |>\n  mutate(\n    smb_replicated = round(smb_replicated, 4),\n    hml_replicated = round(hml_replicated, 4)\n  )\nsummary(lm(smb ~ smb_replicated, data = test))\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020320 -0.001501  0.000027  0.001519  0.014615 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.000143   0.000133   -1.07     0.28    \nsmb_replicated  0.996413   0.004418  225.55   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00355 on 712 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.09e+04 on 1 and 712 DF,  p-value: <2e-16\nsummary(lm(hml ~ hml_replicated, data = test))\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.022250 -0.002933 -0.000101  0.002366  0.027475 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.000294   0.000214    1.38     0.17    \nhml_replicated 0.958849   0.007376  130.00   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0057 on 712 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.96 \nF-statistic: 1.69e+04 on 1 and 712 DF,  p-value: <2e-16"},{"path":"fama-macbeth-regressions.html","id":"fama-macbeth-regressions","chapter":"10 Fama-MacBeth regressions","heading":"10 Fama-MacBeth regressions","text":"regression approach Fama MacBeth (1973) widely used empirical asset pricing studies.\nResearchers use two-stage regression approach estimate risk premiums various markets, predominately stock market.\nEssentially, two-step Fama-MacBeth regressions exploit linear relationship expected returns exposure (priced) risk factors.\nbasic idea regression approach project asset returns factor exposures characteristics resemble exposure risk factor cross-section time period.\n, second step, estimates aggregated across time test risk factor priced.\nprinciple, Fama-MacBeth regressions can used way portfolio sorts introduced previous chapters.\nchapter, present simple implementation Fama MacBeth (1973) introduce concept regressions. use individual stocks test assets estimate risk premium associated three factors included Fama French (1993).Fama-MacBeth procedure simple two-step approach:\nfirst step uses exposures (characteristics) explanatory variables \\(T\\) cross-sectional regressions, .e.,\n\\[\\begin{aligned}r_{,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{,t} + \\lambda^{HML}_t \\beta^{HML}_{,t} + \\epsilon_{,t}\\text{, t}.\\end{aligned}\\]\n, interested compensation \\(\\lambda^{f}_t\\) exposure risk factor \\(\\beta^{f}_{,t}\\) time point, .e., risk premium. Note terminology: \\(\\beta^{f}_{,t}\\) asset-specific characteristic, e.g., factor exposure accounting variable. linear relationship expected returns characteristic given month, expect regression coefficient reflect relationship, .e., \\(\\lambda_t^{f}\\neq0\\).second step, time-series average \\(\\frac{1}{T}\\sum\\limits_{t=1}^T \\hat\\lambda^{f}_t\\) estimates \\(\\hat\\lambda^{f}_t\\) can interpreted risk premium specific risk factor \\(f\\). follow Zaffaroni Zhou (2022) consider standard cross-sectional regression predict future returns. characteristics replaced time \\(t+1\\) variables, regression approach rather captures risk attributes.move implementation, want highlight characteristics, e.g., \\(\\hat\\beta^{f}_{}\\), typically estimated separate step applying actual Fama-MacBeth methodology. can think step 0. might thus worry errors \\(\\hat\\beta^{f}_{}\\) impact risk premiums’ standard errors. Measurement error \\(\\hat\\beta^{f}_{}\\) indeed affects risk premium estimates, .e., lead biased estimates. literature provides adjustments bias H.-Y. Chen, Lee, Lee (2015) also shows bias goes zero \\(T \\\\infty\\). refer Gagliardini, Ossola, Scaillet (2016) -depth discussion also covering case time-varying betas. Moreover, plan use Fama-MacBeth regressions individual stocks: Hou, Xue, Zhang (2020) advocates using weighed-least squares estimate coefficients biased towards small firms. Without adjustment, high number small firms drive coefficient estimates.current chapter relies set packages.Compared previous chapters, introduce broom package (Robinson, Hayes, Couch 2022) tidy estimation output many estimated linear models.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(broom)"},{"path":"fama-macbeth-regressions.html","id":"data-preparation-4","chapter":"10 Fama-MacBeth regressions","heading":"10.1 Data preparation","text":"illustrate Fama MacBeth (1973) monthly CRSP sample use three characteristics explain cross-section returns: market capitalization, book--market ratio, CAPM beta (.e., covariance excess stock returns market excess returns). collect data SQLite-database introduced chapter “Accessing & managing financial data”.use Compustat CRSP data compute book--market ratio (log) market capitalization.\nFurthermore, also use CAPM betas based daily returns computed previous chapters.","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") |>\n  collect()\nbm <- compustat |>\n  mutate(month = floor_date(ymd(datadate), \"month\")) |>\n  left_join(crsp_monthly, by = c(\"gvkey\", \"month\")) |>\n  left_join(beta, by = c(\"permno\", \"month\")) |>\n  transmute(gvkey,\n            bm = be / mktcap,\n            log_mktcap = log(mktcap),\n            beta = beta_monthly,\n            sorting_date = month %m+% months(6))\n\ndata_fama_macbeth <- crsp_monthly |>\n  left_join(bm, by = c(\"gvkey\", \"month\" = \"sorting_date\")) |>\n  group_by(permno) |>\n  arrange(month) |>\n  fill(c(beta, bm, log_mktcap), .direction = \"down\") |>\n  ungroup() |>\n  left_join(crsp_monthly |>\n              select(permno, month, ret_excess_lead = ret) |>\n              mutate(month = month %m-% months(1)),\n            by = c(\"permno\", \"month\")) |>\n  select(permno, month, ret_excess_lead, beta, log_mktcap, bm) |>\n  drop_na()"},{"path":"fama-macbeth-regressions.html","id":"cross-sectional-regression","chapter":"10 Fama-MacBeth regressions","heading":"10.2 Cross-sectional regression","text":"Next, run cross-sectional regressions characteristics explanatory variables month.\nregress returns test assets particular time point asset’s characteristics.\n, get estimate risk premiums \\(\\hat\\lambda^{F_f}_t\\) point time.","code":"\nrisk_premiums <- data_fama_macbeth |>\n  nest(data = c(ret_excess_lead, beta, log_mktcap, bm, permno)) |> \n  mutate(estimates = map(\n    data,\n    ~tidy(lm(ret_excess_lead ~ beta + log_mktcap + bm, data = .x)))) |> \n  unnest(estimates)"},{"path":"fama-macbeth-regressions.html","id":"time-series-aggregation","chapter":"10 Fama-MacBeth regressions","heading":"10.3 Time-series aggregation","text":"Now risk premiums’ estimates period, can average across time-series dimension get expected risk premium characteristic. Similarly, manually create t-test statistics regressor, can compare usual critical values 1.96 2.576 two-tailed significance tests.common adjust autocorrelation reporting standard errors risk premiums. chapter 5, typical procedure computing Newey West (1987) standard errors. recommend data-driven approach Newey West (1994) using NeweyWest() function, note can enforce typical 6 lag settings via NeweyWest(., lag = 6, prewhite = FALSE).Finally, let us interpret results. Stocks higher book--market ratios earn higher expected future returns, line value premium. negative value log market capitalization reflects size premium smaller stocks. Lastly, negative value CAPM betas characteristics line well-established betting beta anomalies, indicating investors borrowing constraints tilt portfolio towards high beta stocks replicate levered market portfolio (Frazzini Pedersen 2014).","code":"\nprice_of_risk <- risk_premiums |>\n  group_by(factor = term) |>\n  summarise(\n    risk_premium = mean(estimate) * 100,\n    t_statistic = mean(estimate) / sd(estimate) * sqrt(n())\n  )\nregressions_for_newey_west <- risk_premiums |>\n  select(month, factor = term, estimate) |>\n  nest(data = c(month, estimate)) |>\n  mutate(\n    model = map(data, ~ lm(estimate ~ 1, .)),\n    mean = map(model, tidy)\n  )\n\nprice_of_risk_newey_west <- regressions_for_newey_west |>\n  mutate(newey_west_se = map_dbl(model, ~ sqrt(NeweyWest(.)))) |>\n  unnest(mean) |>\n  mutate(t_statistic_newey_west = estimate / newey_west_se) |>\n  select(factor,\n    risk_premium = estimate,\n    t_statistic_newey_west\n  )\n\nleft_join(price_of_risk,\n          price_of_risk_newey_west |> \n            select(factor, t_statistic_newey_west),\n          by = \"factor\")# A tibble: 4 × 4\n  factor      risk_premium t_statistic t_statistic_newey_west\n  <chr>              <dbl>       <dbl>                  <dbl>\n1 (Intercept)       1.69         6.74                   5.96 \n2 beta              0.0122       0.115                  0.103\n3 bm                0.126        2.68                   2.33 \n4 log_mktcap       -0.115       -3.27                  -2.93 "},{"path":"fama-macbeth-regressions.html","id":"exercises-7","chapter":"10 Fama-MacBeth regressions","heading":"10.4 Exercises","text":"Download sample test assets Kenneth French’s homepage reevaluate risk premiums industry portfolios instead individual stocks.Use individual stocks weighted-least squares based firm’s size suggested Hou, Xue, Zhang (2020). , repeat Fama-MacBeth regressions without weighting scheme adjustment drop smallest 20% firms month. Compare results three approaches.Implement rolling-window regression time-series estimation factor exposure. Skip one month rolling period including exposures cross-sectional regression avoid look-ahead bias. , adapt cross-sectional regression compute average risk premiums.","code":""},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"fixed-effects-and-clustered-standard-errors","chapter":"11 Fixed effects and clustered standard errors","heading":"11 Fixed effects and clustered standard errors","text":"working regressions empirical finance, sooner later confronted discussions around deal omitted variables bias dependence residuals.\nchapter, provide intuitive introduction two popular concepts fixed effects regressions clustered standard errors. focus classical panel regression common corporate finance literature (e.g., Fazzari et al. 1988; Erickson Whited 2012; Gulen Ion 2015): firm investment modeled function increases firm cash flow firm investment opportunities.Typically, investment regression uses quarterly balance sheet data provided via Compustat allows richer dynamics regressors opportunities construct variables. focus implementation fixed effects clustered standard errors, use annual Compustat data previous chapters leave estimation using quarterly data exercise. demonstrate regression based annual data yields qualitatively similar results estimations based quarterly data literature, namely confirming positive relationships investment two regressors.current chapter relies following set packages.Compared previous chapters, introduce fixest (Bergé 2018) fixed effects regressions, implementation standard error clusters, tidy estimation output.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(fixest)"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"data-preparation-5","chapter":"11 Fixed effects and clustered standard errors","heading":"11.1 Data preparation","text":"use CRSP annual Compustat data sources SQLite-database introduced chapter “Accessing & managing financial data”. particular, Compustat provides balance sheet income statement data firm level, CRSP provides market valuations.classical investment regressions model capital investment firm function operating cash flows Tobin’s q, measure firm’s investment opportunities. start constructing investment cash flows usually normalized lagged total assets firm. following code chunk, construct panel firm-year observations, cross-sectional information firms, well time-series information firm.Tobin’s q ratio market value capital replacement costs. one common regressors corporate finance applications Erickson Whited (2012). follow implementation Gulen Ion (2015) compute Tobin’s q market value equity (mktcap) plus book value assets () minus book value equity () plus deferred taxes (txdb), divided book value assets (). Finally, keep observations variables interest non-missing, report book value assets strictly positive.variable construction typically leads extreme values likely related data issues (e.g., reporting errors), many papers include winsorization variables interest. Winsorization involves replacing values extreme outliers quantiles respective end. following function implements winsorization percentage cut applied either end distributions.proceeding estimations, highly recommend tabulating summary statistics variables enter regression. simple tables allow check plausibility numerical variables, well spot obvious errors outliers. Additionally, panel data, plotting time series variable’s mean number observations useful exercise spot potential problems.","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\",\n                          extended_types = TRUE\n)\n\ncrsp <- tbl(tidy_finance, \"crsp_monthly\") |> \n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") |>\n  collect()\ndata_investment <- compustat |>\n  mutate(month = floor_date(datadate, \"month\")) |> \n  left_join(compustat |> \n              select(gvkey, year, at_lag = at) |> \n              mutate(year = year + 1), \n            by = c(\"gvkey\", \"year\")) |> \n  filter(at > 0, at_lag > 0) |> \n  mutate(investment = capx / at_lag,\n         cash_flows = oancf / at_lag)\n\ndata_investment <- data_investment |> \n  left_join(data_investment |> \n              select(gvkey, year, investment_lead = investment) |> \n              mutate(year = year - 1), \n            by = c(\"gvkey\", \"year\"))\ndata_investment <- data_investment |> \n    left_join(crsp |> \n              select(gvkey, month, mktcap), \n              by = c(\"gvkey\", \"month\")) |> \n  mutate(tobins_q = (mktcap + at - be + txdb) / at)\n\ndata_investment <- data_investment |> \n  select(gvkey, year, investment_lead, cash_flows, tobins_q) |> \n  drop_na() \nwinsorize <- function(x, cut) {\n  x <- replace(x,\n               x > quantile(x, 1 - cut, na.rm = T),\n               quantile(x, 1 - cut, na.rm = T))\n  x <- replace(x,\n               x < quantile(x, cut, na.rm = T),\n               quantile(x, cut, na.rm = T))\n  return(x)\n}\n\ndata_investment <- data_investment |> \n  mutate(across(c(investment_lead, cash_flows, tobins_q), \n                ~ winsorize(., 0.01)))\ndata_investment |>\n  pivot_longer(cols = c(investment_lead, cash_flows, tobins_q), \n               names_to = \"measure\") |> \n  group_by(measure) |>\n  summarize(mean = mean(value),\n            sd = sd(value),\n            min = min(value),\n            q05 = quantile(value, 0.05),\n            q25 = quantile(value, 0.25),\n            q50 = quantile(value, 0.50),\n            q75 = quantile(value, 0.75),\n            q95 = quantile(value, 0.95),\n            max = max(value),\n            n = n()) |>\n  ungroup()# A tibble: 3 × 11\n  measure         mean     sd    min      q05      q25    q50    q75   q95    max\n  <chr>          <dbl>  <dbl>  <dbl>    <dbl>    <dbl>  <dbl>  <dbl> <dbl>  <dbl>\n1 cash_flows    0.0167 0.261  -1.46  -4.44e-1 -0.00269 0.0653 0.132  0.273  0.481\n2 investment_l… 0.0591 0.0784  0      7.72e-4  0.0126  0.0339 0.0727 0.210  0.470\n3 tobins_q      1.98   1.66    0.571  7.91e-1  1.05    1.38   2.18   5.27  10.7  \n# … with 1 more variable: n <int>"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"fixed-effects","chapter":"11 Fixed effects and clustered standard errors","heading":"11.2 Fixed effects","text":"illustrate fixed effects regressions, use fixest package, computationally powerful flexible respect model specifications. start basic investment regression using simple model\n\\[ \\text{Investment}_{,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{,t}+\\beta_2\\text{Tobin's q}_{,t}+\\varepsilon_{,t},\\]\n\\(\\varepsilon_t\\) ..d. normally distributed across time firms. use feols()-function estimate simple model output structure regressions - also use lm().expected, regression output shows significant coefficients variables. Higher cash flows investment opportunities associated higher investment. However, simple model actually may lot omitted variables, coefficients likely biased. lot unexplained variation simple model (indicated rather low adjusted R-squared), bias coefficients potentially severe, true values zero. Note clear cutoffs decide R-squared high low, depends context application comparison different models data.One way tackle issue omitted variable bias get rid much unexplained variation possible including fixed effects - .e., model parameters fixed specific groups (e.g., Wooldridge 2010). essence, group mean fixed effects regressions. simplest group can form investment regression firm level. firm fixed effects regression \n\\[ \\text{Investment}_{,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{,t}+\\beta_2\\text{Tobin's q}_{,t}+\\varepsilon_{,t},\\]\n\\(\\alpha_i\\) firm-specific mean investment across years. fact, also compute firms’ investments deviations firms’ average investments estimate model without fixed effects. idea firm fixed effect remove firm’s average investment, might affected firm-specific variables observe. example, firms specific industry might invest average. observe young firm large investments small concurrent cash flows, happen years. sort variation unwanted related unobserved variables can bias estimates direction.include firm fixed effect, use gvkey (Compustat’s firm identifier) follows:regression output shows lot unexplained variation firm level taken care including firm fixed effect adjusted R-squared rises 50%. fact, interesting look within R-squared shows explanatory power firm’s cash flow Tobin’s q top average investment firm. can also see coefficients changed slightly magnitude sign.another source variation can get rid setting: average investment across firms might vary time due macroeconomic factors affect firms, economic crises. including year fixed effects, can take effect unobservables vary time. two-way fixed effects regression \n\\[ \\text{Investment}_{,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{,t}+\\beta_2\\text{Tobin's q}_{,t}+\\varepsilon_{,t},\\]\n\\(\\alpha_t\\) time fixed effect. can think higher investments economic expansion simultaneously high cash flows.inclusion time fixed effects marginally affect R-squared coefficients, can interpret good thing indicates coefficients driven omitted variable varies time.can improve robustness regression results? Ideally, want get rid unexplained variation firm-year level, means need include variables vary across firm time likely correlated investment. Note include firm-year fixed effects setting cash flows Tobin’s q colinear fixed effects, estimation becomes void.discuss properties estimation errors, want point regression tables heart every empirical analysis compare multiple models. Fortunately, etable() provides convenient way tabulate regression output (many parameters customize even print output LaTeX). recommend printing \\(t\\)-statistics rather standard errors regression tables latter typically hard interpret across coefficients vary size. also print p-values sometimes misinterpreted signal importance observed effects. \\(t\\) statistics provide consistent way interpret changes estimation uncertainty across different model specifications.","code":"\nmodel_ols <- feols(\n  fml = investment_lead ~ cash_flows + tobins_q,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_olsOLS estimation, Dep. Var.: investment_lead\nObservations: 121,241 \nStandard-errors: IID \n            Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)  0.04235   0.000350   120.9 < 2.2e-16 ***\ncash_flows   0.05345   0.000867    61.6 < 2.2e-16 ***\ntobins_q     0.00804   0.000136    59.1 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.076563   Adj. R2: 0.046221\nmodel_fe_firm <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmOLS estimation, Dep. Var.: investment_lead\nObservations: 121,241 \nFixed-effects: gvkey: 13,691\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(>|t|)    \ncash_flows   0.0152   0.000994    15.3 < 2.2e-16 ***\ntobins_q     0.0116   0.000140    82.9 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.050361     Adj. R2: 0.534796\n                 Within R2: 0.061302\nmodel_fe_firmyear <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmyearOLS estimation, Dep. Var.: investment_lead\nObservations: 121,241 \nFixed-effects: gvkey: 13,691,  year: 33\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(>|t|)    \ncash_flows   0.0189   0.000973    19.4 < 2.2e-16 ***\ntobins_q     0.0105   0.000139    75.4 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.049117     Adj. R2: 0.557364\n                 Within R2: 0.052752\netable(model_ols, model_fe_firm, model_fe_firmyear, \n       coefstat = \"tstat\")                        model_ols     model_fe_firm model_fe_firmyear\nDependent Var.:   investment_lead   investment_lead   investment_lead\n                                                                     \n(Intercept)     0.0423*** (120.9)                                    \ncash_flows      0.0534*** (61.62) 0.0152*** (15.31) 0.0189*** (19.43)\ntobins_q        0.0080*** (59.09) 0.0116*** (82.92) 0.0105*** (75.44)\nFixed-Effects:  ----------------- ----------------- -----------------\ngvkey                          No               Yes               Yes\nyear                           No                No               Yes\n_______________ _________________ _________________ _________________\nVCOV type                     IID               IID               IID\nObservations              121,241           121,241           121,241\nR2                        0.04624           0.58733           0.60747\nWithin R2                      --           0.06130           0.05275\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"clustering-standard-errors","chapter":"11 Fixed effects and clustered standard errors","heading":"11.3 Clustering standard errors","text":"Apart biased estimators, usually deal potentially complex dependencies residuals . dependencies residuals invalidate ..d. assumption OLS lead biased standard errors. biased OLS standard errors, reliably interpret statistical significance estimated coefficients.setting, residuals may correlated across years given firm (time-series dependence), , alternatively, residuals may correlated across different firms (cross-section dependence). One common approaches dealing dependence use clustered standard errors (Petersen 2008). idea behind clustering correlation residuals within cluster can form, .e., assume parametric form. number clusters grows, cluster-robust standard errors become consistent (Donald Lang 2007; Wooldridge 2010). natural requirement clustering standard errors practice hence sufficiently large number clusters. Typically, around least 30 50 clusters seen sufficient (Cameron, Gelbach, Miller 2011).Instead relying ..d. assumption, can use cluster option feols-function . code chunk applies one-way clustering firm, well two-way clustering firm year.table shows comparison different assumptions behind standard errors. first column, can see highly significant coefficients cash flows Tobin’s q. clustering standard errors firm level, \\(t\\)-statistics coefficients drop half, indicating high correlation residuals within firms. additionally cluster year, see drop, particularly Tobin’s q, . Even relaxing assumptions behind standard errors, coefficients still comfortably significant \\(t\\) statistics well usual critical values 1.96 2.576 two-tailed significance tests.Inspired Abadie et al. (2017), want close chapter highlighting choosing right dimensions clustering design problem. Even data informative whether clustering matters standard errors, tell whether adjust standard errors clustering. Clustering aggregate levels can hence lead unnecessarily inflated standard errors.","code":"\nmodel_cluster_firm <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = \"gvkey\",\n  data = data_investment\n)\n\nmodel_cluster_firmyear <- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = c(\"gvkey\", \"year\"),\n  data = data_investment\n)\netable(model_fe_firmyear, model_cluster_firm, model_cluster_firmyear, \n       coefstat = \"tstat\")                model_fe_firmyear model_cluster_f.. model_cluster_f..\nDependent Var.:   investment_lead   investment_lead   investment_lead\n                                                                     \ncash_flows      0.0189*** (19.43) 0.0189*** (11.31) 0.0189*** (9.599)\ntobins_q        0.0105*** (75.44) 0.0105*** (35.61) 0.0105*** (16.98)\nFixed-Effects:  ----------------- ----------------- -----------------\ngvkey                         Yes               Yes               Yes\nyear                          Yes               Yes               Yes\n_______________ _________________ _________________ _________________\nVCOV type                     IID         by: gvkey  by: gvkey & year\nObservations              121,241           121,241           121,241\nR2                        0.60747           0.60747           0.60747\nWithin R2                 0.05275           0.05275           0.05275\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"fixed-effects-and-clustered-standard-errors.html","id":"exercises-8","chapter":"11 Fixed effects and clustered standard errors","heading":"11.4 Exercises","text":"Estimate two-way fixed effects model two-way clustered standard errors using quarterly Compustat data WRDS. Note can access quarterly data via tbl(wrds, in_schema(\"comp\", \"fundq\")).Following H.Peters .Taylor (2017), compute Tobin’s q market value outstanding equity mktcap plus book value debt (dltt + dlc) minus current assets atc everything divided book value property, plant equipment ppegt. correlation measures Tobin’s q? impact two-way fixed effects regressions?","code":""},{"path":"factor-selection-via-machine-learning.html","id":"factor-selection-via-machine-learning","chapter":"12 Factor selection via machine learning","heading":"12 Factor selection via machine learning","text":"aim chapter twofold. data science perspective, introduce tidymodels, collection packages modeling machine learning (ML) using tidyverse principles. tidymodels comes handy workflow sorts typical prediction tasks. finance perspective, address factor zoo (John H. Cochrane 2011). previous chapters, illustrate stock characteristics size provide valuable pricing information addition market beta.\nfindings question usefulness Capital Asset Pricing Model.\nfact, last decades, financial economists “discovered” plethora additional factors may correlated marginal utility consumption (thus deserve prominent role pricing applications). search factors explain cross section expected stock returns produced hundreds potential candidates, noted recently Harvey, Liu, Zhu (2016), Mclean Pontiff (2016), Hou, Xue, Zhang (2020).\nTherefore, given multitude proposed risk factors, challenge days rather : believe relevance 300+ risk factors?. recent years, promising methods vast field machine learning (ML) got applied common finance applications. refer Mullainathan Spiess (2017) treatment ML perspective econometrician, Nagel (2021) excellent review ML practices asset pricing, Easley et al. (2020) ML applications (high-frequency) market microstructure, Dixon, Halperin, Bilokon (2020) detailed treatment methodological aspects.introduce Lasso Ridge regression special case penalized regression models. , explain concept cross-validation model tuning Elastic Net regularization popular example. implement showcase entire cycle model specification, training, forecast evaluation within tidymodels universe. tools can generally applied abundance interesting asset pricing problems, apply penalized regressions identifying macro-economic variables asset pricing factors help explain cross-section industry portfolios.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"brief-theoretical-background","chapter":"12 Factor selection via machine learning","heading":"12.1 Brief theoretical background","text":"book empirical work tidy manner, refer many excellent textbook treatments ML methods especially penalized regressions deeper discussion. Excellent material provided, instance, Hastie, Tibshirani, Friedman (2009), Gareth et al. (2013), De Prado (2018). Instead, briefly summarize idea Lasso Ridge regressions well general Elastic Net. , turn fascinating question implement, tune, use models tidymodels workflow.set stage, start definition linear model: suppose data \\((y_t, x_t), t = 1,\\ldots, T\\) \\(x_t\\) \\((K \\times 1)\\) vector regressors \\(y_t\\) response observation \\(t\\).\nlinear model takes form \\(y_t = \\beta' x_t + \\varepsilon_t\\) error term \\(\\varepsilon_t\\) studied abundance. well-known ordinary-least square (OLS) estimator \\((K \\times 1)\\) vector \\(\\beta\\) minimizes sum squared residuals \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\]\noften interested estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML predictive performance time. new observation \\(\\tilde{x}_t\\), linear model generates predictions \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\]\nbest can ?\nreally: Instead minimizing sum squared residuals, penalized linear models can improve predictive performance choosing estimators \\(\\hat{\\beta}\\) lower variance estimator \\(\\hat\\beta^\\text{ols}\\).\ntime, seems appealing restrict set regressors meaningful ones possible. words, \\(K\\) large (number proposed factors asset pricing literature), may desirable feature select reasonable factors set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) redundant factors.clear promised benefits penalized regressions (reducing mean squared error) come cost. cases, reducing variance estimator introduces bias \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). effect bias-variance trade-? understand implications, assume following data-generating process \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] properties \\(\\hat\\beta^\\text{ols}\\) unbiased estimator may desirable circumstances, certainly consider predictive accuracy. instance, mean-squared error (MSE) depends model choice follow: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(\\textbf{x}))^2)=E((f(\\textbf{x})+\\epsilon-\\hat{f}(\\textbf{x}))^2)\\\\\n&= \\underbrace{E((f(\\textbf{x})-\\hat{f}(\\textbf{x}))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(\\textbf{x})^2\\right)+E\\left(f(\\textbf{x})^2\\right)-2E\\left(f(\\textbf{x})\\hat{f}(\\textbf{x})\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(\\textbf{x})^2\\right)+f(\\textbf{x})^2-2f(\\textbf{x})E\\left(\\hat{f}(\\textbf{x})\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(\\textbf{x})\\right)}_{\\text{variance model}}+ \\underbrace{E\\left((f(\\textbf{x})-\\hat{f}(\\textbf{x}))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] model can reduce \\(\\sigma_\\varepsilon^2\\), biased estimator small variance may lower mean squared error unbiased estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"ridge-regression","chapter":"12 Factor selection via machine learning","heading":"12.1.1 Ridge regression","text":"One biased estimator known Ridge regression. Hoerl Kennard (1970) propose minimize sum squared errors simultaneously imposing penalty \\(L_2\\) norm parameters \\(\\hat\\beta\\). Formally, means penalty factor \\(\\lambda\\geq 0\\) minimization problem takes form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). , \\(X = \\left(x_1 \\ldots x_T\\right)'\\) \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). closed-form solution resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda \\right)^{-1}X'y.\\] couple observations worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) \\(\\lambda = 0\\) \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) \\(\\lambda\\rightarrow \\infty\\). Also \\(\\lambda > 0\\), \\(\\left(X'X + \\lambda \\right)\\) non-singular even \\(X'X\\) means \\(\\hat\\beta^\\text{ridge}\\) exists even \\(\\hat\\beta\\) defined. However, note also Ridge estimator requires careful choice hyperparameter \\(\\lambda\\) controls amount regularization: larger value \\(\\lambda\\) implies shrinkage regression coefficient towards 0, smaller value \\(\\lambda\\) reduces bias resulting estimator.Usually, \\(X\\) contains intercept column ones. general rule, associated intercept coefficient penalized. practice, often implies \\(y\\) simply demeaned computing \\(\\hat\\beta^\\text{ridge}\\).statistical properties Ridge estimator? First, bad news \\(\\hat\\beta^\\text{ridge}\\) biased estimator \\(\\beta\\). However, good news (homoscedastic error terms) variance Ridge estimator guaranteed smaller variance ordinary least square estimator. encourage verify two statements exercises. result, face trade-: Ridge regression sacrifices bias achieve smaller variance OLS estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"lasso","chapter":"12 Factor selection via machine learning","heading":"12.1.2 Lasso","text":"alternative Ridge regression Lasso (least absolute shrinkage selection operator). Similar Ridge regression, Lasso (Tibshirani 1996) penalized biased estimator.\nmain difference Ridge regression Lasso shrink coefficients effectively selects variables setting coefficients irrelevant variables zero. Lasso implements \\(L_1\\) penalization parameters : \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| < c.\\] closed form solution \\(\\hat\\beta^\\text{Lasso}\\) maximization problem efficient algorithms exist (e.g., R package glmnet). Like Ridge regression, hyperparameter \\(\\lambda\\) specified beforehand.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"elastic-net","chapter":"12 Factor selection via machine learning","heading":"12.1.3 Elastic Net","text":"Elastic Net (Zou Hastie 2005) combines \\(L_1\\) \\(L_2\\) penalization encourages grouping effect strongly correlated predictors tend model together. general framework considers following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, chose two hyperparameters: shrinkage factor \\(\\lambda\\) weighting parameter \\(\\rho\\). Elastic Net resembles Lasso \\(\\rho = 1\\) Ridge regression \\(\\rho = 0\\). R package glmnet provides efficient algorithms compute coefficients penalized regressions, good exercise implement Ridge Lasso estimation use glmnet package tidymodels back-end.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"data-preparation-6","chapter":"12 Factor selection via machine learning","heading":"12.2 Data preparation","text":"get started, load required packages data. main focus workflow behind amazing tidymodels package collection (Kuhn Wickham 2020).\nKuhn Silge (2018) provide thorough introduction tidymodels components. glmnet (Simon et al. 2011) developed released sync Tibshirani (1996) provides R implementation elastic net estimation. package timetk (Dancho Vaughan 2022b) provides useful tools time series data wrangling.analysis, use four different data sources load SQLite-database introduced chapter “Accessing & managing financial data”. start two different sets factor portfolio returns suggested representing practical risk factor exposure thus relevant comes asset pricing applications.standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, high-minus-low book--market valuation sorts) defined Fama French (1992) Fama French (1993)Monthly q-factor returns Hou, Xue, Zhang (2014). factors contain size factor, investment factor, return--equity factor, expected growth factorNext, include macroeconomic predictors may predict general stock market economy. Macroeconomic variables effectively serve conditioning information inclusion hints relevance conditional models instead unconditional asset pricing. refer interested reader John H. Cochrane (2009) role conditioning information.set macroeconomic predictors comes paper “Comprehensive Look Empirical Performance Equity Premium Prediction” (Welch Goyal 2008). data updated authors 2020 contains monthly variables suggested good predictors equity premium. variables Dividend Price Ratio, Earnings Price Ratio, Stock Variance, Net Equity Expansion, Treasury Bill rate, inflationFinally, need set test assets. aim understand plenty factors macroeconomic variable combinations prove helpful explaining test assets’ cross-section returns.line many existing papers, use monthly portfolio returns 10 different industries according definition Kenneth French’s homepage test assets.combine monthly observations one data frame.data contains 22 columns regressors 13 macro variables 8 factor returns month.\nfigure provides summary statistics 10 monthly industry excess returns percent.\nFIGURE 12.1: Industry excess return distribution. Boxplots indicate monthly dispersion returns 10 different industries\n","code":"\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(furrr)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(timetk)\ntidy_finance <- dbConnect(\n  SQLite(), \n  \"data/tidy_finance.sqlite\", \n  extended_types = TRUE\n)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect() |>\n  rename_with(~ str_c(\"factor_ff_\", .), -month)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\") |>\n  collect() |>\n  rename_with(~ str_c(\"factor_q_\", .), -month)\n\nmacro_predictors <- tbl(tidy_finance, \"macro_predictors\") |>\n  collect() |>\n  rename_with(~ str_c(\"macro_\", .), -month) |>\n  select(-macro_rp_div)\n\nindustries_ff_monthly <- tbl(tidy_finance, \"industries_ff_monthly\") |>\n  collect() |>\n  pivot_longer(-month,\n    names_to = \"industry\", values_to = \"ret\"\n  ) |>\n  mutate(industry = as_factor(industry))\ndata <- industries_ff_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  left_join(factors_q_monthly, by = \"month\") |>\n  left_join(macro_predictors, by = \"month\") |>\n  mutate(\n    ret = ret - factor_ff_rf\n  ) |>\n  select(month, industry, ret, everything()) |>\n  drop_na()\ndata |>\n  group_by(industry) |>\n  mutate(ret = 100 * ret) |>\n  ggplot(aes(x = industry, y = ret)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Industry excess return distribution (in %)\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"the-tidymodels-workflow","chapter":"12 Factor selection via machine learning","heading":"12.3 The tidymodels workflow","text":"illustrate penalized linear regressions, employ tidymodels collection packages modeling ML using tidyverse principles. can simply use install.packages(\"tidymodels\") get access related packages. recommend checking work Kuhn Silge (2018): continuously write great book ‘Tidy Modeling R’ using tidy principles.tidymodels workflow encompasses main stages modeling process: pre-processing data, model fitting, post-processing results. demonstrate , tidymodels provides efficient workflows can update low effort.Using ideas Ridge Lasso regressions, following example guides () pre-processing data (data split variable mutation), (ii) building models, (iii) fitting models, (iv) tuning models create “best” possible predictions.start, restrict analysis just one industry: Manufacturing. first split sample training test set.\npurpose, tidymodels provides function initial_time_split() rsample package (Silge et al. 2022).\nsplit takes last 20% data test set, used model tuning.\nuse test set evaluate predictive accuracy --sample scenario.object split simply keeps track observations training test set.\ncan call training set training(split), can extract test set testing(split).","code":"\nsplit <- initial_time_split(\n  data |>\n    filter(industry == \"Manuf\") |>\n    select(-industry),\n  prop = 4 / 5\n)\nsplit<Training/Testing/Total>\n<517/130/647>"},{"path":"factor-selection-via-machine-learning.html","id":"pre-process-data","chapter":"12 Factor selection via machine learning","heading":"12.3.1 Pre-process data","text":"Recipes help pre-process data training model. Recipes series pre-processing steps variable selection, transformation, conversion qualitative predictors indicator variables. recipe starts formula defines general structure dataset role variable (regressor dependent variable). dataset, recipe contains following steps fit model:formula defines want explain excess returns available predictors. regression equation thus takes form\n\\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t \\] \\(r_t\\) vector industry excess returns time \\(t\\) \\(f_t\\) \\(z_t\\) vectors factor portfolio returns macroeconomic variables.exclude column month analysis.include interaction terms factors macroeconomic predictors.demean scale regressor standard deviation one.table available recipe steps can found . 2022, 150 different processing steps available! One important point: definition recipe trigger calculations yet rather provides description tasks applied. result, easy reuse recipes different models thus make sure outcomes comparable based input.\nexample , make difference whether use input data = training(split) data = testing(split).\nmatters early stage column names types.can apply recipe data suitable structure. code combines two different functions: prep() estimates required parameters training set can applied data sets later. bake() applies processed computations new data.object data_prep contains information related different preprocessing steps applied training data: E.g., necessary compute sample means standard deviations center scale variables.Note resulting data contains 130 observations test set 126 columns. many? Recall recipe states compute every possible interaction term factors predictors, increases dimension data matrix substantially.may ask stage: use recipe instead simply using data wrangling commands mutate() select()? tidymodels beauty lot happening hood. Recall, simple scaling step, actually compute standard deviation column, store value, apply identical transformation different dataset, e.g., testing(split). prepped recipe stores values hands bake() novel dataset. Easy pie tidymodels, isn’t ?","code":"\nrec <- recipe(ret ~ ., data = training(split)) |>\n  step_rm(month) |>\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) |>\n  step_normalize(all_predictors()) |>\n  step_center(ret, skip = TRUE)\ndata_prep <- prep(rec, training(split))\ndata_bake <- bake(data_prep, \n                 new_data = testing(split))\ndata_bake# A tibble: 130 × 126\n  factor_ff_rf factor_ff_mkt_excess factor_ff_smb factor_ff_hml factor_q_me\n         <dbl>                <dbl>         <dbl>         <dbl>       <dbl>\n1        -1.92                0.644        0.298          0.947      0.371 \n2        -1.88                1.27         0.387          0.607      0.527 \n3        -1.88                0.341        1.43           0.836      1.12  \n4        -1.88               -1.80        -0.0411        -0.963     -0.0921\n5        -1.88               -1.29        -0.627         -1.73      -0.850 \n# … with 125 more rows, and 121 more variables: factor_q_ia <dbl>,\n#   factor_q_roe <dbl>, factor_q_eg <dbl>, macro_dp <dbl>, macro_dy <dbl>,\n#   macro_ep <dbl>, macro_de <dbl>, macro_svar <dbl>, macro_bm <dbl>,\n#   macro_ntis <dbl>, macro_tbl <dbl>, macro_lty <dbl>, macro_ltr <dbl>,\n#   macro_tms <dbl>, macro_dfy <dbl>, macro_infl <dbl>, ret <dbl>,\n#   factor_ff_rf_x_macro_dp <dbl>, factor_ff_rf_x_macro_dy <dbl>,\n#   factor_ff_rf_x_macro_ep <dbl>, factor_ff_rf_x_macro_de <dbl>, …"},{"path":"factor-selection-via-machine-learning.html","id":"build-a-model","chapter":"12 Factor selection via machine learning","heading":"12.3.2 Build a model","text":"\nNext, can build actual model based pre-processed data. line definition , estimate regression coefficients Lasso regression get\n\\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned}\\] want emphasize tidymodels workflow model similar, irrespective specific model. see , straightforward fit Ridge regression coefficients - later - Neural networks Random forests basically code. structure always follows: create -called workflow() use fit() function. table available model APIs available .\nnow, start linear regression model given value penalty factor \\(\\lambda\\). setup , mixture denotes value \\(\\rho\\), hence setting mixture = 1 implies Lasso.’s - done! object lm_model contains definition model required information. Note set_engine(\"glmnet\") indicates API character tidymodels workflow: hood, package glmnet heavy lifting, linear_reg() provides unified framework collect inputs. workflow ends combining everything necessary (serious) data science workflow, namely, recipe model.","code":"\nlm_model <- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) |>\n  set_engine(\"glmnet\", intercept = FALSE)\nlm_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(lm_model)\nlm_fit══ Workflow ═════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ─────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_interact()\n• step_normalize()\n• step_center()\n\n── Model ────────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-04\n  mixture = 1\n\nEngine-Specific Arguments:\n  intercept = FALSE\n\nComputational engine: glmnet "},{"path":"factor-selection-via-machine-learning.html","id":"fit-a-model","chapter":"12 Factor selection via machine learning","heading":"12.3.3 Fit a model","text":"workflow , ready use fit(). Typically, use training data fit model.\ntraining data pre-processed according recipe steps, Lasso regression coefficients computed.\nFirst, focus predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) figure illustrates projections entire time series Manufacturing industry portfolio returns. grey area indicates --sample period, use fit model.\nFIGURE 12.2: Monthly realized fitted manufacturing industry risk premia. grey area corresponds sample period.\nestimated coefficients look like? analyze values illustrate difference tidymodels workflow underlying glmnet package, worth computing coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. code estimates coefficients Lasso Ridge regression processed training data sample. Note glmnet actually takes vector y matrix regressors \\(X\\) input. Moreover, glmnet requires choosing penalty parameter \\(\\alpha\\), corresponds \\(\\rho\\) notation . using tidymodels model API, details need consideration.objects fit_lasso fit_ridge contain entire sequence estimated coefficients multiple values penalty factor \\(\\lambda\\). figure illustrates trajectories regression coefficients function penalty factor. Lasso Ridge coefficients converge zero penalty factor increases.\nFIGURE 12.3: Estimated coefficient paths Lasso Ridge regression function penalty parameter\nOne word caution: package glmnet computes estimates coefficients \\(\\hat\\beta\\) based numerical optimization procedures.\nresult, estimated coefficients special case regularization (\\(\\lambda = 0\\)) can deviate standard OLS estimates.","code":"\npredicted_values <- lm_fit |>\n  fit(data = training(split)) |>\n  predict(data |> filter(industry == \"Manuf\")) |>\n  bind_cols(data |> filter(industry == \"Manuf\")) |>\n  select(month,\n    \"Fitted value\" = .pred,\n    \"Realization\" = ret\n  ) |>\n  pivot_longer(-month, names_to = \"Variable\")\n\npredicted_values |>\n  ggplot(aes(x = month, y = value, color = Variable)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Monthly realized and fitted manufacturing industry risk premia\",\n    subtitle = \"The grey area corresponds to the out of sample period.\"\n  ) +\n  scale_x_date(\n    breaks = function(x) seq.Date(from = min(x), \n                                  to = max(x), \n                                  by = \"5 years\"),\n    minor_breaks = function(x) seq.Date(from = min(x), \n                                        to = max(x), \n                                        by = \"1 years\"),\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  annotate(\"rect\",\n    xmin = testing(split) |> pull(month) |> min(),\n    xmax = testing(split) |> pull(month) |> max(),\n    ymin = -Inf, ymax = Inf,\n    alpha = 0.5, fill = \"grey70\"\n  )\nx <- data_bake |>\n  select(-ret) |>\n  as.matrix()\ny <- data_bake |> pull(ret)\n\nfit_lasso <- glmnet(\n  x = x,\n  y = y,\n  alpha = 1,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge <- glmnet(\n  x = x,\n  y = y,\n  alpha = 0,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\nbind_rows(\n  tidy(fit_lasso) |> mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) |> mutate(Model = \"Ridge\")\n) |>\n  rename(\"Variable\" = term) |>\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Lambda\", y = NULL,\n    title = \"Estimated coefficients paths as a function of the penalty factor\"\n  ) +\n  theme(legend.position = \"none\")"},{"path":"factor-selection-via-machine-learning.html","id":"tune-a-model","chapter":"12 Factor selection via machine learning","heading":"12.3.4 Tune a model","text":"compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , simply imposed value penalty hyperparameter \\(\\lambda\\). Model tuning process optimally selecting hyperparameters. tidymodels provides extensive tuning options based -called cross-validation. , refer treatment cross-validation get detailed discussion statistical underpinnings. focus general idea implementation tidymodels.goal choosing \\(\\lambda\\) (hyperparameter, e.g., \\(\\rho\\) elastic net) find way produce predictors \\(\\hat{Y}\\) outcome \\(Y\\) minimizes mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, MSPE directly observable. can compute estimate data random observe entire population.Obviously, train algorithm data use compute error, estimate \\(\\hat{\\text{MSPE}}\\) indicate way better predictive accuracy can expect real --sample data. result called overfitting.Cross-validation technique allows us alleviate problem. approximate true MSPE average many mean squared prediction errors obtained creating predictions \\(K\\) new random samples data, none used train algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). practice, done carving piece data pretending independent sample. divide data training set test set. MSPE test set measure actual predictive ability, use training set fit models aim find optimal hyperparameter values. , divide training sample (several) subsets, fit model grid potential hyperparameter values (e.g., \\(\\lambda\\)), evaluate predictive accuracy independent sample. works follows:Specify grid hyperparameters.Obtain predictors \\(\\hat{y}_i(\\lambda)\\) denote predictors used parameters \\(\\lambda\\).Compute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] K-fold cross-validation, computation \\(K\\) times. Simply pick validation set \\(M=T/K\\) observations random think random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), \\(k=1\\).pick \\(K\\)? Large values \\(K\\) preferable training data better imitates original data. However, larger values \\(K\\) much higher computation time.\ntidymodels provides required tools conduct \\(K\\)-fold cross-validation. just update model specification let tidymodels know parameters tune. case, specify penalty factor \\(\\lambda\\) well mixing factor \\(\\rho\\) free parameters. Note simple change existing workflow update_model().sample, consider time-series cross-validation sample. means tune models 20 random samples length five years validation period four years. grid possible hyperparameters, fit model fold evaluate \\(\\hat{\\text{MSPE}}\\) corresponding validation set. Finally, select model specification lowest MSPE validation set. First, define cross-validation folds based training data ., evaluate performance grid different penalty values. tidymodels provides functionalities construct suitable grid hyperparameters grid_regular. code chunk creates \\(10 \\times 3\\) hyperparameters grid. , function tune_grid() evaluates models fold.tuning process, collect evaluation metrics (root mean-squared error example) identify optimal model. figure illustrates average validation set’s root mean-squared error value \\(\\lambda\\) \\(\\rho\\).figure shows cross-validated mean squared prediction error drops Lasso Elastic Net spikes afterward. Ridge regression, MSPE increases certain threshold. Recall larger regularization, restricted model becomes. Thus, choose model lowest MSPE, exhibits intermediate level regularization.","code":"\nlm_model <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit |>\n  update_model(lm_model)\ndata_folds <- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\n\ndata_folds# Time Series Cross Validation Plan \n# A tibble: 20 × 2\n  splits          id     \n  <list>          <chr>  \n1 <split [60/48]> Slice01\n2 <split [60/48]> Slice02\n3 <split [60/48]> Slice03\n4 <split [60/48]> Slice04\n5 <split [60/48]> Slice05\n# … with 15 more rows\nlm_tune <- lm_fit |>\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),\n    metrics = metric_set(rmse)\n  )\nautoplot(lm_tune) +\n  labs(\n    y = \"Root mean-squared prediction error\",\n    title = \"MSPE for Manufacturing excess returns\",\n    subtitle = \"Lasso (1.0), Ridge (0.0), and Elastic Net (0.5) with different levels of regularization.\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"parallelized-workflow","chapter":"12 Factor selection via machine learning","heading":"12.3.5 Parallelized workflow","text":"starting point question: factors determine industry returns? Avramov et al. (2022) provides Bayesian analysis related research question , choose simplified approach: illustrate entire workflow, now run penalized regressions ten industries.\nwant identify relevant variables fitting Lasso models industry returns time series. specifically, perform cross-validation industry identify optimal penalty factor \\(\\lambda\\).\n, use set finalize_*()-functions take list tibble tuning parameter values update objects values. determining best model, compute final fit entire training set analyze estimated coefficients.First, define Lasso model one tuning parameter.following task can easily parallelized reduce computing time substantially. use parallelization capabilities furrr. Note can also just recycle steps collect function.just happened? principle, exactly instead computing Lasso coefficients one industry, ten parallel. final option seed = TRUE required make cross-validation process reproducible.\nNow, just housekeeping keep variables Lasso set zero. illustrate results heat map.\nFIGURE 12.4: Selected factor macroeconomic predictors selected different industries\nheat map conveys two main insights.\nFirst, see lot white, means many factors, macroeconomic variables, interaction terms relevant explaining cross-section returns across industry portfolios. fact, market factor return--equity factor play role several industries. Second, seems quite heterogeneity across different industries. even market factor selected Lasso Utilities (means proposed model essentially just contains intercept), many factors selected , e.g., High-Tech Energy, coincide .\nwords, seems clear picture need many factors, Lasso provide conses across industries comes pricing abilities.","code":"\nlasso_model <- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit |>\n  update_model(lasso_model)\nselect_variables <- function(input) {\n  # Split into training and testing data\n  split <- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds <- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune <- lm_fit |>\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Finalizing: Identify the best model and fit with the training data\n  lasso_lowest_rmse <- lm_tune |> select_by_one_std_err(\"rmse\")\n  lasso_final <- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit <- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  lasso_final_fit |>\n    extract_fit_parsnip() |>\n    tidy() |>\n    mutate(\n      term = str_remove_all(term, \"factor_|macro_|industry_\")\n    )\n}\n\n# Parallelization\nplan(multisession, workers = availableCores())\n\n# Computation by industry\nselected_factors <- data |>\n  nest(data = -industry) |>\n  mutate(selected_variables = future_map(\n    data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\nselected_factors |>\n  unnest(selected_variables) |>\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) |>\n  add_count(term) |>\n  mutate(\n    term = str_remove_all(term, \"NA|ff_|q_\"),\n    term = str_replace_all(term, \"_x_\", \" \"),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) |>\n  filter(term != \"Other\") |>\n  mutate(term = fct_drop(term)) |>\n  complete(industry, term, fill = list(selected = 0)) |>\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"grey30\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"exercises-9","chapter":"12 Factor selection via machine learning","heading":"12.4 Exercises","text":"Write function requires three inputs, namely, y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), lambda returns Ridge estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function indicate whether \\(X\\) contains vector ones first column, exempt \\(L_2\\) penalty.Compute \\(L_2\\) norm (\\(\\beta'\\beta\\)) regression coefficients based predictive regression previous exercise range \\(\\lambda\\)’s illustrate effect penalization suitable figure.Now, write function requires three inputs, namely,y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), ’lambda` returns Lasso estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function indicate whether \\(X\\) contains vector ones first column, exempt \\(L_1\\) penalty.understand Ridge Lasso regressions , familiarize glmnet() package’s documentation. thoroughly tested well-established package provides efficient code compute penalized regression coefficients Ridge Lasso combinations, commonly called Elastic Nets.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"option-pricing-via-machine-learning","chapter":"13 Option pricing via machine learning","heading":"13 Option pricing via machine learning","text":" Machine learning (ML) seen part artificial intelligence.\nML algorithms build model based training data order make predictions decisions without explicitly programmed .\nML can specified along vast array different branches, chapter focuses -called supervised learning regressions. basic idea supervised learning algorithms build mathematical model data contains inputs desired outputs. chapter, apply well-known methods random forests neural networks simple application option pricing. specifically, going create artificial dataset option prices different values based Black-Scholes pricing equation call options. , train different models learn price call options without prior knowledge theoretical underpinnings famous option pricing equation Black Scholes (1973).roadmap chapter follows: First, briefly introduce regression trees, random forests, neural networks. focus implementation, leave thorough treatment statistical underpinnings textbooks authors real comparative advantage issues.\nshow implement random forests deep neural networks tidy principles using tidymodels tensorflow complicated network structures.order replicate analysis regarding neural networks chapter, install TensorFlow system, requires administrator rights machine. Parts can done within R. Just follow quick-start instructions.Throughout chapter, need following packages.package tidymodels (Kuhn Wickham 2020) provides tidyverse’s take machine learning opinionated collection packages provide consistent workflow common machine learning tasks. keras (Allaire Chollet 2022) high-level neural networks API developed focus enabling fast experimentation Tensorflow.\npackage ranger (Wright Ziegler 2017) provides fast implementation random forests hardhat (Vaughan Kuhn 2022) helper function robust data preprocessing fit time prediction time.","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(keras)\nlibrary(hardhat)\nlibrary(ranger)"},{"path":"option-pricing-via-machine-learning.html","id":"regression-trees-and-random-forests","chapter":"13 Option pricing via machine learning","heading":"13.1 Regression trees and random forests","text":"Regression trees popular ML approach incorporating multiway predictor interactions. Finance, regression trees used increasingly, also context asset pricing, see, e.g. Bryzgalova, Pelger, Zhu (2022).\nTrees fully nonparametric possess logic departs markedly traditional regressions. Trees designed find groups observations behave similarly . tree “grows” sequence steps. step, new “branch” sorts data leftover preceding step bins based one predictor variables. sequential branching slices space predictors rectangular partitions approximates unknown function \\(f(x)\\) average value outcome variable within partition. thourough treatment regression trees, refer Coqueret Guida (2020).Formally, partition predictor space \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). predictor \\(x\\) falls within region \\(R_j\\) estimate \\(f(x)\\) average training observations, \\(\\hat y_i\\), associated predictor \\(x_i\\) also \\(R_j\\). select partition \\(\\mathbf{x}\\) split order create new partitions, find predictor \\(j\\) value \\(s\\) define two new partitions, called \\(R_1(j,s)\\) \\(R_2(j,s)\\), split observations current partition asking \\(x_j\\) bigger \\(s\\):\n\\[R_1(j,s) = \\{\\mathbf{x} \\mid x_j < s\\} \\mbox{   } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}.\\]\npick \\(j\\) \\(s\\), find pair minimizes residual sum square (RSS):\n\\[\\sum_{:\\, x_i \\R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{:\\, x_i \\R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\\]\nNote: Unlike case sample variance, scale number elements \\(R_k(j, s)\\)! chapter penalized regressions, first relevant question : hyperparameter decisions? Instead regularization parameter, trees fully determined number branches used generate partition (sometimes one specifies minimum number observations final branch instead maximum number branches).Models single tree suffer high variance. Random forests address shortcomings decision trees. goal improve predictive performance reduce instability averaging multiple decision trees (forest trees constructed randomness). forest basically implies creating many regression trees averaging predictions. assure individual trees , use bootstrap induce randomness. specifically, build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using training sample. purpose, randomly select features included building tree. observation test set form prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{=1}^B\\hat{y}_{T_i}\\).","code":""},{"path":"option-pricing-via-machine-learning.html","id":"neural-networks","chapter":"13 Option pricing via machine learning","heading":"13.2 Neural networks","text":"Roughly speaking, neural networks propagate information input layer, one multiple hidden layers, output layer. number units (neurons) input layer equal dimension predictors, output layer usually consists one neuron (regression) multiple neurons classification. output layer predicts future data, similar fitted value regression analysis. Neural networks theoretical underpinnings “universal approximators” smooth predictive association (Hornik 1991). complexity, however, ranks neural networks among least transparent, least interpretable, highly parameterized ML tools.\nFinance, applications neural networks can found context many different applications, e.g. Avramov, Cheng, Metzker (2022), Gu, Kelly, Xiu (2020) L. Chen, Pelger, Zhu (2019).neuron applies nonlinear “activation function” \\(f\\) aggregated signal \nsending output next layer\n\\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\]\neasiest case \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions sigmoid (.e., \\(f(x) = (1+e^{-x})^{-1}\\)) ReLu (.e., \\(f(x) = max(x, 0)\\)).Neural networks gain flexibility chaining multiple layers together. Naturally, imposes many degrees freedom network architecture clear theoretical guidance exists. specification neural network requires, minimum, stance depth (number hidden layers), activation function, number neurons, connection structure units (dense sparse), application regularization techniques avoid overfitting. Finally, learning means choose optimal parameters relying numerical optimization, often requires specifying appropriate learning rate.Despite computational challenges, implementation R tedious can use API tensorflow.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"option-pricing","chapter":"13 Option pricing via machine learning","heading":"13.3 Option pricing","text":"apply ML methods relevant field finance, focus option pricing. application core taken Hull (2020). basic form, call options give owner right obligation buy specific stock (underlying) specific price (strike price \\(K\\)) specific date (exercise date \\(T\\)). Black–Scholes price (Black Scholes 1973) call option non-dividend-paying underlying stock given \n\\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right)\n\\end{aligned}\n\\]\n\\(C(S, T)\\) price option function today’s stock price underlying, \\(S\\), time maturity \\(T\\), \\(r_f\\) risk-free interest rate, \\(\\sigma\\) volatility underlying stock return. \\(\\Phi\\) cumulative distribution function standard normal random variable.Black-Scholes equation provides way compute arbitrage-free price call option parameters \\(S, K, r_f, T\\), \\(\\sigma\\) specified (arguably, parameters easy specify except \\(\\sigma\\) estimated). simple R function allows computing price .","code":"\nblack_scholes_price <- function(S = 50, K = 70, r = 0, T = 1, sigma = 0.2) {\n  d1 <- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  value <- S * pnorm(d1) - K * exp(-r * T) * pnorm(d1 - sigma * sqrt(T))\n  return(value)\n}"},{"path":"option-pricing-via-machine-learning.html","id":"learning-black-scholes","chapter":"13 Option pricing via machine learning","heading":"13.4 Learning Black-Scholes","text":"illustrate concept ML showing ML methods learn Black-Scholes equation observing different specifications corresponding prices without us revealing exact pricing equation.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"data-simulation","chapter":"13 Option pricing via machine learning","heading":"13.4.1 Data simulation","text":"end, start simulated data. compute option prices call options grid different combinations times maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), current stock prices (S). code , add idiosyncratic error term observation prices considered exactly reflect values implied Black-Scholes equation.code generates 1.5 million random parameter constellations. values, two observed prices reflecting Black-Scholes prices given random innovation term pollutes observed prices.Next, split data training set (contains 1% observed option prices) test set used final evaluation. Note entire grid possible combinations contains 3148992 different specifications. Thus, sample learn Black-Scholes price contains 31489 observations therefore relatively small.\norder keep analysis reproducible, use set.seed(). random seed specifies start point computer generates random number sequence ensures simulated data across different machines.process training dataset fit different ML models. define recipe defines processing steps purpose. specific case, want explain observed price five variables enter Black-Scholes equation. true price (stored column black_scholes) obviously used fit model. recipe also reflects standardize predictors ensure variable exhibits sample average zero sample standard deviation one.","code":"\noption_prices <- expand_grid(\n  S = 40:60, # Stock price\n  K = 20:90, # Strike price\n  r = seq(from = 0, to = 0.05, by = 0.01), # Risk-free rate\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12), # Time to maturity\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1) # Volatility\n) |>\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map(\n      black_scholes,\n      function(x) x + rnorm(2, sd = 0.15)\n    )\n  ) |>\n  unnest(observed_price)\nset.seed(420)\nsplit <- initial_split(option_prices, prop = 1 / 100)\nrec <- recipe(observed_price ~ .,\n  data = option_prices\n) |>\n  step_rm(black_scholes) |>\n  step_normalize(all_predictors())"},{"path":"option-pricing-via-machine-learning.html","id":"single-layer-networks-and-random-forests","chapter":"13 Option pricing via machine learning","heading":"13.4.2 Single layer networks and random forests","text":"Next, show fit neural network data. Note requires keras installed local machine. function mlp() package parsnip provides functionality initialize single layer, feed-forward neural network. specification defines single layer feed-forward neural network 10 hidden units. set number training iterations epochs = 500. option set_mode(\"regression\") specifies linear activation function output layer.verbose=0 argument prevents logging results. can follow straightforward tidymodel workflow chapter : Define workflow, equip recipe, specify associated model. Finally, fit model training data.familiar tidymodel workflow, piece cake fit models parsnip family.\ninstance, model initializes random forest 50 trees contained ensemble require least 2000 observations node.\nrandom forests trained using package ranger, required installed order run code (loading package necessary, however, provides transparent overview required packages session).Fitting model follows exactly convention neural network .","code":"\nnnet_model <- mlp(\n  epochs = 500,\n  hidden_units = 10,\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"keras\", verbose = FALSE)\nnn_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(nnet_model) |>\n  fit(data = training(split))\nrf_model <- rand_forest(\n  trees = 50,\n  min_n = 2000\n) |>\n  set_engine(\"ranger\") |>\n  set_mode(\"regression\")\nrf_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(rf_model) |>\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning.html","id":"deep-neural-networks","chapter":"13 Option pricing via machine learning","heading":"13.4.3 Deep neural networks","text":"Note tidymodels workflow extremely convenient, sophisticated deep neural networks supported yet (June 2022). reason, code snippet illustrates initialize sequential model three hidden layers 10 units per layer. keras package provides convenient interface flexible enough handle different activation functions. compile() command defines loss function model predictions evaluated.train neural network, provide inputs (x) variable predict (y) fit parameters. Note slightly tedious use method extract_mold(nn_fit). Instead simply using raw data, fit neural network processed data used single-layer feed-forward network. difference simply calling x = training(data) |> select(-observed_price, -black_scholes)? Recall recipe standardizes variables columns unit standard deviation zero mean. , adds consistency ensure models trained using recipe change recipe reflected performance model. final note potentially irritating observation: Note fit() alters keras model - one instances function R alters input function call object model anymore!","code":"\nmodel <- keras_model_sequential() |>\n  layer_dense(\n    input_shape = 5,\n    units = 10,\n    activation = \"sigmoid\"\n  ) |>\n  layer_dense(units = 10, activation = \"sigmoid\") |>\n  layer_dense(units = 10, activation = \"sigmoid\") |>\n  layer_dense(units = 1, activation = \"linear\") |>\n  compile(\n    loss = \"mean_squared_error\"\n  )\nmodelModel: \"sequential_1\"\n_________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #      \n=================================================================================\n dense_5 (Dense)                    (None, 10)                      60           \n dense_4 (Dense)                    (None, 10)                      110          \n dense_3 (Dense)                    (None, 10)                      110          \n dense_2 (Dense)                    (None, 1)                       11           \n=================================================================================\nTotal params: 291\nTrainable params: 291\nNon-trainable params: 0\n_________________________________________________________________________________\nmodel |>\n  fit(\n    x = extract_mold(nn_fit)$predictors |> as.matrix(),\n    y = extract_mold(nn_fit)$outcomes |> pull(observed_price),\n    epochs = 500, verbose = FALSE\n  )"},{"path":"option-pricing-via-machine-learning.html","id":"universal-approximation","chapter":"13 Option pricing via machine learning","heading":"13.4.4 Universal approximation","text":"evaluate results, implement one model: principle, non-linear function can also approximated linear model containing input variables’ polynomial expansions. illustrate , first define new recipe, rec_linear, processes training data even . include polynomials fifth degree predictor add possible pairwise interaction terms. final recipe step, step_lincomb(), removes potentially redundant variables (instance, interaction \\(r^2\\) \\(r^3\\) term \\(r^5\\)). fit Lasso regression model pre-specified penalty term (consult chapter factor selection tune model hyperparameters).","code":"\nrec_linear <- rec |>\n  step_poly(all_predictors(),\n    degree = 5,\n    options = list(raw = T)\n  ) |>\n  step_interact(terms = ~ all_predictors():all_predictors()) |>\n  step_lincomb(all_predictors())\n\nlm_model <- linear_reg(penalty = 0.01) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- workflow() |>\n  add_recipe(rec_linear) |>\n  add_model(lm_model) |>\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning.html","id":"prediction-evaluation","chapter":"13 Option pricing via machine learning","heading":"13.5 Prediction evaluation","text":"Finally, collect predictions compare --sample prediction error evaluated ten thousand new data points. Note evaluation, use call extract_mold() ensure use pre-processing steps testing data across model. also use somewhat advanced functionality forge(), provides easy, consistent, robust pre-processor prediction time.lines , use fitted models generate predictions entire test data set option prices. evaluate absolute pricing error one possible measure pricing accuracy, defined absolute value difference predicted option price theoretical correct option price Black-Scholes model.\nFIGURE 13.1: Absolut prediction error USD different fitted methods. prediction error evaluated sample call options used training.\nresults can summarized follow: ) ML methods seem able price call options observing training test set. ii) average prediction errors increase far -money options. ii) Random forest Lasso seem perform consistently worse prediction option prices Neural networks. iii) complexity deep neural network relative single layer neural network result better --sample predictions.","code":"\nout_of_sample_data <- testing(split) |>\n  slice_sample(n = 10000)\n\npredictive_performance <- model |>\n  predict(forge(\n    out_of_sample_data,\n    extract_mold(nn_fit)$blueprint\n  )$predictors |> as.matrix()) |>\n  as.vector() |>\n  tibble(\"Deep NN\" = _) |>\n  bind_cols(nn_fit |>\n    predict(out_of_sample_data)) |>\n  rename(\"Single layer\" = .pred) |>\n  bind_cols(lm_fit |> predict(out_of_sample_data)) |>\n  rename(\"Lasso\" = .pred) |>\n  bind_cols(rf_fit |> predict(out_of_sample_data)) |>\n  rename(\"Random forest\" = .pred) |>\n  bind_cols(out_of_sample_data) |>\n  pivot_longer(\"Deep NN\":\"Random forest\", names_to = \"Model\") |>\n  mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\npredictive_performance |>\n  ggplot(aes(x = moneyness, y = pricing_error, color = Model)) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE, method = \"gam\") +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Absolut prediction error (USD)\",\n    title = \"Prediction errors: Call option prices\"\n  )"},{"path":"option-pricing-via-machine-learning.html","id":"exercises-10","chapter":"13 Option pricing via machine learning","heading":"13.6 Exercises","text":"Write function takes y matrix predictors X inputs returns characterization relevant parameters regression tree 1 branch.Create function creates predictions new matrix predictors newX based estimated regression tree.Use package rpart grow tree based training data use illustration tools rpart understand characteristics tree deems relevant option pricing.Make use training test set choose optimal depth (number sample splits) tree.Use ‘keras’ initialize sequential neural network can take predictors training dataset input, contains least one hidden layer, generates continuous predictions. sounds harder : see simple regression example . many parameters neural network aim fit ?Next, compile object. important specify loss function. Illustrate difference predictive accuracy different architecture choices.","code":""},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies","chapter":"14 Parametric portfolio policies","heading":"14 Parametric portfolio policies","text":"chapter, introduce different portfolio performance measures evaluate compare portfolio allocation strategies.\npurpose, introduce direct way estimate optimal portfolio weights large-scale cross-sectional applications. precisely, approach Brandt, Santa-Clara, Valkanov (2009) proposes parametrize optimal portfolio weights function stock characteristics directly, instead estimating stock’s expected return, variance, covariances stocks prior step.\nchose weights function characteristics maximize expected utility investor. approach feasible large portfolio dimensions (entire CRSP universe) proposed Brandt, Santa-Clara, Valkanov (2009). review paper Brandt (2010) provides excellent treatment related portfolio choice methods.current chapter relies following set packages:","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)"},{"path":"parametric-portfolio-policies.html","id":"data-preparation-7","chapter":"14 Parametric portfolio policies","heading":"14.1 Data preparation","text":"get started, load monthly CRSP file, forms investment universe. load data SQLite-database introduced chapter 2.evaluate performance portfolios, use monthly market returns benchmark compute CAPM alphas.Next, retrieve stock characteristics shown effect expected returns expected variances (even higher moments) return distribution. particular, record lagged one-year return momentum (momentum_lag), defined compounded return months \\(t − 12\\) \\(t − 2\\) firm. finance, momentum empirically observed tendency rising asset prices rise , falling prices keep falling (Jegadeesh Titman 1993). second characteristic firm’s market equity (size_lag), defined log price per share times number shares outstanding (Banz 1981).\nconstruct correct lagged values, use approach introduced chapter “Accessing & managing financial data”.","code":"\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\ncrsp_monthly_lags <- crsp_monthly |>\n  transmute(permno,\n    month_12 = month %m+% months(12),\n    mktcap\n  )\n\ncrsp_monthly <- crsp_monthly |>\n  inner_join(crsp_monthly_lags,\n    by = c(\"permno\", \"month\" = \"month_12\"),\n    suffix = c(\"\", \"_12\")\n  )\n\ndata_portfolios <- crsp_monthly |>\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_12,\n    size_lag = log(mktcap_lag)\n  ) |>\n  drop_na(contains(\"lag\"))"},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies-1","chapter":"14 Parametric portfolio policies","heading":"14.2 Parametric portfolio policies","text":"basic idea parametric portfolio weights follows. Suppose date \\(t\\) \\(N_t\\) stocks investment universe, stock \\(\\) return \\(r_{, t+1}\\) associated vector firm characteristics \\(x_{, t}\\) time-series momentum market capitalization. investor’s problem choose portfolio weights \\(w_{,t}\\) maximize expected utility portfolio return:\n\\[\\begin{aligned}\n\\max_{w} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{=1}^{N_t}w_{,t}r_{,t+1}\\right)\\right]\n\\end{aligned}\\]\n\\(u(\\cdot)\\) denotes utility function.stock characteristics show ? parameterize optimal portfolio weights function stock characteristic \\(x_{,t}\\) following linear specification portfolio weights:\n\\[w_{,t} = \\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t},\\]\n\\(\\bar{w}_{,t}\\) stock’s weight benchmark portfolio (use value-weighted naive portfolio application ), \\(\\theta\\) vector coefficients going estimate, \\(\\hat{x}_{,t}\\) characteristics stock \\(\\), cross-sectionally standardized zero mean unit standard deviation.Intuitively, portfolio strategy form active portfolio management relative performance benchmark. Deviations benchmark portfolio derived individual stock characteristics. Note construction weights sum one \\(\\sum_{=1}^{N_t}\\hat{x}_{,t} = 0\\) due standardization. Moreover, coefficients constant across assets time. implicit assumption characteristics fully capture aspects joint distribution returns relevant forming optimal portfolios.first implement cross-sectional standardization entire CRSP universe. also keep track (lagged) relative market capitalization relative_mktcap, represent value-weighted benchmark portfolio, n denotes number traded assets \\(N_t\\), use construct naive portfolio benchmark.","code":"\ndata_portfolios <- data_portfolios |>\n  group_by(month) |>\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) |>\n  ungroup() |>\n  select(-mktcap_lag, -altprc)"},{"path":"parametric-portfolio-policies.html","id":"computing-portfolio-weights","chapter":"14 Parametric portfolio policies","heading":"14.3 Computing portfolio weights","text":"Next, move identify optimal choices \\(\\theta\\). rewrite optimization problem together weight parametrization can estimate \\(\\theta\\) maximize objective function based sample\n\\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{=1}^{N_t}\\left(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\right)r_{,t+1}\\right).\n\\end{aligned}\\]\nallocation strategy straightforward number parameters estimate small. Instead tedious specification \\(N_t\\) dimensional vector expected returns \\(N_t(N_t+1)/2\\) free elements covariance matrix, need focus application vector \\(\\theta\\). \\(\\theta\\) contains two elements application - relative deviation benchmark due size momentum.get feeling performance allocation strategy, start arbitrary initial vector \\(\\theta_0\\). next step choose \\(\\theta\\) optimally maximize objective function. automatically detect number parameters counting number columns lagged values.function compute_portfolio_weights() computes portfolio weights \\(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\) according parametrization given value \\(\\theta_0\\). Everything happens within single pipeline, hence provide short walk .first compute characteristic_tilt, tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{, t}\\) resemble deviation benchmark portfolio. Next, compute benchmark portfolio weight_benchmark, can reasonable set portfolio weights. case, choose either value equal-weighted allocation.\nweight_tilt completes picture contains final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt deviate benchmark portfolio depending stock characteristics.final lines go bit implement simple version -short sale constraint. generally straightforward ensure portfolio weight constraints via parameterization, simply normalize portfolio weights enforced positive. Finally, make sure normalized weights sum one . \\[w_{,t}^+ = \\frac{\\max(0, w_{,t})}{\\sum\\limits_{j=1}^{N_t}\\max(0, w_{,t})}.\\]following function computes optimal portfolio weights way just described.next step compute portfolio weights arbitrary vector \\(\\theta_0\\). example , use value-weighted portfolio benchmark allow negative portfolio weights.","code":"\nn_parameters <- sum(str_detect(\n  colnames(data_portfolios), \"lag\"\n))\n\ntheta <- rep(1.5, n_parameters)\n\nnames(theta) <- colnames(data_portfolios)[str_detect(\n  colnames(data_portfolios), \"lag\"\n)]\ncompute_portfolio_weights <- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data |>\n    group_by(month) |>\n    bind_cols(\n      characteristic_tilt = data |>\n        transmute(across(contains(\"lag\"), ~ . / n)) |>\n        as.matrix() %*% theta |> as.numeric()\n    ) |>\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) |>\n    ungroup()\n}\nweights_crsp <- compute_portfolio_weights(theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)"},{"path":"parametric-portfolio-policies.html","id":"portfolio-performance","chapter":"14 Parametric portfolio policies","heading":"14.4 Portfolio performance","text":"computed weights optimal way? likely , picked \\(\\theta_0\\) arbitrarily. evaluate performance allocation strategy, one can think many different approaches. original paper, Brandt, Santa-Clara, Valkanov (2009) focus simple evaluation hypothetical utility agent equipped power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^\\gamma}{1-\\gamma}\\), \\(\\gamma\\) risk aversion factor.noted, Gehrig, Sögner, Westerkamp (2020) warn leading case constant relative risk aversion (CRRA) strong assumptions properties returns, variables used implement parametric portfolio policy, \nparameter space necessary obtain well defined optimization problem.doubt, many ways evaluate portfolio. function provides summary kinds interesting measures can considered relevant. need evaluation measures? depends: original paper Brandt, Santa-Clara, Valkanov (2009) cares expected utility choose \\(\\theta\\). However, want choose optimal values achieve highest performance putting constraints portfolio weights, helpful everything one function.\nLet us take look different portfolio strategies evaluation measures.value-weighted portfolio delivers annualized return 6 percent clearly outperforms tilted portfolio, irrespective whether evaluate expected utility, Sharpe ratio CAPM alpha. can conclude market beta close one strategies (naturally almost identically 1 value-weighted benchmark portfolio). comes distribution portfolio weights, see benchmark portfolio weight takes less extreme positions (lower average absolute weights lower maximum weight). definition, value-weighted benchmark take negative positions, tilted portfolio also takes short positions.","code":"\npower_utility <- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\nevaluate_portfolio <- function(weights_crsp,\n                               full_evaluation = TRUE) {\n  evaluation <- weights_crsp |>\n    group_by(month) |>\n    summarize(\n      return_tilt = weighted.mean(ret_excess, weight_tilt),\n      return_benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) |>\n    pivot_longer(-month,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) |>\n    group_by(model) |>\n    left_join(factors_ff_monthly, by = \"month\") |>\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(12 * portfolio_return),\n      \"SD return\" = 100 * sqrt(12) * sd(portfolio_return),\n      \"Sharpe ratio\" = mean(portfolio_return) / sd(portfolio_return),\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n    )) |>\n    mutate(model = str_remove(model, \"return_\")) |>\n    pivot_longer(-model, names_to = \"measure\") |>\n    pivot_wider(names_from = model, values_from = value)\n\n  if (full_evaluation) {\n    weight_evaluation <- weights_crsp |>\n      select(month, contains(\"weight\")) |>\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") |>\n      group_by(model, month) |>\n      transmute(tibble(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight < 0]),\n        \"Avg. fraction of negative weights\" = sum(weight < 0) / n()\n      )) |>\n      group_by(model) |>\n      summarize(across(-month, ~ 100 * mean(.))) |>\n      mutate(model = str_remove(model, \"weight_\")) |>\n      pivot_longer(-model, names_to = \"measure\") |>\n      pivot_wider(names_from = model, values_from = value)\n    evaluation <- bind_rows(evaluation, weight_evaluation)\n  }\n  return(evaluation)\n}\nevaluate_portfolio(weights_crsp) |>\n  print(n = Inf)# A tibble: 11 × 3\n   measure                            benchmark     tilt\n   <chr>                                  <dbl>    <dbl>\n 1 Expected utility                  -0.249     -0.262  \n 2 Average return                     6.86      -0.604  \n 3 SD return                         15.3       21.0    \n 4 Sharpe ratio                       0.129     -0.00831\n 5 CAPM alpha                         0.000108  -0.00574\n 6 Market beta                        0.992      0.927  \n 7 Absolute weight                    0.0246     0.0631 \n 8 Max. weight                        3.52       3.65   \n 9 Min. weight                        0.0000278 -0.145  \n10 Avg. sum of negative weights       0         78.0    \n11 Avg. fraction of negative weights  0         49.4    "},{"path":"parametric-portfolio-policies.html","id":"optimal-parameter-choice","chapter":"14 Parametric portfolio policies","heading":"14.5 Optimal parameter choice","text":"Next, move choice \\(\\theta\\) actually aims improve () performance measures. first define helper function compute_objective_function(), pass optimizer.may wonder return negative value objective function. simply due common convention optimization procedures search minima default. minimizing negative value objective function, get maximum value result.\nbasic form, R optimization relies function optim(). main inputs, function requires initial guess parameters objective function minimize. Now, fully equipped compute optimal values \\(\\hat\\theta\\), maximize hypothetical expected utility investor.resulting values \\(\\hat\\theta\\) easy interpret: Intuitively, expected utility increases tilting weights value-weighted portfolio towards smaller stocks (negative coefficient size) towards past winners (positive value momentum). findings line well-documented size effect (Banz 1981) momentum anomaly (Jegadeesh Titman 1993).","code":"\ncompute_objective_function <- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data <- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function <- evaluate_portfolio(processed_data,\n    full_evaluation = FALSE\n  ) |>\n    filter(measure == objective_measure) |>\n    pull(tilt)\n\n  return(-objective_function)\n}\noptimal_theta <- optim(\n  par = theta,\n  compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)\n\noptimal_theta$parmomentum_lag     size_lag \n       0.189       -2.007 "},{"path":"parametric-portfolio-policies.html","id":"more-model-specifications","chapter":"14 Parametric portfolio policies","heading":"14.6 More model specifications","text":"portfolio perform different model specifications? purpose, compute performance number different modeling choices based entire CRSP sample. next code chunk performs heavy lifting.Finally, can compare results. table shows summary statistics possible combinations: equal- value-weighted benchmark portfolio, without short-selling constraints, tilted towards maximizing expected utility.results indicate average annualized Sharpe ratio equal-weighted portfolio exceeds Sharpe ratio value-weighted benchmark portfolio. Nevertheless, starting weighted value portfolio benchmark tilting optimally respect momentum small stocks yields highest Sharpe ratio across specifications. Imposing short-sale constraints improve performance portfolios application.","code":"\nfull_model_grid <- expand_grid(\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE),\n  data = list(data_portfolios)\n) |>\n  mutate(optimal_theta = pmap(\n    .l = list(\n      data,\n      value_weighting,\n      allow_short_selling\n    ),\n    .f = ~ optim(\n      par = theta,\n      compute_objective_function,\n      data = ..1,\n      objective_measure = \"Expected utility\",\n      value_weighting = ..2,\n      allow_short_selling = ..3\n    )$par\n  ))\nperformance_table <- full_model_grid |>\n  mutate(\n    processed_data = pmap(\n      .l = list(\n        optimal_theta,\n        data,\n        value_weighting,\n        allow_short_selling\n      ),\n      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)\n    ),\n    portfolio_evaluation = map(processed_data,\n      evaluate_portfolio,\n      full_evaluation = TRUE\n    )\n  ) |>\n  select(\n    value_weighting,\n    allow_short_selling,\n    portfolio_evaluation\n  ) |>\n  unnest(portfolio_evaluation)\n\nperformance_table |>\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) |>\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) |>\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) |>\n  select(\n    measure,\n    `EW    `,\n    `VW    `,\n    sort(contains(\"Optimal\"))\n  ) |>\n  print(n = 11)# A tibble: 11 × 7\n   measure       `EW    ` `VW    ` `VW  Optimal ` `VW (no s.) Op…` `EW  Optimal `\n   <chr>            <dbl>    <dbl>          <dbl>            <dbl>          <dbl>\n 1 Expected uti… -0.250   -2.49e-1       -0.247           -0.247         -0.250  \n 2 Average retu… 10.5      6.86e+0       14.7             13.4           13.0    \n 3 SD return     20.3      1.53e+1       20.6             19.6           22.7    \n 4 Sharpe ratio   0.149    1.29e-1        0.206            0.198          0.166  \n 5 CAPM alpha     0.00231  1.08e-4        0.00649          0.00528        0.00440\n 6 Market beta    1.13     9.92e-1        1.01             1.04           1.14   \n 7 Absolute wei…  0.0246   2.46e-2        0.0379           0.0246         0.0258 \n 8 Max. weight    0.0246   3.52e+0        3.34             2.65           0.0807 \n 9 Min. weight    0.0246   2.78e-5       -0.0327           0             -0.0342 \n10 Avg. sum of …  0        0             27.9              0              2.49   \n11 Avg. fractio…  0        0             38.8              0              7.84   \n# … with 1 more variable: `EW (no s.) Optimal ` <dbl>"},{"path":"parametric-portfolio-policies.html","id":"exercises-11","chapter":"14 Parametric portfolio policies","heading":"14.7 Exercises","text":"estimated parameters \\(\\hat\\theta\\) portfolio performance change objective maximize Sharpe ratio instead hypothetical expected utility?code flexible sense can easily add new firm characteristics. Construct new characteristic evaluate corresponding coefficient \\(\\hat\\theta_i\\).Tweak function optimal_theta()can impose additional performance constraints order determine \\(\\hat\\theta\\) maximizes expected utility constraint market beta 1.portfolio performance resemble realistic --sample backtesting procedure? Verify robustness results first estimating \\(\\hat\\theta\\) based past data . , use recent periods evaluate actual portfolio performance.formulating portfolio problem statistical estimation problem, can easily obtain standard errors coefficients weight function. Brandt, Santa-Clara, Valkanov (2009) provide relevant derivations paper Equation (10). Implement small function computes standard errors \\(\\hat\\theta\\).","code":""},{"path":"constrained-optimization-and-backtesting.html","id":"constrained-optimization-and-backtesting","chapter":"15 Constrained optimization and backtesting","heading":"15 Constrained optimization and backtesting","text":" chapter, conduct portfolio backtesting realistic setting including transaction costs investment constraints -short-selling rules.\nstart standard mean-variance efficient portfolios.\n, introduce constraints step--step.Throughout chapter, use following packages:Compared previous chapters, introduce quadprog package (Berwin . Turlach R port Andreas Weingessel <Andreas.Weingessel@ci.tuwien.ac.> Fortran contributions Cleve Moler dpodi/LINPACK) 2019) perform numerical constrained optimization quadratic objective functions alabama (Varadhan 2022) general non-linear objective functions constraints. ","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(quadprog)\nlibrary(alabama)"},{"path":"constrained-optimization-and-backtesting.html","id":"data-preparation-8","chapter":"15 Constrained optimization and backtesting","heading":"15.1 Data preparation","text":"start loading required data SQLite-database introduced chapter 2. simplicity, restrict investment universe monthly Fama-French industry portfolio returns following application. ","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nindustry_returns <- tbl(tidy_finance, \"industries_ff_monthly\") |>\n  collect()\n\nindustry_returns <- industry_returns |>\n  select(-month)"},{"path":"constrained-optimization-and-backtesting.html","id":"recap-of-portfolio-choice","chapter":"15 Constrained optimization and backtesting","heading":"15.2 Recap of portfolio choice","text":"common objective portfolio optimization find mean-variance efficient portfolio weights, .e., allocation delivers lowest possible return variance given minimum level expected returns.\nextreme case, investor concerned portfolio variance, may choose implement minimum variance portfolio (MVP) weights given solution \n\\[w_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1\\]\n\\(\\Sigma\\) \\((N \\times N)\\) covariance matrix returns. optimal weights \\(\\omega_\\text{mvp}\\) can found analytically \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). terms code, math equivalent following. Next, consider investor aims achieve minimum variance given required expected portfolio return \\(\\bar{\\mu}\\) chooses\n\\[w_\\text{eff}({\\bar{\\mu}}) =\\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\ncan shown (see Exercises) portfolio choice problem can equivalently formulated investor mean-variance preferences risk aversion factor \\(\\gamma\\). investor aims choose portfolio weights \n\\[ w^*_\\gamma = \\arg\\max w' \\mu - \\frac{\\gamma}{2}w'\\Sigma w\\quad \\text{ s.t. } w'\\iota = 1.\\]\nsolution optimal portfolio choice problem :\n\\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\]\nEmpirically, classical solution imposes many problems.\nparticular, estimates \\(\\mu_t\\) noisy short horizons, (\\(N \\times N\\)) matrix \\(\\Sigma_t\\) contains \\(N(N-1)/2\\) distinct elements thus, estimation error huge.\nSeminal papers effect ignoring estimation uncertainty, among others, Brown (1976), Jobson Korkie (1980), “Bayes-Stein estimation portfolio analysis” (1986), Chopra Ziemba (1993).Even worse, asset universe contains assets available time periods \\((N > T)\\), sample covariance matrix longer positive definite inverse \\(\\Sigma^{-1}\\) exist anymore.\naddress estimation issues vast-dimensional covariance matrices, regularization techniques popular tool, see, e.g., Ledoit Wolf (2003), Ledoit Wolf (2004), Ledoit Wolf (2012), Fan, Fan, Lv (2008).Model uncertainty due multiple feasible data generating processes context portfolio choice investigated, instance, Wang (2005), Garlappi, Uppal, Wang (2007), Pflug, Pichler, Wozabal (2012). top estimation uncertainty, transaction costs major concern.\nRebalancing portfolios costly, , therefore, optimal choice depend investor’s current holdings. presence transaction costs, benefits reallocating wealth may smaller costs associated turnover.aspect investigated theoretically, among others, one risky asset Magill Constantinides (1976) Davis Norman (1990).\nSubsequent extensions case multiple assets proposed Balduzzi Lynch (1999) Balduzzi Lynch (2000).\nrecent papers empirical approaches explicitly account transaction costs include Gârleanu Pedersen (2013), DeMiguel, Nogales, Uppal (2014), DeMiguel, Martín-Utrera, Nogales (2015).","code":"\nSigma <- cov(industry_returns)\nw_mvp <- solve(Sigma) %*% rep(1, ncol(Sigma))\nw_mvp <- as.vector(w_mvp / sum(w_mvp))"},{"path":"constrained-optimization-and-backtesting.html","id":"estimation-uncertainty-and-transaction-costs","chapter":"15 Constrained optimization and backtesting","heading":"15.3 Estimation uncertainty and transaction costs","text":"empirical evidence regarding performance mean-variance optimization procedure simply plug sample estimates \\(\\hat \\mu_t\\) \\(\\hat \\Sigma_t\\) can summarized rather briefly: mean-variance optimization performs poorly! literature discusses many proposals overcome empirical issues. instance, one may impose form regularization \\(\\Sigma\\), rely Bayesian priors inspired theoretical asset pricing models (“Optimal Portfolio Choice Parameter Uncertainty” 2007) use high-frequency data improve forecasting (Hautsch, Kyj, Malec 2015).\nOne unifying framework works easily, effectively (even large dimensions), purely inspired economic arguments ex-ante adjustment transaction costs (Hautsch Voigt 2019).Assume returns multivariate normal distribution mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, assume quadratic transaction costs penalize rebalancing \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) := \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\]\ncost parameter \\(\\beta>0\\) \\(\\omega_{t^+} := {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\).\nIntuitively, transaction costs penalize portfolio performance portfolio shifted current holdings \\(\\omega_{t^+}\\) new allocation \\(\\omega_{t+1}\\).\nsetup, transaction costs increase linearly larger rebalancing penalized heavily small adjustments.\nNote \\(\\omega_{t^+}\\) differs mechanically \\(\\omega_t\\) due returns past period.\n, optimal portfolio choice investor mean variance preferences \n\\[\\begin{aligned}\\omega_{t+1} ^* &:=  \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned}\\]\n\n\\[\\mu^*:=\\mu+\\beta \\omega_{t^+} \\quad  \\text{} \\quad \\Sigma^*:=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\\]\nresult, adjusting transaction costs implies standard mean-variance optimal portfolio choice adjusted return parameters \\(\\Sigma^*\\) \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]alternative formulation optimal portfolio can derived follows:\n\\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1.\\]\noptimal weights correspond mean-variance portfolio vector expected returns assets currently exhibit higher weight considered delivering higher expected return.","code":""},{"path":"constrained-optimization-and-backtesting.html","id":"optimal-portfolio-choice","chapter":"15 Constrained optimization and backtesting","heading":"15.4 Optimal portfolio choice","text":"function implements efficient portfolio weight general form, allowing transaction costs (conditional holdings reallocation).\n\\(\\beta=0\\), computation resembles standard mean-variance efficient framework. gamma denotes coefficient risk aversion \\(\\gamma\\),beta transaction cost parameter \\(\\beta\\) w_prev weights rebalancing \\(\\omega_{t^+}\\).portfolio weights indicate efficient portfolio investor risk aversion coefficient \\(\\gamma=2\\) absence transaction costs. positions negative implies short-selling, positions rather extreme. instance, position \\(-1\\) implies investor takes short position worth entire wealth lever long positions assets.\neffect transaction costs different levels risk aversion optimal portfolio choice? following lines code analyze distance minimum variance portfolio portfolio implemented investor different values transaction cost parameter \\(\\beta\\) risk aversion \\(\\gamma\\).code chunk computes optimal weight presence transaction cost different values \\(\\beta\\) \\(\\gamma\\) initial allocation, theoretical optimal minimum variance portfolio.\nStarting initial allocation, investor chooses optimal allocation along efficient frontier reflect risk preferences.\ntransaction costs absent, investor simply implement mean-variance efficient allocation. transaction costs make costly rebalance, optimal portfolio choice reflects shift towards efficient portfolio, whereas current portfolio anchors investment.\nFIGURE 15.1: Portfolio weights different risk aversion transaction cost\nfigure shows initial portfolio always (sample) minimum variance portfolio higher transaction costs parameter \\(\\beta\\), smaller rebalancing initial portfolio (always set minimum variance portfolio weights example). addition, risk aversion \\(\\gamma\\) increases, efficient portfolio closer minimum variance portfolio weights investor desires less rebalancing initial holdings.","code":"\ncompute_efficient_weight <- function(Sigma,\n                                     mu,\n                                     gamma = 2,\n                                     beta = 0, # transaction costs\n                                     w_prev = rep(\n                                       1 / ncol(Sigma),\n                                       ncol(Sigma)\n                                     )) {\n  iota <- rep(1, ncol(Sigma))\n  Sigma_processed <- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed <- mu + beta * w_prev\n\n  Sigma_inverse <- solve(Sigma_processed)\n\n  w_mvp <- Sigma_inverse %*% iota\n  w_mvp <- as.vector(w_mvp / sum(w_mvp))\n  w_opt <- w_mvp + 1 / gamma *\n    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%\n      mu_processed\n  return(as.vector(w_opt))\n}\n\nmu <- colMeans(industry_returns)\ncompute_efficient_weight(Sigma, mu) [1]  1.428  0.270 -1.302  0.375  0.308 -0.152  0.544  0.472 -0.167 -0.776\ntransaction_costs <- expand_grid(\n  gamma = c(2, 4, 8, 20),\n  beta = 20 * qexp((1:99) / 100)\n) |>\n  mutate(\n    weights = map2(\n      .x = gamma,\n      .y = beta,\n      ~ compute_efficient_weight(Sigma,\n        mu,\n        gamma = .x,\n        beta = .y / 10000,\n        w_prev = w_mvp\n      )\n    ),\n    concentration = map_dbl(weights, ~ sum(abs(. - w_mvp)))\n  )\ntransaction_costs |>\n  mutate(risk_aversion = as_factor(gamma)) |>\n  ggplot(aes(\n    x = beta,\n    y = concentration,\n    color = risk_aversion\n  )) +\n  geom_line() +\n  labs(\n    x = \"Transaction cost parameter\",\n    y = \"Distance from MVP\",\n    color = \"Risk aversion\",\n    title = \"Portfolio weights for different risk aversion and transaction cost\"\n  )"},{"path":"constrained-optimization-and-backtesting.html","id":"constrained-optimization","chapter":"15 Constrained optimization and backtesting","heading":"15.5 Constrained optimization","text":"Next, introduce constraints optimization procedure.\noften, typical constraints short-selling restrictions prevent analytical solutions optimal portfolio weights (Short-selling restrictions simply imply negative weights allowed require \\(w_i \\geq 0 \\forall \\)).\nHowever, numerical optimization allows computing solutions constrained problems. purpose mean-variance optimization, rely solve.QP() function package quadprog.function solve.QP() delivers numerical solutions quadratic programming problems form\n\\[\\min(-\\mu \\omega + 1/2 \\omega' \\Sigma \\omega) \\text{ s.t. } ' \\omega >= b_0.\\]\nfunction takes one argument (meq) number equality constraints. Therefore, matrix \\(\\) simply vector ones ensure weights sum one. case short-selling constraints, matrix \\(\\) form\n\\[' = \\begin{pmatrix}1 & 1& \\ldots&1 \\\\1 & 0 &\\ldots&0\\\\0 & 1 &\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}'\\qquad b_0 = \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}.\\]dive constrained optimization, revisit unconstrained problem replicate analytical solutions minimum variance efficient portfolio weights . verify output equal solution.\nNote near() safe way compare two vectors pairwise equal. alternative == sensitive small differences may occur due representation floating points computer near() built tolerance. just discussed, set Amat matrix column ones bvec 1 enforce constraint weights must sum one. meq=1 means one (one) constraints must satisfied equality.result shows indeed numerical procedure recovered optimal weights scenario already know analytic solution.\ncomplex optimization routines, R’s optimization task view provides overview wast optimization landscape R. Next, approach problems analytical solutions exist. First, additionally impose short-sale constraints, implies \\(N\\) inequality constraints form \\(w_i >=0\\).expected, resulting portfolio weights positive (numerical precision). Typically, holdings presence short-sale constraints concentrated among way fewer assets unrestricted case.\ncan verify sum(w_no_short_sale$solution) returns 1. words: solve.QP() provides numerical solution portfolio choice problem mean-variance investor risk aversion gamma = 2 negative holdings forbidden.solve.QP() fast benefits clear problem structure quadratic objective linear constraints. However, optimization often requires flexibility. example, show compute optimal weights, subject -called regulation T-constraint, requires sum absolute portfolio weights smaller 1.5, \\(\\sum\\limits_{=1}^N |w_i| \\leq 1.5\\).\nconstraint implies initial margin requirement 50% , therefore, also non-linear constraint.\nThus, can longer rely solve.QP() defined solve quadratic programming problems linear constraints.\nInstead, rely package alabama, requires separate definition objective constraint functions.Note function constrOptim.nl() requires starting vector parameter values, initial portfolio. hood, alamaba performs numerical optimization searching local minimum function objective() (subject equality constraints equality_constraints() inequality constraints inequality_constraints()).\norder initialize search procedure, starting point provided.\nalgorithm identifies global minimum, starting point matter.figure shows optimal allocation weights across 10 industries four different strategies considered far: minimum variance, efficient portfolio \\(\\gamma\\) = 2, efficient portfolio short-sale constraints, Regulation-T constrained portfolio.\nFIGURE 15.2: Summary optimal allocation weights 10 industry portfolios 4 different allocation strategies.\nresults clearly indicate effect imposing additional constraints: extreme holdings investor implement follows (theoretically optimal) efficient portfolio vanish , e.g., regulation-t constraint.\nmay wonder investor deviate theoretically optimal portfolio imposing potentially arbitrary constraints.\nshort answer : efficient portfolio efficient true parameters data generating process correspond estimated parameters \\(\\hat\\Sigma\\) \\(\\hat\\mu\\).\nEstimation uncertainty may thus lead inefficient allocations. imposing restrictions, implicitly shrink set possible weights prevent extreme allocations result error-maximization due estimation uncertainty (Jagannathan Ma 2003).move , want propose final allocation strategy, reflects somewhat realistic structure transaction costs instead quadratic specification used . function computes efficient portfolio weights adjusting transaction costs form \\(\\beta\\sum\\limits_{=1}^N |(w_{, t+1} - w_{, t^+})|\\). closed-form solution exists, rely non-linear optimization procedures.","code":"\nn_industries <- ncol(industry_returns)\n\nw_mvp_numerical <- solve.QP(\n  Dmat = Sigma,\n  dvec = rep(0, n_industries),\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(w_mvp, w_mvp_numerical$solution))[1] TRUE\nw_efficient_numerical <- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(compute_efficient_weight(Sigma, mu), w_efficient_numerical$solution))[1] TRUE\nw_no_short_sale <- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(1, diag(n_industries)),\n  bvec = c(1, rep(0, n_industries)),\n  meq = 1\n)\nw_no_short_sale$solution [1]  6.34e-01 -1.91e-17  1.20e-16 -3.47e-18  6.78e-18 -6.24e-17  1.02e-01\n [8]  2.64e-01  3.38e-22 -2.22e-16\ninitial_weights <- rep(\n  1 / n_industries,\n  n_industries\n)\n\nobjective <- function(w, gamma = 2) {\n  -t(w) %*% (1 + mu) +\n    gamma / 2 * t(w) %*% Sigma %*% w\n}\n\ninequality_constraints <- function(w, reg_t = 1.5) {\n  return(reg_t - sum(abs(w)))\n}\n\nequality_constraints <- function(w) {\n  return(sum(w) - 1)\n}\n\nw_reg_t <- constrOptim.nl(\n  par = initial_weights,\n  hin = inequality_constraints,\n  fn = objective,\n  heq = equality_constraints,\n  control.outer = list(trace = FALSE)\n)\nw_reg_t$par [1]  4.11e-01 -2.07e-02 -9.00e-02  3.37e-02  8.03e-02 -2.25e-08  3.08e-01\n [8]  3.52e-01  6.25e-02 -1.37e-01\ntibble(\n  `No short-sale` = w_no_short_sale$solution,\n  `Minimum Variance` = w_mvp,\n  `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n  `Regulation-T` = w_reg_t$par,\n  Industry = colnames(industry_returns)\n) |>\n  pivot_longer(-Industry,\n    names_to = \"Strategy\",\n    values_to = \"weights\"\n  ) |>\n  ggplot(aes(\n    fill = Strategy,\n    y = weights,\n    x = Industry\n  )) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() +\n  labs(\n    y = \"Allocation weight\",\n    title = \" Optimal allocations for different investment rules\"\n  ) +\n  scale_y_continuous(labels = percent)\ncompute_efficient_weight_L1_TC <- function(mu,\n                                           Sigma,\n                                           gamma = 2,\n                                           beta = 0,\n                                           initial_weights = rep(\n                                             1 / ncol(Sigma),\n                                             ncol(Sigma)\n                                           )) {\n  objective <- function(w) {\n    -t(w) %*% mu +\n      gamma / 2 * t(w) %*% Sigma %*% w +\n      (beta / 10000) / 2 * sum(abs(w - initial_weights))\n  }\n\n  w_optimal <- constrOptim.nl(\n    par = initial_weights,\n    fn = objective,\n    heq = function(w) {\n      sum(w) - 1\n    },\n    control.outer = list(trace = FALSE)\n  )\n\n  return(w_optimal$par)\n}"},{"path":"constrained-optimization-and-backtesting.html","id":"out-of-sample-backtesting","chapter":"15 Constrained optimization and backtesting","heading":"15.6 Out-of-sample backtesting","text":"sake simplicity, committed one fundamental error computing portfolio weights (Arnott, Harvey, Markowitz 2019). used full sample data determine optimal allocation. implement strategy beginning 2000s, need know returns evolve 2020.\ninteresting methodological point view, evaluate performance portfolios reasonable --sample fashion. next backtesting application three strategies. backtest, recompute optimal weights just based past available data.\nlines define general setup. consider 120 periods past update parameter estimates recomputing portfolio weights. , update portfolio weights costly affects performance. portfolio weights determine portfolio return. period later, current portfolio weights changed form foundation transaction costs incurred next period. consider three different competing strategies: mean-variance efficient portfolio, mean-variance efficient portfolio ex-ante adjustment transaction costs, naive portfolio, allocates wealth equally across different assets.also define two helper functions: one adjust weights due returns one performance evaluation, compute realized returns net transaction costs.following code chunk performs rolling-window estimation. period, estimation window contains returns available current period.\nNote use sample variance covariance matrix ignore estimation \\(\\hat\\mu\\) entirely, might use advanced estimators practice.Finally, get evaluation portfolio strategies net--transaction costs. Note compute annualized returns standard deviations. results clearly speak mean-variance optimization. Turnover huge investor considers portfolio’s expected return variance. Effectively, mean-variance portfolio generates negative annualized return adjusting transaction costs. time, naive portfolio turns perform well. fact, performance gains transaction-cost adjusted mean-variance portfolio small. --sample Sharpe ratio slightly higher naive portfolio. Note extreme effect turnover penalization turnover: MV (TC) effectively resembles buy--hold strategy updates portfolio estimated parameters \\(\\hat\\mu_t\\) \\(\\hat\\Sigma_t\\)indicate current allocation far away optimal theoretical portfolio.","code":"\nwindow_length <- 120\nperiods <- nrow(industry_returns) - window_length\n\nbeta <- 50\ngamma <- 2\n\nperformance_values <- matrix(NA,\n  nrow = periods,\n  ncol = 3\n) # A matrix to collect all returns\ncolnames(performance_values) <- c(\"raw_return\", \"turnover\", \"net_return\")\n\nperformance_values <- list(\n  \"MV (TC)\" = performance_values,\n  \"Naive\" = performance_values,\n  \"MV\" = performance_values\n)\n\nw_prev_1 <- w_prev_2 <- w_prev_3 <- rep(\n  1 / n_industries,\n  n_industries\n)\nadjust_weights <- function(w, next_return) {\n  w_prev <- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance <- function(w, w_previous, next_return, beta = 50) {\n  raw_return <- as.matrix(next_return) %*% w\n  turnover <- sum(abs(w - w_previous))\n  net_return <- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\nfor (p in 1:periods) {\n  returns_window <- industry_returns[p:(p + window_length - 1), ]\n  next_return <- industry_returns[p + window_length, ] |> as.matrix()\n\n  Sigma <- cov(returns_window)\n  mu <- 0 * colMeans(returns_window)\n\n  # Transaction-cost adjusted portfolio\n  w_1 <- compute_efficient_weight_L1_TC(\n    mu = mu,\n    Sigma = Sigma,\n    beta = beta,\n    gamma = gamma,\n    initial_weights = w_prev_1\n  )\n\n  performance_values[[1]][p, ] <- evaluate_performance(w_1,\n    w_prev_1,\n    next_return,\n    beta = beta\n  )\n\n  w_prev_1 <- adjust_weights(w_1, next_return)\n\n  # Naive portfolio\n  w_2 <- rep(1 / n_industries, n_industries)\n\n  performance_values[[2]][p, ] <- evaluate_performance(\n    w_2,\n    w_prev_2,\n    next_return\n  )\n\n  w_prev_2 <- adjust_weights(w_2, next_return)\n\n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 <- compute_efficient_weight(\n    Sigma = Sigma,\n    mu = mu,\n    gamma = gamma\n  )\n\n  performance_values[[3]][p, ] <- evaluate_performance(\n    w_3,\n    w_prev_3,\n    next_return\n  )\n\n  w_prev_3 <- adjust_weights(w_3, next_return)\n}\nperformance <- lapply(\n  performance_values,\n  as_tibble\n) |>\n  bind_rows(.id = \"strategy\")\n\nperformance |>\n  group_by(strategy) |>\n  summarize(\n    Mean = 12 * mean(100 * net_return),\n    SD = sqrt(12) * sd(100 * net_return),\n    `Sharpe ratio` = if_else(Mean > 0,\n      Mean / SD,\n      NA_real_\n    ),\n    Turnover = 100 * mean(turnover)\n  )# A tibble: 3 × 5\n  strategy   Mean    SD `Sharpe ratio` Turnover\n  <chr>     <dbl> <dbl>          <dbl>    <dbl>\n1 MV       -0.637  12.4         NA     214.    \n2 MV (TC)  12.1    15.1          0.802   0.0311\n3 Naive    12.1    15.1          0.801   0.229 "},{"path":"constrained-optimization-and-backtesting.html","id":"exercises-12","chapter":"15 Constrained optimization and backtesting","heading":"15.7 Exercises","text":"argue investor quadratic utility function certainty equivalent \\[\\max_w CE(w) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\]\nfaces equivalent optimization problem framework portfolio weights chosen aim minimize volatility given pre-specified level expected returns\n\\[\\min_w \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ } \\iota'\\omega = 1. \\] Proof equivalence optimal portfolio weights cases.Consider portfolio choice problem transaction-cost adjusted certainty equivalent maximization risk aversion parameter \\(\\gamma\\)\n\\[\\omega_{t+1} ^* :=  \\arg\\max_{\\omega \\\\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\]\n\\(\\Sigma\\) \\(\\mu\\) (estimators ) variance-covariance matrix returns vector expected returns. Assume now transaction costs quadratic rebalancing proportional stock illiquidity \n\\[\\nu_t\\left(\\omega, B\\right) := \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) diagonal matrix \\(ill_1, \\ldots, ill_N\\). Derive closed-form solution mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based transaction cost specification . Discuss effect illiquidity \\(ill_i\\) individual portfolio weights relative investor myopically ignores transaction costs decision.Use solution previous exercise update function compute_efficient_weight can compute optimal weights conditional matrix \\(B\\) illiquidity measures.Illustrate evolution optimal weights naive portfolio efficient portfolio mean-standard deviation diagram.always optimal choose \\(\\beta\\) optimization problem value used evaluating portfolio performance? words: Can optimal choose theoretically sub-optimal portfolios based transaction cost considerations reflect actual incurred costs? Evaluate --sample Sharpe ratio transaction costs range different values imposed \\(\\beta\\) values.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"cover-design.html","id":"cover-design","chapter":"A Cover design","heading":"A Cover design","text":"cover book inspired fast growing generative art community R.\nGenerative art refers art whole part created use autonomous system.\nInstead creating random dynamics rely core book: evolution financial markets.\nThus, circle logo corresponds daily market returns within one year sample. colors determined standard deviation market returns particular year. lines code replicate entire figure.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(SQLite(), \n                          \"data/tidy_finance.sqlite\", \n                          extended_types = TRUE)\nmfac <- tbl(tidy_finance, \n            \"factors_ff_daily\") %>%\n  collect()\n\ncp <- coord_polar(direction = -1, clip = \"on\")\ncp$is_free <- function() TRUE\n\nplot_data <- mfac %>%\n  select(date, mkt_excess) %>%\n  group_by(year = lubridate::floor_date(date, \"year\")) %>%\n  mutate(group_id = cur_group_id())\n\nplot_data <- plot_data %>%\n  mutate(\n    group_id = if_else(group_id >= 28, group_id + 4, group_id + 0),\n    group_id = if_else(group_id >= 36, group_id + 4, group_id + 0),\n    group_id = if_else(group_id >= 44, group_id + 4, group_id + 0)\n  ) %>%\n  bind_rows(plot_data %>%\n    filter(group_id %in% c(28:31, 36:39, 44:47)) %>%\n    mutate(mkt_excess = NA)) %>%\n  group_by(group_id) %>%\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) %>%\n  filter(year >= \"1961-01-01\")\n\ncolors <- wes_palette(\"Zissou1\", n_groups(plot_data), type = \"continuous\")\nlevels <- plot_data %>%\n  distinct(group_id, vola) %>%\n  arrange(vola) %>%\n  pull(vola)\n\nplot <- plot_data %>%\n  mutate(vola = factor(vola, levels = levels)) %>%\n  ggplot() +\n  aes(x = day, y = mkt_excess, group = group_id, fill = vola) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = as_factor(vola)\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id, ncol = 8, scales = \"free\") +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\n# ggsave(\n#   plot = plot, width = 8, height = 9,\n#   filename = \"cover.jpg\", bg = \"white\"\n# )"},{"path":"clean-enhanced-trace-with-r.html","id":"clean-enhanced-trace-with-r","chapter":"B Clean enhanced TRACE with R","heading":"B Clean enhanced TRACE with R","text":"appendix contains code clean enhanced TRACE R. also available via following Github gist. Hence, also source function devtools::source_gist(\"3a05b3ab281563b2e94858451c2eb3a4\"). need function Chapter 4 download clean enhanced TRACE trade messages following Dick-Nielsen (2009) Dick-Nielsen (2014) enhanced TRACE specifically. WRDS provides SAS code clean enhanced TRACE data.function takes vector CUSIPs (cusips), connection WRDS (connection) explained chapter 3, start end date (start_date end_date, respectively). Specifying many CUSIPs result slow downloads potential failure due size request WRDS. dates within coverage TRACE , .e., starting 2002, dates supplied using class date. output function contains valid trade messages selected CUSIPs specified period.","code":"\nclean_enhanced_trace <- function(cusips, \n                                 connection, \n                                 start_date = as.Date(\"2002-01-01\"), \n                                 end_date = tody()) {\n  # Function checks ---------------------------------------------------------\n  # Input parameters\n  ## Cusips\n  if(length(cusips) == 0 | any(is.na(cusips))) stop(\"Check cusips.\")\n  \n  ## Dates\n  if(!is.Date(start_date) | !is.Date(end_date)) stop(\"Dates needed\")\n  if(start_date < as.Date(\"2002-01-01\")) stop(\"TRACE starts later.\")\n  if(end_date > today()) stop(\"TRACE does not predict the future.\")\n  if(start_date >= end_date) stop(\"Date conflict.\")\n  \n  ## Connection\n  if(!dbIsValid(connection)) stop(\"Connection issue.\")\n  \n  # Packages (required)\n  library(tidyverse)\n  library(lubridate)\n  library(dbplyr)\n  library(RPostgres)\n  \n  # Enhanced Trace ----------------------------------------------------------\n  # Main file\n  trace_all <- tbl(connection, \n                   in_schema(\"trace\", \"trace_enhanced\")) |> \n    filter(cusip_id %in% cusips) |>\n    filter(trd_exctn_dt >= start_date & trd_exctn_dt <= end_date) |> \n    select(cusip_id, msg_seq_nb, orig_msg_seq_nb,\n           entrd_vol_qt, rptd_pr, rpt_side_cd, cntra_mp_id,\n           trd_exctn_dt, trd_exctn_tm, trd_rpt_dt, trd_rpt_tm, \n           pr_trd_dt, trc_st, asof_cd, wis_fl, \n           days_to_sttl_ct, stlmnt_dt, spcl_trd_fl) |>\n    collect()\n  \n  # Enhanced Trace: Post 06-02-2012 -----------------------------------------\n  # Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR <- trace_all |> \n    filter((trc_st == \"T\" | trc_st == \"R\"),\n           trd_rpt_dt >= as.Date(\"2012-02-06\"))\n  \n  # Cancelations (trc_st = X) and correction cancelations (trc_st = C)\n  trace_post_XC <- trace_all |>\n    filter((trc_st == \"X\" | trc_st == \"C\"),\n           trd_rpt_dt >= as.Date(\"2012-02-06\"))\n  \n  # Cleaning corrected and cancelled trades\n  trace_post_TR <- trace_post_TR |>\n    anti_join(trace_post_XC,\n              by = c(\"cusip_id\", \"msg_seq_nb\", \"entrd_vol_qt\", \n                     \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\", \n                     \"trd_exctn_dt\", \"trd_exctn_tm\"))\n  \n  # Reversals (trc_st = Y)\n  trace_post_Y <- trace_all |>\n    filter(trc_st == \"Y\",\n           trd_rpt_dt >= as.Date(\"2012-02-06\"))\n  \n  # Clean reversals\n  ## match the orig_msg_seq_nb of the Y-message to \n  ## the msg_seq_nb of the main message\n  trace_post <- trace_post_TR |>\n    anti_join(trace_post_Y,\n              by = c(\"cusip_id\", \"msg_seq_nb\" = \"orig_msg_seq_nb\", \n                     \"entrd_vol_qt\", \"rptd_pr\", \"rpt_side_cd\", \n                     \"cntra_mp_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"))\n  \n  \n  # Enhanced TRACE: Pre 06-02-2012 ------------------------------------------\n  # Cancelations (trc_st = C)\n  trace_pre_C <- trace_all |>\n    filter(trc_st == \"C\",\n           trd_rpt_dt < as.Date(\"2012-02-06\"))\n  \n  # Trades w/o cancelations\n  ## match the orig_msg_seq_nb of the C-message \n  ## to the msg_seq_nb of the main message\n  trace_pre_T <- trace_all |>\n    filter(trc_st == \"T\",\n           trd_rpt_dt < as.Date(\"2012-02-06\")) |>\n    anti_join(trace_pre_C, \n              by = c(\"cusip_id\", \"msg_seq_nb\" = \"orig_msg_seq_nb\", \n                     \"entrd_vol_qt\", \"rptd_pr\", \"rpt_side_cd\", \n                     \"cntra_mp_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"))\n  \n  # Corrections (trc_st = W) - W can also correct a previous W\n  trace_pre_W <- trace_all |>\n    filter(trc_st == \"W\",\n           trd_rpt_dt < as.Date(\"2012-02-06\"))\n  \n  # Implement corrections in a loop\n  ## Correction control\n  correction_control <- nrow(trace_pre_W)\n  correction_control_last <- nrow(trace_pre_W)\n  \n  ## Correction loop\n  while(correction_control > 0) {\n    # Corrections that correct some msg\n    trace_pre_W_correcting <- trace_pre_W |>\n      semi_join(trace_pre_T, \n                by = c(\"cusip_id\", \"trd_exctn_dt\",\n                       \"orig_msg_seq_nb\" = \"msg_seq_nb\"))\n    \n    # Corrections that do not correct some msg\n    trace_pre_W <- trace_pre_W |>\n      anti_join(trace_pre_T, \n                by = c(\"cusip_id\", \"trd_exctn_dt\",\n                       \"orig_msg_seq_nb\" = \"msg_seq_nb\"))\n    \n    # Delete msgs that are corrected and add correction msgs\n    trace_pre_T <- trace_pre_T |>\n      anti_join(trace_pre_W_correcting, \n                by = c(\"cusip_id\", \"trd_exctn_dt\",\n                       \"msg_seq_nb\" = \"orig_msg_seq_nb\")) |>\n      union_all(trace_pre_W_correcting) \n    \n    # Escape if no corrections remain or they cannot be matched\n    correction_control <- nrow(trace_pre_W)\n    if(correction_control == correction_control_last) {\n      correction_control <- 0 \n    }\n    correction_control_last <- nrow(trace_pre_W)\n  }\n  \n  \n  # Clean reversals\n  ## Record reversals\n  trace_pre_R <- trace_pre_T |>\n    filter(asof_cd == 'R') |>\n    group_by(cusip_id, trd_exctn_dt, entrd_vol_qt, \n             rptd_pr, rpt_side_cd, cntra_mp_id) |>\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |>\n    mutate(seq = row_number()) |>\n    ungroup()\n  \n  ## Remove reversals and the reversed trade\n  trace_pre <- trace_pre_T |> \n    filter(is.na(asof_cd) | !(asof_cd %in% c('R', 'X', 'D'))) |> \n    group_by(cusip_id, trd_exctn_dt, entrd_vol_qt, \n             rptd_pr, rpt_side_cd, cntra_mp_id) |> \n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |> \n    mutate(seq = row_number()) |> \n    ungroup() |> \n    anti_join(trace_pre_R,\n              by = c(\"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\", \n                     \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\", \"seq\")) |> \n    select(-seq)\n  \n  \n  # Agency trades -----------------------------------------------------------\n  # Combine pre and post trades\n  trace_clean <- trace_post |> \n    union_all(trace_pre)\n  \n  # Keep angency sells and unmatched agency buys\n  ## Agency sells\n  trace_agency_sells <- trace_clean |> \n    filter(cntra_mp_id == \"D\",\n           rpt_side_cd == \"S\")\n  \n  # Agency buys that are unmatched\n  trace_agency_buys_filtered <- trace_clean |> \n    filter(cntra_mp_id == \"D\",\n           rpt_side_cd == \"B\") |> \n    anti_join(trace_agency_sells, \n              by = c(\"cusip_id\", \"trd_exctn_dt\", \n                     \"entrd_vol_qt\", \"rptd_pr\"))\n  \n  # Agency clean\n  trace_clean <- trace_clean |> \n    filter(cntra_mp_id == \"C\")  |> \n    union_all(trace_agency_sells) |> \n    union_all(trace_agency_buys_filtered) \n  \n  \n  # Additional Filters ------------------------------------------------------\n  trace_add_filters <- trace_clean |> \n    mutate(days_to_sttl_ct2 = stlmnt_dt - trd_exctn_dt) |> \n    filter(is.na(days_to_sttl_ct) | as.numeric(days_to_sttl_ct) <= 7,\n           is.na(days_to_sttl_ct2) | as.numeric(days_to_sttl_ct2) <= 7,\n           wis_fl == \"N\",\n           is.na(spcl_trd_fl) | spcl_trd_fl == \"\",\n           is.na(asof_cd) | asof_cd == \"\")\n  \n  \n  # Output ------------------------------------------------------------------\n  # Only keep necessary columns\n  trace_final <- trace_add_filters |> \n    arrange(cusip_id, trd_exctn_dt, trd_exctn_tm) |> \n    select(cusip_id, trd_exctn_dt, trd_exctn_tm, \n           rptd_pr, entrd_vol_qt, rpt_side_cd, cntra_mp_id) |> \n    mutate(trd_exctn_tm = format(as_datetime(trd_exctn_tm), \"%H:%M:%S\")) \n  \n  # Return\n  return(trace_final)\n}"}]
