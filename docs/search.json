[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "Experimental and external contributions based on Tidy Finance with R. Contribute your ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Data for Tidy Finance Readers without Access to WRDS\n\n\n14 min\n\n\n\nData\n\n\n\nR code to generate dummy data that can be used to run the code chunks in Tidy Finance with R or Python\n\n\n\nChristoph Scheuch\n\n\nSep 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Raw TRACE Data to a Local SQLite Database\n\n\n37 min\n\n\n\nData\n\n\n\nAn R code that converts TRACE files from FINRA into a SQLite for facilitated analysis and filtering\n\n\n\nKevin Riehl, Lukas Müller\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\n13 min\n\n\n\nRecommender System\n\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms\n\n\n\nChristoph Scheuch\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Standard Errors in Portfolio Sorts\n\n\n39 min\n\n\n\nReplications\n\n\n\nAn all-in-one implementation of non-standard errors in portfolio sorts\n\n\n\nPatrick Weiss\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruction of a Historical S&P 500 Total Return Index\n\n\n8 min\n\n\n\nData\n\n\n\nAn approximation of total returns using Robert Shiller’s stock market data\n\n\n\nChristoph Scheuch\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n\n5 min\n\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at Workshops for Ukraine\n\n\n2 min\n\n\n\nWorkshops\n\n\n\nYou can learn Tidy Finance and support Ukraine at the same time\n\n\n\nPatrick Weiss\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at the useR!2022 Conference\n\n\n1 min\n\n\n\nConferences\n\n\n\nTidy Finance presentation at the gathering supported by the R Foundation\n\n\n\nPatrick Weiss\n\n\nJun 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html",
    "href": "blog/tidy-finance-dummy-data/index.html",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "",
    "text": "Since we published our book Tidy Finance with R, we have received feedback from readers who don’t have access to WRDS that they cannot run the code we provide. To alleviate their constraints, we decided to create a dummy database that contains all tables and corresponding columns such that all code chunks in our book can be executed with this dummy database. The resulting database can be found through this link (around 50 MB). Just download the database and put it into your data folder (I already renamed it to tidy_finance.sqlite).\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\nTo generate the dummy database, we use the following packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nWe use the original database as an input and initialize a dummy version where we later store the output tables.\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"../../data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ntidy_finance_dummy &lt;- dbConnect(\n  SQLite(),\n  \"../../data/tidy_finance_dummy.sqlite\",\n  extended_types = TRUE\n)\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over 10 years that we then use to create yearly, monthly, and daily data, respectively.\nset.seed(1234)\n\nstart_date &lt;- as.Date(\"2003-01-01\")\nend_date &lt;- as.Date(\"2022-12-31\")\n\ntime_series_years &lt;- seq(year(start_date), year(end_date), 1)\ntime_series_months &lt;- seq(start_date, end_date, \"1 month\")\ntime_series_days &lt;- seq(start_date, end_date, \"1 day\")"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-macro-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-macro-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create macro dummy data",
    "text": "Create macro dummy data\nWe start by creating dummy data for the macroeconomic tables. The following code does this by replacing the original data with random numbers while preserving the time structure (either daily or monthly) and column names. We iterate over each table name in the macro_tables vector. For each table, we fetch the data from the database and store it in a data_original table. The code checks the column names of data_original to determine if the data is structured by months or by days. Depending on the structure, it sets the appropriate time series and date column name. For each relevant column (i.e., excluding columns with dates), the code generates a command to replace its values with random numbers drawn from a uniform distribution between 0 and 1. We then use the !!! operator to unlist and execute the list of commands. This trick actually helps us to avoid typing the same function for each column individually. Finally, each table with dummy data is written to the new database with dummy data.\nNote that we do not put any meaningful structure on the macro tables because they can be freely downloaded from the original sources - check out Accessing and Managing Financial Data to replace the dummy data with the actual data.\n\nmacro_tables &lt;- c(\n  \"cpi_monthly\", \"factors_ff3_daily\", \n  \"factors_ff3_monthly\", \"factors_ff5_monthly\", \n  \"factors_q_monthly\", \"industries_ff_monthly\", \n  \"macro_predictors\"\n)\n\nfor (table_name in macro_tables) {\n  data_original &lt;- tbl(tidy_finance, table_name) |&gt;\n    collect() |&gt; \n    drop_na()\n  \n  if (\"month\" %in% names(data_original)) {\n    time_series &lt;- time_series_months\n    date_column &lt;- \"month\"\n  } else if (\"date\" %in% names(data_original)) {\n    time_series &lt;- time_series_days\n    date_column &lt;- \"date\"\n  }\n  \n  relevant_columns &lt;- data_original |&gt; \n    select(-contains(c(\"month\", \"date\"))) |&gt; \n    names()\n  \n  commands &lt;- unlist(\n    map(\n      relevant_columns, \n      ~rlang::exprs(!!..1 := runif(n()))\n    )\n  )\n  \n  data_dummy &lt;- tibble(\n    !!sym(date_column) := time_series\n    ) |&gt; \n    mutate(\n      !!!commands\n    )\n  \n  dbWriteTable(\n    tidy_finance_dummy, \n    table_name,\n    data_dummy,\n    overwrite = TRUE\n  )\n}"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-stock-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-stock-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create stock dummy data",
    "text": "Create stock dummy data\nLet us move on the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique permno and gvkey values, as well as associated exchcd, exchange, industry, and siccd values. The generated data is based on the characteristics of stocks in the crsp_monthly table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the permno-gvkey combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\nnumber_of_stocks &lt;- 100\n\ncrsp_stocks &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  group_by(permno) |&gt; \n  filter(month == max(month, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  select(permno, gvkey, industry, exchange, exchcd, siccd) |&gt; \n  collect()\n\nindustries &lt;- crsp_stocks |&gt; \n  filter(industry != \"Missing\") |&gt; \n  count(industry) |&gt; \n  mutate(prob = n / sum(n))\n\nexchanges &lt;- crsp_stocks |&gt; \n  filter(exchange != \"Other\") |&gt; \n  count(exchange) |&gt; \n  mutate(prob = n / sum(n))\n\nstock_identifiers &lt;- 1:number_of_stocks |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        permno = x,\n        gvkey = as.character(x + 10000),\n        exchange = sample(exchanges$exchange, 1, \n                          prob = exchanges$prob),\n        industry = sample(industries$industry, 1, \n                          prob = industries$prob)\n      ) |&gt; \n        mutate(\n          exchcd = case_when(\n            exchange == \"NYSE\" ~ sample(c(1, 31), n()),\n            exchange == \"AMEX\" ~ sample(c(2, 32), n()),\n            exchange == \"NASDAQ\" ~ sample(c(3, 33), n())\n          ),\n          siccd = case_when(\n            industry == \"Agriculture\" ~ sample(1:999, n()),\n            industry == \"Mining\" ~ sample(1000:1499, n()),\n            industry == \"Construction\" ~ sample(1500:1799, n()),\n            industry == \"Manufacturing\" ~ sample(1800:3999, n()),\n            industry == \"Transportation\" ~ sample(4000:4899, n()),\n            industry == \"Utilities\" ~ sample(4900:4999, n()),\n            industry == \"Wholesale\" ~ sample(5000:5199, n()),\n            industry == \"Retail\" ~ sample(5200:5999, n()),\n            industry == \"Finance\" ~ sample(6000:6799, n()),\n            industry == \"Services\" ~ sample(7000:8999, n()),\n            industry == \"Public\" ~ sample(9000:9999, n())\n          )\n        )\n    }\n  )\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the stock_panel_yearly panel. To achieve this, we combine the stock_identifiers table with a new table containing the variable year from time_series_years. The expand_grid() function ensures that we get all possible combinations of the two tables. After combining, we select only the gvkey and year columns for our final yearly panel.\nNext, we construct the stock_panel_monthly panel. Similar to the yearly panel, we use the expand_grid() function to combine stock_identifiers with a new table that has the month variable from time_series_months. After merging, we select the columns permno, gvkey, month, siccd, industry, exchcd, and exchange to form our monthly panel.\nLastly, we create the stock_panel_daily panel. We combine stock_identifiers with a table containing the date variable from time_series_days. After merging, we retain only the permno and date columns for our daily panel.\n\nstock_panel_yearly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(year = time_series_years)\n) |&gt; \n  select(gvkey, year)\n\nstock_panel_monthly &lt;- expand_grid(\n  stock_identifiers, \n  tibble(month = time_series_months)\n) |&gt; \n  select(permno, gvkey, month, siccd, industry, exchcd, exchange)\n\nstock_panel_daily &lt;- expand_grid(\n  stock_identifiers, \n  tibble(date = time_series_days)\n)|&gt; \n  select(permno, date)\n\n\nDummy beta table\nWe then proceed to create dummy beta values for our stock_panel_monthly table. We generate monthly beta values beta_monthly using the rnorm() function with a mean and standard deviation of 1. For daily beta values beta_daily, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the rnorm() function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\nbeta_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    beta_monthly = rnorm(n(), mean = 1, sd = 1),\n    beta_daily = beta_monthly + rnorm(n()) / 100\n  )\n\ndbWriteTable(\n  tidy_finance_dummy,\n  \"beta\", \n  beta_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy compustat table\nTo create dummy firm characteristics, we take all columns from the compustat table and create random numbers between 0 and 1 using the same trick as with the macro data tables. For simplicity, we set the datadate for each firm-year observation to the last day of the year, although it is empirically not the case.\n\nrelevant_columns &lt;- tbl(tidy_finance, \"compustat\") |&gt; \n  select(-c(gvkey, datadate, year)) |&gt; \n  names()\n\ncommands &lt;- unlist(\n  map(\n    relevant_columns, \n    ~rlang::exprs(!!..1 := runif(n()))\n  )\n)\n\ncompustat_dummy &lt;- stock_panel_yearly |&gt; \n  mutate(\n    datadate = ymd(str_c(year, \"12\", \"31\")),\n    !!!commands\n  )\n\ndbWriteTable(\n  tidy_finance_dummy, \n  \"compustat\", \n  compustat_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_monthly table\nThe crsp_monthly table only lacks a few more columns compared to stock_panel_monthly: the returns ret drawn from a normal distribution, the excess returns ret_excess with small deviations from the returns, the shares outstanding shrout and the last price per month altprc both drawn from uniform distributions, and the market capitalization mktcap as the product of shrout and altprc.\n\ntbl(tidy_finance, \"crsp_monthly\")\n\n# Source:   table&lt;crsp_monthly&gt; [?? x 14]\n# Database: sqlite 3.41.2 [C:\\Users\\christoph.scheuch\\Documents\\Github\\tidy-finance\\website\\data\\tidy_finance.sqlite]\n  permno date       month           ret  shrout altprc exchcd siccd\n   &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  10057 1969-09-30 1969-09-01 -0.0167  2038000   22.1      1  3541\n2  10057 1969-10-31 1969-10-01  0.0328  2038000   22.5      1  3541\n3  10057 1969-11-28 1969-11-01 -0.00556 2038000   22.4      1  3541\n4  10057 1969-12-31 1969-12-01  0.0279  3854000   23        1  3541\n5  10057 1970-01-30 1970-01-01 -0.0652  3854000   21.5      1  3541\n# ℹ more rows\n# ℹ 6 more variables: mktcap &lt;dbl&gt;, mktcap_lag &lt;dbl&gt;,\n#   exchange &lt;chr&gt;, industry &lt;chr&gt;, ret_excess &lt;dbl&gt;, gvkey &lt;chr&gt;\n\ncrsp_monthly_dummy &lt;- stock_panel_monthly |&gt; \n  mutate(\n    date = ceiling_date(month, \"month\") - 1,\n    ret = pmax(rnorm(n()), -1),\n    ret_excess = pmax(ret - runif(n(), 0, 0.0025), -1),\n    shrout = runif(n(), 1, 50) * 1000,\n    altprc = runif(n(), 0, 1000),\n    mktcap = shrout * altprc\n  ) |&gt; \n  group_by(permno) |&gt; \n  arrange(month) |&gt; \n  mutate(mktcap_lag = lag(mktcap)) |&gt; \n  ungroup()\n\ndbWriteTable(\n  tidy_finance_dummy, \n  \"crsp_monthly\",\n  crsp_monthly_dummy,\n  overwrite = TRUE\n)\n\n\n\nDummy crsp_daily table\nThe crsp_daily table only contains a month column and the daily excess returns ret_excess as additional columns to stock_panel_daily.\n\ncrsp_daily_dummy &lt;- stock_panel_daily |&gt; \n  mutate(\n    month = floor_date(date, \"month\"),\n    ret_excess = pmax(rnorm(n()), -1)\n  )\n\ndbWriteTable(\n  tidy_finance_dummy,\n  \"crsp_daily\",\n  crsp_daily_dummy, \n  overwrite = TRUE\n)"
  },
  {
    "objectID": "blog/tidy-finance-dummy-data/index.html#create-bond-dummy-data",
    "href": "blog/tidy-finance-dummy-data/index.html#create-bond-dummy-data",
    "title": "Dummy Data for Tidy Finance Readers without Access to WRDS",
    "section": "Create bond dummy data",
    "text": "Create bond dummy data\nLastly, we move to the bond data that we use in our books.\n\nDummy mergent data\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: maturity, offering_amt, interest_frequency, coupon, and sic_code. We use these probabilities to sample a small cross-section of bonds with completely made up complete_cusip, issue_id, and issuer_id.\n\nnumber_of_bonds &lt;- 100\n\nmergent &lt;- tbl(tidy_finance, \"mergent\") |&gt; \n  collect()\n\nmaturities &lt;- mergent |&gt; \n  count(days = maturity - offering_date) |&gt; \n  mutate(probability = n / sum(n))\n\namounts &lt;- mergent |&gt; \n  count(offering_amt) |&gt; \n  mutate(probability = n / sum(n))\n\nfrequencies &lt;- mergent |&gt; \n  count(interest_frequency) |&gt; \n  mutate(probability = n / sum(n))\n\ncoupons &lt;- mergent |&gt; \n  count(coupon) |&gt; \n  mutate(probability = n / sum(n))\n\nsic &lt;- mergent |&gt; \n  count(sic_code) |&gt; \n  mutate(probability = n / sum(n))\n\nmergent_dummy &lt;- 1:number_of_bonds |&gt; \n  map_df(\n    function(x) {\n      tibble(\n        complete_cusip = str_to_upper(\n          str_c(\n            sample(c(letters, 0:9), 12, replace = TRUE), \n            collapse = \"\"\n          )\n        ),\n      )\n    }\n  ) |&gt; \n  mutate(\n    maturity = sample(time_series_days, n(), replace = TRUE),\n    offering_amt = sample(\n      amounts$offering_amt, n(), \n      prob = amounts$probability, \n      replace = TRUE\n    ),\n    offering_date = maturity - sample(\n      maturities$days, n(),\n      prob = maturities$probability, \n      replace = TRUE\n    ),\n    dated_date = offering_date - sample(-10:10, n(), replace = TRUE),\n    interest_frequency = sample(\n      frequencies$interest_frequency, n(), \n      prob = frequencies$probability, \n      replace = TRUE\n    ),\n    coupon = sample(\n      coupons$coupon, n(), \n      prob = coupons$probability, \n      replace = TRUE\n    ),\n    last_interest_date = pmax(maturity, offering_date, dated_date),\n    issue_id = row_number(),\n    issuer_id = sample(1:250, n(), replace = TRUE),\n    sic_code = sample(\n      sic$sic_code, n(), \n      prob = sic$probability, \n      replace = TRUE\n    )\n  )\n  \ndbWriteTable(\n  tidy_finance_dummy, \n  \"mergent\", \n  mergent_dummy, \n  overwrite = TRUE\n)\n\n\n\nDummy trace_enhanced data\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy mergent data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book.\n\nstart_date &lt;- as.Date(\"2014-01-01\")\nend_date &lt;- as.Date(\"2016-11-30\")\n\nbonds_panel &lt;- expand_grid(\n  mergent_dummy |&gt; \n    select(cusip_id = complete_cusip),\n  tibble(\n    trd_exctn_dt = seq(start_date, end_date, \"1 day\")\n  )\n)\n\ntrace_enhanced_dummy &lt;- bind_rows(\n  bonds_panel, bonds_panel, \n  bonds_panel, bonds_panel, \n  bonds_panel) |&gt; \n  mutate(\n    trd_exctn_tm = str_c(\n      sample(0:24, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE), \":\", \n      sample(0:60, n(), replace = TRUE)\n    ),\n    rptd_pr = runif(n(), 10, 200),\n    entrd_vol_qt = sample(1:20, n(), replace = TRUE) * 1000,\n    yld_pt = runif(n(), -10, 10),\n    rpt_side_cd = sample(c(\"B\", \"S\"), n(), replace = TRUE),\n    cntra_mp_id = sample(c(\"C\", \"D\"), n(), replace = TRUE)\n  ) \n  \ndbWriteTable(\n  tidy_finance_dummy, \n  \"trace_enhanced\", \n  trace_enhanced_dummy, \n  overwrite = TRUE\n)\n\nAs stated in the introduction, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books. You can find the database with the dummy data here."
  }
]