[
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support Tidy Finance",
    "section": "",
    "text": "Tidy Finance is and will remain an open-source project. We are grateful for all the support we have received so far. Of course, we do not force anybody to support us, but every gesture is very much appreciated. We have three options if you want to give something back and support our efforts. Moreover, most options come at no additional cost to you, i.e., they just increase our share of the pie. Who does not appreciate a little more pie?"
  },
  {
    "objectID": "support.html#get-your-copy-of-the-book",
    "href": "support.html#get-your-copy-of-the-book",
    "title": "Support Tidy Finance",
    "section": "Get your copy of the book",
    "text": "Get your copy of the book\n\nYou can read the free online version of Tidy Finance on this website. However, you can also get your own physical copy! The book comes with many perks, such as the joy of holding something in your hand, a fresh smell, and it certainly looks good in your library. If you decide to buy your own copy, please consider using our affiliate links below. No extra cost to you, just some pie for us. Pick your preferred outlet:\n\nRoutledge\nAmazon.com\nAmazon.co.uk\nAmazon.de\n\nNote that some affiliate links track your behavior on the site and can be flagged as suspicious by your browser. We exclusively use the official paths provided by the respective vendors."
  },
  {
    "objectID": "support.html#spread-the-word",
    "href": "support.html#spread-the-word",
    "title": "Support Tidy Finance",
    "section": "Spread the word",
    "text": "Spread the word\nThe project grows with the attention it receives from the community. Therefore, making people aware of Tidy Finance is a great way to support it. There are certainly many possibilities how you can spread the word. For example, you could\n\nContribute to the Tidy Finance blog\nCite the book in one of your projects\nUse Tidy Finance as a teaching resource and let us know\nConnect with us and share posts about Tidy Finance via social media\n\nThese are just a few suggestions, yet highly effective. In any case, we rely on your support to share Tidy Finance within your own community."
  },
  {
    "objectID": "support.html#buy-us-a-coffee",
    "href": "support.html#buy-us-a-coffee",
    "title": "Support Tidy Finance",
    "section": "Buy us a coffee",
    "text": "Buy us a coffee\nEvery task requires some fuel. In particular, one key ingredient to completing the mental efforts that culminate in Tidy Finance is, of course, coffee. Hence, if you appreciate Tidy Finance, let us have a coffee. We are grateful for every small contribution to sustain our caffeine levels. Moreover, higher caffeine levels positively correlate with new content on Tidy Finance. It is a win-win situation!"
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html",
    "href": "r/value-and-bivariate-sorts.html",
    "title": "Value and Bivariate Sorts",
    "section": "",
    "text": "In this chapter, we extend univariate portfolio analysis to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for the Fama and French three factors. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)"
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#data-preparation",
    "href": "r/value-and-bivariate-sorts.html#data-preparation",
    "title": "Value and Bivariate Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we load the necessary data from our SQLite-database introduced in Chapters 2-4. We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Chapter 8.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n    select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt; \n  collect() |&gt;\n  drop_na()\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date. To achieve this, we use the function floor_date().\n\nbook_equity &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(gvkey, datadate, be) |&gt;\n  collect() |&gt;\n  drop_na() |&gt;\n  mutate(month = floor_date(ymd(datadate), \"month\"))"
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#book-to-market-ratio",
    "href": "r/value-and-bivariate-sorts.html#book-to-market-ratio",
    "title": "Value and Bivariate Sorts",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias - we must not include data in forming a portfolio that is not public knowledge at the time. Of course, researchers have more information when looking into the past than agents had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nWe continue to lag market capitalization and firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.1\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fill()-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by group_by() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nme &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month %m+% months(1)) |&gt;\n  select(permno, sorting_date, me = mktcap)\n\nbm &lt;- book_equity |&gt;\n  inner_join(crsp_monthly, by = c(\"gvkey\", \"month\")) |&gt;\n  mutate(\n    bm = be / mktcap,\n    sorting_date = month %m+% months(6),\n    comp_date = sorting_date\n  ) |&gt;\n  select(permno, gvkey, sorting_date, comp_date, bm)\n\ndata_for_sorts &lt;- crsp_monthly |&gt;\n  left_join(bm, by = c(\"permno\", \"gvkey\", \"month\" = \"sorting_date\")) |&gt;\n  left_join(me, by = c(\"permno\", \"month\" = \"sorting_date\")) |&gt;\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap_lag, me, bm, exchange, comp_date\n  )\n\ndata_for_sorts &lt;- data_for_sorts |&gt;\n  arrange(permno, gvkey, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(bm, comp_date) |&gt;\n  ungroup() |&gt; \n  filter(comp_date &gt; month %m-% months(12)) |&gt;\n  select(-comp_date) |&gt;\n  drop_na()\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function allowing for the specification of exchanges to use for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             n_portfolios, \n                             exchanges) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange %in% exchanges) |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  return(assigned_portfolios)\n}\n\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis."
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#independent-sorts",
    "href": "r/value-and-bivariate-sorts.html#independent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Independent Sorts",
    "text": "Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_me, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_me = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"me\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    )) |&gt;\n  group_by(month, portfolio_bm, portfolio_me) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium &lt;- value_portfolios |&gt;\n  group_by(month, portfolio_bm) |&gt;\n  summarize(ret = mean(ret), .groups = \"drop_last\") |&gt;\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n                              ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)\n\n[1] 0.384\n\n\nThe resulting annualized value premium is 4.608 percent."
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#dependent-sorts",
    "href": "r/value-and-bivariate-sorts.html#dependent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Dependent Sorts",
    "text": "Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts ensure an equal amount of stocks within each portfolio.\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable = \"me\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(portfolio_me = assign_portfolio(\n    data = pick(everything()),\n    sorting_variable = \"me\",\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) |&gt;\n  group_by(month, portfolio_me) |&gt;\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = \"bm\",\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    )) |&gt;\n  group_by(month, portfolio_me, portfolio_bm) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )\n\nvalue_premium &lt;- value_portfolios |&gt;\n  group_by(month, portfolio_bm) |&gt;\n  summarize(ret = mean(ret), .groups = \"drop_last\") |&gt;\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] -\n    ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)\n\n[1] 0.329\n\n\nThe value premium from dependent sorts is 3.948 percent per year.\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Chapter 8, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices."
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#exercises",
    "href": "r/value-and-bivariate-sorts.html#exercises",
    "title": "Value and Bivariate Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nIn Chapter 8, we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\nWhen we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\nAs for the size premium, also the value premium constructed here does not follow Fama and French (1993). Implement a p-hacking setup as in Chapter 8 to find a premium that comes closest to their HML premium."
  },
  {
    "objectID": "r/value-and-bivariate-sorts.html#footnotes",
    "href": "r/value-and-bivariate-sorts.html#footnotes",
    "title": "Value and Bivariate Sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in the Chapter 8 on p-hacking.↩︎"
  },
  {
    "objectID": "r/trace-and-fisd.html",
    "href": "r/trace-and-fisd.html",
    "title": "TRACE and FISD",
    "section": "",
    "text": "In this chapter, we dive into the US corporate bond market. Bond markets are far more diverse than stock markets, as most issuers have multiple bonds outstanding simultaneously with potentially very different indentures. This market segment is exciting due to its size (roughly 10 trillion USD outstanding), heterogeneity of issuers (as opposed to government bonds), market structure (mostly over-the-counter trades), and data availability. We introduce how to use bond characteristics from FISD and trade reports from TRACE and provide code to download and clean TRACE in R.\nMany researchers study liquidity in the US corporate bond market O’Hara and Zhou (2021). We do not cover bond returns here, but you can compute them from TRACE data. Instead, we refer to studies on the topic such as Bessembinder et al. (2008), Bai, Bali, and Wen (2019), and Kelly, Palhares, and Pruitt (2021) and a survey by Huang and Shi (2021). Moreover, WRDS includes bond returns computed from TRACE data at a monthly frequency.\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(RSQLite)\nlibrary(RPostgres)\nlibrary(devtools)\nCompared to previous chapters, we load the devtools package (Wickham et al. 2022) to source code that we provided to the public via gist."
  },
  {
    "objectID": "r/trace-and-fisd.html#bond-data-from-wrds",
    "href": "r/trace-and-fisd.html#bond-data-from-wrds",
    "title": "TRACE and FISD",
    "section": "Bond Data from WRDS",
    "text": "Bond Data from WRDS\nBoth bond databases we need are available on WRDS to which we establish the RPostgres connection described in the previous chapter. Additionally, we connect to our local SQLite-database to store the data we download.\n\nwrds &lt;- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)"
  },
  {
    "objectID": "r/trace-and-fisd.html#mergent-fisd",
    "href": "r/trace-and-fisd.html#mergent-fisd",
    "title": "TRACE and FISD",
    "section": "Mergent FISD",
    "text": "Mergent FISD\nFor research on US corporate bonds, the Mergent Fixed Income Securities Database (FISD) is the primary resource for bond characteristics. There is a detailed manual on WRDS, so we only cover the necessary subjects here. FISD data comes in two main variants, namely, centered on issuers or issues. In either case, the most useful identifiers are CUSIPs. 9-digit CUSIPs identify securities issued by issuers. The issuers can be identified from the first six digits of a security CUSIP, which is also called 6-digit CUSIP. Both stocks and bonds have CUSIPs. This connection would, in principle, allow matching them easily, but due to changing issuer details, this approach only yields small coverage.\nWe use the issue-centered version of FISD to identify the subset of US corporate bonds that meet the standard criteria (Bessembinder, Maxwell, and Venkataraman 2006). The WRDS table fisd_mergedissue contains most of the information we need on a 9-digit CUSIP level. Due to the diversity of corporate bonds, details in the indenture vary significantly. We focus on common bonds that make up the majority of trading volume in this market without diverging too much in indentures.\nThe following chunk connects to the data and selects the bond sample to remove certain bond types that are less commonly (see, e.g., Dick-Nielsen, Feldhütter, and Lando 2012; O’Hara and Zhou 2021, among many others).\n\nmergent &lt;- tbl(\n  wrds,\n  in_schema(\"fisd\", \"fisd_mergedissue\")\n) |&gt;\n  filter(\n    security_level == \"SEN\", # senior bonds\n    slob == \"N\" | is.na(slob), # secured lease obligation\n    is.na(security_pledge), # unsecured bonds\n    asset_backed == \"N\" | is.na(asset_backed), # not asset backed\n    defeased == \"N\" | is.na(defeased), # not defeased\n    is.na(defeased_date),\n    bond_type %in% c(\n      \"CDEB\", # US Corporate Debentures\n      \"CMTN\", # US Corporate MTN (Medium Term Note)\n      \"CMTZ\", # US Corporate MTN Zero\n      \"CZ\", # US Corporate Zero,\n      \"USBN\" # US Corporate Bank Note\n    ), \n    pay_in_kind != \"Y\" | is.na(pay_in_kind), # not payable in kind\n    is.na(pay_in_kind_exp_date),\n    yankee == \"N\" | is.na(yankee), # no foreign issuer\n    canadian == \"N\" | is.na(canadian), # not Canadian\n    foreign_currency == \"N\", # USD\n    coupon_type %in% c(\n      \"F\", # fixed coupon\n      \"Z\" # zero coupon\n    ), \n    is.na(fix_frequency),\n    coupon_change_indicator == \"N\",\n    interest_frequency %in% c(\n      \"0\", # per year\n      \"1\",\n      \"2\",\n      \"4\",\n      \"12\"\n    ),\n    rule_144a == \"N\", # publicly traded\n    private_placement == \"N\" | is.na(private_placement),\n    defaulted == \"N\", # not defaulted\n    is.na(filing_date),\n    is.na(settlement),\n    convertible == \"N\", # not convertible\n    is.na(exchange),\n    putable == \"N\" | is.na(putable), # not putable\n    unit_deal == \"N\" | is.na(unit_deal), # not issued with another security\n    exchangeable == \"N\" | is.na(exchangeable), # not exchangeable\n    perpetual == \"N\", # not perpetual\n    preferred_security == \"N\" | is.na(preferred_security) # not preferred\n  ) |&gt; \n  select(\n    complete_cusip, maturity,\n    offering_amt, offering_date,\n    dated_date, \n    interest_frequency, coupon,\n    last_interest_date, \n    issue_id, issuer_id\n  ) |&gt;\n  collect()\n\nWe also pull issuer information from fisd_mergedissuer regarding the industry and country of the firm that issued a particular bond. Then, we filter to include only US-domiciled firms’ bonds. We match the data by issuer_id.\n\nmergent_issuer &lt;- tbl(wrds, in_schema(\"fisd\", \"fisd_mergedissuer\")) |&gt;\n  select(issuer_id, sic_code, country_domicile) |&gt;\n  collect()\n\nmergent &lt;- mergent |&gt;\n  inner_join(mergent_issuer, by = \"issuer_id\") |&gt;\n  filter(country_domicile == \"USA\") |&gt;\n  select(-country_domicile)\n\nFinally, we save the bond characteristics to our local database. This selection of bonds also constitutes the sample for which we will collect trade reports from TRACE below.\n\n  dbWriteTable(\n    conn = tidy_finance,\n    name = \"mergent\",\n    value = mergent,\n    overwrite = TRUE\n  )\n\nThe FISD database also contains other data. The issue-based file contains information on covenants, i.e., restrictions included in bond indentures to limit specific actions by firms (e.g., Handler, Jankowitsch, and Weiss 2021). Moreover, FISD also provides information on bond ratings. We do not need either here."
  },
  {
    "objectID": "r/trace-and-fisd.html#trace",
    "href": "r/trace-and-fisd.html#trace",
    "title": "TRACE and FISD",
    "section": "TRACE",
    "text": "TRACE\nThe Financial Industry Regulatory Authority (FINRA) provides the Trade Reporting and Compliance Engine (TRACE). In TRACE, dealers that trade corporate bonds must report such trades individually. Hence, we observe trade messages in TRACE that contain information on the bond traded, the trade time, price, and volume. TRACE comes in two variants; standard and enhanced TRACE. We show how to download and clean enhanced TRACE as it contains uncapped volume, a crucial quantity missing in the standard distribution. Moreover, enhanced TRACE also provides information on the respective parties’ roles and the direction of the trade report. These items become essential in cleaning the messages.\nWhy do we repeatedly talk about cleaning TRACE? Trade messages are submitted within a short time window after a trade is executed (less than 15 minutes). These messages can contain errors, and the reporters subsequently correct them or they cancel a trade altogether. The cleaning needs are described by Dick-Nielsen (2009) in detail, and Dick-Nielsen (2014) shows how to clean the enhanced TRACE data using SAS. We do not go into the cleaning steps here, since the code is lengthy and serves no educational purpose. However, downloading and cleaning enhanced TRACE data is straightforward with our setup.\nWe store code for cleaning enhanced TRACE with R on the following Github gist. as a function. The appendix also contains the code for reference. We only need to source the code from the gist, which we can do with source_gist(). Alternatively, you can also go to the gist, download it, and source() the respective R-file. The clean_enhanced_trace() function takes a vector of CUSIPs, a connection to WRDS explained in Chapter 3, and a start and end date, respectively.\n\nsource_gist(\"3a05b3ab281563b2e94858451c2eb3a4\")\n\nThe TRACE database is considerably large. Therefore, we only download subsets of data at once. Specifying too many CUSIPs over a long time horizon will result in very long download times and a potential failure due to the size of the request to WRDS. The size limit depends on many parameters, and we cannot give you a guideline here. If we were working with the complete TRACE data for all CUSIPs above, splitting the data into 100 parts takes roughly two hours using our setup. For the applications in this book, we need data around the Paris Agreement in December 2015 and download the data in ten sets, which we define below.\n\nmergent_cusips &lt;- mergent |&gt;\n  pull(complete_cusip)\n\nmergent_parts &lt;- split(\n  mergent_cusips,\n  rep(1:10, \n      length.out = length(mergent_cusips))\n)\n\nFinally, we run a loop in the same style as in Chapter 3 where we download daily returns from CRSP. For each of the CUSIP sets defined above, we call the cleaning function and save the resulting output. We add new data to the existing table for batch two and all following batches.\n\nfor (j in 1:length(mergent_parts)) {\n  trace_enhanced &lt;- clean_enhanced_trace(\n    cusips = mergent_parts[[j]],\n    connection = wrds,\n    start_date = ymd(\"2014-01-01\"),\n    end_date = ymd(\"2016-11-30\")\n  )\n\n  dbWriteTable(\n      conn = tidy_finance,\n      name = \"trace_enhanced\",\n      value = trace_enhanced,\n      overwrite = ifelse(j == 1, TRUE, FALSE),\n      append = ifelse(j != 1, TRUE, FALSE)\n    )\n\n}"
  },
  {
    "objectID": "r/trace-and-fisd.html#insights-into-corporate-bonds",
    "href": "r/trace-and-fisd.html#insights-into-corporate-bonds",
    "title": "TRACE and FISD",
    "section": "Insights into Corporate Bonds",
    "text": "Insights into Corporate Bonds\nWhile many news outlets readily provide information on stocks and the underlying firms, corporate bonds are not covered frequently. Additionally, the TRACE database contains trade-level information, potentially new to students. Therefore, we provide you with some insights by showing some summary statistics.\nWe start by looking into the number of bonds outstanding over time and compare it to the number of bonds traded in our sample. First, we compute the number of bonds outstanding for each quarter around the Paris Agreement from 2014 to 2016.\n\nbonds_outstanding &lt;- expand_grid(\"date\" = seq(ymd(\"2014-01-01\"),\n                                              ymd(\"2016-11-30\"), \n                                              by = \"quarter\"), \n                                 \"complete_cusip\" = mergent$complete_cusip) |&gt; \n  left_join(mergent |&gt; select(complete_cusip, \n                              offering_date,\n                              maturity), \n            by = \"complete_cusip\") |&gt; \n  mutate(offering_date = floor_date(offering_date),\n         maturity = floor_date(maturity)) |&gt; \n  filter(date &gt;= offering_date & date &lt;= maturity) |&gt; \n  count(date) |&gt; \n  mutate(type = \"Outstanding\")\n\nNext, we look at the bonds traded each quarter in the same period. Notice that we load the complete trace table from our database, as we only have a single part of it in the environment from the download loop from above.\n\ntrace_enhanced &lt;- tbl(tidy_finance, \"trace_enhanced\") |&gt;\n  collect()\n\nbonds_traded &lt;- trace_enhanced |&gt; \n  mutate(date = floor_date(trd_exctn_dt, \"quarters\")) |&gt; \n  group_by(date) |&gt; \n  summarize(n = length(unique(cusip_id)),\n            type = \"Traded\",\n            .groups = \"drop\") \n\nFinally, we plot the two time series in Figure 1.\n\nbonds_outstanding |&gt; \n  bind_rows(bonds_traded) |&gt; \n  ggplot(aes(\n    x = date, \n    y = n, \n    color = type, \n    linetype = type\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Number of bonds outstanding and traded each quarter\"\n  )\n\n\n\n\nFigure 1: The number of corporate bonds outstanding each quarter as reported by Mergent FISD and the number of traded bonds from enhanced TRACE between 2014 and end of 2016.\n\n\n\n\nWe see that the number of bonds outstanding increases steadily between 2014 and 2016. During our sample period of trade data, we see that the fraction of bonds trading each quarter is roughly 60%. The relatively small number of traded bonds means that many bonds do not trade through an entire quarter. This lack of trading activity illustrates the generally low level of liquidity in the corporate bond market, where it can be hard to trade specific bonds. Does this lack of liquidity mean that corporate bond markets are irrelevant in terms of their size? With over 7,500 traded bonds each quarter, it is hard to say that the market is small. However, let us also investigate the characteristics of issued corporate bonds. In particular, we consider maturity (in years), coupon, and offering amount (in million USD).\n\nmergent |&gt;\n  mutate(maturity = as.numeric(maturity - offering_date) / 365,\n         offering_amt = offering_amt / 10^3) |&gt; \n  pivot_longer(cols = c(maturity, coupon, offering_amt),\n               names_to = \"measure\") |&gt;\n  drop_na() |&gt; \n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )\n\n# A tibble: 3 × 8\n  measure        mean     sd   min   q05    q50     q95    max\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 coupon         5.92   2.66 0     1.85    6.12    9.88    39 \n2 maturity       9.93   9.22 0.164 1.09    7.65   30.0    100.\n3 offering_amt 382.   570.   0.001 0.801 200    1400    15000 \n\n\nWe see that the average bond in our sample period has an offering amount of over 357 million USD with a median of 200 million USD, which both cannot be considered small. The average bond has a maturity of 10 years and pays around 6% in coupons.\nFinally, let us compute some summary statistics for the trades in this market. To this end, we show a summary based on aggregate information daily. In particular, we consider the trade size (in million USD) and the number of trades.\n\ntrace_enhanced |&gt; \n  group_by(trd_exctn_dt) |&gt; \n  summarize(trade_size = sum(entrd_vol_qt * rptd_pr / 100) / 10^6,\n            trade_number = n(),\n            .groups = \"drop\") |&gt; \n  pivot_longer(cols = c(trade_size, trade_number),\n               names_to = \"measure\") |&gt; \n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value)\n  )\n\n# A tibble: 2 × 8\n  measure        mean    sd   min    q05    q50    q95    max\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 trade_number 25921. 5460. 438   17851. 26025  34458. 40889 \n2 trade_size   12968. 3574.  17.2  6138. 13408. 17851. 20905.\n\n\nOn average, nearly 26 billion USD of corporate bonds are traded daily in nearly 13,000 transactions. We can hence conclude that the corporate bond market is indeed significant in terms of trading volume and activity."
  },
  {
    "objectID": "r/trace-and-fisd.html#exercises",
    "href": "r/trace-and-fisd.html#exercises",
    "title": "TRACE and FISD",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize the amount outstanding of all bonds over time and describe the resulting graph.\nCompute the number of days each bond is traded (accounting for the bonds’ maturities and issuances). Start by looking at the number of bonds traded each day in a graph similar to the one above. How many bonds trade on more than 75% of trading days?\nWRDS provides more information from Mergent FISD. In particular, they also provide rating information in fisd_ratings. Download the ratings for the bond sample. Then, plot the distribution of ratings in a histogram.\nDownload the TRACE data until the end of September 2021. Hint: you want to download the data completely new, rather than adding the new information. Can you find a reason why?"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html",
    "href": "r/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama and French Factors",
    "section": "",
    "text": "In this chapter, we provide a replication of the famous Fama and French factor portfolios. The Fama and French three-factor model (see Fama and French 1993) is a cornerstone of asset pricing. On top of the market factor represented by the traditional CAPM beta, the model includes the size and value factors to explain the cross section of returns. We introduce both factors in Chapter 9, and their definition remains the same. Size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#data-preparation",
    "href": "r/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama and French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it. Hence, there is nothing new below and we only load data from our SQLite-database introduced in Chapters 2-4.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ndata_ff &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(\n    permno, gvkey, month, ret_excess,\n    mktcap, mktcap_lag, exchange\n  ) |&gt;\n  collect() |&gt;\n  drop_na()\n\nbook_equity &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n    select(gvkey, datadate, be) |&gt;\n    collect() |&gt;\n    drop_na()\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  select(month, smb, hml) |&gt;\n  collect()\n\nYet when we start merging our data set for computing the premiums, there are a few differences to Chapter 9. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity.\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of distinct() at the end of the chunk below.\n\nme_ff &lt;- data_ff |&gt;\n  filter(month(month) == 6) |&gt;\n  mutate(sorting_date = month %m+% months(1)) |&gt;\n  select(permno, sorting_date, me_ff = mktcap)\n\nme_ff_dec &lt;- data_ff |&gt;\n  filter(month(month) == 12) |&gt;\n  mutate(sorting_date = ymd(str_c(year(month) + 1, \"0701)\"))) |&gt;\n  select(permno, gvkey, sorting_date, bm_me = mktcap)\n\nbm_ff &lt;- book_equity |&gt;\n  mutate(sorting_date = ymd(str_c(year(datadate) + 1, \"0701\"))) |&gt;\n  select(gvkey, sorting_date, bm_be = be) |&gt;\n  inner_join(me_ff_dec, by = c(\"gvkey\", \"sorting_date\")) |&gt;\n  mutate(bm_ff = bm_be / bm_me) |&gt;\n  select(permno, sorting_date, bm_ff)\n\nvariables_ff &lt;- me_ff |&gt;\n  inner_join(bm_ff, by = c(\"permno\", \"sorting_date\")) |&gt;\n  drop_na() |&gt;\n  distinct(permno, sorting_date, .keep_all = TRUE)"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "r/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama and French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30%- and 70%-percentiles, and they use independent sorts. The sorts for book-to-market require an adjustment to the function in Chapter 9 because the seq() we would produce does not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = c(0, 0.3, 0.7, 1) to the function. Additionally, we perform an inner_join() with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             percentiles) {\n  breakpoints &lt;- data |&gt;\n    filter(exchange == \"NYSE\") |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = percentiles,\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  return(assigned_portfolios)\n}\n\nportfolios_ff &lt;- variables_ff |&gt;\n  inner_join(data_ff, by = c(\"permno\" = \"permno\", \"sorting_date\" = \"month\")) |&gt;\n  group_by(sorting_date) |&gt;\n  mutate(\n    portfolio_me = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = me_ff,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = bm_ff,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) |&gt;\n  select(permno, sorting_date, portfolio_me, portfolio_bm)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios_ff &lt;- data_ff |&gt;\n  mutate(sorting_date = case_when(\n    month(month) &lt;= 6 ~ ymd(str_c(year(month) - 1, \"0701\")),\n    month(month) &gt;= 7 ~ ymd(str_c(year(month), \"0701\"))\n  )) |&gt;\n  inner_join(portfolios_ff, by = c(\"permno\", \"sorting_date\"))"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#fama-and-french-factor-returns",
    "href": "r/replicating-fama-and-french-factors.html#fama-and-french-factor-returns",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Factor Returns",
    "text": "Fama and French Factor Returns\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama and French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally.\n\nfactors_ff_monthly_replicated &lt;- portfolios_ff |&gt;\n  group_by(portfolio_me, portfolio_bm, month) |&gt;\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_me = unique(portfolio_me),\n    portfolio_bm = unique(portfolio_bm), .groups = \"drop\"\n  ) |&gt;\n  group_by(month) |&gt;\n  summarize(\n    smb_replicated = mean(ret[portfolio_me == 1]) -\n      mean(ret[portfolio_me == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) -\n      mean(ret[portfolio_bm == 1])\n  )"
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "r/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama and French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. However, we did not follow their procedure strictly. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using lm(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest &lt;- factors_ff_monthly |&gt;\n  inner_join(factors_ff_monthly_replicated, by = \"month\") |&gt;\n  mutate(\n    smb_replicated = round(smb_replicated, 4),\n    hml_replicated = round(hml_replicated, 4)\n  )\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient and R-squared are at 99%.\n\nsummary(lm(smb ~ smb_replicated, data = test))\n\n\nCall:\nlm(formula = smb ~ smb_replicated, data = test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02035 -0.00159  0.00001  0.00156  0.01483 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.000129   0.000131   -0.98     0.33    \nsmb_replicated  0.995344   0.004344  229.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00353 on 724 degrees of freedom\nMultiple R-squared:  0.986, Adjusted R-squared:  0.986 \nF-statistic: 5.25e+04 on 1 and 724 DF,  p-value: &lt;2e-16\n\n\nThe replication of the HML factor is also a success, although at a slightly lower level with coefficient and R-squared around 96%.\n\nsummary(lm(hml ~ hml_replicated, data = test))\n\n\nCall:\nlm(formula = hml ~ hml_replicated, data = test)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.023348 -0.002983 -0.000091  0.002269  0.028783 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.000317   0.000219    1.45     0.15    \nhml_replicated 0.965357   0.007477  129.12   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00587 on 724 degrees of freedom\nMultiple R-squared:  0.958, Adjusted R-squared:  0.958 \nF-statistic: 1.67e+04 on 1 and 724 DF,  p-value: &lt;2e-16\n\n\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French premiums, although we cannot see their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data."
  },
  {
    "objectID": "r/replicating-fama-and-french-factors.html#exercises",
    "href": "r/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama and French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Pick one of them, e.g. OP (operating profitability) and try to replicate the portfolio sort return time series provided on his homepage."
  },
  {
    "objectID": "r/parametric-portfolio-policies.html",
    "href": "r/parametric-portfolio-policies.html",
    "title": "Parametric Portfolio Policies",
    "section": "",
    "text": "In this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of Brandt, Santa-Clara, and Valkanov (2009) proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock’s expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of the characteristics, which maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brandt, Santa-Clara, and Valkanov (2009). See the review paper Brandt (2010) for an excellent treatment of related portfolio choice methods.\nThe current chapter relies on the following set of packages:\nlibrary(tidyverse)\nlibrary(RSQLite)"
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#data-preparation",
    "href": "r/parametric-portfolio-policies.html#data-preparation",
    "title": "Parametric Portfolio Policies",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite-database introduced in Chapters 2-4.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, ret_excess, mktcap, mktcap_lag) |&gt;\n  collect()\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  select(month, mkt_excess) |&gt;\n  collect()\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. In particular, we record the lagged one-year return momentum (momentum_lag), defined as the compounded return between months \\(t-13\\) and \\(t-2\\) for each firm. In finance, momentum is the empirically observed tendency for rising asset prices to rise further, and falling prices to keep falling (Jegadeesh and Titman 1993). The second characteristic is the firm’s market equity (size_lag), defined as the log of the price per share times the number of shares outstanding (Banz 1981). To construct the correct lagged values, we use the approach introduced in Chapter 3.\n\ncrsp_monthly_lags &lt;- crsp_monthly |&gt;\n  transmute(permno,\n    month_13 = month %m+% months(13),\n    mktcap\n  )\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  inner_join(crsp_monthly_lags,\n    by = c(\"permno\", \"month\" = \"month_13\"),\n    suffix = c(\"\", \"_13\")\n  )\n\ndata_portfolios &lt;- crsp_monthly |&gt;\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_13,\n    size_lag = log(mktcap_lag)\n  ) |&gt;\n  drop_na(contains(\"lag\"))"
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "href": "r/parametric-portfolio-policies.html#parametric-portfolio-policies",
    "title": "Parametric Portfolio Policies",
    "section": "Parametric Portfolio Policies",
    "text": "Parametric Portfolio Policies\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date \\(t\\) we have \\(N_t\\) stocks in the investment universe, where each stock \\(i\\) has a return of \\(r_{i, t+1}\\) and is associated with a vector of firm characteristics \\(x_{i, t}\\) such as time-series momentum or the market capitalization. The investor’s problem is to choose portfolio weights \\(w_{i,t}\\) to maximize the expected utility of the portfolio return: \\[\\begin{aligned}\n\\max_{\\omega} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}\\omega_{i,t}r_{i,t+1}\\right)\\right]\n\\end{aligned}\\] where \\(u(\\cdot)\\) denotes the utility function.\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic \\(x_{i,t}\\) with the following linear specification for the portfolio weights: \\[\\omega_{i,t} = \\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t},\\] where \\(\\bar{\\omega}_{i,t}\\) is a stock’s weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), \\(\\theta\\) is a vector of coefficients which we are going to estimate, and \\(\\hat{x}_{i,t}\\) are the characteristics of stock \\(i\\), cross-sectionally standardized to have zero mean and unit standard deviation.\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction the weights sum up to one as \\(\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0\\) due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization relative_mktcap, which will represent the value-weighted benchmark portfolio, while n denotes the number of traded assets \\(N_t\\), which we use to construct the naive portfolio benchmark.\n\ndata_portfolios &lt;- data_portfolios |&gt;\n  group_by(month) |&gt;\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) |&gt;\n  ungroup() |&gt;\n  select(-mktcap_lag)"
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#computing-portfolio-weights",
    "href": "r/parametric-portfolio-policies.html#computing-portfolio-weights",
    "title": "Parametric Portfolio Policies",
    "section": "Computing Portfolio Weights",
    "text": "Computing Portfolio Weights\nNext, we move on to identify optimal choices of \\(\\theta\\). We rewrite the optimization problem together with the weight parametrization and can then estimate \\(\\theta\\) to maximize the objective function based on our sample \\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned}\\] The allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the \\(N_t\\) dimensional vector of expected returns and the \\(N_t(N_t+1)/2\\) free elements of the covariance matrix, all we need to focus on in our application is the vector \\(\\theta\\). \\(\\theta\\) contains only two elements in our application - the relative deviation from the benchmark due to size and momentum.\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector \\(\\theta_0\\). The next step is to choose \\(\\theta\\) optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values.\n\nn_parameters &lt;- sum(str_detect(\n  colnames(data_portfolios), \"lag\"\n))\n\ntheta &lt;- rep(1.5, n_parameters)\n\nnames(theta) &lt;- colnames(data_portfolios)[str_detect(\n  colnames(data_portfolios), \"lag\"\n)]\n\nThe function compute_portfolio_weights() below computes the portfolio weights \\(\\bar{\\omega}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\) according to our parametrization for a given value \\(\\theta_0\\). Everything happens within a single pipeline. Hence, we provide a short walk-through.\nWe first compute characteristic_tilt, the tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}\\) which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio weight_benchmark, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. weight_tilt completes the picture and contains the final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt which deviate from the benchmark portfolio depending on the stock characteristics.\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again: \\[\\omega_{i,t}^+ = \\frac{\\max(0, \\omega_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, \\omega_{i,t})}.\\]\nThe following function computes the optimal portfolio weights in the way just described.\n\ncompute_portfolio_weights &lt;- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data |&gt;\n    group_by(month) |&gt;\n    bind_cols(\n      characteristic_tilt = data |&gt;\n        transmute(across(contains(\"lag\"), ~ . / n)) |&gt;\n        as.matrix() %*% theta |&gt; as.numeric()\n    ) |&gt;\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) |&gt;\n    ungroup()\n}\n\nIn the next step, we compute the portfolio weights for the arbitrary vector \\(\\theta_0\\). In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\nweights_crsp &lt;- compute_portfolio_weights(\n  theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)"
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#portfolio-performance",
    "href": "r/parametric-portfolio-policies.html#portfolio-performance",
    "title": "Parametric Portfolio Policies",
    "section": "Portfolio Performance",
    "text": "Portfolio Performance\n Are the computed weights optimal in any way? Most likely not, as we picked \\(\\theta_0\\) arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, Brandt, Santa-Clara, and Valkanov (2009) focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^{(1-\\gamma)}}{1-\\gamma}\\), where \\(\\gamma\\) is the risk aversion factor.\n\npower_utility &lt;- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\n\nWe want to note that Gehrig, Sögner, and Westerkamp (2020) warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: the original paper Brandt, Santa-Clara, and Valkanov (2009) only cares about the expected utility to choose \\(\\theta\\). However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\nevaluate_portfolio &lt;- function(weights_crsp,\n                               full_evaluation = TRUE) {\n  evaluation &lt;- weights_crsp |&gt;\n    group_by(month) |&gt;\n    summarize(\n      return_tilt = weighted.mean(ret_excess, weight_tilt),\n      return_benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) |&gt;\n    pivot_longer(-month,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) |&gt;\n    group_by(model) |&gt;\n    left_join(factors_ff_monthly, by = \"month\") |&gt;\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(12 * portfolio_return),\n      \"SD return\" = 100 * sqrt(12) * sd(portfolio_return),\n      \"Sharpe ratio\" = sqrt(12) * mean(portfolio_return) / sd(portfolio_return),\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n    )) |&gt;\n    mutate(model = str_remove(model, \"return_\")) |&gt;\n    pivot_longer(-model, names_to = \"measure\") |&gt;\n    pivot_wider(names_from = model, values_from = value)\n\n  if (full_evaluation) {\n    weight_evaluation &lt;- weights_crsp |&gt;\n      select(month, contains(\"weight\")) |&gt;\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") |&gt;\n      group_by(model, month) |&gt;\n      transmute(tibble(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight &lt; 0]),\n        \"Avg. fraction of negative weights\" = sum(weight &lt; 0) / n()\n      )) |&gt;\n      group_by(model) |&gt;\n      summarize(across(-month, ~ 100 * mean(.))) |&gt;\n      mutate(model = str_remove(model, \"weight_\")) |&gt;\n      pivot_longer(-model, names_to = \"measure\") |&gt;\n      pivot_wider(names_from = model, values_from = value)\n    evaluation &lt;- bind_rows(evaluation, weight_evaluation)\n  }\n  return(evaluation)\n}\n\n Let us take a look at the different portfolio strategies and evaluation measures.\n\nevaluate_portfolio(weights_crsp) |&gt;\n  print(n = Inf)\n\n# A tibble: 11 × 3\n   measure                            benchmark     tilt\n   &lt;chr&gt;                                  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Expected utility                  -0.249     -0.261  \n 2 Average return                     7.08       0.166  \n 3 SD return                         15.3       20.9    \n 4 Sharpe ratio                       0.463      0.00793\n 5 CAPM alpha                         0.000129  -0.00538\n 6 Market beta                        0.992      0.950  \n 7 Absolute weight                    0.0249     0.0637 \n 8 Max. weight                        3.55       3.68   \n 9 Min. weight                        0.0000277 -0.145  \n10 Avg. sum of negative weights       0         77.9    \n11 Avg. fraction of negative weights  0         49.5    \n\n\nThe value-weighted portfolio delivers an annualized return of more than 6 percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically 1 for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions."
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#optimal-parameter-choice",
    "href": "r/parametric-portfolio-policies.html#optimal-parameter-choice",
    "title": "Parametric Portfolio Policies",
    "section": "Optimal Parameter Choice",
    "text": "Optimal Parameter Choice\nNext, we move to a choice of \\(\\theta\\) that actually aims to improve some (or all) of the performance measures. We first define a helper function compute_objective_function(), which we then pass to an optimizer.\n\ncompute_objective_function &lt;- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data &lt;- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function &lt;- evaluate_portfolio(processed_data,\n    full_evaluation = FALSE\n  ) |&gt;\n    filter(measure == objective_measure) |&gt;\n    pull(tilt)\n\n  return(-objective_function)\n}\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, R optimization relies on the function optim(). As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of \\(\\hat\\theta\\), which maximize the hypothetical expected utility of the investor.\n\noptimal_theta &lt;- optim(\n  par = theta,\n  compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)\n\noptimal_theta$par\n\nmomentum_lag     size_lag \n        0.31        -1.99 \n\n\nThe resulting values of \\(\\hat\\theta\\) are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect (Banz 1981) and the momentum anomaly (Jegadeesh and Titman 1993)."
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#more-model-specifications",
    "href": "r/parametric-portfolio-policies.html#more-model-specifications",
    "title": "Parametric Portfolio Policies",
    "section": "More Model Specifications",
    "text": "More Model Specifications\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\nfull_model_grid &lt;- expand_grid(\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE),\n  data = list(data_portfolios)\n) |&gt;\n  mutate(optimal_theta = pmap(\n    .l = list(\n      data,\n      value_weighting,\n      allow_short_selling\n    ),\n    .f = ~ optim(\n      par = theta,\n      compute_objective_function,\n      data = ..1,\n      objective_measure = \"Expected utility\",\n      value_weighting = ..2,\n      allow_short_selling = ..3\n    )$par\n  ))\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility.\n\nperformance_table &lt;- full_model_grid |&gt;\n  mutate(\n    processed_data = pmap(\n      .l = list(\n        optimal_theta,\n        data,\n        value_weighting,\n        allow_short_selling\n      ),\n      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)\n    ),\n    portfolio_evaluation = map(processed_data,\n      evaluate_portfolio,\n      full_evaluation = TRUE\n    )\n  ) |&gt;\n  select(\n    value_weighting,\n    allow_short_selling,\n    portfolio_evaluation\n  ) |&gt;\n  unnest(portfolio_evaluation)\n\nperformance_table |&gt;\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) |&gt;\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) |&gt;\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) |&gt;\n  select(\n    measure,\n    `EW    `,\n    `VW    `,\n    sort(contains(\"Optimal\"))\n  ) |&gt;\n  print(n = 11)\n\n# A tibble: 11 × 7\n   measure     `EW    ` `VW    ` `VW  Optimal ` `VW (no s.) Optimal `\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Expected u… -0.250   -2.49e-1       -0.246                -0.247  \n 2 Average re… 10.7      7.08e+0       14.9                  13.6    \n 3 SD return   20.3      1.53e+1       20.5                  19.5    \n 4 Sharpe rat…  0.525    4.63e-1        0.727                 0.698  \n 5 CAPM alpha   0.00231  1.29e-4        0.00659               0.00531\n 6 Market beta  1.13     9.92e-1        1.01                  1.04   \n 7 Absolute w…  0.0249   2.49e-2        0.0381                0.0249 \n 8 Max. weight  0.0249   3.55e+0        3.37                  2.69   \n 9 Min. weight  0.0249   2.77e-5       -0.0352                0      \n10 Avg. sum o…  0        0             27.4                   0      \n11 Avg. fract…  0        0             38.6                   0      \n# ℹ 2 more variables: `EW  Optimal ` &lt;dbl&gt;,\n#   `EW (no s.) Optimal ` &lt;dbl&gt;\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application."
  },
  {
    "objectID": "r/parametric-portfolio-policies.html#exercises",
    "href": "r/parametric-portfolio-policies.html#exercises",
    "title": "Parametric Portfolio Policies",
    "section": "Exercises",
    "text": "Exercises\n\nHow do the estimated parameters \\(\\hat\\theta\\) and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\nThe code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient \\(\\hat\\theta_i\\).\nTweak the function optimal_theta() such that you can impose additional performance constraints in order to determine \\(\\hat\\theta\\), which maximizes expected utility under the constraint that the market beta is below 1.\nDoes the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating \\(\\hat\\theta\\) based on past data only. Then, use more recent periods to evaluate the actual portfolio performance.\nBy formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. Brandt, Santa-Clara, and Valkanov (2009) provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for \\(\\hat\\theta\\)."
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html",
    "href": "r/option-pricing-via-machine-learning.html",
    "title": "Option Pricing via Machine Learning",
    "section": "",
    "text": "This chapter covers machine learning methods in option pricing. First, we briefly introduce regression trees, random forests, and neural networks - these methods are advocated as highly flexible universal approximators, capable of recovering highly nonlinear structures in the data. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues. We show how to implement random forests and deep neural networks with tidy principles using tidymodels or TensorFlow for more complicated network structures.\nMachine learning (ML) is seen as a part of artificial intelligence. ML algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so. While ML can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to a simple application in option pricing. More specifically, we create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to learn how to price call options without prior knowledge of the theoretical underpinnings of the famous option pricing equation by Black and Scholes (1973).\nThroughout this chapter, we need the following packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(keras)\nlibrary(hardhat)\nlibrary(ranger)\nlibrary(glmnet)\nThe package keras (Allaire and Chollet 2022) is a high-level neural networks API developed with a focus on enabling fast experimentation with Tensorflow. The package ranger (Wright and Ziegler 2017) provides a fast implementation for random forests and hardhat (Vaughan and Kuhn 2022) is a helper function to for robust data preprocessing at fit time and prediction time."
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "href": "r/option-pricing-via-machine-learning.html#regression-trees-and-random-forests",
    "title": "Option Pricing via Machine Learning",
    "section": "Regression Trees and Random Forests",
    "text": "Regression Trees and Random Forests\nRegression trees are a popular ML approach for incorporating multiway predictor interactions. In Finance, regression trees are gaining popularity, also in the context of asset pricing (see, e.g. Bryzgalova, Pelger, and Zhu 2022). Trees possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each other. A tree grows in a sequence of steps. At each step, a new branch sorts the data leftover from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into partitions and approximates the unknown function \\(f(x)\\) which yields the relation between the predictors \\(x\\) and the outcome variable \\(y\\) with the average value of the outcome variable within each partition. For a more thorough treatment of regression trees, we refer to Coqueret and Guida (2020).\nFormally, we partition the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For any predictor \\(x\\) that falls within region \\(R_j\\), we estimate \\(f(x)\\) with the average of the training observations, \\(\\hat y_i\\), for which the associated predictor \\(x_i\\) is also in \\(R_j\\). Once we select a partition \\(x\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, called \\(R_1(j,s)\\) and \\(R_2(j,s)\\), which split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\): \\[R_1(j,s) = \\{x \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{x \\mid x_j \\geq s\\}.\\] To pick \\(j\\) and \\(s\\), we find the pair that minimizes the residual sum of square (RSS): \\[\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\\] As in Chapter 14 in the context of penalized regressions, the first relevant question is: what are the hyperparameter decisions? Instead of a regularization parameter, trees are fully determined by the number of branches used to generate a partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).\nModels with a single tree may suffer from high predictive variance. Random forests address these shortcomings of decision trees. The goal is to improve the predictive performance and reduce instability by averaging multiple decision trees. A forest basically implies creating many regression trees and averaging their predictions. To assure that the individual trees are not the same, we use a bootstrap to induce randomness. More specifically, we build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using the training sample. For that purpose, we randomly select features to be included in the building of each tree. For each observation in the test set we then form a prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{i=1}^B\\hat{y}_{T_i}\\)."
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#neural-networks",
    "href": "r/option-pricing-via-machine-learning.html#neural-networks",
    "title": "Option Pricing via Machine Learning",
    "section": "Neural Networks",
    "text": "Neural Networks\nRoughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as universal approximators for any smooth predictive association (Hornik 1991). Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized ML tools. In finance, applications of neural networks can be found in the context of many different contexts, e.g. Avramov, Cheng, and Metzker (2022), Chen, Pelger, and Zhu (2019), and Gu, Kelly, and Xiu (2020).\nEach neuron applies a nonlinear activation function \\(f\\) to its aggregated signal before sending its output to the next layer \\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\] Here, \\(\\theta\\) are the parameters to fit, \\(N^l\\) denotes the number of units (a hyperparameter to tune), and \\(z_j\\) are the input variables which can be either the raw data or, in the case of multiple chained layers, the outcome from a previous layers \\(z_j = x_k-1\\). While the easiest case with \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions are sigmoid (i.e., \\(f(x) = (1+e^{-x})^{-1}\\)) or ReLu (i.e., \\(f(x) = max(x, 0)\\)).\nNeural networks gain their flexibility from chaining multiple layers together. Naturally, this imposes many degrees of freedom on the network architecture for which no clear theoretical guidance exists. The specification of a neural network requires, at a minimum, a stance on depth (number of hidden layers), the activation function, the number of neurons, the connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, learning means to choose optimal parameters relying on numerical optimization, which often requires specifying an appropriate learning rate. Despite these computational challenges, implementation in R is not tedious at all because we can use the API to TensorFlow."
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#option-pricing",
    "href": "r/option-pricing-via-machine-learning.html#option-pricing",
    "title": "Option Pricing via Machine Learning",
    "section": "Option Pricing",
    "text": "Option Pricing\nTo apply ML methods in a relevant field of finance, we focus on option pricing. The application in its core is taken from Hull (2020). In its most basic form, call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price \\(K\\)) at a specific date (the exercise date \\(T\\)). The Black–Scholes price (Black and Scholes 1973) of a call option for a non-dividend-paying underlying stock is given by \\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right)\n\\end{aligned}\n\\] where \\(C(S, T)\\) is the price of the option as a function of today’s stock price of the underlying, \\(S\\), with time to maturity \\(T\\), \\(r_f\\) is the risk-free interest rate, and \\(\\sigma\\) is the volatility of the underlying stock return. \\(\\Phi\\) is the cumulative distribution function of a standard normal random variable.\nThe Black-Scholes equation provides a way to compute the arbitrage-free price of a call option once the parameters \\(S, K, r_f, T\\), and \\(\\sigma\\) are specified (arguably, in a realistic context, all parameters are easy to specify except for \\(\\sigma\\) which has to be estimated). A simple R function allows computing the price as we do below.\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  d1 &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  value &lt;- S * pnorm(d1) - K * exp(-r * T) * pnorm(d1 - sigma * sqrt(T))\n\n  return(value)\n}"
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#learning-black-scholes",
    "href": "r/option-pricing-via-machine-learning.html#learning-black-scholes",
    "title": "Option Pricing via Machine Learning",
    "section": "Learning Black-Scholes",
    "text": "Learning Black-Scholes\nWe illustrate the concept of ML by showing how ML methods learn the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation.\n\nData simulation\nTo that end, we start with simulated data. We compute option prices for call options for a grid of different combinations of times to maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), and current stock prices (S). In the code below, we add an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\nIn order to keep the analysis reproducible, we use set.seed(). A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines.\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:60,\n  K = 20:90,\n  r = seq(from = 0, to = 0.05, by = 0.01),\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map(\n      black_scholes,\n      function(x) x + rnorm(2, sd = 0.15)\n    )\n  ) |&gt;\n  unnest(observed_price)\n\nThe code above generates more than 1.5 million random parameter constellations. For each of these values, two observed prices reflecting the Black-Scholes prices are given and a random innovation term pollutes the observed prices. The intuition of this application is simple: the simulated data provides many observations of option prices - by using the Black-Scholes equation we can evaluate the actual predictive performance of a ML method, which would be hard in a realistic context were the actual arbitrage-free price would be unknown.\nNext, we split the data into a training set (which contains 1% of all the observed option prices) and a test set that will only be used for the final evaluation. Note that the entire grid of possible combinations contains 3148992 different specifications. Thus, the sample to learn the Black-Scholes price contains only 31,489 observations and is therefore relatively small.\n\nsplit &lt;- initial_split(option_prices, prop = 1 / 100)\n\nWe process the training dataset further before we fit the different ML models. We define a recipe() that defines all processing steps for that purpose. For our specific case, we want to explain the observed price by the five variables that enter the Black-Scholes equation. The true price (stored in column black_scholes) should obviously not be used to fit the model. The recipe also reflects that we standardize all predictors to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.\n\nrec &lt;- recipe(observed_price ~ .,\n  data = option_prices\n) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())\n\n\n\nSingle layer networks and random forests\nNext, we show how to fit a neural network to the data. Note that this requires that keras is installed on your local machine. The function mlp() from the package parsnip provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with 10 hidden units. We set the number of training iterations to epochs = 500. The option set_mode(\"regression\") specifies a linear activation function for the output layer.\n\nnnet_model &lt;- mlp(\n  epochs = 500,\n  hidden_units = 10,\n) |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"keras\", verbose = FALSE)\n\nThe verbose = FALSE argument prevents logging the results to the console. We can follow the straightforward tidymodel workflow as in the chapter before: define a workflow, equip it with the recipe, and specify the associated model. Finally, fit the model with the training data.\n\nnn_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(nnet_model) |&gt;\n  fit(data = training(split))\n\nOnce you are familiar with the tidymodels workflow, it is a piece of cake to fit other models from the parsnip family. For instance, the model below initializes a random forest with 50 trees contained in the ensemble, where we require at least 2000 observations in a node. The random forests are trained using the package ranger, which is required to be installed in order to run the code below.\n\nrf_model &lt;- rand_forest(\n  trees = 50,\n  min_n = 2000\n) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nFitting the model follows exactly the same convention as for the neural network before.\n\nrf_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(data = training(split))\n\n\n\nDeep neural networks\nA deep neural network is a neural network with multiple layers between the input and output layers. By chaining multiple layers together, more complex structures can be represented with fewer parameters than simple shallow (one-layer) networks as the one implemented above. For instance, image or text recognition are typical tasks where deep neural networks are used (for applications of deep neural networks in finance, see, for instance, Jiang, Kelly, and Xiu 2022; Jensen et al. 2022).\nNote that while the tidymodels workflow is extremely convenient, these more sophisticated multi-layer (so-called deep) neural networks are not supported by tidymodels yet (as of September 2022). Instead, an implementation of a deep neural network in R requires additional computational tools. For that reason, the code snippet below illustrates how to initialize a sequential model with three hidden layers with 10 units per layer. The keras package provides a convenient interface and is flexible enough to handle different activation functions. The compile() command defines the loss function with which the model predictions are evaluated.\n\nmodel &lt;- keras_model_sequential() |&gt;\n  layer_dense(\n    input_shape = 5,\n    units = 10,\n    activation = \"sigmoid\"\n  ) |&gt;\n  layer_dense(units = 10, activation = \"sigmoid\") |&gt;\n  layer_dense(units = 10, activation = \"sigmoid\") |&gt;\n  layer_dense(units = 1, activation = \"linear\") |&gt;\n  compile(\n    loss = \"mean_squared_error\"\n  )\nmodel\n\nModel: \"sequential_1\"\n_____________________________________________________________________\n Layer (type)                  Output Shape               Param #    \n=====================================================================\n dense_5 (Dense)               (None, 10)                 60         \n dense_4 (Dense)               (None, 10)                 110        \n dense_3 (Dense)               (None, 10)                 110        \n dense_2 (Dense)               (None, 1)                  11         \n=====================================================================\nTotal params: 291\nTrainable params: 291\nNon-trainable params: 0\n_____________________________________________________________________\n\n\nTo train the neural network, we provide the inputs (x) and the variable to predict (y) and then fit the parameters. Note the slightly tedious use of the method extract_mold(nn_fit). Instead of simply using the raw data, we fit the neural network with the same processed data that is used for the single-layer feed-forward network. What is the difference to simply calling x = training(data) |&gt; select(-observed_price, -black_scholes)? Recall that the recipe standardizes the variables such that all columns have unit standard deviation and zero mean. Further, it adds consistency if we ensure that all models are trained using the same recipe such that a change in the recipe is reflected in the performance of any model. A final note on a potentially irritating observation: fit() alters the keras model - this is one of the few instances, where a function in R alters the input such that after the function call the object model is not same anymore!\n\nmodel |&gt;\n  fit(\n    x = extract_mold(nn_fit)$predictors |&gt; as.matrix(),\n    y = extract_mold(nn_fit)$outcomes |&gt; pull(observed_price),\n    epochs = 500, verbose = FALSE\n  )\n\n\n\nUniversal approximation\nBefore we evaluate the results, we implement one more model. In principle, any non-linear function can also be approximated by a linear model containing the input variables’ polynomial expansions. To illustrate this, we first define a new recipe, rec_linear, which processes the training data even further. We include polynomials up to the fifth degree of each predictor and then add all possible pairwise interaction terms. The final recipe step, step_lincomb(), removes potentially redundant variables (for instance, the interaction between \\(r^2\\) and \\(r^3\\) is the same as the term \\(r^5\\)). We fit a Lasso regression model with a pre-specified penalty term (consult Chapter 14 on how to tune the model hyperparameters).\n\nrec_linear &lt;- rec |&gt;\n  step_poly(all_predictors(),\n    degree = 5,\n    options = list(raw = T)\n  ) |&gt;\n  step_interact(terms = ~ all_predictors():all_predictors()) |&gt;\n  step_lincomb(all_predictors())\n\nlm_model &lt;- linear_reg(penalty = 0.01) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- workflow() |&gt;\n  add_recipe(rec_linear) |&gt;\n  add_model(lm_model) |&gt;\n  fit(data = training(split))"
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#prediction-evaluation",
    "href": "r/option-pricing-via-machine-learning.html#prediction-evaluation",
    "title": "Option Pricing via Machine Learning",
    "section": "Prediction Evaluation",
    "text": "Prediction Evaluation\nFinally, we collect all predictions to compare the out-of-sample prediction error evaluated on ten thousand new data points. Note that for the evaluation, we use the call to extract_mold() to ensure that we use the same pre-processing steps for the testing data across each model. We also use the somewhat advanced functionality in forge(), which provides an easy, consistent, and robust pre-processor at prediction time.\n\nout_of_sample_data &lt;- testing(split) |&gt;\n  slice_sample(n = 10000)\n\npredictive_performance &lt;- model |&gt;\n  predict(forge(\n    out_of_sample_data,\n    extract_mold(nn_fit)$blueprint\n  )$predictors |&gt; as.matrix()) |&gt;\n  as.vector() |&gt;\n  tibble(\"Deep NN\" = _) |&gt;\n  bind_cols(nn_fit |&gt;\n    predict(out_of_sample_data)) |&gt;\n  rename(\"Single layer\" = .pred) |&gt;\n  bind_cols(lm_fit |&gt; predict(out_of_sample_data)) |&gt;\n  rename(\"Lasso\" = .pred) |&gt;\n  bind_cols(rf_fit |&gt; predict(out_of_sample_data)) |&gt;\n  rename(\"Random forest\" = .pred) |&gt;\n  bind_cols(out_of_sample_data) |&gt;\n  pivot_longer(\"Deep NN\":\"Random forest\", names_to = \"Model\") |&gt;\n  mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nIn the lines above, we use each of the fitted models to generate predictions for the entire test data set of option prices. We evaluate the absolute pricing error as one possible measure of pricing accuracy, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model. We show the results graphically in Figure 15.1.\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE, method = \"gam\") +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Absolut prediction error (USD)\",\n    title = \"Prediction errors of call option prices for different models\",\n    linetype = NULL\n  )\n\n`geom_smooth()` using formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nAbsolut prediction error in USD for the different fitted methods. The prediction error is evaluated on a sample of call options that were not used for training.\n\n\n\n\nThe results can be summarized as follows:\n\nAll ML methods seem to be able to price call options after observing the training test set.\nThe average prediction errors increase for far in-the-money options.\nRandom forest and the Lasso seem to perform consistently worse in prediction option prices than the neural networks.\nThe complexity of the deep neural network relative to the single-layer neural network does not result in better out-of-sample predictions."
  },
  {
    "objectID": "r/option-pricing-via-machine-learning.html#exercises",
    "href": "r/option-pricing-via-machine-learning.html#exercises",
    "title": "Option Pricing via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that takes y and a matrix of predictors X as inputs and returns a characterization of the relevant parameters of a regression tree with 1 branch.\nCreate a function that creates predictions for a new matrix of predictors newX based on the estimated regression tree.\nUse the package rpart to grow a tree based on the training data and use the illustration tools in rpart to understand which characteristics the tree deems relevant for option pricing.\nMake use of a training and a test set to choose the optimal depth (number of sample splits) of the tree.\nUse keras to initialize a sequential neural network that can take the predictors from the training data set as input, contains at least one hidden layer, and generates continuous predictions. This sounds harder than it is: see a simple regression example here. How many parameters does the neural network you aim to fit have?\nCompile the object from the previous exercise. It is important that you specify a loss function. Illustrate the difference in predictive accuracy for different architecture choices."
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "Preface",
    "section": "",
    "text": "This website is the online version of Tidy Finance with R, a book published via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, and Patrick Weiss.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\n\n\n\nSupport Tidy Finance\n\n\n\nBuy our book via your preferred vendor or support us with coffee here.\n\n\n\n\nFinancial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from undergrad to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common data sets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham and Grolemund (2016) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nWe believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham.\n\n\n\n\n\n\nBefore we continue, make sure you have all the software you need for this book:\n\nInstall R and RStudio. To get a walk-through of the installation for every major operating system, follow the steps outlined in this summary. The whole process should be done in a few clicks. If you wonder about the difference: R is an open-source language and environment for statistical computing and graphics, free to download and use. While R runs the computations, RStudio is an integrated development environment that provides an interface by adding many convenient features and tools. We suggest doing all the coding in RStudio.\nOpen RStudio and install the tidyverse. Not sure how it works? You will find helpful information on how to install packages in this brief summary.\n\nIf you are new to R, we recommend starting with the following sources:\n\nA very gentle and good introduction to the workings of R can be found in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham and Grolemund (2016), is available online and for free: R for Data Science explains the majority of the tools we use in our book.\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages.\n\n\n\n\nWe met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute teaching award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is affiliated with Reykjavik University and Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in a top journal in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237\n\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}\n\n\n\nThis book was written in RStudio using bookdown (Xie 2016). The webiste was rendered using quarto (Allaire et al. 2022) and it is hosted via GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.3.0 (2023-04-21, Already Tomorrow) and the following packages: \n\n\n\n\n\nPackage\nVersion\n\n\n\n\nRPostgres\n1.4.5\n\n\nRSQLite\n2.3.1\n\n\nalabama\n2022.4-1\n\n\nbroom\n1.0.4\n\n\ndbplyr\n2.3.2\n\n\ndevtools\n2.4.5\n\n\ndplyr\n1.1.2\n\n\nfixest\n0.11.1\n\n\nforcats\n1.0.0\n\n\nfrenchdata\n0.2.0\n\n\nfurrr\n0.3.1\n\n\nggplot2\n3.4.2\n\n\nglmnet\n4.1-7\n\n\ngoogledrive\n2.1.0\n\n\nhardhat\n1.3.0\n\n\nhexSticker\n0.4.9\n\n\njsonlite\n1.8.4\n\n\nkableExtra\n1.3.4\n\n\nkeras\n2.11.1\n\n\nlmtest\n0.9-40\n\n\npurrr\n1.0.1\n\n\nquadprog\n1.5-8\n\n\nranger\n0.15.1\n\n\nreadr\n2.1.4\n\n\nreadxl\n1.4.2\n\n\nrenv\n0.17.3\n\n\nrlang\n1.1.1\n\n\nrmarkdown\n2.21\n\n\nsandwich\n3.0-2\n\n\nscales\n1.2.1\n\n\nslider\n0.3.0\n\n\nstringr\n1.5.0\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.0\n\n\ntidyquant\n1.0.7\n\n\ntidyr\n1.3.0\n\n\ntidyverse\n2.0.0\n\n\ntimetk\n2.8.3\n\n\nwesanderson\n0.3.6"
  },
  {
    "objectID": "r/index.html#why-does-this-book-exist",
    "href": "r/index.html#why-does-this-book-exist",
    "title": "Preface",
    "section": "",
    "text": "Financial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future."
  },
  {
    "objectID": "r/index.html#who-should-read-this-book",
    "href": "r/index.html#who-should-read-this-book",
    "title": "Preface",
    "section": "",
    "text": "We write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from undergrad to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed."
  },
  {
    "objectID": "r/index.html#what-will-you-learn",
    "href": "r/index.html#what-will-you-learn",
    "title": "Preface",
    "section": "",
    "text": "The book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common data sets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet the data chapters provide an important background necessary for data management in all other chapters."
  },
  {
    "objectID": "r/index.html#what-wont-you-learn",
    "href": "r/index.html#what-wont-you-learn",
    "title": "Preface",
    "section": "",
    "text": "This book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the nitty-gritty choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham and Grolemund (2016) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages)."
  },
  {
    "objectID": "r/index.html#why-r",
    "href": "r/index.html#why-r",
    "title": "Preface",
    "section": "",
    "text": "We believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019)."
  },
  {
    "objectID": "r/index.html#why-tidy",
    "href": "r/index.html#why-tidy",
    "title": "Preface",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats (Wickham 2021), and lubridate (Grolemund and Wickham 2011).\n\n\n\n\n\n\nNote\n\n\n\nThroughout the book we use the native pipe |&gt;, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %&gt;% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to this blog post second edition by Hadley Wickham."
  },
  {
    "objectID": "r/index.html#prerequisites",
    "href": "r/index.html#prerequisites",
    "title": "Preface",
    "section": "",
    "text": "Before we continue, make sure you have all the software you need for this book:\n\nInstall R and RStudio. To get a walk-through of the installation for every major operating system, follow the steps outlined in this summary. The whole process should be done in a few clicks. If you wonder about the difference: R is an open-source language and environment for statistical computing and graphics, free to download and use. While R runs the computations, RStudio is an integrated development environment that provides an interface by adding many convenient features and tools. We suggest doing all the coding in RStudio.\nOpen RStudio and install the tidyverse. Not sure how it works? You will find helpful information on how to install packages in this brief summary.\n\nIf you are new to R, we recommend starting with the following sources:\n\nA very gentle and good introduction to the workings of R can be found in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham and Grolemund (2016), is available online and for free: R for Data Science explains the majority of the tools we use in our book.\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages."
  },
  {
    "objectID": "r/index.html#about-the-authors",
    "href": "r/index.html#about-the-authors",
    "title": "Preface",
    "section": "",
    "text": "We met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute teaching award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is affiliated with Reykjavik University and Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in a top journal in financial economics."
  },
  {
    "objectID": "r/index.html#license",
    "href": "r/index.html#license",
    "title": "Preface",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows:\n\nScheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237\n\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}"
  },
  {
    "objectID": "r/index.html#colophon",
    "href": "r/index.html#colophon",
    "title": "Preface",
    "section": "",
    "text": "This book was written in RStudio using bookdown (Xie 2016). The webiste was rendered using quarto (Allaire et al. 2022) and it is hosted via GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.3.0 (2023-04-21, Already Tomorrow) and the following packages: \n\n\n\n\n\nPackage\nVersion\n\n\n\n\nRPostgres\n1.4.5\n\n\nRSQLite\n2.3.1\n\n\nalabama\n2022.4-1\n\n\nbroom\n1.0.4\n\n\ndbplyr\n2.3.2\n\n\ndevtools\n2.4.5\n\n\ndplyr\n1.1.2\n\n\nfixest\n0.11.1\n\n\nforcats\n1.0.0\n\n\nfrenchdata\n0.2.0\n\n\nfurrr\n0.3.1\n\n\nggplot2\n3.4.2\n\n\nglmnet\n4.1-7\n\n\ngoogledrive\n2.1.0\n\n\nhardhat\n1.3.0\n\n\nhexSticker\n0.4.9\n\n\njsonlite\n1.8.4\n\n\nkableExtra\n1.3.4\n\n\nkeras\n2.11.1\n\n\nlmtest\n0.9-40\n\n\npurrr\n1.0.1\n\n\nquadprog\n1.5-8\n\n\nranger\n0.15.1\n\n\nreadr\n2.1.4\n\n\nreadxl\n1.4.2\n\n\nrenv\n0.17.3\n\n\nrlang\n1.1.1\n\n\nrmarkdown\n2.21\n\n\nsandwich\n3.0-2\n\n\nscales\n1.2.1\n\n\nslider\n0.3.0\n\n\nstringr\n1.5.0\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.0\n\n\ntidyquant\n1.0.7\n\n\ntidyr\n1.3.0\n\n\ntidyverse\n2.0.0\n\n\ntimetk\n2.8.3\n\n\nwesanderson\n0.3.6"
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html",
    "href": "r/fixed-effects-and-clustered-standard-errors.html",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "",
    "text": "In this chapter, we provide an intuitive introduction to the two popular concepts of fixed effects regressions and clustered standard errors. When working with regressions in empirical finance, you will sooner or later be confronted with discussions around how you deal with omitted variables bias and dependence in your residuals. The concepts we introduce in this chapter are designed to address such concerns.\nWe focus on a classical panel regression common to the corporate finance literature (e.g., Fazzari et al. 1988; Erickson and Whited 2012; Gulen and Ion 2015): firm investment modeled as a function that increases in firm cash flow and firm investment opportunities.\nTypically, this investment regression uses quarterly balance sheet data provided via Compustat because it allows for richer dynamics in the regressors and more opportunities to construct variables. As we focus on the implementation of fixed effects and clustered standard errors, we use the annual Compustat data from our previous chapters and leave the estimation using quarterly data as an exercise. We demonstrate below that the regression based on annual data yields qualitatively similar results to estimations based on quarterly data from the literature, namely confirming the positive relationships between investment and the two regressors.\nThe current chapter relies on the following set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(fixest)\nCompared to previous chapters, we introduce fixest (Bergé 2018) for the fixed effects regressions, the implementation of standard error clusters, and tidy estimation output."
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#data-preparation",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and annual Compustat as data sources from our SQLite-database introduced in Chapters 2-4. In particular, Compustat provides balance sheet and income statement data on a firm level, while CRSP provides market valuations. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(gvkey, month, mktcap) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(datadate, gvkey, year, at, be, capx, oancf, txdb) |&gt;\n  collect()\n\nThe classical investment regressions model the capital investment of a firm as a function of operating cash flows and Tobin’s q, a measure of a firm’s investment opportunities. We start by constructing investment and cash flows which are usually normalized by lagged total assets of a firm. In the following code chunk, we construct a panel of firm-year observations, so we have both cross-sectional information on firms as well as time-series information for each firm.\n\ndata_investment &lt;- compustat |&gt;\n  mutate(month = floor_date(datadate, \"month\")) |&gt;\n  left_join(compustat |&gt;\n    select(gvkey, year, at_lag = at) |&gt;\n    mutate(year = year + 1),\n  by = c(\"gvkey\", \"year\")\n  ) |&gt;\n  filter(at &gt; 0, at_lag &gt; 0) |&gt;\n  mutate(\n    investment = capx / at_lag,\n    cash_flows = oancf / at_lag\n  )\n\ndata_investment &lt;- data_investment |&gt;\n  left_join(data_investment |&gt;\n    select(gvkey, year, investment_lead = investment) |&gt;\n    mutate(year = year - 1),\n  by = c(\"gvkey\", \"year\")\n  )\n\nTobin’s q is the ratio of the market value of capital to its replacement costs. It is one of the most common regressors in corporate finance applications (e.g., Fazzari et al. 1988; Erickson and Whited 2012). We follow the implementation of Gulen and Ion (2015) and compute Tobin’s q as the market value of equity (mktcap) plus the book value of assets (at) minus book value of equity (be) plus deferred taxes (txdb), all divided by book value of assets (at). Finally, we only keep observations where all variables of interest are non-missing, and the reported book value of assets is strictly positive.\n\ndata_investment &lt;- data_investment |&gt;\n  left_join(crsp_monthly,\n  by = c(\"gvkey\", \"month\")\n  ) |&gt;\n  mutate(tobins_q = (mktcap + at - be + txdb) / at)\n\ndata_investment &lt;- data_investment |&gt;\n  select(gvkey, year, investment_lead, cash_flows, tobins_q) |&gt;\n  drop_na()\n\nAs the variable construction typically leads to extreme values that are most likely related to data issues (e.g., reporting errors), many papers include winsorization of the variables of interest. Winsorization involves replacing values of extreme outliers with quantiles on the respective end. The following function implements the winsorization for any percentage cut that should be applied on either end of the distributions. In the specific example, we winsorize the main variables (investment, cash_flows, and tobins_q) at the 1 percent level.\n\nwinsorize &lt;- function(x, cut) {\n  x &lt;- replace(\n    x,\n    x &gt; quantile(x, 1 - cut, na.rm = T),\n    quantile(x, 1 - cut, na.rm = T)\n  )\n  x &lt;- replace(\n    x,\n    x &lt; quantile(x, cut, na.rm = T),\n    quantile(x, cut, na.rm = T)\n  )\n  return(x)\n}\n\ndata_investment &lt;- data_investment |&gt;\n  mutate(across(\n    c(investment_lead, cash_flows, tobins_q),\n    ~ winsorize(., 0.01)\n  ))\n\nBefore proceeding to any estimations, we highly recommend tabulating summary statistics of the variables that enter the regression. These simple tables allow you to check the plausibility of your numerical variables, as well as spot any obvious errors or outliers. Additionally, for panel data, plotting the time series of the variable’s mean and the number of observations is a useful exercise to spot potential problems.\n\ndata_investment |&gt;\n  pivot_longer(\n    cols = c(investment_lead, cash_flows, tobins_q),\n    names_to = \"measure\"\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 9\n  measure      mean     sd    min      q05    q50   q95    max      n\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n1 cash_flows 0.0145 0.266  -1.50  -4.57e-1 0.0649 0.273  0.480 124194\n2 investmen… 0.0584 0.0778  0      7.27e-4 0.0333 0.208  0.467 124194\n3 tobins_q   1.99   1.69    0.571  7.92e-1 1.38   5.33  10.8   124194"
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#fixed-effects",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nTo illustrate fixed effects regressions, we use the fixest package, which is both computationally powerful and flexible with respect to model specifications. We start out with the basic investment regression using the simple model \\[ \\text{Investment}_{i,t+1} = \\alpha + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\varepsilon_t\\) is i.i.d. normally distributed across time and firms. We use the feols()-function to estimate the simple model so that the output has the same structure as the other regressions below, but you could also use lm().\n\nmodel_ols &lt;- feols(\n  fml = investment_lead ~ cash_flows + tobins_q,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_ols\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 124,194 \nStandard-errors: IID \n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  0.04241   0.000342   124.1 &lt; 2.2e-16 ***\ncash_flows   0.05144   0.000835    61.6 &lt; 2.2e-16 ***\ntobins_q     0.00767   0.000132    58.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.076031   Adj. R2: 0.044472\n\n\nAs expected, the regression output shows significant coefficients for both variables. Higher cash flows and investment opportunities are associated with higher investment. However, the simple model actually may have a lot of omitted variables, so our coefficients are most likely biased. As there is a lot of unexplained variation in our simple model (indicated by the rather low adjusted R-squared), the bias in our coefficients is potentially severe, and the true values could be above or below zero. Note that there are no clear cutoffs to decide when an R-squared is high or low, but it depends on the context of your application and on the comparison of different models for the same data.\nOne way to tackle the issue of omitted variable bias is to get rid of as much unexplained variation as possible by including fixed effects - i.e., model parameters that are fixed for specific groups (e.g., Wooldridge 2010). In essence, each group has its own mean in fixed effects regressions. The simplest group that we can form in the investment regression is the firm level. The firm fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\alpha_i\\) is the firm fixed effect and captures the firm-specific mean investment across all years. In fact, you could also compute firms’ investments as deviations from the firms’ average investments and estimate the model without the fixed effects. The idea of the firm fixed effect is to remove the firm’s average investment, which might be affected by firm-specific variables that you do not observe. For example, firms in a specific industry might invest more on average. Or you observe a young firm with large investments but only small concurrent cash flows, which will only happen in a few years. This sort of variation is unwanted because it is related to unobserved variables that can bias your estimates in any direction.\nTo include the firm fixed effect, we use gvkey (Compustat’s firm identifier) as follows:\n\nmodel_fe_firm &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firm\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 124,194 \nFixed-effects: gvkey: 13,904\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(&gt;|t|)    \ncash_flows   0.0146   0.000963    15.2 &lt; 2.2e-16 ***\ntobins_q     0.0113   0.000136    82.6 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.050072     Adj. R2: 0.533321\n                 Within R2: 0.059463\n\n\nThe regression output shows a lot of unexplained variation at the firm level that is taken care of by including the firm fixed effect as the adjusted R-squared rises above 50%. In fact, it is more interesting to look at the within R-squared that shows the explanatory power of a firm’s cash flow and Tobin’s q on top of the average investment of each firm. We can also see that the coefficients changed slightly in magnitude but not in sign.\nThere is another source of variation that we can get rid of in our setting: average investment across firms might vary over time due to macroeconomic factors that affect all firms, such as economic crises. By including year fixed effects, we can take out the effect of unobservables that vary over time. The two-way fixed effects regression is then \\[ \\text{Investment}_{i,t+1} = \\alpha_i + \\alpha_t + \\beta_1\\text{Cash Flows}_{i,t}+\\beta_2\\text{Tobin's q}_{i,t}+\\varepsilon_{i,t},\\] where \\(\\alpha_t\\) is the time fixed effect. Here you can think of higher investments during an economic expansion with simultaneously high cash flows.\n\nmodel_fe_firmyear &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  se = \"iid\",\n  data = data_investment\n)\nmodel_fe_firmyear\n\nOLS estimation, Dep. Var.: investment_lead\nObservations: 124,194 \nFixed-effects: gvkey: 13,904,  year: 34\nStandard-errors: IID \n           Estimate Std. Error t value  Pr(&gt;|t|)    \ncash_flows   0.0182   0.000941    19.3 &lt; 2.2e-16 ***\ntobins_q     0.0102   0.000135    75.5 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.048796     Adj. R2: 0.556677\n                 Within R2: 0.051589\n\n\nThe inclusion of time fixed effects did only marginally affect the R-squared and the coefficients, which we can interpret as a good thing as it indicates that the coefficients are not driven by an omitted variable that varies over time.\nHow can we further improve the robustness of our regression results? Ideally, we want to get rid of unexplained variation at the firm-year level, which means we need to include more variables that vary across firm and time and are likely correlated with investment. Note that we cannot include firm-year fixed effects in our setting because then cash flows and Tobin’s q are colinear with the fixed effects, and the estimation becomes void.\nBefore we discuss the properties of our estimation errors, we want to point out that regression tables are at the heart of every empirical analysis, where you compare multiple models. Fortunately, the etable() provides a convenient way to tabulate the regression output (with many parameters to customize and even print the output in LaTeX). We recommend printing \\(t\\)-statistics rather than standard errors in regression tables because the latter are typically very hard to interpret across coefficients that vary in size. We also do not print p-values because they are sometimes misinterpreted to signal the importance of observed effects (Wasserstein and Lazar 2016). The \\(t\\)-statistics provide a consistent way to interpret changes in estimation uncertainty across different model specifications.\n\netable(model_ols, model_fe_firm, model_fe_firmyear,\n  coefstat = \"tstat\"\n)\n\n                        model_ols     model_fe_firm\nDependent Var.:   investment_lead   investment_lead\n                                                   \nConstant        0.0424*** (124.1)                  \ncash_flows      0.0514*** (61.59) 0.0146*** (15.15)\ntobins_q        0.0077*** (58.21) 0.0113*** (82.63)\nFixed-Effects:  ----------------- -----------------\ngvkey                          No               Yes\nyear                           No                No\n_______________ _________________ _________________\nVCOV type                     IID               IID\nObservations              124,194           124,194\nR2                        0.04449           0.58557\nWithin R2                      --           0.05946\n\n                model_fe_firmyear\nDependent Var.:   investment_lead\n                                 \nConstant                         \ncash_flows      0.0182*** (19.31)\ntobins_q        0.0102*** (75.55)\nFixed-Effects:  -----------------\ngvkey                         Yes\nyear                          Yes\n_______________ _________________\nVCOV type                     IID\nObservations              124,194\nR2                        0.60643\nWithin R2                 0.05159\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#clustering-standard-errors",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Clustering Standard Errors",
    "text": "Clustering Standard Errors\nApart from biased estimators, we usually have to deal with potentially complex dependencies of our residuals with each other. Such dependencies in the residuals invalidate the i.i.d. assumption of OLS and lead to biased standard errors. With biased OLS standard errors, we cannot reliably interpret the statistical significance of our estimated coefficients.\nIn our setting, the residuals may be correlated across years for a given firm (time-series dependence), or, alternatively, the residuals may be correlated across different firms (cross-section dependence). One of the most common approaches to dealing with such dependence is the use of clustered standard errors (Petersen 2008). The idea behind clustering is that the correlation of residuals within a cluster can be of any form. As the number of clusters grows, the cluster-robust standard errors become consistent (Donald and Lang 2007; Wooldridge 2010). A natural requirement for clustering standard errors in practice is hence a sufficiently large number of clusters. Typically, around at least 30 to 50 clusters are seen as sufficient (Cameron, Gelbach, and Miller 2011).\nInstead of relying on the iid assumption, we can use the cluster option in the feols-function as above. The code chunk below applies both one-way clustering by firm as well as two-way clustering by firm and year.\n\nmodel_cluster_firm &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = \"gvkey\",\n  data = data_investment\n)\n\nmodel_cluster_firmyear &lt;- feols(\n  investment_lead ~ cash_flows + tobins_q | gvkey + year,\n  cluster = c(\"gvkey\", \"year\"),\n  data = data_investment\n)\n\n The table below shows the comparison of the different assumptions behind the standard errors. In the first column, we can see highly significant coefficients on both cash flows and Tobin’s q. By clustering the standard errors on the firm level, the \\(t\\)-statistics of both coefficients drop in half, indicating a high correlation of residuals within firms. If we additionally cluster by year, we see a drop, particularly for Tobin’s q, again. Even after relaxing the assumptions behind our standard errors, both coefficients are still comfortably significant as the \\(t\\) statistics are well above the usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\netable(model_fe_firmyear, model_cluster_firm, model_cluster_firmyear,\n  coefstat = \"tstat\"\n)\n\n                model_fe_firmyear model_cluster_f..\nDependent Var.:   investment_lead   investment_lead\n                                                   \ncash_flows      0.0182*** (19.31) 0.0182*** (11.19)\ntobins_q        0.0102*** (75.55) 0.0102*** (35.64)\nFixed-Effects:  ----------------- -----------------\ngvkey                         Yes               Yes\nyear                          Yes               Yes\n_______________ _________________ _________________\nVCOV type                     IID         by: gvkey\nObservations              124,194           124,194\nR2                        0.60643           0.60643\nWithin R2                 0.05159           0.05159\n\n                model_cluster_f...1\nDependent Var.:     investment_lead\n                                   \ncash_flows        0.0182*** (9.473)\ntobins_q          0.0102*** (16.39)\nFixed-Effects:    -----------------\ngvkey                           Yes\nyear                            Yes\n_______________   _________________\nVCOV type          by: gvkey & year\nObservations                124,194\nR2                          0.60643\nWithin R2                   0.05159\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspired by Abadie et al. (2017), we want to close this chapter by highlighting that choosing the right dimensions for clustering is a design problem. Even if the data is informative about whether clustering matters for standard errors, they do not tell you whether you should adjust the standard errors for clustering. Clustering at too aggregate levels can hence lead to unnecessarily inflated standard errors."
  },
  {
    "objectID": "r/fixed-effects-and-clustered-standard-errors.html#exercises",
    "href": "r/fixed-effects-and-clustered-standard-errors.html#exercises",
    "title": "Fixed Effects and Clustered Standard Errors",
    "section": "Exercises",
    "text": "Exercises\n\nEstimate the two-way fixed effects model with two-way clustered standard errors using quarterly Compustat data from WRDS. Note that you can access quarterly data via tbl(wrds, in_schema(\"comp\", \"fundq\")).\nFollowing Peters and Taylor (2017), compute Tobin’s q as the market value of outstanding equity mktcap plus the book value of debt (dltt + dlc) minus the current assets atc and everything divided by the book value of property, plant and equipment ppegt. What is the correlation between the measures of Tobin’s q? What is the impact on the two-way fixed effects regressions?"
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html",
    "href": "r/factor-selection-via-machine-learning.html",
    "title": "Factor Selection via Machine Learning",
    "section": "",
    "text": "The aim of this chapter is twofold. From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning (ML) using tidyverse principles. tidymodels comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the notion of factor zoo (Cochrane 2011) using ML methods. We introduce Lasso and Ridge regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model tuning with Elastic Net regularization as a popular example. We implement and showcase the entire cycle from model specification, training, and forecast evaluation within the tidymodels universe. While the tools can generally be applied to an abundance of interesting asset pricing problems, we apply penalized regressions for identifying macroeconomic variables and asset pricing factors that help explain a cross-section of industry portfolios.\nIn previous chapters, we illustrate that stock characteristics such as size provide valuable pricing information in addition to the market beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists discovered a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role in pricing applications). The search for factors that explain the cross-section of expected stock returns has produced hundreds of potential candidates, as noted more recently by Harvey, Liu, and Zhu (2016), Mclean and Pontiff (2016), and Hou, Xue, and Zhang (2020). Therefore, given the multitude of proposed risk factors, the challenge these days rather is: do we believe in the relevance of 300+ risk factors? During recent years, promising methods from the field of ML got applied to common finance applications. We refer to Mullainathan and Spiess (2017) for a treatment of ML from the perspective of an econometrician, Nagel (2021) for an excellent review of ML practices in asset pricing, Easley et al. (2020) for ML applications in (high-frequency) market microstructure, and Dixon, Halperin, and Bilokon (2020) for a detailed treatment of all methodological aspects."
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "href": "r/factor-selection-via-machine-learning.html#brief-theoretical-background",
    "title": "Factor Selection via Machine Learning",
    "section": "Brief Theoretical Background",
    "text": "Brief Theoretical Background\nThis is a book about doing empirical work in a tidy manner, and we refer to any of the many excellent textbook treatments of ML methods and especially penalized regressions for some deeper discussion. Excellent material is provided, for instance, by Hastie, Tibshirani, and Friedman (2009), Gareth et al. (2013), and De Prado (2018). Instead, we briefly summarize the idea of Lasso and Ridge regressions as well as the more general Elastic Net. Then, we turn to the fascinating question on how to implement, tune, and use such models with the tidymodels workflow.\nTo set the stage, we start with the definition of a linear model: suppose we have data \\((y_t, x_t), t = 1,\\ldots, T\\), where \\(x_t\\) is a \\((K \\times 1)\\) vector of regressors and \\(y_t\\) is the response for observation \\(t\\). The linear model takes the form \\(y_t = \\beta' x_t + \\varepsilon_t\\) with some error term \\(\\varepsilon_t\\) and has been studied in abundance. The well-known ordinary-least square (OLS) estimator for the \\((K \\times 1)\\) vector \\(\\beta\\) minimizes the sum of squared residuals and is then \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\] \nWhile we are often interested in the estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML is about the predictive performance most of the time. For a new observation \\(\\tilde{x}_t\\), the linear model generates predictions such that \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\] Is this the best we can do? Not really: instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by choosing other estimators \\(\\hat{\\beta}\\) with lower variance than the estimator \\(\\hat\\beta^\\text{ols}\\). At the same time, it seems appealing to restrict the set of regressors to a few meaningful ones if possible. In other words, if \\(K\\) is large (such as for the number of proposed factors in the asset pricing literature), it may be a desirable feature to select reasonable factors and set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) for some redundant factors.\nIt should be clear that the promised benefits of penalized regressions, i.e., reducing the mean squared error (MSE), come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). What is the effect of such a bias-variance trade-off? To understand the implications, assume the following data-generating process for \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] We want to recover \\(f(x)\\), which denotes some unknown functional which maps the relationship between \\(x\\) and \\(y\\). While the properties of \\(\\hat\\beta^\\text{ols}\\) as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) could be more desirable: For instance, the MSE depends on our model choice as follows: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(x))^2)=E((f(x)+\\epsilon-\\hat{f}(x))^2)\\\\\n&= \\underbrace{E((f(x)-\\hat{f}(x))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance of model}}+ \\underbrace{E\\left((f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] While no model can reduce \\(\\sigma_\\varepsilon^2\\), a biased estimator with small variance may have a lower MSE than an unbiased estimator.\n\nRidge regression\n\nOne biased estimator is known as Ridge regression. Hoerl and Kennard (1970) propose to minimize the sum of squared errors while simultaneously imposing a penalty on the \\(L_2\\) norm of the parameters \\(\\hat\\beta\\). Formally, this means that for a penalty factor \\(\\lambda\\geq 0\\) the minimization problem takes the form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). Here \\(c\\geq 0\\) is a constant that depends on the choice of \\(\\lambda\\). The larger \\(\\lambda\\), the smaller \\(c\\) (technically speaking, there is a one-to-one relationship between \\(\\lambda\\), which corresponds to the Lagrangian of the minimization problem above and \\(c\\)). Here, \\(X = \\left(x_1 \\ldots x_T\\right)'\\) and \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). A closed-form solution for the resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda I\\right)^{-1}X'y.\\] A couple of observations are worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) for \\(\\lambda = 0\\) and \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) for \\(\\lambda\\rightarrow \\infty\\). Also for \\(\\lambda &gt; 0\\), \\(\\left(X'X + \\lambda I\\right)\\) is non-singular even if \\(X'X\\) is which means that \\(\\hat\\beta^\\text{ridge}\\) exists even if \\(\\hat\\beta\\) is not defined. However, note also that the Ridge estimator requires careful choice of the hyperparameter \\(\\lambda\\) which controls the amount of regularization: a larger value of \\(\\lambda\\) implies shrinkage of the regression coefficient toward 0, a smaller value of \\(\\lambda\\) reduces the bias of the resulting estimator.\n\nNote, that \\(X\\) usually contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that \\(y\\) is simply demeaned before computing \\(\\hat\\beta^\\text{ridge}\\).\n\nWhat about the statistical properties of the Ridge estimator? First, the bad news is that \\(\\hat\\beta^\\text{ridge}\\) is a biased estimator of \\(\\beta\\). However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is guaranteed to be smaller than the variance of the ordinary least square estimator. We encourage you to verify these two statements in the exercises. As a result, we face a trade-off: The Ridge regression sacrifices some unbiasedness to achieve a smaller variance than the OLS estimator.\n\n\nLasso\n\nAn alternative to Ridge regression is the Lasso (least absolute shrinkage and selection operator). Similar to Ridge regression, the Lasso (Tibshirani 1996) is a penalized and biased estimator. The main difference to Ridge regression is that Lasso does not only shrink coefficients but effectively selects variables by setting coefficients for irrelevant variables to zero. Lasso implements a \\(L_1\\) penalization on the parameters such that: \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| &lt; c(\\lambda).\\] There is no closed form solution for \\(\\hat\\beta^\\text{Lasso}\\) in the above maximization problem but efficient algorithms exist (e.g., the R package glmnet). Like for Ridge regression, the hyperparameter \\(\\lambda\\) has to be specified beforehand.\n\n\nElastic Net\nThe Elastic Net (Zou and Hastie 2005) combines \\(L_1\\) with \\(L_2\\) penalization and encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. This more general framework considers the following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, we have to chose two hyperparameters: the shrinkage factor \\(\\lambda\\) and the weighting parameter \\(\\rho\\). The Elastic Net resembles Lasso for \\(\\rho = 0\\) and Ridge regression for \\(\\rho = 1\\). While the R package glmnet provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the glmnet package or the tidymodels back-end."
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#data-preparation",
    "href": "r/factor-selection-via-machine-learning.html#data-preparation",
    "title": "Factor Selection via Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the required packages and data. The main focus is on the workflow behind the tidymodels package collection (Kuhn and Wickham 2020). Kuhn and Silge (2018) provide a thorough introduction into all tidymodels components. glmnet (Simon et al. 2011) was developed and released in sync with Tibshirani (1996) and provides an R implementation of Elastic Net estimation. The package timetk (Dancho and Vaughan 2022) provides useful tools for time series data wrangling.\n\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scales)\nlibrary(furrr)\nlibrary(glmnet)\nlibrary(timetk)\n\nIn this analysis, we use four different data sources that we load from our SQLite-database introduced in Chapters 2-4. We start with two different sets of factor portfolio returns which have been suggested as representing practical risk factor exposure and thus should be relevant when it comes to asset pricing applications.\n\nThe standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts) defined in Fama and French (1992) and Fama and French (1993)\nMonthly q-factor returns from Hou, Xue, and Zhang (2014). The factors contain the size factor, the investment factor, the return-on-equity factor, and the expected growth factor\n\nNext, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2009) on the role of conditioning information.\n\nOur set of macroeconomic predictors comes from Welch and Goyal (2008). The data has been updated by the authors until 2021 and contains monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, and inflation\n\nFinally, we need a set of test assets. The aim is to understand which of the plenty factors and macroeconomic variable combinations prove helpful in explaining our test assets’ cross-section of returns. In line with many existing papers, we use monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"factor_ff_\", .), -month)\n\nfactors_q_monthly &lt;- tbl(tidy_finance, \"factors_q_monthly\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"factor_q_\", .), -month)\n\nmacro_predictors &lt;- tbl(tidy_finance, \"macro_predictors\") |&gt;\n  collect() |&gt;\n  rename_with(~ str_c(\"macro_\", .), -month) |&gt;\n  select(-macro_rp_div)\n\nindustries_ff_monthly &lt;- tbl(tidy_finance, \"industries_ff_monthly\") |&gt;\n  collect() |&gt;\n  pivot_longer(-month,\n    names_to = \"industry\", values_to = \"ret\"\n  ) |&gt;\n  mutate(industry = as_factor(industry))\n\nWe combine all the monthly observations into one data frame.\n\ndata &lt;- industries_ff_monthly |&gt;\n  left_join(factors_ff_monthly, by = \"month\") |&gt;\n  left_join(factors_q_monthly, by = \"month\") |&gt;\n  left_join(macro_predictors, by = \"month\") |&gt;\n  mutate(\n    ret = ret - factor_ff_rf\n  ) |&gt;\n  select(month, industry, ret_excess = ret, everything()) |&gt;\n  drop_na()\n\nOur data contains 22 columns of regressors with the 13 macro variables and 8 factor returns for each month. Figure 1 provides summary statistics for the 10 monthly industry excess returns in percent.\n\ndata |&gt;\n  group_by(industry) |&gt;\n  ggplot(aes(x = industry, y = ret_excess)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Excess return distributions by industry in percent\"\n  ) +\n  scale_y_continuous(\n    labels = percent\n  )\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries."
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "href": "r/factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "title": "Factor Selection via Machine Learning",
    "section": "The tidymodels Workflow",
    "text": "The tidymodels Workflow\nTo illustrate penalized linear regressions, we employ the tidymodels collection of packages for modeling and ML using tidyverse principles. You can simply use install.packages(\"tidymodels\") to get access to all the related packages. We recommend checking out the work of Kuhn and Silge (2018): They continuously write on their great book ‘Tidy Modeling with R’ using tidy principles.\nThe tidymodels workflow encompasses the main stages of the modeling process: pre-processing of data, model fitting, and post-processing of results. As we demonstrate below, tidymodels provides efficient workflows that you can update with low effort.\nUsing the ideas of Ridge and Lasso regressions, the following example guides you through (i) pre-processing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.\nTo start, we restrict our analysis to just one industry: Manufacturing. We first split the sample into a training and a test set. For that purpose, tidymodels provides the function initial_time_split() from the rsample package (Silge et al. 2022). The split takes the last 20% of the data as a test set, which is not used for any model tuning. We use this test set to evaluate the predictive accuracy in an out-of-sample scenario.\n\nsplit &lt;- initial_time_split(\n  data |&gt;\n    filter(industry == \"Manuf\") |&gt;\n    select(-industry),\n  prop = 4 / 5\n)\nsplit\n\n&lt;Training/Testing/Total&gt;\n&lt;527/132/659&gt;\n\n\nThe object split simply keeps track of the observations of the training and the test set. We can call the training set with training(split), while we can extract the test set with testing(split).\n\nPre-process data\nRecipes help you pre-process your data before training your model. Recipes are a series of pre-processing steps such as variable selection, transformation, or conversion of qualitative predictors to indicator variables. Each recipe starts with a formula that defines the general structure of the dataset and the role of each variable (regressor or dependent variable). For our dataset, our recipe contains the following steps before we fit any model:\n\nOur formula defines that we want to explain excess returns with all available predictors. The regression equation thus takes the form \\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t \\] where \\(r_t\\) is the vector of industry excess returns at time \\(t\\) and \\(\\tilde f_t\\) and \\(\\tilde z_t\\) are the (standardized) vectors of factor portfolio returns and macroeconomic variables\nWe exclude the column month from the analysis\nWe include all interaction terms between factors and macroeconomic predictors\nWe demean and scale each regressor such that the standard deviation is one\n\n\nrec &lt;- recipe(ret_excess ~ ., data = training(split)) |&gt;\n  step_rm(month) |&gt;\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_center(ret_excess, skip = TRUE)\n\nA table of all available recipe steps can be found in the tidymodels documentation. As of 2023, more than 150 different processing steps are available! One important point: The definition of a recipe does not trigger any calculations yet but rather provides a description of the tasks to be applied. As a result, it is very easy to reuse recipes for different models and thus make sure that the outcomes are comparable as they are based on the same input. In the example above, it does not make a difference whether you use the input data = training(split) or data = testing(split). All that matters at this early stage are the column names and types.\nWe can apply the recipe to any data with a suitable structure. The code below combines two different functions: prep() estimates the required parameters from a training set that can be applied to other data sets later. bake() applies the processed computations to new data.\n\ndata_prep &lt;- prep(rec, training(split))\n\nThe object data_prep contains information related to the different preprocessing steps applied to the training data: E.g., it is necessary to compute sample means and standard deviations to center and scale the variables.\n\ndata_bake &lt;- bake(data_prep,\n  new_data = testing(split)\n)\ndata_bake\n\n# A tibble: 132 × 126\n  factor_ff_rf factor_ff_mkt_excess factor_ff_smb factor_ff_hml\n         &lt;dbl&gt;                &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        -1.80              1.37            0.139         1.14 \n2        -1.80              0.333          -0.833         0.121\n3        -1.80              0.656           0.399         0.297\n4        -1.80              0.00426         0.717        -0.756\n5        -1.84              0.529          -0.177        -0.972\n# ℹ 127 more rows\n# ℹ 122 more variables: factor_q_me &lt;dbl&gt;, factor_q_ia &lt;dbl&gt;,\n#   factor_q_roe &lt;dbl&gt;, factor_q_eg &lt;dbl&gt;, macro_dp &lt;dbl&gt;,\n#   macro_dy &lt;dbl&gt;, macro_ep &lt;dbl&gt;, macro_de &lt;dbl&gt;,\n#   macro_svar &lt;dbl&gt;, macro_bm &lt;dbl&gt;, macro_ntis &lt;dbl&gt;,\n#   macro_tbl &lt;dbl&gt;, macro_lty &lt;dbl&gt;, macro_ltr &lt;dbl&gt;,\n#   macro_tms &lt;dbl&gt;, macro_dfy &lt;dbl&gt;, macro_infl &lt;dbl&gt;, …\n\n\nNote that the resulting data contains the 132 observations from the test set and 126 columns. Why so many? Recall that the recipe states to compute every possible interaction term between the factors and predictors, which increases the dimension of the data matrix substantially.\nYou may ask at this stage: why should I use a recipe instead of simply using the data wrangling commands such as mutate() or select()? tidymodels beauty is that a lot is happening under the hood. Recall, that for the simple scaling step, you actually have to compute the standard deviation of each column, then store this value, and apply the identical transformation to a different dataset, e.g., testing(split). A prepped recipe stores these values and hands them on once you bake() a novel dataset. Easy as pie with tidymodels, isn’t it?\n\n\nBuild a model\n Next, we can build an actual model based on our pre-processed data. In line with the definition above, we estimate regression coefficients of a Lasso regression such that we get \\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned}\\] We want to emphasize that the tidymodels workflow for any model is very similar, irrespective of the specific model. As you will see further below, it is straightforward to fit Ridge regression coefficients and - later - Neural networks or Random forests with basically the same code. The structure is always as follows: create a so-called workflow() and use the fit() function. A table with all available model APIs is available here. For now, we start with the linear regression model with a given value for the penalty factor \\(\\lambda\\). In the setup below, mixture denotes the value of \\(\\rho\\), hence setting mixture = 1 implies the Lasso.\n\nlm_model &lt;- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) |&gt;\n  set_engine(\"glmnet\", intercept = FALSE)\n\nThat’s it - we are done! The object lm_model contains the definition of our model with all required information. Note that set_engine(\"glmnet\") indicates the API character of the tidymodels workflow: Under the hood, the package glmnet is doing the heavy lifting, while linear_reg() provides a unified framework to collect the inputs. The workflow ends with combining everything necessary for the serious data science workflow, namely, a recipe and a model.\n\nlm_fit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(lm_model)\nlm_fit\n\n══ Workflow ═════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ─────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_interact()\n• step_normalize()\n• step_center()\n\n── Model ────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-04\n  mixture = 1\n\nEngine-Specific Arguments:\n  intercept = FALSE\n\nComputational engine: glmnet \n\n\n\n\nFit a model\nWith the workflow from above, we are ready to use fit(). Typically, we use training data to fit the model. The training data is pre-processed according to our recipe steps, and the Lasso regression coefficients are computed. First, we focus on the predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure 2 illustrates the projections for the entire time series of the manufacturing industry portfolio returns. The grey area indicates the out-of-sample period, which we did not use to fit the model.\n\npredicted_values &lt;- lm_fit |&gt;\n  fit(data = training(split)) |&gt;\n  predict(data |&gt; filter(industry == \"Manuf\")) |&gt;\n  bind_cols(data |&gt; filter(industry == \"Manuf\")) |&gt;\n  select(month,\n    \"Fitted value\" = .pred,\n    \"Realization\" = ret_excess\n  ) |&gt;\n  pivot_longer(-month, names_to = \"Variable\")\n\n\npredicted_values |&gt;\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = Variable,\n    linetype = Variable\n    )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Monthly realized and fitted manufacturing industry risk premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"5 years\"\n      )\n    },\n    minor_breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"1 years\"\n      )\n    },\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  annotate(\"rect\",\n    xmin = testing(split) |&gt; pull(month) |&gt; min(),\n    xmax = testing(split) |&gt; pull(month) |&gt; max(),\n    ymin = -Inf, ymax = Inf,\n    alpha = 0.5, fill = \"grey70\"\n  )\n\n\n\n\nFigure 2: The grey area corresponds to the out of sample period.\n\n\n\n\nWhat do the estimated coefficients look like? To analyze these values and to illustrate the difference between the tidymodels workflow and the underlying glmnet package, it is worth computing the coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample. Note that glmnet actually takes a vector y and the matrix of regressors \\(X\\) as input. Moreover, glmnet requires choosing the penalty parameter \\(\\alpha\\), which corresponds to \\(\\rho\\) in the notation above. When using the tidymodels model API, such details do not need consideration.\n\nx &lt;- data_bake |&gt;\n  select(-ret_excess) |&gt;\n  as.matrix()\ny &lt;- data_bake |&gt; pull(ret_excess)\n\nfit_lasso &lt;- glmnet(\n  x = x,\n  y = y,\n  alpha = 1,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge &lt;- glmnet(\n  x = x,\n  y = y,\n  alpha = 0,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nThe objects fit_lasso and fit_ridge contain an entire sequence of estimated coefficients for multiple values of the penalty factor \\(\\lambda\\). Figure 3 illustrates the trajectories of the regression coefficients as a function of the penalty factor. Both Lasso and Ridge coefficients converge to zero as the penalty factor increases.\n\nbind_rows(\n  tidy(fit_lasso) |&gt; mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) |&gt; mutate(Model = \"Ridge\")\n) |&gt;\n  rename(\"Variable\" = term) |&gt;\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Penalty factor (lambda)\", y = NULL,\n    title = \"Estimated coefficient paths for different penalty factors\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 3: The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\n\n\n\n\n\nOne word of caution: The package glmnet computes estimates of the coefficients \\(\\hat\\beta\\) based on numerical optimization procedures. As a result, the estimated coefficients for the special case with no regularization (\\(\\lambda = 0\\)) can deviate from the standard OLS estimates.\n\n\n\nTune a model\nTo compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , we simply imposed a value for the penalty hyperparameter \\(\\lambda\\). Model tuning is the process of optimally selecting such hyperparameters. tidymodels provides extensive tuning options based on so-called cross-validation. Again, we refer to any treatment of cross-validation to get a more detailed discussion of the statistical underpinnings. Here we focus on the general idea and the implementation with tidymodels.\nThe goal for choosing \\(\\lambda\\) (or any other hyperparameter, e.g., \\(\\rho\\) for the Elastic Net) is to find a way to produce predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, the MSPE is not directly observable. We can only compute an estimate because our data is random and because we do not observe the entire population.\nObviously, if we train an algorithm on the same data that we use to compute the error, our estimate \\(\\hat{\\text{MSPE}}\\) would indicate way better predictive accuracy than what we can expect in real out-of-sample data. The result is called overfitting.\nCross-validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many MSPE obtained by creating predictions for \\(K\\) new random samples of the data, none of them used to train the algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). In practice, this is done by carving out a piece of our data and pretending it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the optimal hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g., \\(\\lambda\\)), and evaluate the predictive accuracy on an independent sample. This works as follows:\n\nSpecify a grid of hyperparameters\nObtain predictors \\(\\hat{y}_i(\\lambda)\\) to denote the predictors for the used parameters \\(\\lambda\\)\nCompute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] With K-fold cross-validation, we do this computation \\(K\\) times. Simply pick a validation set with \\(M=T/K\\) observations at random and think of these as random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), with \\(k=1\\)\n\nHow should you pick \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original data. However, larger values of \\(K\\) will have much higher computation time. tidymodels provides all required tools to conduct \\(K\\)-fold cross-validation. We just have to update our model specification and let tidymodels know which parameters to tune. In our case, we specify the penalty factor \\(\\lambda\\) as well as the mixing factor \\(\\rho\\) as free parameters. Note that it is simple to change an existing workflow with update_model().\n\nlm_model &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- lm_fit |&gt;\n  update_model(lm_model)\n\nFor our sample, we consider a time-series cross-validation sample. This means that we tune our models with 20 random samples of length five years with a validation period of four years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate \\(\\hat{\\text{MSPE}}\\) in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.\n\ndata_folds &lt;- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\ndata_folds\n\n# Time Series Cross Validation Plan \n# A tibble: 20 × 2\n  splits          id     \n  &lt;list&gt;          &lt;chr&gt;  \n1 &lt;split [60/48]&gt; Slice01\n2 &lt;split [60/48]&gt; Slice02\n3 &lt;split [60/48]&gt; Slice03\n4 &lt;split [60/48]&gt; Slice04\n5 &lt;split [60/48]&gt; Slice05\n# ℹ 15 more rows\n\n\nThen, we evaluate the performance for a grid of different penalty values. tidymodels provides functionalities to construct a suitable grid of hyperparameters with grid_regular. The code chunk below creates a \\(10 \\times 3\\) hyperparameters grid. Then, the function tune_grid() evaluates all the models for each fold.\n\nlm_tune &lt;- lm_fit |&gt;\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),\n    metrics = metric_set(rmse)\n  )\n\nAfter the tuning process, we collect the evaluation metrics (the root mean-squared error in our example) to identify the optimal model. Figure 4 illustrates the average validation set’s root mean-squared error for each value of \\(\\lambda\\) and \\(\\rho\\).\n\nautoplot(lm_tune) + \n  aes(linetype = `Proportion of Lasso Penalty`) + \n  guides(linetype = \"none\") +\n  labs(\n    x = \"Penalty factor (lambda)\",\n    y = \"Root MSPE\",\n    title = \"Root MSPE for different penalty factors\"\n  )\n\n\n\n\nFigure 4: Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\n\n\n\n\nFigure 4 shows that the cross-validated MSPE drops for Lasso and Elastic Net and spikes afterward. For Ridge regression, the MSPE increases above a certain threshold. Recall that the larger the regularization, the more restricted the model becomes. Thus, we would choose the model with the lowest MSPE.\n\n\nParallelized workflow\nOur starting point was the question: Which factors determine industry returns? While Avramov et al. (2022) provide a Bayesian analysis related to the research question above, we choose a simplified approach: To illustrate the entire workflow, we now run the penalized regressions for all ten industries. We want to identify relevant variables by fitting Lasso models for each industry returns time series. More specifically, we perform cross-validation for each industry to identify the optimal penalty factor \\(\\lambda\\). Then, we use the set of finalize_*()-functions that take a list or tibble of tuning parameter values and update objects with those values. After determining the best model, we compute the final fit on the entire training set and analyze the estimated coefficients.\nFirst, we define the Lasso model with one tuning parameter.\n\nlasso_model &lt;- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |&gt;\n  set_engine(\"glmnet\")\n\nlm_fit &lt;- lm_fit |&gt;\n  update_model(lasso_model)\n\nThe following task can be easily parallelized to reduce computing time substantially. We use the parallelization capabilities of furrr. Note that we can also just recycle all the steps from above and collect them in a function.\n\nselect_variables &lt;- function(input) {\n  # Split into training and testing data\n  split &lt;- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds &lt;- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune &lt;- lm_fit |&gt;\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Identify the best model and fit with the training data\n  lasso_lowest_rmse &lt;- lm_tune |&gt; select_by_one_std_err(\"rmse\")\n  lasso_final &lt;- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit &lt;- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  estimated_coefficients &lt;- lasso_final_fit |&gt;\n    extract_fit_parsnip() |&gt;\n    tidy() |&gt;\n    mutate(\n      term = str_remove_all(term, \"factor_|macro_|industry_\")\n    )\n\n  return(estimated_coefficients)\n}\n\n# Parallelization\nplan(multisession, workers = availableCores())\n\n# Computation by industry\nselected_factors &lt;- data |&gt;\n  nest(data = -industry) |&gt;\n  mutate(selected_variables = future_map(\n    data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\n\nWhat has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry, we did it for ten in parallel. The final option seed = TRUE is required to make the cross-validation process reproducible. Now, we just have to do some housekeeping and keep only variables that Lasso does not set to zero. We illustrate the results in a heat map in Figure 5.\n\nselected_factors |&gt;\n  unnest(selected_variables) |&gt;\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) |&gt;\n  add_count(term) |&gt;\n  mutate(\n    term = str_remove_all(term, \"NA|ff_|q_\"),\n    term = str_replace_all(term, \"_x_\", \" \"),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) |&gt;\n  filter(term != \"Other\") |&gt;\n  mutate(term = fct_drop(term)) |&gt;\n  complete(industry, term, fill = list(selected = 0)) |&gt;\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"grey30\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )\n\n\n\n\nFigure 5: Grey areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.\n\n\n\n\nThe heat map in Figure 5 conveys two main insights. First, we see a lot of white, which means that many factors, macroeconomic variables, and interaction terms are not relevant for explaining the cross-section of returns across the industry portfolios. In fact, only the market factor and the return-on-equity factor play a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While barely any variable is selected by Lasso for Utilities, many factors are selected for, e.g., High-Tech and Durable, but they do not coincide at all. In other words, there seems to be a clear picture that we do not need many factors, but Lasso does not provide a factor that consistently provides pricing abilities across industries."
  },
  {
    "objectID": "r/factor-selection-via-machine-learning.html#exercises",
    "href": "r/factor-selection-via-machine-learning.html#exercises",
    "title": "Factor Selection via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and then returns the Ridge estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of penalization in a suitable figure.\nNow, write a function that requires three inputs, namely,y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and ’lambda` and then returns the Lasso estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_1\\) penalty.\nAfter you understand what Ridge and Lasso regressions are doing, familiarize yourself with the glmnet() package’s documentation. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients for Ridge and Lasso and for combinations, commonly called Elastic Nets."
  },
  {
    "objectID": "r/cover-and-logo-design.html",
    "href": "r/cover-and-logo-design.html",
    "title": "Cover and Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_daily &lt;- tbl(\n  tidy_finance,\n  \"factors_ff_daily\"\n) |&gt;\n  collect()\n\ndata_plot &lt;- factors_ff_daily |&gt;\n  select(date, mkt_excess) |&gt;\n  group_by(year = floor_date(date, \"year\")) |&gt;\n  mutate(group_id = cur_group_id())\n\ndata_plot &lt;- data_plot |&gt;\n  group_by(group_id) |&gt;\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |&gt;\n  filter(year &gt;= \"1962-01-01\" & year &lt;= \"2021-12-31\")\n\nlevels &lt;- data_plot |&gt;\n  distinct(group_id, vola) |&gt;\n  arrange(vola) |&gt;\n  pull(vola)\n\ncp &lt;- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free &lt;- function() TRUE\ncolors &lt;- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover &lt;- data_plot |&gt;\n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"images/cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"2021-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"images/logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"images/logo-website.png\",\n)\n\nHere is the code to generate the vector graphics for our buttons.\n\nbutton_r &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"2000-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) \n\nggsave(\n plot = button_r +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-r-blue.svg\",\n)\n\nggsave(\n plot = button_r +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[4]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-r-orange.svg\",\n)\n\nbutton_python &lt;- data_plot |&gt;\n  ungroup() |&gt; \n  filter(year == \"1991-01-01\") |&gt; \n  mutate(vola = factor(vola, levels = levels)) |&gt;\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) \n\nggsave(\n plot = button_python +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-python-blue.svg\",\n)\n\nggsave(\n plot = button_python +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[4]), \n width = 100,\n height = 100,\n units = \"px\",\n filename = \"images/button-python-orange.svg\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson."
  },
  {
    "objectID": "r/clean-enhanced-trace-with-r.html",
    "href": "r/clean-enhanced-trace-with-r.html",
    "title": "Clean Enhanced TRACE with R",
    "section": "",
    "text": "This appendix contains code to clean enhanced TRACE with R. It is also available via the following Github gist. Hence, you could also source the function with devtools::source_gist(\"3a05b3ab281563b2e94858451c2eb3a4\"). We need this function in Chapter 4 to download and clean enhanced TRACE trade messages following Dick-Nielsen (2009) and Dick-Nielsen (2014) for enhanced TRACE specifically. WRDS provides SAS code to clean enhanced TRACE data.\nThe function takes a vector of CUSIPs (in cusips), a connection to WRDS (connection) explained in Chapter 3, and a start and end date (start_date and end_date, respectively). Specifying too many CUSIPs will result in very slow downloads and a potential failure due to the size of the request to WRDS. The dates should be within the coverage of TRACE itself, i.e., starting after 2002, and the dates should be supplied using the class date. The output of the function contains all valid trade messages for the selected CUSIPs over the specified period.\n\nclean_enhanced_trace &lt;- function(cusips,\n                                 connection,\n                                 start_date = as.Date(\"2002-01-01\"),\n                                 end_date = today()) {\n\n  # Packages (required)\n  library(tidyverse)\n  library(dbplyr)\n  library(RPostgres)\n\n  # Function checks ---------------------------------------------------------\n  # Input parameters\n  ## Cusips\n  if (length(cusips) == 0 | any(is.na(cusips))) stop(\"Check cusips.\")\n\n  ## Dates\n  if (!is.Date(start_date) | !is.Date(end_date)) stop(\"Dates needed\")\n  if (start_date &lt; as.Date(\"2002-01-01\")) stop(\"TRACE starts later.\")\n  if (end_date &gt; today()) stop(\"TRACE does not predict the future.\")\n  if (start_date &gt;= end_date) stop(\"Date conflict.\")\n\n  ## Connection\n  if (!dbIsValid(connection)) stop(\"Connection issue.\")\n\n  # Enhanced Trace ----------------------------------------------------------\n  # Main file\n  trace_all &lt;- tbl(\n    connection,\n    in_schema(\"trace\", \"trace_enhanced\")\n  ) |&gt;\n    filter(cusip_id %in% cusips) |&gt;\n    filter(trd_exctn_dt &gt;= start_date & trd_exctn_dt &lt;= end_date) |&gt;\n    select(\n      cusip_id, msg_seq_nb, orig_msg_seq_nb,\n      entrd_vol_qt, rptd_pr, yld_pt, rpt_side_cd, cntra_mp_id,\n      trd_exctn_dt, trd_exctn_tm, trd_rpt_dt, trd_rpt_tm,\n      pr_trd_dt, trc_st, asof_cd, wis_fl,\n      days_to_sttl_ct, stlmnt_dt, spcl_trd_fl\n    ) |&gt;\n    collect()\n\n  # Enhanced Trace: Post 06-02-2012 -----------------------------------------\n  # Trades (trc_st = T) and correction (trc_st = R)\n  trace_post_TR &lt;- trace_all |&gt;\n    filter(\n      (trc_st == \"T\" | trc_st == \"R\"),\n      trd_rpt_dt &gt;= as.Date(\"2012-02-06\")\n    )\n\n  # Cancelations (trc_st = X) and correction cancelations (trc_st = C)\n  trace_post_XC &lt;- trace_all |&gt;\n    filter(\n      (trc_st == \"X\" | trc_st == \"C\"),\n      trd_rpt_dt &gt;= as.Date(\"2012-02-06\")\n    )\n\n  # Cleaning corrected and cancelled trades\n  trace_post_TR &lt;- trace_post_TR |&gt;\n    anti_join(trace_post_XC,\n      by = c(\n        \"cusip_id\", \"msg_seq_nb\", \"entrd_vol_qt\",\n        \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\",\n        \"trd_exctn_dt\", \"trd_exctn_tm\"\n      )\n    )\n\n  # Reversals (trc_st = Y)\n  trace_post_Y &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"Y\",\n      trd_rpt_dt &gt;= as.Date(\"2012-02-06\")\n    )\n\n  # Clean reversals\n  ## match the orig_msg_seq_nb of the Y-message to\n  ## the msg_seq_nb of the main message\n  trace_post &lt;- trace_post_TR |&gt;\n    anti_join(trace_post_Y,\n      by = c(\"cusip_id\",\n        \"msg_seq_nb\" = \"orig_msg_seq_nb\",\n        \"entrd_vol_qt\", \"rptd_pr\", \"rpt_side_cd\",\n        \"cntra_mp_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"\n      )\n    )\n\n\n  # Enhanced TRACE: Pre 06-02-2012 ------------------------------------------\n  # Cancelations (trc_st = C)\n  trace_pre_C &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"C\",\n      trd_rpt_dt &lt; as.Date(\"2012-02-06\")\n    )\n\n  # Trades w/o cancelations\n  ## match the orig_msg_seq_nb of the C-message\n  ## to the msg_seq_nb of the main message\n  trace_pre_T &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"T\",\n      trd_rpt_dt &lt; as.Date(\"2012-02-06\")\n    ) |&gt;\n    anti_join(trace_pre_C,\n      by = c(\"cusip_id\",\n        \"msg_seq_nb\" = \"orig_msg_seq_nb\",\n        \"entrd_vol_qt\", \"rptd_pr\", \"rpt_side_cd\",\n        \"cntra_mp_id\", \"trd_exctn_dt\", \"trd_exctn_tm\"\n      )\n    )\n\n  # Corrections (trc_st = W) - W can also correct a previous W\n  trace_pre_W &lt;- trace_all |&gt;\n    filter(\n      trc_st == \"W\",\n      trd_rpt_dt &lt; as.Date(\"2012-02-06\")\n    )\n\n  # Implement corrections in a loop\n  ## Correction control\n  correction_control &lt;- nrow(trace_pre_W)\n  correction_control_last &lt;- nrow(trace_pre_W)\n\n  ## Correction loop\n  while (correction_control &gt; 0) {\n    # Corrections that correct some msg\n    trace_pre_W_correcting &lt;- trace_pre_W |&gt;\n      semi_join(trace_pre_T,\n        by = c(\"cusip_id\", \"trd_exctn_dt\",\n          \"orig_msg_seq_nb\" = \"msg_seq_nb\"\n        )\n      )\n\n    # Corrections that do not correct some msg\n    trace_pre_W &lt;- trace_pre_W |&gt;\n      anti_join(trace_pre_T,\n        by = c(\"cusip_id\", \"trd_exctn_dt\",\n          \"orig_msg_seq_nb\" = \"msg_seq_nb\"\n        )\n      )\n\n    # Delete msgs that are corrected and add correction msgs\n    trace_pre_T &lt;- trace_pre_T |&gt;\n      anti_join(trace_pre_W_correcting,\n        by = c(\"cusip_id\", \"trd_exctn_dt\",\n          \"msg_seq_nb\" = \"orig_msg_seq_nb\"\n        )\n      ) |&gt;\n      union_all(trace_pre_W_correcting)\n\n    # Escape if no corrections remain or they cannot be matched\n    correction_control &lt;- nrow(trace_pre_W)\n    if (correction_control == correction_control_last) {\n      correction_control &lt;- 0\n    }\n    correction_control_last &lt;- nrow(trace_pre_W)\n  }\n\n\n  # Clean reversals\n  ## Record reversals\n  trace_pre_R &lt;- trace_pre_T |&gt;\n    filter(asof_cd == \"R\") |&gt;\n    group_by(\n      cusip_id, trd_exctn_dt, entrd_vol_qt,\n      rptd_pr, rpt_side_cd, cntra_mp_id\n    ) |&gt;\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |&gt;\n    mutate(seq = row_number()) |&gt;\n    ungroup()\n\n  ## Remove reversals and the reversed trade\n  trace_pre &lt;- trace_pre_T |&gt;\n    filter(is.na(asof_cd) | !(asof_cd %in% c(\"R\", \"X\", \"D\"))) |&gt;\n    group_by(\n      cusip_id, trd_exctn_dt, entrd_vol_qt,\n      rptd_pr, rpt_side_cd, cntra_mp_id\n    ) |&gt;\n    arrange(trd_exctn_tm, trd_rpt_dt, trd_rpt_tm) |&gt;\n    mutate(seq = row_number()) |&gt;\n    ungroup() |&gt;\n    anti_join(trace_pre_R,\n      by = c(\n        \"cusip_id\", \"trd_exctn_dt\", \"entrd_vol_qt\",\n        \"rptd_pr\", \"rpt_side_cd\", \"cntra_mp_id\", \"seq\"\n      )\n    ) |&gt;\n    select(-seq)\n\n\n  # Agency trades -----------------------------------------------------------\n  # Combine pre and post trades\n  trace_clean &lt;- trace_post |&gt;\n    union_all(trace_pre)\n\n  # Keep angency sells and unmatched agency buys\n  ## Agency sells\n  trace_agency_sells &lt;- trace_clean |&gt;\n    filter(\n      cntra_mp_id == \"D\",\n      rpt_side_cd == \"S\"\n    )\n\n  # Agency buys that are unmatched\n  trace_agency_buys_filtered &lt;- trace_clean |&gt;\n    filter(\n      cntra_mp_id == \"D\",\n      rpt_side_cd == \"B\"\n    ) |&gt;\n    anti_join(trace_agency_sells,\n      by = c(\n        \"cusip_id\", \"trd_exctn_dt\",\n        \"entrd_vol_qt\", \"rptd_pr\"\n      )\n    )\n\n  # Agency clean\n  trace_clean &lt;- trace_clean |&gt;\n    filter(cntra_mp_id == \"C\") |&gt;\n    union_all(trace_agency_sells) |&gt;\n    union_all(trace_agency_buys_filtered)\n\n\n  # Additional Filters ------------------------------------------------------\n  trace_add_filters &lt;- trace_clean |&gt;\n    mutate(days_to_sttl_ct2 = stlmnt_dt - trd_exctn_dt) |&gt;\n    filter(\n      is.na(days_to_sttl_ct) | as.numeric(days_to_sttl_ct) &lt;= 7,\n      is.na(days_to_sttl_ct2) | as.numeric(days_to_sttl_ct2) &lt;= 7,\n      wis_fl == \"N\",\n      is.na(spcl_trd_fl) | spcl_trd_fl == \"\",\n      is.na(asof_cd) | asof_cd == \"\"\n    )\n\n\n  # Output ------------------------------------------------------------------\n  # Only keep necessary columns\n  trace_final &lt;- trace_add_filters |&gt;\n    arrange(cusip_id, trd_exctn_dt, trd_exctn_tm) |&gt;\n    select(\n      cusip_id, trd_exctn_dt, trd_exctn_tm,\n      rptd_pr, entrd_vol_qt, yld_pt, rpt_side_cd, cntra_mp_id\n    ) |&gt;\n    mutate(trd_exctn_tm = format(as_datetime(trd_exctn_tm), \"%H:%M:%S\"))\n\n  # Return\n  return(trace_final)\n}\n\n\n\n\n\nReferences\n\nDick-Nielsen, Jens. 2009. “Liquidity biases in TRACE.” The Journal of Fixed Income 19 (2): 43–55. https://doi.org/10.3905/jfi.2009.19.2.043.\n\n\n———. 2014. “How to clean enhanced TRACE data.” Working Paper. https://ssrn.com/abstract=2337908."
  },
  {
    "objectID": "r/beta-estimation.html",
    "href": "r/beta-estimation.html",
    "title": "Beta Estimation",
    "section": "",
    "text": "In this chapter, we introduce an important concept in financial economics: the exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio. The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas. We do not go into details about the foundations of market beta but simply refer to any treatment of the CAPM for further information. Instead, we provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: rolling-window estimation and parallelization.\nWe use the following packages throughout this chapter:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(slider)\nlibrary(furrr)\nCompared to previous chapters, we introduce slider (Vaughan 2021) for sliding window functions, and furrr (Vaughan and Dancho 2022) to apply mapping functions in parallel."
  },
  {
    "objectID": "r/beta-estimation.html#estimating-beta-using-monthly-returns",
    "href": "r/beta-estimation.html#estimating-beta-using-monthly-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta using Monthly Returns",
    "text": "Estimating Beta using Monthly Returns\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly CRSP data from our SQLite-database introduced in the previous Chapters 2-4.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, industry, ret_excess) |&gt;\n  collect()\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  select(month, mkt_excess) |&gt;\n  collect()\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(factors_ff_monthly, by = \"month\")\n\nTo estimate the CAPM regression coefficients\n\\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t}\n\\] we regress stock excess returns ret_excess on excess returns of the market portfolio mkt_excess. R provides a simple solution to estimate (linear) models with the function lm(). lm() requires a formula as input that is specified in a compact symbolic form. An expression of the form y ~ model is interpreted as a specification that the response y is modeled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. In addition to standard linear models, lm() provides a lot of flexibility. You should check out the documentation for more information. To start, we restrict the data only to the time series of observations in CRSP that correspond to Apple’s stock (i.e., to permno 14593 for Apple) and compute \\(\\hat\\alpha_i\\) as well as \\(\\hat\\beta_i\\).\n\nfit &lt;- lm(ret_excess ~ mkt_excess,\n  data = crsp_monthly |&gt;\n    filter(permno == \"14593\")\n)\n\nsummary(fit)\n\n\nCall:\nlm(formula = ret_excess ~ mkt_excess, data = filter(crsp_monthly, \n    permno == \"14593\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5169 -0.0598  0.0001  0.0636  0.3944 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.01034    0.00521    1.99    0.048 *  \nmkt_excess   1.39419    0.11576   12.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.114 on 490 degrees of freedom\nMultiple R-squared:  0.228, Adjusted R-squared:  0.227 \nF-statistic:  145 on 1 and 490 DF,  p-value: &lt;2e-16\n\n\nlm() returns an object of class lm which contains all information we usually care about with linear models. summary() returns an overview of the estimated parameters. coefficients(fit) would return only the estimated coefficients. The output above indicates that Apple moves excessively with the market as the estimated \\(\\hat\\beta_i\\) is above one (\\(\\hat\\beta_i \\approx 1.4\\))."
  },
  {
    "objectID": "r/beta-estimation.html#rolling-window-estimation",
    "href": "r/beta-estimation.html#rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Rolling-Window Estimation",
    "text": "Rolling-Window Estimation\nAfter we estimated the regression coefficients on an example, we scale the estimation of \\(\\beta_i\\) to a whole different level and perform rolling-window estimations for the entire CRSP sample. The following function implements the CAPM regression for a data frame (or a part thereof) containing at least min_obs observations to avoid huge fluctuations if the time series is too short. If the condition is violated, that is, the time series is too short, the function returns a missing value.\n\nestimate_capm &lt;- function(data, min_obs = 1) {\n  if (nrow(data) &lt; min_obs) {\n    beta &lt;- as.numeric(NA)\n  } else {\n    fit &lt;- lm(ret_excess ~ mkt_excess, data = data)\n    beta &lt;- as.numeric(coefficients(fit)[2])\n  }\n  return(beta)\n}\n\nNext, we define a function that does the rolling estimation. The slide_period function is able to handle months in its window input in a straightforward manner. We thus avoid using any time-series package (e.g., zoo) and converting the data to fit the package functions, but rather stay in the world of the tidyverse.\nThe following function takes input data and slides across the month vector, considering only a total of months months. The function essentially performs three steps: (i) arrange all rows, (ii) compute betas by sliding across months, and (iii) return a tibble with months and corresponding beta estimates (again particularly useful in the case of daily data). As we demonstrate further below, we can also apply the same function to daily returns data.\n\nroll_capm_estimation &lt;- function(data, months, min_obs) {\n  data &lt;- data |&gt;\n    arrange(month)\n\n  betas &lt;- slide_period_vec(\n    .x = data,\n    .i = data$month,\n    .period = \"month\",\n    .f = ~ estimate_capm(., min_obs),\n    .before = months - 1,\n    .complete = FALSE\n  )\n\n  return(tibble(\n    month = unique(data$month),\n    beta = betas\n  ))\n}\n\nBefore we attack the whole CRSP sample, let us focus on a couple of examples for well-known firms.\n\nexamples &lt;- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n)\n\nIf we want to estimate rolling betas for Apple, we can use mutate(). We take a total of 5 years of data and require at least 48 months with return data to compute our betas. Check out the exercises if you want ot compute beta for different time periods.\n\nbeta_example &lt;- crsp_monthly |&gt;\n  filter(permno == examples$permno[1]) |&gt;\n  mutate(roll_capm_estimation(pick(everything()), months = 60, min_obs = 48)) |&gt;\n  drop_na()\nbeta_example\n\n# A tibble: 445 × 6\n  permno month      industry      ret_excess mkt_excess  beta\n   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1  14593 1984-12-01 Manufacturing     0.170      0.0184  2.05\n2  14593 1985-01-01 Manufacturing    -0.0108     0.0799  1.90\n3  14593 1985-02-01 Manufacturing    -0.152      0.0122  1.88\n4  14593 1985-03-01 Manufacturing    -0.112     -0.0084  1.89\n5  14593 1985-04-01 Manufacturing    -0.0467    -0.0096  1.90\n# ℹ 440 more rows\n\n\nIt is actually quite simple to perform the rolling-window estimation for an arbitrary number of stocks, which we visualize in the following code chunk and the resulting Figure 1.\n\nbeta_examples &lt;- crsp_monthly |&gt;\n  inner_join(examples, by = \"permno\") |&gt;\n  group_by(permno) |&gt;\n  mutate(roll_capm_estimation(pick(everything()), months = 60, min_obs = 48)) |&gt;\n  ungroup() |&gt;\n  select(permno, company, month, beta) |&gt;\n  drop_na()\n\nbeta_examples |&gt;\n  ggplot(aes(\n    x = month, \n    y = beta, \n    color = company,\n    linetype = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of data\"\n  )\n\n\n\n\nFigure 1: The CAPM betas are estimated with monthly data and a rolling window of length 5 years based on adjusted excess returns from CRSP. We use market excess returns from Kenneth French data library."
  },
  {
    "objectID": "r/beta-estimation.html#parallelized-rolling-window-estimation",
    "href": "r/beta-estimation.html#parallelized-rolling-window-estimation",
    "title": "Beta Estimation",
    "section": "Parallelized Rolling-Window Estimation",
    "text": "Parallelized Rolling-Window Estimation\nEven though we could now just apply the function using group_by() on the whole CRSP sample, we advise against doing it as it is computationally quite expensive. Remember that we have to perform rolling-window estimations across all stocks and time periods. However, this estimation problem is an ideal scenario to employ the power of parallelization. Parallelization means that we split the tasks which perform rolling-window estimations across different workers (or cores on your local machine).\nFirst, we nest() the data by permno. Nested data means we now have a list of permno with corresponding time series data and an industry label. We get one row of output for each unique combination of non-nested variables which are permno and industry.\n\ncrsp_monthly_nested &lt;- crsp_monthly |&gt;\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_monthly_nested\n\n# A tibble: 30,072 × 3\n  permno industry      data              \n   &lt;dbl&gt; &lt;chr&gt;         &lt;list&gt;            \n1  10042 Mining        &lt;tibble [264 × 3]&gt;\n2  10043 Services      &lt;tibble [159 × 3]&gt;\n3  10066 Services      &lt;tibble [23 × 3]&gt; \n4  10067 Manufacturing &lt;tibble [12 × 3]&gt; \n5  10068 Finance       &lt;tibble [8 × 3]&gt;  \n# ℹ 30,067 more rows\n\n\nAlternatively, we could have created the same nested data by excluding the variables that we do not want to nest, as in the following code chunk. However, for many applications it is desirable to explicitly state the variables that are nested into the data list-column, so that the reader can track what ends up in there.\n\ncrsp_monthly_nested &lt;- crsp_monthly |&gt;\n  nest(data = -c(permno, industry))\n\nNext, we want to apply the roll_capm_estimation() function to each stock. This situation is an ideal use case for map(), which takes a list or vector as input and returns an object of the same length as the input. In our case, map() returns a single data frame with a time series of beta estimates for each stock. Therefore, we use unnest() to transform the list of outputs to a tidy data frame.\n\ncrsp_monthly_nested |&gt;\n  inner_join(examples, by = \"permno\") |&gt;\n  mutate(beta = map(\n    data,\n    ~ roll_capm_estimation(., months = 60, min_obs = 48)\n  )) |&gt;\n  unnest(beta) |&gt;\n  select(permno, month, beta_monthly = beta) |&gt;\n  drop_na()\n\n# A tibble: 1,410 × 3\n  permno month      beta_monthly\n   &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1  10107 1990-03-01         1.39\n2  10107 1990-04-01         1.38\n3  10107 1990-05-01         1.43\n4  10107 1990-06-01         1.43\n5  10107 1990-07-01         1.45\n# ℹ 1,405 more rows\n\n\nHowever, instead, we want to perform the estimations of rolling betas for different stocks in parallel. If you have a Windows machine, it makes most sense to define multisession, which means that separate R processes are running in the background on the same machine to perform the individual jobs. If you check out the documentation of plan(), you can also see other ways to resolve the parallelization in different environments.\n\nplan(multisession, workers = availableCores())\n\nUsing eight cores, the estimation for our sample of around 25k stocks takes around 20 minutes. Of course, you can speed up things considerably by having more cores available to share the workload or by having more powerful cores. Notice the difference in the code below? All you need to do is to replace map() with future_map().\n\nbeta_monthly &lt;- crsp_monthly_nested |&gt;\n  mutate(beta = future_map(\n    data, ~ roll_capm_estimation(., months = 60, min_obs = 48)\n  )) |&gt;\n  unnest(c(beta)) |&gt;\n  select(permno, month, beta_monthly = beta) |&gt;\n  drop_na()"
  },
  {
    "objectID": "r/beta-estimation.html#estimating-beta-using-daily-returns",
    "href": "r/beta-estimation.html#estimating-beta-using-daily-returns",
    "title": "Beta Estimation",
    "section": "Estimating Beta using Daily Returns",
    "text": "Estimating Beta using Daily Returns\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns.\nFirst, we load daily CRSP data. Note that the sample is large compared to the monthly data, so make sure to have enough memory available.\n\ncrsp_daily &lt;- tbl(tidy_finance, \"crsp_daily\") |&gt;\n  select(permno, month, date, ret_excess) |&gt;\n  collect()\n\nWe also need the daily Fama-French market excess returns.\n\nfactors_ff_daily &lt;- tbl(tidy_finance, \"factors_ff_daily\") |&gt;\n  select(date, mkt_excess) |&gt;\n  collect()\n\nWe make sure to keep only relevant data to save memory space. However, note that your machine might not have enough memory to read the whole daily CRSP sample. In this case, we refer you to the exercises and try working with loops as in Chapter 3.\n\ncrsp_daily &lt;- crsp_daily |&gt;\n  inner_join(factors_ff_daily, by = \"date\") |&gt;\n  select(permno, month, ret_excess, mkt_excess)\n\nJust like above, we nest the data by permno for parallelization.\n\ncrsp_daily_nested &lt;- crsp_daily |&gt;\n  nest(data = c(month, ret_excess, mkt_excess))\n\nThis is what the estimation looks like for a couple of examples using map(). For the daily data, we use the same function as above but only take 3 months of data and require at least 50 daily return observations in these months. These restrictions help us to retrieve somewhat smooth coefficient estimates.\n\ncrsp_daily_nested |&gt;\n  inner_join(examples, by = \"permno\") |&gt;\n  mutate(beta_daily = map(\n    data,\n    ~ roll_capm_estimation(., months = 3, min_obs = 50)\n  )) |&gt;\n  unnest(c(beta_daily)) |&gt;\n  select(permno, month, beta_daily = beta) |&gt;\n  drop_na()\n\n# A tibble: 1,591 × 3\n  permno month      beta_daily\n   &lt;dbl&gt; &lt;date&gt;          &lt;dbl&gt;\n1  10107 1986-05-01      0.898\n2  10107 1986-06-01      0.906\n3  10107 1986-07-01      0.822\n4  10107 1986-08-01      0.900\n5  10107 1986-09-01      1.01 \n# ℹ 1,586 more rows\n\n\nFor the sake of completeness, we tell our session again to use multiple workers for parallelization.\n\nplan(multisession, workers = availableCores())\n\nThe code chunk for beta estimation using daily returns now looks very similar to the one for monthly data. The whole estimation takes around 30 minutes using eight cores and 16gb memory.\n\nbeta_daily &lt;- crsp_daily_nested |&gt;\n  mutate(beta_daily = future_map(\n    data, ~ roll_capm_estimation(., months = 3, min_obs = 50)\n  )) |&gt;\n  unnest(c(beta_daily)) |&gt;\n  select(permno, month, beta_daily = beta) |&gt;\n  drop_na()"
  },
  {
    "objectID": "r/beta-estimation.html#comparing-beta-estimates",
    "href": "r/beta-estimation.html#comparing-beta-estimates",
    "title": "Beta Estimation",
    "section": "Comparing Beta Estimates",
    "text": "Comparing Beta Estimates\nWhat is a typical value for stock betas? To get some feeling, we illustrate the dispersion of the estimated \\(\\hat\\beta_i\\) across different industries and across time below. Figure 2 shows that typical business models across industries imply different exposure to the general market economy. However, there are barely any firms that exhibit a negative exposure to the market factor.\n\ncrsp_monthly |&gt;\n  left_join(beta_monthly, by = c(\"permno\", \"month\")) |&gt;\n  drop_na(beta_monthly) |&gt;\n  group_by(industry, permno) |&gt;\n  summarize(beta = mean(beta_monthly), \n            .groups = \"drop\") |&gt;\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Firm-specific beta distributions by industry\"\n  )\n\n\n\n\nFigure 2: The box plots show the average firm-specific beta estimates by industry.\n\n\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. Figure 3 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks the correlation with the market increases while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\nbeta_monthly |&gt;\n  drop_na(beta_monthly) |&gt;\n  group_by(month) |&gt;\n  reframe(\n    x = quantile(beta_monthly, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1)\n  ) |&gt;\n  ggplot(aes(\n    x = month, \n    y = x, \n    color = as_factor(quantile),\n    linetype = as_factor(quantile)\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly deciles of estimated betas\",\n  )\n\n\n\n\nFigure 3: Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\n\n\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. Then, we use the table to plot a comparison of beta estimates for our example stocks in Figure 4.\n\nbeta &lt;- beta_monthly |&gt;\n  full_join(beta_daily, by = c(\"permno\", \"month\")) |&gt;\n  arrange(permno, month)\n\nbeta |&gt;\n  inner_join(examples, by = \"permno\") |&gt;\n  pivot_longer(cols = c(beta_monthly, beta_daily)) |&gt;\n  drop_na() |&gt;\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name, \n    linetype = name\n    )) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL, \n    title = \"Comparison of beta estimates using monthly and daily data\"\n  )\n\n\n\n\nFigure 4: CAPM betas are computed using 5 years of monthly or 3 months of daily data. The two lines show the monthly estimates based on a rolling window for few exemplary stocks.\n\n\n\n\nThe estimates in Figure 4 look as expected. As you can see, it really depends on the estimation window and data frequency how your beta estimates turn out.\nFinally, we write the estimates to our database such that we can use them in later chapters.\n\n  dbWriteTable(tidy_finance,\n    \"beta\",\n    value = beta,\n    overwrite = TRUE\n  )\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive helps us discover potential errors in our data preparation or estimation procedure. For instance, suppose there was a gap in our output where we do not have any betas. In this case, we would have to go back and check all previous steps to find out what went wrong.\n\nbeta_long &lt;- crsp_monthly |&gt;\n  left_join(beta, by = c(\"permno\", \"month\")) |&gt;\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long |&gt;\n  group_by(month, name) |&gt;\n  summarize(share = sum(!is.na(value)) / n()) |&gt;\n  ggplot(aes(\n    x = month, \n    y = share, \n    color = name,\n    linetype = name\n    )) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\n\n`summarise()` has grouped output by 'month'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nFigure 5: The two lines show the share of securities with beta estimates using 5 years of monthly or 3 months of daily data.\n\n\n\n\nFigure 5 does not indicate any troubles, so let us move on to the next check.\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\n\nbeta_long |&gt;\n  select(name, value) |&gt;\n  drop_na() |&gt;\n  group_by(name) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  )\n\n# A tibble: 2 × 9\n  name          mean    sd   min    q05   q50   q95   max       n\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 beta_daily   0.749 0.926 -43.7 -0.447 0.686  2.23  56.6 3233745\n2 beta_monthly 1.10  0.713 -13.0  0.125 1.03   2.32  10.3 2102854\n\n\nThe summary statistics also look plausible for the two estimation procedures.\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators should be at least positively correlated (although not perfectly as the estimators are based on different sample periods and frequencies).\n\nbeta |&gt;\n  select(beta_daily, beta_monthly) |&gt;\n  cor(use = \"complete.obs\")\n\n             beta_daily beta_monthly\nbeta_daily        1.000        0.323\nbeta_monthly      0.323        1.000\n\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data as most readers should be able to replicate them due to potential memory limitations that might arise with the daily data."
  },
  {
    "objectID": "r/beta-estimation.html#exercises",
    "href": "r/beta-estimation.html#exercises",
    "title": "Beta Estimation",
    "section": "Exercises",
    "text": "Exercises\n\nCompute beta estimates based on monthly data using 1, 3, and 5 years of data and impose a minimum number of observations of 10, 28, and 48 months with return data, respectively. How strongly correlated are the estimated betas?\nCompute beta estimates based on monthly data using 5 years of data and impose different numbers of minimum observations. How does the share of permno-month observations with successful beta estimates vary across the different requirements? Do you find a high correlation across the estimated betas?\nInstead of using future_map(), perform the beta estimation in a loop (using either monthly or daily data) for a subset of 100 permnos of your choice. Verify that you get the same results as with the parallelized code from above.\nFilter out the stocks with negative betas. Do these stocks frequently exhibit negative betas, or do they resemble estimation errors?\nCompute beta estimates for multi-factor models such as the Fama-French 3 factor model. For that purpose, you extend your regression to \\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\sum\\limits_{j=1}^k\\beta_{i,k}(r_{j, t}-r_{f,t})+\\varepsilon_{i, t}\n\\] where \\(r_{j, t}\\) are the \\(k\\) factor returns. Thus, you estimate 4 parameters (\\(\\alpha_i\\) and the slope coefficients). Provide some summary statistics of the cross-section of firms and their exposure to the different factors."
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html",
    "href": "python/wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nThis chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the sqlite-database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nFirst, we load the packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom pandas.tseries.offsets import DateOffset"
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "python/wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with Python as SQL. We use the sqlalchemy package to establish a connection to the WRDS database because it already contains a suitable driver.1\n\nfrom sqlalchemy import create_engine\n\nTo establish a connection, you use the function create_engine() with a connection string. Note that you need to replace the user and password arguments with your own credentials. We defined system variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world (these system variables are stored in the Python environment and loaded with the os.environ() function).\nAdditionally, you have to use multi-factor (i.e., two-factor) authentication since May 2023 when establishing a remote connection to WRDS. You have two choices to provide the additional identification. First, if you have Duo Push enabled for your WRDS account, you will receive a push notification on your mobile phone when trying to establish a connection with the code below. Upon accepting the notification, you can continue your work. Second, you can log in to a WRDS website that requires multi-factor authentication with your username and the same IP address. Once you have successfully identified yourself on the website, your username-IP combination will be remembered for 30 days, and you can comfortably use the remote connection below.\n\nimport os \nconnection_string = (\"postgresql+psycopg2://\" + \n                      os.environ[\"USER\"] + \":\" + \n                      os.environ[\"PASSWORD\"] +\n                      \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\")\nwrds = create_engine(connection_string, pool_pre_ping=True)\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition)."
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "python/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds engine object that we just created to first access monthly CRSP return data. Actually, we need three tables to get the desired data: (i) the CRSP monthly security file msf, (ii) the identifying information msenames, and (iii) the delisting information msedelist.\nWe use the three remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we keep only data in the time windows of interest, (ii) we keep only US-listed stocks as identified via share codes shrcd 10 and 11, and (iii) we keep only months within permno-specific start dates namedt and end dates nameendt. In addition, we add delisting codes and returns. You can read up in the great textbook of Bali, Engle, and Murray (2016) for an extensive discussion on the filters we apply in the code below.\n\ncrsp_monthly_query = (\n  \"\"\"SELECT msf.permno, msf.date, \n            date_trunc('month', msf.date)::date as month,\n            msf.ret, msf.shrout, msf.altprc, \n            msenames.exchcd, msenames.siccd,\n            msedelist.dlret, msedelist.dlstcd\n        FROM crsp.msf AS msf\n        LEFT JOIN crsp.msenames as msenames\n               ON msf.permno = msenames.permno AND \n                  msenames.namedt &lt;= msf.date AND\n                  msf.date &lt;= msenames.nameendt\n        LEFT JOIN crsp.msedelist as msedelist\n               ON msf.permno = msedelist.permno AND\n                  date_trunc('month', msf.date)::date = \n                    date_trunc('month', msedelist.dlstdt)::date\n        WHERE msf.date BETWEEN '01/01/1960' AND '12/31/2021' AND\n              msenames.shrcd IN (10, 11)\"\"\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=crsp_monthly_query,\n    con=wrds,\n    dtype={\"permno\": np.int64, \"exchcd\": np.int64, \"siccd\": np.int64},\n    parse_dates={\"date\", \"month\"})\n  .assign(shrout = lambda x: x[\"shrout\"] * 1000)\n)\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares shrout and the last traded price in a month altprc. Note that in contrast to returns ret, these two variables are not adjusted ex-post for any corporate actions like stock splits. Moreover, the altprc is negative whenever the last traded price does not exist, and CRSP decides to report the mid-quote of the last available order book instead. Hence, we take the absolute value of the market cap. We also keep the market cap in millions of USD just for convenience as we do not want to print huge numbers in our figures and tables. In addition, we set zero market cap to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly = (crsp_monthly\n  .assign(mktcap = lambda x: abs(x[\"shrout\"] * x[\"altprc\"] / 1000000))\n  .assign(mktcap = lambda x: x[\"mktcap\"].replace(0, np.nan))\n)\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag = (crsp_monthly\n  .assign(\n    month = lambda x: x[\"month\"] + DateOffset(months=1),\n    mktcap_lag = lambda x: x[\"mktcap\"]\n   )\n  .get([\"permno\", \"month\", \"mktcap_lag\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(mktcap_lag, \n         how=\"left\", \n         on=[\"permno\", \"month\"])\n)\n\nNext, we follow Bali, Engle, and Murray (2016) in transforming listing exchange codes to explicit exchange names.\n\ndef assign_exchange(exchcd):\n    if exchcd in [1, 31]:\n        return \"NYSE\"\n    elif exchcd in [2, 32]:\n        return \"AMEX\"\n    elif exchcd in [3, 33]:\n        return \"NASDAQ\"\n    else:\n        return \"Other\"\n\ncrsp_monthly[\"exchange\"] = crsp_monthly[\"exchcd\"].apply(assign_exchange)\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ndef assign_industry(siccd):\n    if 1 &lt;= siccd &lt;= 999:\n        return \"Agriculture\"\n    elif 1000 &lt;= siccd &lt;= 1499:\n        return \"Mining\"\n    elif 1500 &lt;= siccd &lt;= 1799:\n        return \"Construction\"\n    elif 2000 &lt;= siccd &lt;= 3999:\n        return \"Manufacturing\"\n    elif 4000 &lt;= siccd &lt;= 4899:\n        return \"Transportation\"\n    elif 4900 &lt;= siccd &lt;= 4999:\n        return \"Utilities\"\n    elif 5000 &lt;= siccd &lt;= 5199:\n        return \"Wholesale\"\n    elif 5200 &lt;= siccd &lt;= 5999:\n        return \"Retail\"\n    elif 6000 &lt;= siccd &lt;= 6799:\n        return \"Finance\"\n    elif 7000 &lt;= siccd &lt;= 8999:\n        return \"Services\"\n    elif 9000 &lt;= siccd &lt;= 9999:\n        return \"Public\"\n    else:\n        return \"Missing\"\n\ncrsp_monthly[\"industry\"] = crsp_monthly[\"siccd\"].apply(assign_industry)\n\nWe also construct returns adjusted for delistings as described by Bali, Engle, and Murray (2016). The delisting of a security usually results when a company ceases operations, declares bankruptcy, merges, does not meet listing requirements, or seeks to become private. The adjustment tries to reflect the returns of investors who bought the stock in the month before the delisting and held it until the delisting date. After this transformation, we can drop the delisting returns and codes.\n\nconditions_delisting = [\n    crsp_monthly[\"dlstcd\"].isna(),\n    (~crsp_monthly[\"dlstcd\"].isna()) & (~crsp_monthly[\"dlret\"].isna()),\n    crsp_monthly[\"dlstcd\"].isin([500, 520, 580, 584]) | \n        ((crsp_monthly[\"dlstcd\"] &gt;= 551) & (crsp_monthly[\"dlstcd\"] &lt;= 574)),\n    crsp_monthly[\"dlstcd\"] == 100\n]\n\nchoices_delisting = [\n    crsp_monthly[\"ret\"],\n    crsp_monthly[\"dlret\"],\n    -0.30,\n    crsp_monthly[\"ret\"]\n]\n\ncrsp_monthly = (crsp_monthly\n  .assign(\n    ret_adj = np.select(conditions_delisting, choices_delisting, default=-1)\n  )\n  .drop(columns=[\"dlret\", \"dlstcd\"])\n)\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop adjusted returns and the risk-free rate from our tibble. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the tibble factors_ff_monthly.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\nfactors_ff_monthly = pd.read_sql_query(\n  sql=\"SELECT month, rf FROM factors_ff_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}}\n)\n  \ncrsp_monthly = (crsp_monthly\n  .merge(factors_ff_monthly, \n         how=\"left\", \n         on=\"month\")\n  .assign(ret_excess = lambda x: x[\"ret_adj\"] - x[\"rf\"])\n  .assign(ret_excess = lambda x: x[\"ret_excess\"].clip(lower=-1))\n  .drop(columns = [\"ret_adj\", \"rf\"])\n)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly = (crsp_monthly\n  .dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n)\n\nFinally, we store the monthly CRSP file in our database.\n\n(crsp_monthly\n  .assign(\n    date = lambda x: (x[\"date\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"),\n    month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\")\n  )\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)"
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "python/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2021, there were 2,779 stocks with a primary listing on NASDAQ, 1,395 on NYSE, 145 on AMEX, and only one belonged to the other category.\n\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format\n\nsecurities_per_exchange = (crsp_monthly\n  .groupby([\"exchange\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nsecurities_per_exchange_figure = (ggplot(securities_per_exchange, \n        aes(x=\"date\", y=\"n\", \n            color=\"exchange\"))\n  + geom_line()\n  + labs(x=\"\", y=\"\", color=\"\",\n         title=\"Monthly number of securities by listing exchange\")\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nsecurities_per_exchange_figure.draw()\nFigure 1: ?(caption)\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange. All values in Figure 2 are at the end of 2021 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\n\ncpi_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM cpi_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}}\n)\n\nmarket_cap_per_exchange = (crsp_monthly\n  .merge(cpi_monthly, \n         how=\"left\", \n         on=\"month\")\n  .groupby([\"month\", \"exchange\"])\n  .apply(\n    lambda group: pd.Series({\n        \"mktcap\": (group[\"mktcap\"].sum() / group[\"cpi\"].mean())\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_exchange_figure = (ggplot(market_cap_per_exchange, \n        aes(x=\"month\", y=\"mktcap / 1000\", \n            color=\"exchange\", linetype=\"exchange\"))\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by listing exchange in billions of Dec 2021 USD\"\n  )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nmarket_cap_per_exchange_figure.draw()\nFigure 2: ?(caption)\n\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\n\nsecurities_per_industry = (crsp_monthly\n  .groupby([\"industry\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nsecurities_per_industry_figure =(ggplot(securities_per_industry, \n        aes(x=\"date\", y=\"n\", color=\"industry\"))\n  + geom_line()\n  + labs(x=\"\", y=\"\", color=\"\",\n         title=\"Monthly number of securities by industry\")\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nsecurities_per_industry_figure.draw()\nFigure 3: ?(caption)\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2021 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\n\nmarket_cap_per_industry = (crsp_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"month\")\n  .groupby([\"month\", \"industry\"])\n  .apply(\n    lambda group: pd.Series({\n        \"mktcap\": (group[\"mktcap\"].sum() / group[\"cpi\"].mean())\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_industry_figure = (ggplot(market_cap_per_industry, \n        aes(x=\"month\", y=\"mktcap / 1000\", color=\"industry\"))\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", \n      title=\"Monthly market cap by industry in billions of Dec 2021 USD\"\n  )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nmarket_cap_per_industry_figure.draw()\nFigure 4: ?(caption)"
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "python/wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data. While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire data set to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easy to handle. That is, instead of downloading data about many stocks all at once, download the data in small batches for each stock consecutively. Such operations can be implemented in for()-loops, where we download, prepare, and store the data for a single stock in each iteration. This operation might nonetheless take a couple of hours, so you have to be patient either way (we often run such code overnight). To keep track of the progress, you can use txtProgressBar(). Eventually, we end up with more than 68 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno, date, and month alongside the excess returns. We thus ensure that our local database contains only the data we actually use and that we can load the full daily data into our memory later. Notice that we also use the function dbWriteTable() here with the option to append the new data to an existing table, when we process the second and all following batches.\n\n# TODO: Migrate to SQLite\n\nTo the best of our knowledge, the daily CRSP data does not require any adjustments like the monthly data. The adjustment of the monthly data comes from the fact that CRSP aggregates daily data into monthly observations and has to decide which prices and returns to record if a stock gets delisted. In the daily data, there is simply no price or return after delisting, so there is also no aggregation problem."
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "python/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat data",
    "text": "Preparing Compustat data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies. We follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, (ii) in the standard format (i.e., consolidated information in standard presentation), and (iii) only data in the desired time window.\n\ncompustat_query = (\n  \"\"\"SELECT gvkey, datadate, seq, ceq, at, lt, txditc, txdb, \n            itcb, pstkrv, pstkl, pstk, capx, oancf\n        FROM comp.funda\n        WHERE indfmt = 'INDL' AND \n              datafmt = 'STD' AND \n              consol = 'C' AND\n              datadate BETWEEN '01/01/1960' AND '12/31/2021'\"\"\"\n)\n\ncompustat = (pd.read_sql_query(\n    sql=compustat_query,\n    con=wrds,\n    dtype={\"gvkey\": np.int64},\n    parse_dates={\"datadate\"})\n)\n\nNext, we calculate the book value of preferred stock and equity inspired by the variable definition in Ken French’s data library. Note that we set negative or zero equity to missing which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat = (compustat\n  .assign(be = lambda x: (x[\"seq\"].combine_first(x[\"ceq\"] + x[\"pstk\"])\n            .combine_first(x[\"at\"] - x[\"lt\"]) +\n            x[\"txditc\"].combine_first(x[\"txdb\"] + x[\"itcb\"]).fillna(0) -\n            x[\"pstkrv\"].combine_first(x[\"pstkl\"]).combine_first(x[\"pstk\"])\n            .fillna(0)))\n  .assign(\n    be = lambda x: x[\"be\"].apply(lambda y: np.nan if y &lt;= 0 else y)\n  )\n)\n\nWe keep only the last available information for each firm-year group. Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2021). Therefore, datadate is not the date when data was made available to the public. Check out the exercises for more insights into the peculiarities of datadate.\n\ncompustat = (compustat\n  .assign(\n    year = lambda x: pd.DatetimeIndex(x[\"datadate\"]).year\n  )\n  .sort_values(\"datadate\")\n  .groupby([\"gvkey\", \"year\"], as_index=False)\n  .tail(1)\n)\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n(compustat\n  .assign(\n    datadate = lambda x: (x[\"datadate\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\")\n  )\n  .to_sql(name=\"compustat\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)"
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "python/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP). The linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via the SQL verb CURRENT_DATE.\n\nccmxpf_linktable_query = (\n  \"\"\"SELECT lpermno AS permno, gvkey, linkdt, \n            COALESCE(linkenddt, CURRENT_DATE) AS linkenddt\n        FROM crsp.ccmxpf_linktable\n        WHERE linktype IN ('LU', 'LC') AND\n              linkprim IN ('P', 'C') AND\n              usedflag = 1\"\"\"\n)\n\nccmxpf_linktable = (pd.read_sql_query(\n    sql=ccmxpf_linktable_query,\n    con=wrds,\n    dtype={\"permno\": np.int64, \"gvkey\": np.int64},\n    parse_dates={\"linkdt\", \"linkenddt\"})\n)\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links = (crsp_monthly\n  .merge(ccmxpf_linktable, how=\"inner\", on=\"permno\")\n  .query(\"~gvkey.isnull() & (date &gt;= linkdt) & (date &lt;= linkenddt)\")\n  .get([\"permno\", \"gvkey\", \"date\"])\n)\n\ncrsp_monthly = (crsp_monthly\n  .merge(ccm_links, how=\"left\", on=[\"permno\", \"date\"])\n)\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\n(crsp_monthly\n  .assign(\n    date = lambda x: (x[\"date\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"),\n    month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\")\n  )\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 60s but hovers around 80% for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\n\nfrom mizani.formatters import percent_format\n\nshare_with_be = (crsp_monthly\n  .assign(\n    year = lambda x: pd.DatetimeIndex(x[\"month\"]).year\n  )\n  .sort_values(\"date\")\n  .groupby([\"permno\", \"year\"], as_index=False)\n  .tail(1)\n  .merge(compustat, how=\"left\", on=[\"gvkey\", \"year\"])\n  .groupby([\"exchange\", \"year\"])\n  .apply(lambda x: pd.Series({\n         \"share\": x[\"permno\"][~x[\"be\"].isnull()].nunique()/x[\"permno\"].nunique()\n    }))\n  .reset_index()\n)\n\nshare_with_be_figure = (ggplot(share_with_be, \n        aes(x=\"year\", y=\"share\", color=\"exchange\"))\n  + geom_line()\n  + labs(x=\"\", y=\"\", color=\"\",\n         title=\"Share of securities with book equity values by exchange\")\n  + scale_y_continuous(labels=percent_format())\n  + coord_cartesian(ylim=(0, 1))\n)\nshare_with_be_figure.draw()\nFigure 5: ?(caption)"
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#exercises",
    "href": "python/wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCompute mkt_cap_lag using shift() rather than joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are they different?\nIn the main part, we look at the distribution of market capitalization across exchanges and industries. Now, plot the average market capitalization of firms for each exchange and industry. What do you find?\ndatadate refers to the date to which the fiscal year of a corresponding firm refers to. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat data in funda and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?"
  },
  {
    "objectID": "python/wrds-crsp-and-compustat.html#footnotes",
    "href": "python/wrds-crsp-and-compustat.html#footnotes",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn alternative to establish a connection to WRDS is to use the (WRDS-Py)[https://pypi.org/project/wrds/] library. We chose to work with sqlalchemy to show how to access `PostgreSQL´ engines in general.↩︎"
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html",
    "href": "python/univariate-portfolio-sorts.html",
    "title": "Univariate Portfolio Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Chapter 9.\nA univariate portfolio sort considers only one sorting variable \\(x_{t-1,i}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\).\nThe objective is to assess the cross-sectional relation between \\(x_{t-1,i}\\) and, typically, stock excess returns \\(r_{t,i}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nThe current chapter relies on the following set of packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom plotnine import *\n\nimport statsmodels.api as sm\nfrom mizani.formatters import percent_format"
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#data-preparation",
    "href": "python/univariate-portfolio-sorts.html#data-preparation",
    "title": "Univariate Portfolio Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start with loading the required data from our SQLite-database introduced in Chapters 2-4. In particular, we use the monthly CRSP sample as our asset universe. Once we form our portfolios, we use the Fama-French market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the tibble with market betas computed in the previous chapter.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT permno, month, ret_excess, mktcap_lag FROM crsp_monthly\",\n    con=tidy_finance,\n    dtype={\"permno\": np.int32},\n    parse_dates={\"month\": {\"unit\": \"D\",\"origin\": \"unix\"}})\n  .dropna()\n)\n\nfactors_ff_monthly = (pd.read_sql_query(\n    sql=\"SELECT month, mkt_excess FROM factors_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\",\"origin\": \"unix\"}})\n  .dropna()\n)\n\nbeta = (pd.read_sql_query(\n    sql=\"SELECT permno, month, beta_monthly FROM beta\",\n    con=tidy_finance,\n    dtype={\"permno\": np.int32},\n    parse_dates={\"month\": {\"unit\": \"D\",\"origin\": \"unix\"}})\n  .dropna()\n)"
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "href": "python/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "title": "Univariate Portfolio Sorts",
    "section": "Sorting by Market Beta",
    "text": "Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as crsp_monthly['beta_lag'] = crsp_monthly.groupby('permno')['beta'].shift(1) instead. This procedure, however, does not work correctly if there are non-explicit missing values in the time series.\n\nbeta_lag = (beta\n  .assign(month = lambda x: x[\"month\"] + pd.DateOffset(months=1))\n  .get([\"permno\", \"month\", \"beta_monthly\"])\n  .rename(columns={\"beta_monthly\": \"beta_lag\"})\n  .dropna()\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(beta_lag, \n         how=\"inner\", \n         on=[\"permno\", \"month\"])\n)\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in np.average().\n\nbeta_portfolios = (data_for_sorts\n    .groupby(\"month\", group_keys=False)\n    .apply(lambda x: x.assign(portfolio=pd.qcut(x[\"beta_lag\"], \n                                                q=[0, 0.5, 1], \n                                                labels=[\"low\", \"high\"])))\n    .groupby([\"portfolio\",\"month\"], group_keys=False)\n    .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n    .reset_index(name=\"ret\")\n    )"
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#performance-evaluation",
    "href": "python/univariate-portfolio-sorts.html#performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero, i.e., you do not need to invest money to realize this strategy in the absence of frictions.\n\nbeta_longshort = (beta_portfolios\n  .pivot_table(index=\"month\", \n               columns=\"portfolio\", \n               values=\"ret\")\n  .reset_index()\n  .assign(long_short = lambda x: x[\"high\"] - x[\"low\"])\n)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. Researchers often default on choosing a pre-specified lag length of 6 months.\n\nmodel_fit = (sm.OLS\n  .from_formula(\"long_short ~ 1\", data=beta_longshort)\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n  .summary(slim=True)\n)\nmodel_fit\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlong_short\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nNo. Observations:\n695\nF-statistic:\nnan\n\n\nCovariance Type:\nHAC\nProb (F-statistic):\nnan\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n0.0003\n0.001\n0.222\n0.824\n-0.002\n0.003\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 6 lags and without small sample correction\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint hence does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high beta stocks by shorting low beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate."
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "python/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "Univariate Portfolio Sorts",
    "section": "Functional Programming for Portfolio Sorts",
    "text": "Functional Programming for Portfolio Sorts\nNow we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we define a function that give us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use np.quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the pd.cut() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases, where the number of portfolio constituents differs substantially due to the distribution of the characteristics, require careful consideration and, depending on the application, might require customized sorting approaches.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios):\n    breakpoints = np.quantile(data[sorting_variable].dropna(), \n                              np.linspace(0, 1, n_portfolios + 1), \n                              method=\"linear\")\n    assigned_portfolios = pd.cut(data[sorting_variable],\n                                 bins=breakpoints,\n                                 labels=range(1, breakpoints.size),\n                                 include_lowest=True)\n    return assigned_portfolios\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"month\", group_keys=False)\n  .apply(lambda x: x.assign(portfolio = assign_portfolio(x, 'beta_lag', 10)))\n  .groupby([\"portfolio\", \"month\"], group_keys=False)\n  .apply(lambda x: x.assign(ret=np.average(x[\"ret_excess\"], \n                                           weights=x[\"mktcap_lag\"])))\n  .merge(factors_ff_monthly, \n         how=\"left\", \n         on=\"month\")\n)"
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#more-performance-evaluation",
    "href": "python/univariate-portfolio-sorts.html#more-performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "More Performance Evaluation",
    "text": "More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary = (beta_portfolios\n  .groupby(\"portfolio\", group_keys=False)\n  .apply(lambda x: x.assign(alpha=sm.OLS.from_formula(\n    formula=\"ret ~ 1 + mkt_excess\", data=x).fit().params[0],\n                                      beta=sm.OLS.from_formula(\n                                      formula=\"ret ~ 1 + mkt_excess\", data=x)\n                                      .fit().params[1],\n                                      ret=x[\"ret\"].mean()).tail(1))\n  .get([\"portfolio\", \"alpha\", \"beta\", \"ret\"])\n  .reset_index(drop=True)\n)\n\nFigure 1 illustrates the CAPM alphas of beta-sorted portfolios. It shows that low beta portfolios tend to exhibit positive alphas, while high beta portfolios exhibit negative alphas.\n\nplot_beta_portfolios_summary = (ggplot(beta_portfolios_summary, \n    aes(x=\"portfolio\", y=\"alpha\", fill=\"portfolio\"))\n  + geom_bar(stat=\"identity\")\n  + labs(title=\"CAPM alphas of beta-sorted portfolios\",\n         x=\"Portfolio\", y=\"CAPM alpha\", fill=\"Portfolio\")\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\"))\nplot_beta_portfolios_summary.draw()\n\n\n\n\nFigure 1: Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\n\n\n\n\nThese results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero."
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "href": "python/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "title": "Univariate Portfolio Sorts",
    "section": "The Security Market Line and Beta Portfolios",
    "text": "The Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 2 illustrates the security market line: We see that (not surprisingly) the high beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm = (sm.OLS.from_formula(\"ret ~ 1 + beta\", data=beta_portfolios_summary)\n            .fit().params)\n\nplot_sml_capm = (ggplot(beta_portfolios_summary,\n    aes(x=\"beta\", y=\"ret\", color=\"portfolio\"))\n  + geom_point()\n  + geom_abline(intercept=0,\n                slope=factors_ff_monthly[\"mkt_excess\"].mean(),\n                linetype=\"solid\")\n  + geom_abline(intercept=sml_capm[\"Intercept\"],\n                slope=sml_capm[\"beta\"],\n                linetype=\"dashed\")\n  + scale_y_continuous(labels=percent_format(),\n                       limits=(0, factors_ff_monthly[\"mkt_excess\"].mean()*2))\n  + scale_x_continuous(limits=(0, 2))\n  + labs(x=\"Beta\", y=\"Excess return\", color=\"Portfolio\",\n         title=\"Average portfolio excess returns and average beta estimates\")\n)\nplot_sml_capm.draw()\n\n\n\n\nFigure 2: Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort = (beta_portfolios\n  .assign(portfolio=lambda x: x[\"portfolio\"]\n          .apply(lambda y: \"high\" if y == x[\"portfolio\"].max()\n                                 else (\"low\" if y == x[\"portfolio\"].min()\n                                       else y)))\n  .query(\"portfolio in ['low', 'high']\")\n  .pivot_table(index=\"month\", columns=\"portfolio\", values=\"ret\")\n  .assign(long_short=lambda x: x[\"high\"] - x[\"low\"])\n  .merge(factors_ff_monthly, how=\"left\", on=\"month\")\n)\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\nmodel_fit = (sm.OLS.from_formula(\"long_short ~ 1\", data=beta_longshort)\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n  .summary(slim=True)\n)\nmodel_fit\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlong_short\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nNo. Observations:\n695\nF-statistic:\nnan\n\n\nCovariance Type:\nHAC\nProb (F-statistic):\nnan\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n0.0023\n0.003\n0.750\n0.453\n-0.004\n0.008\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 1 lags and without small sample correction\n\n\nHowever, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha, although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting against beta factor (Frazzini and Pedersen 2014). Betting against beta corresponds to a strategy that shorts high beta stocks and takes a (levered) long position in low beta stocks. If borrowing constraints prevent investors from taking positions on the SML they are instead incentivized to buy high beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital constraint investors with lower risk aversion.\n\nmodel_fit = (sm.OLS.from_formula(\"long_short ~ 1 + mkt_excess\", data=beta_longshort)\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n  .summary(slim=True)\n)\nmodel_fit\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlong_short\nR-squared:\n0.451\n\n\nModel:\nOLS\nAdj. R-squared:\n0.451\n\n\nNo. Observations:\n695\nF-statistic:\n262.8\n\n\nCovariance Type:\nHAC\nProb (F-statistic):\n2.35e-50\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.0045\n0.002\n-2.014\n0.044\n-0.009\n-0.000\n\n\nmkt_excess\n1.1654\n0.072\n16.212\n0.000\n1.024\n1.306\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 1 lags and without small sample correction\n\n\nFigure 3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates no consistent striking patterns over the last years - each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort_year = (beta_longshort\n  .assign(year=lambda x: x[\"month\"].dt.year)\n  .groupby(\"year\")\n  .aggregate(low = (\"low\", lambda x: 1-(1 + x).prod()),\n             high = (\"high\", lambda x: 1-(1 + x).prod()),\n             long_short = (\"long_short\", lambda x: 1-(1 + x).prod()))\n  .reset_index()\n  .melt(id_vars=\"year\", var_name=\"name\", value_name=\"value\")\n)\n\nplot_beta_longshort_year = (ggplot(\n  beta_longshort_year, aes(x=\"year\", y=\"value\", fill=\"name\"))\n  + geom_col(position='dodge')\n  + facet_wrap(\"~name\", ncol=1)\n  + theme(legend_position=\"none\")\n  + scale_color_discrete(guide=False)\n  + scale_y_continuous(labels=percent_format())\n  + labs(x=\"\", y=\"\", title=\"Annual returns of beta portfolios\")\n)\nplot_beta_longshort_year.draw()\n\n\n\n\nFigure 3: We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\nOverall, this chapter shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next chapter, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis."
  },
  {
    "objectID": "python/univariate-portfolio-sorts.html#exercises",
    "href": "python/univariate-portfolio-sorts.html#exercises",
    "title": "Univariate Portfolio Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nTake the two long-short beta strategies based on different numbers of portfolios and compare the returns. Is there a significant difference in returns? How do the Sharpe ratios compare between the strategies? Find one additional portfolio evaluation statistic and compute it.\nWe plotted the alphas of the ten beta portfolios above. Write a function that tests these estimates for significance. Which portfolios have significant alphas?\nThe analysis here is based on betas from monthly returns. However, we also computed betas from daily returns. Re-run the analysis and point out differences in the results.\nGiven the results in this chapter, can you define a long-short strategy that yields positive abnormal returns (i.e., alphas)? Plot the cumulative excess return of your strategy and the market excess return for comparison."
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html",
    "href": "python/replicating-fama-and-french-factors.html",
    "title": "Replicating Fama and French Factors",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we provide a replication of the famous Fama and French factor portfolios. The Fama and French three-factor model (see Fama and French 1993) is a cornerstone of asset pricing. On top of the market factor represented by the traditional CAPM beta, the model includes the size and value factors to explain the cross section of returns. We introduce both factors in Chapter 9, and their definition remains the same. Size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nThe current chapter relies on this set of packages.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#data-preparation",
    "href": "python/replicating-fama-and-french-factors.html#data-preparation",
    "title": "Replicating Fama and French Factors",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use CRSP and Compustat as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it. Hence, there is nothing new below and we only load data from our database introduced in Chapters 2-4.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\ndata_ff = (pd.read_sql_query(\n    sql=\"SELECT permno, gvkey, month, ret_excess, mktcap, mktcap_lag, exchange FROM crsp_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\nbook_equity = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be FROM compustat\",\n    con=tidy_finance,\n    parse_dates={\"datadate\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\nfactors_ff_monthly = (pd.read_sql_query(\n    sql=\"SELECT month, smb, hml FROM factors_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n  .dropna()\n)\n\nYet when we start merging our data set for computing the premiums, there are a few differences to Chapter 9. First, Fama and French form their portfolios in June of year \\(t\\), whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year \\(t+1\\).\nSecond, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year \\(t - 1\\) and the book equity reported in year \\(t-1\\), i.e., the datadate is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity.\nTo implement all these time lags, we again employ the temporary sorting_date-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year. We ensure this by a call of drop_duplicates() at the end of the chunk below.\n\nme_ff = (data_ff\n  .query(\"month.dt.month == 6\")\n  .assign(sorting_date = lambda x: x[\"month\"] + pd.DateOffset(months=1))\n  .get([\"permno\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"me_ff\"})\n)\n\nme_ff_dec = (data_ff\n  .query(\"month.dt.month == 12\")\n  .assign(sorting_date = lambda x: x[\"month\"] + pd.DateOffset(months=7))\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"bm_me\"})\n)\n\nbm_ff = (book_equity\n  .assign(\n    sorting_date = lambda x: pd.to_datetime((x[\"datadate\"].dt.year + 1).astype(str) + \"0701\", format=\"%Y%m%d\")\n    )\n  .rename(columns={\"be\": \"bm_be\"})\n  .merge(me_ff_dec, \n         how=\"inner\", \n         on=[\"gvkey\", \"sorting_date\"])\n  .assign(bm_ff = lambda x: x[\"bm_be\"] / x[\"bm_me\"])\n  .get([\"permno\", \"sorting_date\", \"bm_ff\"])\n)\n\nvariables_ff = (me_ff\n  .merge(bm_ff, \n         how=\"inner\", \n         on=[\"permno\", \"sorting_date\"])\n )"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "href": "python/replicating-fama-and-french-factors.html#portfolio-sorts",
    "title": "Replicating Fama and French Factors",
    "section": "Portfolio Sorts",
    "text": "Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30%- and 70%-percentiles, and they use independent sorts. The sorts for book-to-market require an adjustment to the function in Chapter 9 because the seq() we would produce does not produce the right breakpoints. Instead of n_portfolios, we now specify percentiles, which take the breakpoint-sequence as an object specified in the function’s call. Specifically, we give percentiles = c(0, 0.3, 0.7, 1) to the function. Additionally, we perform a join with our return data to ensure that we only use traded stocks when computing the breakpoints as a first step.\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    breakpoints = (data\n        .query(\"exchange == 'NYSE'\")\n        .get(sorting_variable)\n        .quantile(percentiles, interpolation = \"linear\")\n        )\n    breakpoints.iloc[0] = -np.Inf\n    breakpoints.iloc[breakpoints.size-1] = np.Inf\n    assigned_portfolios = pd.cut(data[sorting_variable],\n                                 bins=breakpoints,\n                                 labels=pd.Series(range(1, breakpoints.size)),\n                                 include_lowest=True)\n    return assigned_portfolios\n\nportfolios_ff = (variables_ff\n  .merge(data_ff,\n         how=\"inner\",\n         left_on=[\"permno\", \"sorting_date\"], right_on=[\"permno\", \"month\"])\n  .groupby(\"sorting_date\", group_keys=False)\n  .apply(lambda x: x\n         .assign(portfolio_me = assign_portfolio(x, 'me_ff', [0, 0.5, 1]),\n                 portfolio_bm = assign_portfolio(x, 'bm_ff', [0, 0.3, 0.7, 1])))\n  .reset_index(drop=True)\n  .get([\"permno\", \"sorting_date\", \"portfolio_me\", \"portfolio_bm\"])\n)\n\nNext, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column sorting_date in our return data by setting the date to sort on to July of \\(t-1\\) if the month is June (of year \\(t\\)) or earlier or to July of year \\(t\\) if the month is July or later.\n\nportfolios_ff = (data_ff\n  .assign(sorting_date = lambda x: pd.to_datetime(\n            x[\"month\"].apply(lambda x: str(x.year - 1) + \"0701\" if x.month &lt;= 6 else str(x.year) + \"0701\")))\n  .merge(portfolios_ff, how=\"inner\", on=[\"permno\", \"sorting_date\"])\n)"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#fama-and-french-factor-returns",
    "href": "python/replicating-fama-and-french-factors.html#fama-and-french-factor-returns",
    "title": "Replicating Fama and French Factors",
    "section": "Fama and French Factor Returns",
    "text": "Fama and French Factor Returns\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama and French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally.\n\nfactors_ff_monthly_replicated = (portfolios_ff\n  .groupby([\"portfolio_me\", \"portfolio_bm\", \"month\"])\n  .apply(lambda x: pd.Series({\"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])}))\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\n    \"smb_replicated\": x[\"ret\"][x[\"portfolio_me\"] == 1].mean() - x[\"ret\"][x[\"portfolio_me\"] == 2].mean(),\n    \"hml_replicated\": x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - x[\"ret\"][x[\"portfolio_bm\"] == 1].mean()\n    }))\n  .round(decimals=4)\n)"
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "href": "python/replicating-fama-and-french-factors.html#replication-evaluation",
    "title": "Replicating Fama and French Factors",
    "section": "Replication Evaluation",
    "text": "Replication Evaluation\nIn the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. However, we did not follow their procedure strictly. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using smf.ols(). If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance).\n\ntest = (factors_ff_monthly\n  .merge(factors_ff_monthly_replicated, \n         how=\"inner\", \n         on=\"month\")\n)\n\nThe results for the SMB factor are quite convincing as all three criteria outlined above are met and the coefficient and R-squared are at 99%.\n\nmodel_smb = smf.ols(formula=\"smb ~ smb_replicated\", data=test).fit()\nmodel_smb.summary(slim=True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsmb\nR-squared:\n0.986\n\n\nModel:\nOLS\nAdj. R-squared:\n0.986\n\n\nNo. Observations:\n726\nF-statistic:\n5.223e+04\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n0.00\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-0.0001\n0.000\n-0.993\n0.321\n-0.000\n0.000\n\n\nsmb_replicated\n0.9957\n0.004\n228.540\n0.000\n0.987\n1.004\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe replication of the HML factor is also a success, although at a slightly lower level with coefficient and R-squared around 94%.\n\nmodel_hml = smf.ols(formula=\"hml ~ hml_replicated\", data=test).fit()\nmodel_hml.summary(slim=True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nhml\nR-squared:\n0.958\n\n\nModel:\nOLS\nAdj. R-squared:\n0.958\n\n\nNo. Observations:\n726\nF-statistic:\n1.671e+04\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n0.00\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0003\n0.000\n1.431\n0.153\n-0.000\n0.001\n\n\nhml_replicated\n0.9656\n0.007\n129.258\n0.000\n0.951\n0.980\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French premiums, although we cannot see their underlying code. From our perspective, a perfect match is only possible with additional information from the maintainers of the original data."
  },
  {
    "objectID": "python/replicating-fama-and-french-factors.html#exercises",
    "href": "python/replicating-fama-and-french-factors.html#exercises",
    "title": "Replicating Fama and French Factors",
    "section": "Exercises",
    "text": "Exercises\n\nFama and French (1993) claim that their sample excludes firms until they have appeared in Compustat for two years. Implement this additional filter and compare the improvements of your replication effort.\nOn his homepage, Kenneth French provides instructions on how to construct the most common variables used for portfolio sorts. Pick one of them, e.g. OP (operating profitability) and try to replicate the portfolio sort return time series provided on his homepage."
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Preface",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\n\n\n\n\nOur book Tidy Finance with R received great feedback from students and teachers alike. However, one of the most common feedback that we received was that many interested coders are constrained and have to use Python in their institution. We really love R for data analysis tasks, but we of course acknowledge the flexibility and popularity of Python. So we decided to increase our team of authors with a Python expert and translate our original work following the same tidy principles.\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\n\n\n\n\nChristoph Frey is a Quantitative Researcher at Pinechip Capital GmbH and previously worked as an Assistant Professor at the Erasmus Universiteit Rotterdam.\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute teaching award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is affiliated with Reykjavik University and Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in a top journal in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain."
  },
  {
    "objectID": "python/index.html#why-does-this-book-exist",
    "href": "python/index.html#why-does-this-book-exist",
    "title": "Preface",
    "section": "",
    "text": "Our book Tidy Finance with R received great feedback from students and teachers alike. However, one of the most common feedback that we received was that many interested coders are constrained and have to use Python in their institution. We really love R for data analysis tasks, but we of course acknowledge the flexibility and popularity of Python. So we decided to increase our team of authors with a Python expert and translate our original work following the same tidy principles."
  },
  {
    "objectID": "python/index.html#why-tidy",
    "href": "python/index.html#why-tidy",
    "title": "Preface",
    "section": "",
    "text": "As you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\n\nIn its essence, tidy data follows these three principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans."
  },
  {
    "objectID": "python/index.html#about-the-authors",
    "href": "python/index.html#about-the-authors",
    "title": "Preface",
    "section": "",
    "text": "Christoph Frey is a Quantitative Researcher at Pinechip Capital GmbH and previously worked as an Assistant Professor at the Erasmus Universiteit Rotterdam.\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute teaching award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is affiliated with Reykjavik University and Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in a top journal in financial economics."
  },
  {
    "objectID": "python/index.html#license",
    "href": "python/index.html#license",
    "title": "Preface",
    "section": "",
    "text": "This book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain."
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html",
    "href": "python/accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we suggest a way to organize your financial data. Everybody, who has experience with data, is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nimport pandas as pd\nimport numpy as np\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\nstart_date = \"1960-01-01\"\nend_date = \"2021-12-31\""
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#fama-french-data",
    "href": "python/accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, the pandas-datareader package provides a simple interface to read data from Ken French’s Data Library. \n\nimport pandas_datareader as pdr\n\nWe can use the pdr.DataReader() function of the package to download monthly Fama-French factors. The set 3 Factors includes the return time series of the market, size, and value factors alongside the risk-free rates. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the site, check the raw data files to appreciate the time you can save thanks to pandas-datareader.\n\nfactors_ff_monthly_raw = pdr.DataReader(name=\"F-F_Research_Data_Factors\",\n                                        data_source=\"famafrench\", \n                                        start=start_date, \n                                        end=end_date)[0]\n\nfactors_ff_monthly = (factors_ff_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(month = lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns = {\"mkt-rf\" : \"mkt_excess\"})\n)\n\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\4118113231.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  factors_ff_monthly_raw = pdr.DataReader(name=\"F-F_Research_Data_Factors\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\4118113231.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  factors_ff_monthly_raw = pdr.DataReader(name=\"F-F_Research_Data_Factors\",\n\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff_daily_raw = pdr.DataReader(\"F-F_Research_Data_Factors_daily\",\n                                      \"famafrench\", \n                                      start_date, \n                                      end_date)[0]\n\nfactors_ff_daily = (factors_ff_daily_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns = {\"mkt-rf\" : \"mkt_excess\"})\n)\n\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\3554812465.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  factors_ff_daily_raw = pdr.DataReader(\"F-F_Research_Data_Factors_daily\",\n\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\n                                           \"famafrench\", \n                                           start_date, \n                                           end_date)[0]\n\nindustries_ff_monthly = (industries_ff_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(month = lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n)\n\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\nC:\\Users\\christoph.scheuch\\AppData\\Local\\Temp\\ipykernel_19936\\918757163.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  industries_ff_monthly_raw = pdr.DataReader(\"10_Industry_Portfolios\",\n\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling pdr.famafrench.get_available_datasets()."
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#q-factors",
    "href": "python/accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within pd.read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link = (\"http://global-q.org/uploads/1/2/2/6/122679606/\" +\n                          \"q5_factors_monthly_2021.csv\")\nfactors_q_monthly=(pd.read_csv(factors_q_monthly_link)\n  .assign(month = lambda x: (pd.to_datetime(x[\"year\"].astype(str) \n                              + \"-\" + x[\"month\"].astype(str) + \"-01\")))\n  .drop(columns=[\"R_F\", \"R_MKT\", \"year\"])\n  .rename(columns = lambda x: x.replace(\"R_\", \"\").lower())\n  .query(\"month &gt;= @start_date and month &lt;= @end_date\")\n  .assign(**{col: lambda x: x[col] / 100 for col in [\"me\", \"ia\", \"roe\", \"eg\"]})\n)"
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "python/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2021 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google drive location, we need additional packages to access the data directly from our Python session. Usually, you need to authenticate if you interact with Google drive directly in Python. Since the data is stored via a public link, we can proceed without any authentication.\n\nsheet_id = \"1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2\"\nsheet_name = \"macro_predictors.xlsx\"\nmacro_predictors_link = (\"https://docs.google.com/spreadsheets/d/\" + sheet_id + \n                         \"/gviz/tq?tqx=out:csv&sheet=\" + sheet_name)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997)\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors = (pd.read_csv(macro_predictors_link, thousands=\",\")\n  .assign(month = lambda x: pd.to_datetime(x[\"yyyymm\"], format=\"%Y%m\"),\n          IndexDiv = lambda x: x[\"Index\"] + x[\"D12\"],\n          logret= lambda x: (np.log(x[\"IndexDiv\"]) - \n                                np.log(x[\"IndexDiv\"].shift(1))),\n          Rfree = lambda x: np.log(x[\"Rfree\"] + 1),\n          rp_div = lambda x: x[\"logret\"] - x[\"Rfree\"].shift(-1),\n          dp = lambda x: np.log(x[\"D12\"]) - np.log(x[\"Index\"]),\n          dy = lambda x: np.log(x[\"D12\"]) - np.log(x[\"D12\"].shift(1)),\n          ep = lambda x: np.log(x[\"E12\"]) - np.log(x[\"Index\"]),\n          de = lambda x: np.log(x[\"D12\"]) - np.log(x[\"E12\"]),\n          tms = lambda x: x[\"lty\"] - x[\"tbl\"],\n          dfy = lambda x: x[\"BAA\"] - x[\"AAA\"])\n  .get([\"month\", \"rp_div\", \"dp\", \"dy\", \"ep\", \"de\", \"svar\", \"b/m\", \"ntis\", \n        \"tbl\", \"lty\", \"ltr\", \"tms\", \"dfy\", \"infl\"])\n  .query(\"month &gt;= @start_date and month &lt;= @end_date\")\n  .dropna()\n)"
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "python/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar pandas-datareader package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\ncpi_monthly = (pdr.DataReader(\"CPIAUCNS\", \n                              \"fred\", \n                              start_date, \n                              end_date)\n  .reset_index(names=\"month\")\n  .rename(columns = {\"CPIAUCNS\" : \"cpi\"})\n  .assign(cpi=lambda x: x[\"cpi\"] / x[\"cpi\"].iloc[-1])\n)\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key."
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "python/accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases.\n\nimport sqlite3\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance.db in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function to_sql(), which copies the data to our SQLite-database. Before we copy the data to the database, we convert the date to UNIX integers, which allows us to smoothly share the data between R and Python. We follow the approach recommended by pandas for this conversion.\n\n(factors_ff_monthly\n  .assign(month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"))\n  .to_sql(name=\"factors_ff_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n\n744\n\n\nIf we want to have the whole table in memory, we need to call pd.read_sql_query() with the corresponding query. You will see that we regularly load the data into the memory in the next chapters.\n\n(pd.read_sql_query(\n    sql=\"SELECT month, rf FROM factors_ff_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n)\n\n\n\n\n\n\n\n\nmonth\nrf\n\n\n\n\n0\n1960-01-01\n0.0033\n\n\n1\n1960-02-01\n0.0029\n\n\n2\n1960-03-01\n0.0035\n\n\n3\n1960-04-01\n0.0019\n\n\n4\n1960-05-01\n0.0027\n\n\n...\n...\n...\n\n\n739\n2021-08-01\n0.0000\n\n\n740\n2021-09-01\n0.0000\n\n\n741\n2021-10-01\n0.0000\n\n\n742\n2021-11-01\n0.0000\n\n\n743\n2021-12-01\n0.0001\n\n\n\n\n744 rows × 2 columns\n\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\n(factors_ff_daily\n  .assign(date = lambda x: (x[\"date\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"))\n  .to_sql(name=\"factors_ff_daily\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(industries_ff_monthly\n  .assign(month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"))\n  .to_sql(name=\"industries_ff_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(factors_q_monthly\n  .assign(month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"))\n  .to_sql(name=\"factors_q_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(macro_predictors\n  .assign(month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"))\n  .to_sql(name=\"macro_predictors\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(cpi_monthly\n  .assign(month = lambda x: (x[\"month\"]- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1d\"))\n  .to_sql(name=\"cpi_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n\nFrom now on, all you need to do to access data that is stored in the database is to follow two steps: (i) Establish the connection to the SQLite database and (ii) execute the query to fetch the data. For your convenience, the following steps show all you need in a compact fashion.\n\nimport pandas\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\nfactors_q_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM factors_q_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n)"
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "python/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the execute() function.\n\ntidy_finance.execute(\"VACUUM\")\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read up in this tutorial."
  },
  {
    "objectID": "python/accessing-and-managing-financial-data.html#exercises",
    "href": "python/accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via pd.read_csv(). Validate that you get the same data as via the pandas-datareader package.\nDownload the Fama-French 5 factors using the pandas-datareader package. Use pdr.famafrench.get_available_datasets() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the resulting rf, mkt_excess, smb, and hml columns to factors_ff_monthly. Explain any differences you might find."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "Experimental and external contributions based on Tidy Finance with R. Contribute your ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Raw TRACE Data to a Local SQLite Database\n\n\n37 min\n\n\n\nData\n\n\n\nAn R code that converts TRACE files from FINRA into a SQLite for facilitated analysis and filtering\n\n\n\nKevin Riehl, Lukas Müller\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Collaborative Filtering: Building A Stock Recommender\n\n\n13 min\n\n\n\nRecommender System\n\n\n\nA simple implementation for prototyping multiple collaborative filtering algorithms\n\n\n\nChristoph Scheuch\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Standard Errors in Portfolio Sorts\n\n\n39 min\n\n\n\nReplications\n\n\n\nAn all-in-one implementation of non-standard errors in portfolio sorts\n\n\n\nPatrick Weiss\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruction of a Historical S&P 500 Total Return Index\n\n\n8 min\n\n\n\nData\n\n\n\nAn approximation of total returns using Robert Shiller’s stock market data\n\n\n\nChristoph Scheuch\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n\n5 min\n\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at Workshops for Ukraine\n\n\n2 min\n\n\n\nWorkshops\n\n\n\nYou can learn Tidy Finance and support Ukraine at the same time\n\n\n\nPatrick Weiss\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Finance at the useR!2022 Conference\n\n\n1 min\n\n\n\nConferences\n\n\n\nTidy Finance presentation at the gathering supported by the R Foundation\n\n\n\nPatrick Weiss\n\n\nJun 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/user-conference-2022/index.html",
    "href": "blog/user-conference-2022/index.html",
    "title": "Tidy Finance at the useR!2022 Conference",
    "section": "",
    "text": "We had the pleasure of presenting our book at the virtual useR!2022 conference with 1,227 registered participants from 96 countries. It was great fun! Since it was recorded, you can find the video of my presentation below."
  },
  {
    "objectID": "blog/op-ed-tidy-finance/index.html",
    "href": "blog/op-ed-tidy-finance/index.html",
    "title": "What is Tidy Finance?",
    "section": "",
    "text": "Empirical finance can be tedious. Many standard tasks, such as cleaning data or forming factor portfolios, require a lot of effort. The code to produce even seminal results is typically opaque. Why should researchers have to reinvent the wheel over and over again?\nTidy Finance with R is our take on how to conduct empirical research in financial economics from scratch. Whether you are an industry professional looking to expand your quant skills, a graduate student diving into the finance world, or an academic researcher, this book shows you how to use R to master applications in asset pricing, portfolio optimization, risk management, and option pricing.\nWe wrote this book to provide a comprehensive guide to using R for financial analysis. Our book collects all the tools we wish we would have had at hand at the beginning of our graduate studies in finance. Without transparent code for standard procedures, numerous replication efforts (and their failures) feel like a waste of resources. We have been there, as probably everybody working with data has. Since we kicked off our careers, we have constantly updated our methods, coding styles, and workflows. Our book reflects our lessons learned. By sharing them, we aim to help others avoid dead ends.\nWorking on problems that countless others have already solved in secrecy is not just tedious, it even may have detrimental effects. In a recent study1 together with hundreds of research teams from across the globe, Albert J. Menkveld, the best-publishing Dutch economist according to Economentop 40, shows that without a standard path to do empirical analysis, results may vary substantially. Even if teams set out to analyze the same research question based on the same data, implementation details are important and deserve more than treatment as subtleties.\nThere will always be multiple acceptable ways to test relevant research questions. So why should it matter that our book lifts our curtain on reproducible finance by providing a fully transparent code base for many typical financial applications? First and foremost, we hope to inspire others to make their research truly reproducible. This is not a purely theoretical exercise: our examples start with data preparation and conclude with communicating results to get readers to do empirical analysis on real data as fast as possible. We believe that the need for precise academic writing does not stop where the code begins. Understanding and agreeing on standard procedures hopefully frees up resources to focus on what matters: a novel research project, a seminar paper, or a thorough analysis for your employer. If our book helps to provide a foundation for discussions on which determinants render code useful, we have achieved much more than we were hoping for at the beginning of this project.\nUnlike typical stylized examples, our book starts with the problems of any serious research project. The often overlooked challenge behind empirical research may seem trivial at first glance: we need data to conduct our analyses. Finance is not an exception: raw data, often hidden behind proprietary financial data sources, requires cleaning before there is any hope of extracting valuable insights from it. While you can despise data cleaning, you cannot ignore it.\nWe describe and provide the code to prepare typical open-source and proprietary financial data sources (e.g., CRSP, Compustat, Mergent FISD, TRACE). We reuse these data in all the subsequent chapters, which we keep as self-contained as possible. The empirical applications range from key concepts of empirical asset pricing (beta estimation, portfolio sorts, performance analysis, Fama-French factors) to modeling and machine learning applications (fixed effects estimation, clustering standard errors, difference-in-difference estimators, ridge regression, Lasso, Elastic net, random forests, neural networks) and portfolio optimization techniques.\nNecessarily, our book reflects our opinionated perspective on how to perform empirical analyses. From our experience as researchers and instructors, we believe in the value of the workflows we teach and apply daily. The entire book rests on two core concepts: coding principles using the tidyverse family of R packages and tidy data.\nWe base our book entirely on the open-source programming language R. R and the tidyverse community provide established tools to perform typical data science tasks, ranging from cleaning and manipulation to plotting and modeling. R is hence the ideal environment to develop an accessible code base for future finance students. The concept of tidy data refers to organizing financial data in a structured and consistent way, allowing for easy analysis and understanding.2 Taken together, tidy data and code help achieve the ultimate goal: to provide a fundamentally human-centered experience that makes it easier to teach, learn, and replicate the code of others – or even your own!\nWe are convinced that empirical research in finance is in desperate need of reproducible code to form standards for otherwise repetitive tasks. Instructors and researchers have already reached out to us with grateful words about our book. Tidy Finance finds its way into lecture halls across the globe already today. Various recent developments support our call for increased transparency. For instance, Cam Harvey, the former editor of the Journal of Finance, and a former president of the American Finance Association, openly argues that the profession needs to tackle the replication crisis.3 Top journals in financial economics increasingly adopt code and data-sharing policies to increase transparency. The industry and academia are aware and concerned (if not alarmed) about these issues, which is why we believe that the timing for publishing Tidy Finance with R could not be better."
  },
  {
    "objectID": "blog/op-ed-tidy-finance/index.html#footnotes",
    "href": "blog/op-ed-tidy-finance/index.html#footnotes",
    "title": "What is Tidy Finance?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J. et al. (2022). “Non-standard Errors”. http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nWigglesworth, R. (2021). The hidden ‘replication crisis’ of finance. Financial Times. https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0↩︎"
  },
  {
    "objectID": "blog/historical-sp-500-total-return/index.html",
    "href": "blog/historical-sp-500-total-return/index.html",
    "title": "Construction of a Historical S&P 500 Total Return Index",
    "section": "",
    "text": "I wanted to simulate simple equity savings plans over long time horizons and many different initiation periods for a story with the German news portal t-online. The good thing is that the S&P 500 index provides a great starting point as it is easily available since 1928 via Yahoo Finance. However, I wanted my savings plans to be accumulating, i.e., all cash distributions are reinvested in the savings plan. The S&P index is inadequate for this situation as it is a price index that only tracks its components’ price movements. The S&P 500 Total Return Index tracks the overall performance of the S&P 500 and would be the solution to my problem, but it is only available since 1988.\nFortunately, I came up with a solution using data provided by Robert Shiller and provide the complete code below for future reference. If you spot any errors or have better suggestions, please feel free to create an issue.\nThis is the set of packages I use throughout this post.\n\nlibrary(tidyverse) # for overall grammar\nlibrary(tidyquant) # to download data from yahoo finance\nlibrary(glue)      # to automatically construct figure captions\nlibrary(scales)    # for nicer axis labels \nlibrary(readxl)    # to read Shiller's data \n\nFirst, let us download the S&P 500 Total Return Index from Yahoo Finance. I only consider the closing prices of the last day of each month because my savings plans only transfer funds once a month. In principle, you could also approximate the daily time series, but I believe it will be noiser because Shiller only provides monthly data.\n\nsp500_recent &lt;-  tq_get(\"^SP500TR\", get = \"stock.prices\",\n                        from = \"1988-01-04\", to = \"2023-01-31\") |&gt;\n  select(date, total_return_index = close) |&gt;\n  drop_na() |&gt;\n  group_by(month = ceiling_date(date, \"month\")-1) |&gt;\n  arrange(date) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  select(month, total_return_index)\n\nNext, I download data from Robert Shiller’s website that he used in his great book Irrational Excuberance. I create a temporary file and read the relevant sheet. In particular, the data contains monthly S&P 500 price and dividend data. The original file has a bit of annoying date format that I have to correct before parsing.\n\ntemp &lt;- tempfile(fileext = \".xls\")\n\ndownload.file(url = \"http://www.econ.yale.edu/~shiller/data/ie_data.xls\",\n              destfile = temp, mode='wb')\n\nshiller_historical &lt;- read_excel(temp, sheet = \"Data\", skip = 7) |&gt;\n  transmute(month = ceiling_date(ymd(str_replace(str_c(Date, \".01\"), \"\\\\.1\\\\.\", \"\\\\.10\\\\.\")), \"month\")-1,\n            price = as.numeric(P),\n            dividend = as.numeric(D)) \n\nTo construct the total return index, I need a return that includes dividends. In the next code chunk, I compute monthly total returns of the S&P 500 index by incorporating the monthly dividend paid on the index in the corresponding month. Note that Shiller’s data contains the 12-month moving sum of monthly dividends, hence the division by 12. Admittedly, this is a brute force approximation, but I couldn’t come up with a better solution so far.\n\nshiller_historical &lt;- shiller_historical |&gt;\n  arrange(month) |&gt;\n  mutate(ret = (price + dividend / 12) / lag(price) - 1)\n\nBefore I go back in time, let us check whether the total return computed above is able to match the actual total return since 1988. I start with the first total return index number that is available and use the cumulative product of returns from above to construct the check time series.\n\ncheck &lt;- shiller_historical |&gt;\n  full_join(sp500_recent, by = \"month\") |&gt;\n  filter(!is.na(total_return_index)) |&gt;\n  arrange(month) |&gt;\n  mutate(ret = if_else(row_number() == 1, 0, ret), # ignore first month return\n         total_return_check = total_return_index[1] * cumprod(1 + ret)) |&gt;\n  drop_na()\n\nThe correlation between the actual time series and the check is remarkably high which gives me confidence in the method I propose here.\n\ncheck |&gt; \n  select(total_return_index, total_return_check) |&gt;  \n  cor()\n\n                   total_return_index total_return_check\ntotal_return_index              1.000              0.999\ntotal_return_check              0.999              1.000\n\n\nIn addition, the visual inspection of the two time series in Figure 1 corroborates my confidence. Note that both the actual and the simulated total return indexes start at the same index value.\n\ncheck |&gt;\n  select(month, Actual = total_return_index, Simulated = total_return_check) |&gt;\n  pivot_longer(cols = -month) |&gt;\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = comma)+ \n  labs(x = NULL, y = NULL, color = NULL,\n       title = \"Actual and simulated S&P 500 Total Return index\",\n       subtitle = glue(\"Both indexes start at {min(check$month)}\"))\n\n\n\n\nFigure 1: Simluated and actual S&P 500 Total Return index data move closely together.\n\n\n\n\nNow, let us use the same logic to construct the total return index for the time before 1988. Note that I just sort the months in descending order and divide by the cumulative product of the total return from Shiller’s data.\n\nsp500_historical &lt;- sp500_recent |&gt; \n  filter(month == min(month)) |&gt;\n  full_join(shiller_historical |&gt;\n              filter(month &lt;= min(sp500_recent$month)), by = \"month\") |&gt;\n  arrange(desc(month)) |&gt;\n  mutate(ret = if_else(row_number() == 1, 0, ret),\n         total_return_index = total_return_index[1] / cumprod(1 + ret))\n\nBefore we take a look at the results, I also add the S&P price index from Yahoo Finance for comparison.\n\nsp500_price_index &lt;- tq_get(\"^GSPC\", get = \"stock.prices\",\n                            from = \"1928-01-01\", to = \"2023-01-31\") |&gt;\n  select(date, price_index = close) |&gt;\n  drop_na() |&gt;\n  group_by(month = ceiling_date(date, \"month\") - 1) |&gt;\n  arrange(date) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  select(month, price_index)\n\nFinally, let us combine (i) the actual S&P 500 Total Return Index from 1988 until 2023, (ii) the simulated S&P 500 total return index before 1988, and (iii) the S&P 500 price index from 1928 until 2023.\n\nsp500_monthly &lt;- sp500_recent|&gt;\n  bind_rows(sp500_historical |&gt;\n              filter(month &lt; min(sp500_recent$month))  |&gt;\n              select(month, total_return_index)) |&gt;\n  full_join(sp500_price_index |&gt; \n              select(month, price_index), by = \"month\") |&gt;\n  filter(month &gt;= \"1928-01-01\")  |&gt;\n  arrange(month)\nsp500_monthly\n\n# A tibble: 1,141 × 3\n  month      total_return_index price_index\n  &lt;date&gt;                  &lt;dbl&gt;       &lt;dbl&gt;\n1 1928-01-31               1.20        17.6\n2 1928-02-29               1.21        17.3\n3 1928-03-31               1.20        19.3\n4 1928-04-30               1.26        19.8\n5 1928-05-31               1.35        20  \n# ℹ 1,136 more rows\n\n\nFigure 2 shows the dramatic differences in cumulative returns if you only consider price changes, as the S&P 500 Index does, versus total returns with reinvested capital gains. Note that I plot the indexes in log scale, otherwise everything until the last couple of decades would look like a flat line. I believe it is also important to keep the differences between price and performance indexes in mind whenever you compare equity indexes across countries. For instance, the DAX is a performance index by default and should never be compared with the S&P 500 price index.\n\nsp500_monthly |&gt;\n  select(month, \n         `Price Index` = price_index, \n         `Total Return Index` = total_return_index) |&gt;\n  pivot_longer(cols = -month) |&gt;\n  group_by(name) |&gt;\n  arrange(month) |&gt;\n  mutate(value = value / value[1] * 100) |&gt;\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_log10(labels = comma) +\n  scale_x_date(expand = c(0, 0), date_breaks = \"10 years\", date_labels = \"%Y\") + \n  labs(x = NULL, y = NULL, color = NULL,\n       title = \"S&P 500 Price index and Total Return index since 1928\",\n       subtitle = glue(\"Both indexes are normalized to 100 at {min(sp500_monthly$month)}\"))\n\n\n\n\nFigure 2: Using total return data yields dramatically higher cumulative returns over a few decades."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html",
    "href": "blog/convert-raw-trace-data/index.html",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "",
    "text": "Corporate bond research is gaining momentum, but data availability can be a challenge for most researchers. While FINRA makes its TRACE Enhanced Data on U.S. corporate bond trading available through vendors such as Wharton Research Data Services (WRDS), not every researcher has access to these services. Alternatively, FINRA offers its Enhanced Historical Data and Academic Data (the latter exclusively) as one-off purchases. However, organizing this data can be complex and time consuming due to the nested and zipped structure of raw TXT files, making it cumbersome for researchers to explore the exciting world of fixed income securities.\nOur R code enables you organizing the TRACE academic data and TRACE enhanced data by eliminating the complexities of multiple, nested files, simplifying the data conversion process. Drawing from the existing SAS-based solution of Dick-Nielsen (2014) 1 and (2019) 2, you can now easily convert TRACE data into a single, organized SQLite database, allowing for seamless and efficient downstream analysis.\nHere are the key benefits our solution offers:\nWith our R code, you can simplify your corporate bond research and overcome current limitations. Don’t let complex data organizing hold you back! Prepare yourself to be immersed in the extraordinary realm of TRACE, the key to unlock a deeper understanding of over-the-counter transactions and to unveil the mysterious pathways of fixed-income securities. With TRACE as your guide, you will embark on a journey of discovery, unraveling market trends and unearthing invaluable insights that will forever enrich your understanding of this mesmerizing world. Embrace the power of TRACE and unveil the secrets that lie within.\nIn the following, we will discuss first, the structure of the bond trading data “TRACE” provided by FINRA, second the proposed normalized database schema for the generated SQLite database, and finally third, give you a brief hands-on guide to downloading data and using our R code to generate the SQLite."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html#final-remarks",
    "href": "blog/convert-raw-trace-data/index.html#final-remarks",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "Final Remarks",
    "text": "Final Remarks\nWe hope that this blog post is helpful to the reader working with TRACE academic data and TRACE enhanced data and in facilitating analyses in this emerging, exciting, and promising field of research.\nAt the end, we would like to thank the editors of Tidy Finance for their helpful suggestions and support with writing this blog post, and our supervisor Prof. Dr. Dirk Schiereck from the Chair of Corporate Finance at TU Darmstadt (Germany) who made the data available for this project."
  },
  {
    "objectID": "blog/convert-raw-trace-data/index.html#footnotes",
    "href": "blog/convert-raw-trace-data/index.html#footnotes",
    "title": "Convert Raw TRACE Data to a Local SQLite Database",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDick-Nielsen, J. (2014). How to clean enhanced TRACE data. Available at [SSRN 2337908] (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2337908).↩︎\nDick-Nielsen, J., & Poulsen, T. K. (2019). How to clean academic trace data. Available at [SSRN 3456082] (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3456082).↩︎"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html",
    "href": "blog/nse-portfolio-sorts/index.html",
    "title": "Non-standard errors in portfolio sorts",
    "section": "",
    "text": "Welcome to our latest blog post where we delve into the intriguing world of non-standard errors1, a crucial aspect of academic research, which also is addressed in financial economics. These non-standard errors often stem from the methodological decisions we make, adding an extra dimension of uncertainty to the estimates we report. I had the pleasure of working with Stefan Voigt on a contribution to Menkveld et al. (2023), which was an exciting opportunity to shape the first discussions on non-standard errors.\nOne of the goals of Tidy Finance has always been focused on promoting reproducibility in finance research. We began this endeavor by introducing a chapter on Size sorts and p-hacking, which initiated some analysis of portfolio sorting choices. Recently, my fellow authors, Dominik Walter and Rüdiger Weber, and I published an update to our working paper, Non-Standard Errors in Portfolio Sorts2. This paper delves into the impact of methodological choices on return differentials derived from portfolio sorts. Our conclusion? We need to accept non-standard errors in empirical asset pricing. By reporting the entire range of return differentials, we simultaneously deepen our economic understanding of return anomalies and improve trust.\nThis blog post will guide you through conducting portfolio sorts which keep non-standard errors in perspective. We explore the multitude of possible decisions that can impact return differentials, allowing us to estimate a premium’s distribution rather than a single return differential. The code is inspired by Tidy Finance with R and the replication code for Walter, Weber, and Weiss (2023), which can be found in this Github repository. By the end of this post, you will have the knowledge to sort portfolios based on asset growth in nearly 70,000 different ways.\nWhile this post is detailed, it is not overly complex. However, if you are new to R or portfolio sorts, I recommend first checking out our chapter on Size sorts and p-hacking. Due to the length constraints, we will be skipping over certain details related to implementation and the economic background (you can find these in the WWW paper). If there are particular aspects you would like to delve into further, please feel free to reach out."
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#merge-data",
    "href": "blog/nse-portfolio-sorts/index.html#merge-data",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Merge data",
    "text": "Merge data\nOne key decision node is the sorting variable lag. However, merging data is an expensive operation, and doing it repeatedly is unnecessary. Hence, we merge the data in the three possible lag configurations and store them as separate tibbles. Thereby, we can later reference the correct table instead of merging the desired output.\nFirst, let us consider the Fama-French (FF) lag. Here, we consider accounting information published in year \\(t-1\\) starting from July of year \\(t\\). That is, we use the accounting information published 6 to 18 months ago. We first match the accounting data to the stock market data before we fill in the missing observations. A few pitfalls exist when using the fill()-function. First, one might easily forget to order and group the data. Second, the function does not care how outdated the information becomes. In principle, you can end up with data that is decades old. Therefore, we ensure that these filled data points are not older than 12 months. This is achieved with the new variable sv_age_check, which serves as a filter for outdated data. Finally, notice that this code provides much flexibility. All variables with prefixes sv_ and filter_ get filled. So you can easily adapt my code to your needs.\n\ndata_FF &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"year\") %m+% months(18),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n# Fill variables and ensure timeliness of data\ndata_FF &lt;- data_FF |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)\n\nNext, we create the basis with lags of three and six months. The process is exactly the same as above for the FF lag, but without the floor_date() as we apply a constant lag to all observations. Again, we make sure that after the call to fill() our information does not become too old.\n\n# 3 months\n## Merge data\ndata_3m &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(3),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_3m &lt;- data_3m |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)\n\n# 6 months\n## Merge data\ndata_6m &lt;- crsp_monthly |&gt;\n  mutate(sorting_date = month) |&gt;\n  left_join(\n    compustat |&gt;\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(6),\n        sv_age_check = sorting_date\n      ) |&gt;\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_6m &lt;- data_6m |&gt;\n  arrange(permno, month) |&gt;\n  group_by(permno, gvkey) |&gt;\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |&gt;\n  ungroup() |&gt;\n  filter(sv_age_check &gt; month %m-% months(12)) |&gt;\n  select(-sv_age_check, -sorting_date, -gvkey)"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#functions",
    "href": "blog/nse-portfolio-sorts/index.html#functions",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Functions",
    "text": "Functions\nWe write functions that complete specific tasks and then combine them to generate the desired output. Breaking it up into smaller steps makes the whole process more tractable and easier to test.\n\nSelect the sample\nThe first function gets the name handle_data() because it is intended to select the sample according to the sample construction choices. The function first selects the data based on the desired sorting variable lag (specified in sv_lag). Then, we apply the various filters we discussed above. As you see, this is relatively simple, but it already covers our sample construction nodes.\n\nhandle_data &lt;- function(include_financials, include_utilities,\n                        drop_smallNYSE_at, drop_price_at, drop_stock_age_at,\n                        drop_earnings, drop_bookequity,\n                        sv_lag) {\n  # Select dataset\n  if (sv_lag == \"FF\") data_all &lt;- data_FF\n  if (sv_lag == \"3m\") data_all &lt;- data_3m\n  if (sv_lag == \"6m\") data_all &lt;- data_6m\n\n  # Size filter based on NYSE percentile\n  if (drop_smallNYSE_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      group_by(month) |&gt;\n      mutate(NYSE_breakpoint = quantile(\n        mktcap_lag[exchange == \"NYSE\"],\n        drop_smallNYSE_at\n      )) |&gt;\n      ungroup() |&gt;\n      filter(mktcap_lag &gt;= NYSE_breakpoint) |&gt;\n      select(-NYSE_breakpoint)\n  }\n\n  # Exclude industries\n  data_all &lt;- data_all |&gt;\n    filter(if (include_financials) TRUE else !grepl(\"Finance\", industry)) |&gt;\n    filter(if (include_utilities) TRUE else !grepl(\"Utilities\", industry))\n\n  # Book equity filter\n  if (drop_bookequity) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_be &gt; 0)\n  }\n\n  # Earnings filter\n  if (drop_earnings) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_earnings &gt; 0)\n  }\n\n  # Stock age filter\n  if (drop_stock_age_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_stock_age &gt;= drop_stock_age_at)\n  }\n\n  # Price filter\n  if (drop_price_at &gt; 0) {\n    data_all &lt;- data_all |&gt;\n      filter(filter_price &gt;= drop_price_at)\n  }\n\n  # Define ME\n  data_all &lt;- data_all |&gt;\n    mutate(me = mktcap_lag) |&gt;\n    drop_na(me) |&gt;\n    select(-starts_with(\"filter_\"), -industry)\n\n  # Return\n  return(data_all)\n}\n\n\n\nAssign portfolios\nNext, we define a function that assigns portfolios based on the specified sorting variable, the number of portfolios, and the exchanges. The function only works on a single cross-section of data, i.e., it has to be applied to individual months of data. The central part of the function is to compute the \\(n\\) breakpoints based on the exchange filter. Then, findInterval() assigns the respective portfolio number.\nThe function also features two sanity checks. First, it does not assign portfolios if there are too few stocks in the cross-section. Second, sometimes the sorting variable creates scenarios where some portfolios are overpopulated. For example, if the variable in question is bounded from below by 0. In such a case, an unexpectedly large number of firms might end up in the lowest bucket, covering multiple quantiles.\n\nassign_portfolio &lt;- function(data, sorting_variable,\n                             n_portfolios, exchanges) {\n  # Escape small sets (i.e., less than 10 firms per portfolio)\n  if (nrow(data) &lt; n_portfolios * 10) {\n    return(NA)\n  }\n\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    filter(grepl(exchanges, exchange)) |&gt;\n    pull(all_of(sorting_variable)) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull(all_of(sorting_variable)),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n\n  # Check if breakpoints are well defined\n  if (length(unique(breakpoints)) == n_portfolios + 1) {\n    return(portfolios)\n  } else {\n    print(breakpoints)\n    cat(paste0(\n      \"\\n Breakpoint issue! Month \",\n      as.Date(as.numeric(cur_group())),\n      \"\\n\"\n    ))\n    stop()\n  }\n}\n\n\n\nSingle and double sorts\nOur goal is to construct portfolios for single sorts, independent double sorts, and dependent double sorts. Hence, our next three functions do exactly that. The double sorts considered always take a first sort on market equity (the variable me) before sorting on the actual sorting variable.\nLet us start with single sorts. As you see, we group by month as the function assign_portfolio() we wrote above handles one cross-section at a time. The rest of the function just passes the arguments to the portfolio assignment.\n\nsort_single &lt;- function(data, sorting_variable,\n                        exchanges, n_portfolios_main) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio = assign_portfolio(\n      data = pick(all_of(sorting_variable), exchange),\n      sorting_variable = sorting_variable,\n      n_portfolios = n_portfolios_main,\n      exchanges = exchanges\n    )) |&gt;\n    drop_na(portfolio) |&gt;\n    ungroup()\n}\n\nFor double sorts, things are more interesting. First, we have the issue of independent and dependent double sorts. An independent sort considers the two sorting variables (size and asset growth) independently. In contrast, dependent sorts are, in our case, first sorting on size and within these buckets on asset growth. We group by the secondary portfolio to achieve the dependent sort before generating the main portfolios. Second, we need to generate an overall portfolio of the two sorts - we will see this later.\n\nsort_double_independent &lt;- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(\n      portfolio_secondary = assign_portfolio(\n        data = pick(me, exchange),\n        sorting_variable = \"me\",\n        n_portfolios = n_portfolios_secondary,\n        exchanges = exchanges\n      ),\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |&gt;\n    drop_na(portfolio_main, portfolio_secondary) |&gt;\n    ungroup()\n}\n\nsort_double_dependent &lt;- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio_secondary = assign_portfolio(\n      data = pick(me, exchange),\n      sorting_variable = \"me\",\n      n_portfolios = n_portfolios_secondary,\n      exchanges = exchanges\n    )) |&gt;\n    drop_na(portfolio_secondary) |&gt;\n    group_by(month, portfolio_secondary) |&gt;\n    mutate(\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |&gt;\n    drop_na(portfolio_main) |&gt;\n    ungroup()\n}\n\n\n\nAnnual vs monthly rebalancing\nNow, we still have one decision node to cover: Rebalancing. We can either rebalance annually in July or monthly. To achieve this, we write two more functions - the last functions before finishing up. Let us start with monthly rebalancing because it is much easier. All we need to do is to use the assigned portfolio numbers to generate portfolio returns. Inside the function, we use three if() calls to decide the sorting method. Notice that the double sorts use the simple average for aggregating the extreme portfolios of the size buckets.\n\nrebalance_monthly &lt;- function(data, sorting_variable, sorting_method,\n                              n_portfolios_main, n_portfolios_secondary,\n                              exchanges, value_weighted) {\n  # Single sort\n  if (sorting_method == \"single\") {\n    data_rets &lt;- data |&gt;\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    data_rets &lt;- data |&gt;\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    data_rets &lt;- data |&gt;\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n\n  return(data_rets)\n}\n\nNow, let us move to the annual rebalancing. Here, we first assign a portfolio on the data in July based on single or independent/dependent double sorts. Then, we fill the remaining months forward before computing returns. Hence, we need one extra step for each sort.\n\nrebalance_annually &lt;- function(data, sorting_variable, sorting_method,\n                               n_portfolios_main, n_portfolios_secondary,\n                               exchanges, value_weighted) {\n  data_sorting &lt;- data |&gt;\n    filter(month(month) == 7) |&gt;\n    group_by(month)\n\n  # Single sort\n  if (sorting_method == \"single\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |&gt;\n      select(permno, month, portfolio) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    # Assign portfolios\n    data_sorting &lt;- data_sorting |&gt;\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |&gt;\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |&gt;\n      mutate(sorting_month = month)\n  }\n\n  # Compute portfolio return\n  if (sorting_method == \"single\") {\n    data |&gt;\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |&gt;\n      group_by(permno) |&gt;\n      arrange(month) |&gt;\n      fill(portfolio, sorting_month) |&gt;\n      filter(sorting_month &gt;= month %m-% months(12)) |&gt;\n      drop_na(portfolio) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  } else {\n    data |&gt;\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |&gt;\n      group_by(permno) |&gt;\n      arrange(month) |&gt;\n      fill(portfolio_main, portfolio_secondary, portfolio, sorting_month) |&gt;\n      filter(sorting_month &gt;= month %m-% months(12)) |&gt;\n      drop_na(portfolio_main, portfolio_secondary) |&gt;\n      group_by(month, portfolio) |&gt;\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |&gt;\n      group_by(month, portfolio_main) |&gt;\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |&gt;\n      rename(portfolio = portfolio_main)\n  }\n}\n\n\n\nCombining the functions\nNow, we have everything to compute our 69,120 portfolio sorts for asset growth to understand the variation our decisions induce. To achieve this, our function considers all choices as arguments and passes them to the sample selection and portfolio construction functions.\nFinally, the function computes the return differential for each month. Since we are only interested in the mean here, we simply take the mean of these time series and call it our premium estimate.\n\nexecute_sorts &lt;- function(sorting_variable, drop_smallNYSE_at,\n                          include_financials, include_utilities,\n                          drop_bookequity, drop_earnings,\n                          drop_stock_age_at, drop_price_at,\n                          sv_lag, formation_time,\n                          n_portfolios_main, sorting_method,\n                          n_portfolios_secondary, exchanges,\n                          value_weighted) {\n  # Select data\n  data_sorts &lt;- handle_data(\n    include_financials = include_financials,\n    include_utilities = include_utilities,\n    drop_smallNYSE_at = drop_smallNYSE_at,\n    drop_price_at = drop_price_at,\n    drop_stock_age_at = drop_stock_age_at,\n    drop_earnings = drop_earnings,\n    drop_bookequity = drop_bookequity,\n    sv_lag = sv_lag\n  )\n\n  # Rebalancing\n  ## Monthly\n  if (formation_time == \"monthly\") {\n    data_return &lt;- rebalance_monthly(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  ## Annually\n  if (formation_time == \"FF\") {\n    data_return &lt;- rebalance_annually(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  # Compute return differential\n  data_return |&gt;\n    group_by(month) |&gt;\n    summarize(\n      premium = ret[portfolio == max(portfolio)] - ret[portfolio == min(portfolio)],\n      .groups = \"drop\"\n    ) |&gt;\n    pull(premium) |&gt;\n    mean() * 100\n}"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#applying-the-functions",
    "href": "blog/nse-portfolio-sorts/index.html#applying-the-functions",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Applying the functions",
    "text": "Applying the functions\nFinally, we have data, decisions, and functions. Indeed, we are now ready to implement the portfolio sort, right? Yes! Just let me briefly discuss how the implementation works.\nWe have a grid with 69,120 specifications. For each of these specifications, we want to estimate a return differential. This is most easily achieved with a pmap() call. However, we want to parallelize the operation to leverage the multiple cores of our device. Hence, we have to use the package furrr and a future_pmap() instead. As a side note, in most cases we also have to increase the maximum memory for each worker, which can be done with options().\n\nlibrary(furrr)\n\noptions(future.globals.maxSize = 891289600)\n\nplan(multisession, workers = availableCores())\n\nWith the parallel environment set and ready to go, we map the arguments our function needs into the final function execute_sorts() from above. Then, we go and have some tea. And some lunch, breakfast, second breakfast, and so on. In short, it takes a while - depending on your device even more than a day. Each result requires roughly 13 seconds, but you must remember that you are computing 69,120 results.\n\ndata_premia &lt;- setup_grid |&gt;\n  mutate(premium_estimate = future_pmap(\n    .l = list(\n      sorting_variable, drop_smallNYSE_at, include_financials,\n      include_utilities, drop_bookequity, drop_earnings,\n      drop_stock_age_at, drop_price_at, sv_lag,\n      formation_time, n_portfolios_main, sorting_method,\n      n_portfolios_secondary, exchanges, value_weighted\n    ),\n    .f = ~ execute_sorts(\n      sorting_variable = ..1,\n      drop_smallNYSE_at = ..2,\n      include_financials = ..3,\n      include_utilities = ..4,\n      drop_bookequity = ..5,\n      drop_earnings = ..6,\n      drop_stock_age_at = ..7,\n      drop_price_at = ..8,\n      sv_lag = ..9,\n      formation_time = ..10,\n      n_portfolios_main = ..11,\n      sorting_method = ..12,\n      n_portfolios_secondary = ..13,\n      exchanges = ..14,\n      value_weighted = ..15\n    )\n  ))\n\nNow you have all the estimates for the premium. However, one last step has to be considered when you actually investigate the premium. The portfolio sorting algorithm we constructed here is always long in the firms with a high value for the sorting variable and short in firms with low characteristics. This provides a very general way of doing it. Hence, you simply correct for this effect at the end by multiplying the column by -1 if your sorting variable predicts an inverse relation between the sorting variable and expected returns. Otherwise, you can ignore the following chunk.\n\ndata_premia &lt;- data_premia |&gt;\n  mutate(premium_estimate = premium_estimate * -1)"
  },
  {
    "objectID": "blog/nse-portfolio-sorts/index.html#footnotes",
    "href": "blog/nse-portfolio-sorts/index.html#footnotes",
    "title": "Non-standard errors in portfolio sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMenkveld, A. J. et al. (2023). “Non-standard Errors”, Journal of Finance (forthcoming). http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWalter, D., Weber, R., and Weiss, P. (2023). “Non-Standard Errors in Portfolio Sorts”. http://dx.doi.org/10.2139/ssrn.4164117↩︎\nCooper, M. J., Gulen, H., and Schill, M. J. (2008). “Asset growth and the cross‐section of stock returns”, The Journal of Finance, 63(4), 1609-1651.↩︎"
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html",
    "href": "blog/tidy-collaborative-filtering/index.html",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "",
    "text": "Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver personalized content to users based on a large pool of potentially relevant information. In this blog post, I illustrate the concept of recommender systems by building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks to investors who already have their own portfolios and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide meaningful inspiration for other investors. The resulting stock recommendations of course do not constitute any investment advice and rather serve an illustrative purpose.\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers, and editors of financial magazines. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender.\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering, and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the recommender approaches and refer to Ricci et al. (2011)1 for a comprehensive exposition."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#collaborative-filtering",
    "href": "blog/tidy-collaborative-filtering/index.html#collaborative-filtering",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\nIn collaborative filtering, recommendations are based on past user interactions and items to produce new recommendations. The central notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach, which essentially searches nearest neighbors based on recorded transactions and is hence model-free, and the model-based approach, where new representations of users and items are built based on some generative pre-estimated model. Theoretically, the memory-based approach has a low bias (since no latent model is assumed) but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model. It has a relatively higher bias but a lower variance, i.e., recommendations are more stable since they come from a model.\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem)."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#content-based-filering",
    "href": "blog/tidy-collaborative-filtering/index.html#content-based-filering",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Content-Based Filering",
    "text": "Content-Based Filering\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (the user will like the item or not) or more generally a regression problem (which rating will the user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has a few interactions then the resulting model becomes easily unstable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors, hence considering both input simultaneously, and putting them into a neural network.\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#hybrid-recommenders",
    "href": "blog/tidy-collaborative-filtering/index.html#hybrid-recommenders",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Hybrid Recommenders",
    "text": "Hybrid Recommenders\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), chosing among different component (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#roc-curves",
    "href": "blog/tidy-collaborative-filtering/index.html#roc-curves",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "ROC curves",
    "text": "ROC curves\nThe first visualization approach comes from signal-detection and is called “Receiver Operating Characteristic” (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\nTPR = TP / (TP + FN) (i.e., share of true positive recommendations relative to all known portfolios)\nFPR = FP / (FP + TN) (i.e., share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\nresults_tbl |&gt;\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which is not surprising). Association rules, on the other hand, constitute the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#precision-recall-curves",
    "href": "blog/tidy-collaborative-filtering/index.html#precision-recall-curves",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Precision-Recall Curves",
    "text": "Precision-Recall Curves\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\nPrecision = TP / (TP + FP) (i.e., correctly recommended items relative to total recommended items)\nRecall = TP / (TP + FN) (i.e., correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\nresults_tbl |&gt;\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall curves of stock recommender algorithms\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n\n\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach."
  },
  {
    "objectID": "blog/tidy-collaborative-filtering/index.html#footnotes",
    "href": "blog/tidy-collaborative-filtering/index.html#footnotes",
    "title": "Tidy Collaborative Filtering: Building A Stock Recommender",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRicci, F, Rokach, L., Shapira, B. and Kantor, P. (2011). “Recommender Systems Handbook”. https://link.springer.com/book/10.1007/978-0-387-85820-3.↩︎\nHahsler, M. (2022). “recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms”, R package version 1.0.3. https://CRAN.R-project.org/package=recommenderlab.↩︎\nBreese, J.S., Heckerman, D. and Kadie, C. (1998). “Empirical Analysis of Predictive Algorithms for Collaborative Filtering”, Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Madison, 43-52. https://arxiv.org/pdf/1301.7363.pdf.↩︎"
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html",
    "href": "blog/workshops-for-ukraine/index.html",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "",
    "text": "Dariia Mykhailyshyna, an Economics PhD student at the University of Bologna, set up a collection of workshops that can be accessed in exchange for a donation in support of Ukraine. We contributed two workshops based on our book Tidy Finance With R. The seminars are recorded and available on demand, and the collection is continuously expanded with interesting topics. Check out the extensive workshop program to register for upcoming events and get recordings and materials of the previous workshops.\nYou can find the workshop descriptions of our contributions below. We believe in making a humanitarian contribution to this cause and would appreciate it if you consider this tremendous effort."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html#financial-data-in-r",
    "href": "blog/workshops-for-ukraine/index.html#financial-data-in-r",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "Financial Data in R",
    "text": "Financial Data in R\nThis workshop explores financial data available for research and practical applications in financial economics. It relies on material available on www.tidy-finance.org and covers: (1) How to access freely available data from Yahoo!Finance and other vendors. (2) Where to find the data most commonly used in academic research. This main part covers data from CRSP, Compustat, and TRACE. (3) How to store and access data for your research project efficiently. (4) What other data providers are available and how to access their services within R."
  },
  {
    "objectID": "blog/workshops-for-ukraine/index.html#empirical-asset-pricing-in-r",
    "href": "blog/workshops-for-ukraine/index.html#empirical-asset-pricing-in-r",
    "title": "Tidy Finance at Workshops for Ukraine",
    "section": "Empirical Asset Pricing in R",
    "text": "Empirical Asset Pricing in R\nThis workshop explores empirical asset pricing and combines explanations of theoretical concepts with practical implementations. The course relies on material available on www.tidy-finance.org and proceeds in three steps: (1) We dive into the most used data sources and show how to work with data from WRDS, forming the basis for the analysis. We also briefly introduce some other possible sources of financial data. (2) We show how to implement the capital asset pricing model in rolling-window regressions. (3) We introduce the widely used method of portfolio sorts in empirical asset pricing. During the workshop, we will combine some theoretical insights with hands-on implementations in R."
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "Contribute to Tidy Finance",
    "section": "",
    "text": "Join our mission to support reproducible finance by contributing to the Tidy Finance Blog. The blog follows our endeavors to increase transparency in financial economics and opens a new channel for you. We actively encourage the finance community to share their insights on coding.\nTidy Finance is an open-source project to augment how research is conducted. We hope to change the way people think about sharing their code. The Tidy Finance Blog is open to everybody who wants to share their code, discuss paper replication, preparation of typical datasets or highlight novel empirical applications in financial economics. The blog is an excellent tool to promote your research and share your thought processes with the community.\nWe strive to make Tidy Finance a trusted, up-to-date resource for all topics in financial economics. Therefore, we commit our time to ensure high-quality content - a commitment that we extend to the Tidy Finance Blog by providing editorial-like processes. While we aim to be as open as possible, we will also carefully assess all contributions."
  },
  {
    "objectID": "contribute.html#who-should-contribute-to-the-tidy-finance-blog",
    "href": "contribute.html#who-should-contribute-to-the-tidy-finance-blog",
    "title": "Contribute to Tidy Finance",
    "section": "Who should contribute to the Tidy Finance Blog?",
    "text": "Who should contribute to the Tidy Finance Blog?\nEverybody should consider writing a blog post on Tidy Finance. We do not exclude anybody from the finance community. We always appreciate relevant contributions that help the target audiences of Tidy Finance (i.e., researchers, students, and professionals). Let us move forward together."
  },
  {
    "objectID": "contribute.html#how-can-i-contribute",
    "href": "contribute.html#how-can-i-contribute",
    "title": "Contribute to Tidy Finance",
    "section": "How can I contribute?",
    "text": "How can I contribute?\nIf you want to contribute to the Tidy Finance Blog, we provide a simple three-step process for your blog post.\n\nSend a proposal for your blog post to blog@tidy-finance.org.\nAfter our confirmation, write and submit your blog post.\nWork with us on potential revisions before reading your published post.\n\nWe are grateful for every contribution. This process is our attempt to provide fairness to all parties. From the perspective of the readers, the other authors, and the whole project, ensuring the quality and relevance of all contributions is paramount. Hence, we cannot publish all blog posts without thoroughly reviewing the submissions.\nWe ask you to submit a proposal before you write an entire post to ensure your efforts are not wasted. In this proposal, we want to learn about your idea and how it will contribute to Tidy Finance. Therefore, do not hesitate to submit a proposal. We guarantee full anonymity and an open attitude to your suggestion. Nevertheless, we do not make promises regarding a positive decision.\nOnce you receive our thumbs-up, you can start working on your contribution. The target format is a quarto document we can render as part of our website repository. When you have a first draft ready, send us an HTML output, and we will provide timely feedback.\nFinally, we will publish your post on the Tidy Finance blog with your name added to the list of contributors to reproducible finance. Unfortunately, we cannot provide you with a cape. Then again, not all heroes wear capes."
  },
  {
    "objectID": "contribute.html#part-of-the-future",
    "href": "contribute.html#part-of-the-future",
    "title": "Contribute to Tidy Finance",
    "section": "Part of the future",
    "text": "Part of the future\nThe Tidy Finance Blog is the place to contribute stand-alone applications in financial economics with guaranteed quality and relevance. While you publish under an open-source license, you retain sole authorship of your work. Furthermore, in future editions of “Tidy Finance with R,” we will acknowledge external contributions directly by referring to the Tidy Finance blog and citing your work."
  },
  {
    "objectID": "contribute.html#what-if-i-do-not-use-the-tidyverse-or-r",
    "href": "contribute.html#what-if-i-do-not-use-the-tidyverse-or-r",
    "title": "Contribute to Tidy Finance",
    "section": "What if I do not use the tidyverse or R?",
    "text": "What if I do not use the tidyverse or R?\nTidy Finance is strongly connected to the tidyverse and R. Yet, we do not rule out extending our horizon and learning new tricks. You can make non-tidyverse and non-R suggestions. However, we need a sufficient reason to break with our tradition."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-noncommercial-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-noncommercial-sharealike-4.0-international-public-license",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "python/fama-macbeth-regressions.html",
    "href": "python/fama-macbeth-regressions.html",
    "title": "Fama-MacBeth Regressions",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we present a simple implementation of Fama and MacBeth (1973), a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in Fama and French (1993).\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\nThe Fama-MacBeth procedure is a simple two-step approach: The first step uses the exposures (characteristics) as explanatory variables in \\(T\\) cross-sectional regressions. For example, if \\(r_{i,t+1}\\) denote the excess returns of asset \\(i\\) in month \\(t+1\\), then the famous Fama-French three factor model implies the following return generating process (see also Campbell et al. 1998): \\[\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned}\\] Here, we are interested in the compensation \\(\\lambda^{f}_t\\) for the exposure to each risk factor \\(\\beta^{f}_{i,t}\\) at each time point, i.e., the risk premium. Note the terminology: \\(\\beta^{f}_{i,t}\\) is a asset-specific characteristic, e.g., a factor exposure or an accounting variable. If there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., \\(\\lambda_t^{f}\\neq0\\).\nIn the second step, the time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) of the estimates \\(\\hat\\lambda^{f}_t\\) can then be interpreted as the risk premium for the specific risk factor \\(f\\). We follow Zaffaroni and Zhou (2022) and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time \\(t+1\\) variables, then the regression approach captures risk attributes rather than risk premiums.\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., \\(\\hat\\beta^{f}_{i}\\), are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a step 0. You might thus worry that the errors of \\(\\hat\\beta^{f}_{i}\\) impact the risk premiums’ standard errors. Measurement error in \\(\\hat\\beta^{f}_{i}\\) indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias (see, e.g., Shanken 1992; Kim 1995; Chen, Lee, and Lee 2015, among others) but also shows that the bias goes to zero as \\(T \\to \\infty\\). We refer to Gagliardini, Ossola, and Scaillet (2016) for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks: Hou, Xue, and Zhang (2020) advocates using weighed-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\nThe current chapter relies on this set of packages.\nimport pandas as pd\nimport numpy as np\nfrom pandas.tseries.offsets import DateOffset\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#data-preparation",
    "href": "python/fama-macbeth-regressions.html#data-preparation",
    "title": "Fama-MacBeth Regressions",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe illustrate Fama and MacBeth (1973) with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our database introduced in Chapters 2-4.\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, gvkey, month, ret_excess, mktcap from crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}}\n)\n\ncompustat = pd.read_sql_query(\n  sql=\"SELECT datadate, gvkey, be FROM compustat\",\n  con=tidy_finance,\n  parse_dates={\"datadate\": {\"unit\": \"D\", \"origin\": \"unix\"}}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT month, permno, beta_monthly from beta\",\n  con=tidy_finance,\n  parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}}\n)\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization. Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\n\ncharacteristics = (compustat\n  .assign(month = lambda x: x[\"datadate\"].dt.to_period(\"M\").dt.to_timestamp())\n  .merge(crsp_monthly, how=\"left\", on=[\"gvkey\", \"month\"], )\n  .merge(beta, how=\"left\", on=[\"permno\", \"month\"])\n  .assign(bm = lambda x: x[\"be\"] / x[\"mktcap\"],\n          log_mktcap = lambda x: np.log(x[\"mktcap\"]),\n          sorting_date = lambda x: x[\"month\"] + DateOffset(months = 6))\n  .get([\"gvkey\", \"bm\", \"log_mktcap\", \"beta_monthly\", \"sorting_date\"])\n  .rename(columns={\"beta_monthly\": \"beta\"})\n)\n\ndata_fama_macbeth = (crsp_monthly\n  .merge(characteristics, \n         how=\"left\",\n         left_on=[\"gvkey\", \"month\"], right_on=[\"gvkey\", \"sorting_date\"])\n  .sort_values([\"month\", \"permno\"])\n  .groupby(\"permno\")\n  .apply(lambda x: x.assign(\n    beta = x[\"beta\"].fillna(method=\"ffill\"),\n    bm = x[\"bm\"].fillna(method=\"ffill\"),\n    log_mktcap = x[\"log_mktcap\"].fillna(method=\"ffill\"))\n  )\n  .reset_index(drop=True)\n)\n\ndata_fama_macbeth_lagged = (data_fama_macbeth\n  .assign(month = lambda x: x[\"month\"] - DateOffset(months=1))\n  .get([\"permno\", \"month\", \"ret_excess\"])\n  .rename(columns={\"ret_excess\": \"ret_excess_lead\"})\n)\n\ndata_fama_macbeth = (data_fama_macbeth\n  .merge(data_fama_macbeth_lagged, how=\"left\", on=[\"permno\", \"month\"])\n  .get([\"permno\", \"month\", \"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n  .dropna()\n)"
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#cross-sectional-regression",
    "href": "python/fama-macbeth-regressions.html#cross-sectional-regression",
    "title": "Fama-MacBeth Regressions",
    "section": "Cross-sectional Regression",
    "text": "Cross-sectional Regression\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums \\(\\hat\\lambda^{f}_t\\) for each point in time. \n\nrisk_premiums = (data_fama_macbeth\n  .groupby(\"month\", as_index=False)\n  .apply(lambda x: smf.ols(\"ret_excess_lead ~ beta + log_mktcap + bm\", x)\n                    .fit().params)\n)"
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#time-series-aggregation",
    "href": "python/fama-macbeth-regressions.html#time-series-aggregation",
    "title": "Fama-MacBeth Regressions",
    "section": "Time-Series Aggregation",
    "text": "Time-Series Aggregation\nNow that we have the risk premiums’ estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the \\(t\\)-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\nprice_of_risk = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series(\n    {\"risk_premium\": 100 * x.mean(),\n     \"t_statistic\": x.mean() / x.std() * np.sqrt(len(x))}))\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in Chapter 7, the typical procedure for this is computing Newey and West (1987) standard errors.\n\nprice_of_risk_newey_west = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: x[\"estimate\"].mean() /\n         smf.ols(\"estimate ~ 1\", x)\n         .fit(cov_type = \"HAC\", cov_kwds = {\"maxlags\": 6})\n         .bse)\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\n(price_of_risk\n  .merge(price_of_risk_newey_west, on=\"factor\")\n  .round(3)\n)\n\n\n\n\n\n\n\n\nfactor\nrisk_premium\nt_statistic\nt_statistic_newey_west\n\n\n\n\n0\nIntercept\n1.327\n5.208\n4.628\n\n\n1\nbeta\n0.008\n0.080\n0.073\n\n\n2\nbm\n0.138\n2.968\n2.599\n\n\n3\nlog_mktcap\n-0.114\n-3.185\n-3.037\n\n\n\n\n\n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns."
  },
  {
    "objectID": "python/fama-macbeth-regressions.html#exercises",
    "href": "python/fama-macbeth-regressions.html#exercises",
    "title": "Fama-MacBeth Regressions",
    "section": "Exercises",
    "text": "Exercises\n\nDownload a sample of test assets from Kenneth French’s homepage and reevaluate the risk premiums for industry portfolios instead of individual stocks.\nUse individual stocks with weighted-least squares based on a firm’s size as suggested by Hou, Xue, and Zhang (2020). Then, repeat the Fama-MacBeth regressions without the weighting scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches.\nImplement a rolling-window regression for the time-series estimation of the factor exposure. Skip one month after each rolling period before including the exposures in the cross-sectional regression to avoid a look-ahead bias. Then, adapt the cross-sectional regression and compute the average risk premiums."
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html",
    "href": "python/introduction-to-tidy-finance.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nThe main aim of this chapter is to familiarize yourself with ´pandas´ and ´numpy´, the main workhorses for data analysis in Python. We start by downloading and visualizing stock data from Yahoo!Finance. Then we move to a simple portfolio choice problem and construct the efficient frontier. These examples introduce you to our approach of Tidy Finance."
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "href": "python/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with Stock Market Data",
    "text": "Working with Stock Market Data\nAt the start of each session, we load the required packages. Throughout the entire book, we always use pandas and numpy to perform a number of data manipulations. In this chapter, we also load the convenient yfinance package to download price data.\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\n\nWe first download daily prices for one stock symbol, e.g., the Apple stock, AAPL, directly from the data provider Yahoo!Finance. To download the data, you can use the function yf.download(). \n\nprices = (yf.download(tickers=\"AAPL\", \n                      start=\"2000-01-01\", \n                      end=\"2021-12-31\")\n  .reset_index()\n  .assign(symbol = \"AAPL\")\n  .rename(columns = {\"Date\": \"date\", \n                     \"Open\": \"open\", \n                     \"High\": \"high\",\n                     \"Low\": \"low\",\n                     \"Close\": \"close\", \n                     \"Adj Close\": \"adjusted\", \n                     \"Volume\": \"volume\"\n                    })\n)\nprices.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadjusted\nvolume\nsymbol\n\n\n\n\n0\n2000-01-03\n0.936384\n1.004464\n0.907924\n0.999442\n0.849469\n535796800\nAAPL\n\n\n1\n2000-01-04\n0.966518\n0.987723\n0.903460\n0.915179\n0.777850\n512377600\nAAPL\n\n\n2\n2000-01-05\n0.926339\n0.987165\n0.919643\n0.928571\n0.789232\n778321600\nAAPL\n\n\n3\n2000-01-06\n0.947545\n0.955357\n0.848214\n0.848214\n0.720933\n767972800\nAAPL\n\n\n4\n2000-01-07\n0.861607\n0.901786\n0.852679\n0.888393\n0.755083\n460734400\nAAPL\n\n\n\n\n\n\n\n yf.download() downloads stock market data from Yahoo!Finance if you do not specify another data source. The above code chunk returns a data frame with eight quite self-explanatory columns: date, the market prices at the open, high, low, and close, the adjusted price in USD, the daily volume (in the number of traded shares), and the symbol. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the plotnine package to visualize the time series of adjusted prices in Figure 1 . This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012).\n\nfrom plotnine import *\n\nprices_figure = (ggplot(prices, aes(y=\"adjusted\", \n                                    x=\"date\"))\n + geom_line()\n + labs(x=\"\", y=\"\",\n        title=\"Apple stock prices between beginning of 2000 and end of 2021\")\n)\nprices_figure.draw()\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n Instead of analyzing prices, we compute daily net returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted day \\(t\\) price. In that context, the function pct_change() is helpful because it computes this percentage change.\n\nreturns = (prices\n  .sort_values(\"date\")\n  .assign(ret = lambda x: x[\"adjusted\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n)\nreturns.head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nret\n\n\n\n\n0\nAAPL\n2000-01-03\nNaN\n\n\n1\nAAPL\n2000-01-04\n-0.084310\n\n\n2\nAAPL\n2000-01-05\n0.014633\n\n\n3\nAAPL\n2000-01-06\n-0.086539\n\n\n4\nAAPL\n2000-01-07\n0.047369\n\n\n\n\n\n\n\nThe resulting data frame contains three columns, where the last contains the daily returns (ret). Note that the first entry naturally contains a missing value (NaN) because there is no previous price. Obviously, the use of pct_change() would be meaningless if the time series is not ordered by ascending dates. The function sort_values() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use the parameter ascending=False.\nFor the upcoming examples, we remove missing values as these would require separate treatment when computing, e.g., sample averages. In general, however, make sure you understand why NA values occur and carefully examine if you can simply get rid of these observations.\n\nreturns = returns.dropna() \n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. Additionally, we add a dashed line that indicates the 5 percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most 5 percent. The 5 percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to stylized facts of returns.\n\nfrom mizani.formatters import percent_format\n\nquantile_05 = returns[\"ret\"].quantile(0.05)\n\nreturns_figure = (ggplot(returns, aes(x=\"ret\"))\n + geom_histogram(bins=100)\n + geom_vline(aes(xintercept=quantile_05), linetype=\"dashed\")\n + labs(x=None, y=None,\n        title=\"Distribution of daily Apple stock returns\")\n + scale_x_continuous(labels=percent_format())\n)\nreturns_figure.draw()\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical 5 percent quantile.\n\n\n\n\nHere, bins = 100 determines the number of bins used in the illustration and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns. A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.\n\nreturns[\"ret\"].describe()\n\ncount    5534.000000\nmean        0.001297\nstd         0.025237\nmin        -0.518692\n25%        -0.010100\n50%         0.000942\n75%         0.013111\nmax         0.139049\nName: ret, dtype: float64\n\n\nWe see that the maximum daily return was {python} round(returns[\"ret\"].max() * 100, 3) percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing .groupby(returns[\"date\"].dt.year), where the call .dt.year returns the year of a date variable. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the return distribution.\n\nreturns[\"ret\"].groupby(returns[\"date\"].dt.year).describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2000\n251.0\n-0.003457\n0.054940\n-0.518692\n-0.034425\n-0.001702\n0.027153\n0.136858\n\n\n2001\n248.0\n0.002329\n0.039338\n-0.171712\n-0.022873\n-0.001162\n0.026942\n0.128567\n\n\n2002\n252.0\n-0.001212\n0.030526\n-0.150372\n-0.018749\n-0.002787\n0.017930\n0.084558\n\n\n2003\n252.0\n0.001857\n0.023360\n-0.081421\n-0.012068\n0.001549\n0.014642\n0.113492\n\n\n2004\n252.0\n0.004702\n0.025470\n-0.055785\n-0.009004\n0.002835\n0.015525\n0.131573\n\n\n2005\n252.0\n0.003490\n0.024478\n-0.092105\n-0.010068\n0.003440\n0.016910\n0.091168\n\n\n2006\n251.0\n0.000949\n0.024265\n-0.063326\n-0.014082\n-0.001525\n0.014302\n0.118299\n\n\n2007\n251.0\n0.003664\n0.023757\n-0.070206\n-0.008942\n0.002557\n0.017957\n0.105359\n\n\n2008\n253.0\n-0.002646\n0.036666\n-0.179195\n-0.023983\n-0.001043\n0.018687\n0.139049\n\n\n2009\n252.0\n0.003819\n0.021369\n-0.050164\n-0.008984\n0.002112\n0.015498\n0.067592\n\n\n2010\n252.0\n0.001832\n0.016856\n-0.049598\n-0.006217\n0.002332\n0.011379\n0.076867\n\n\n2011\n252.0\n0.001040\n0.016539\n-0.055940\n-0.009266\n0.001067\n0.011128\n0.058888\n\n\n2012\n250.0\n0.001299\n0.018566\n-0.064357\n-0.007969\n0.000473\n0.011957\n0.088741\n\n\n2013\n252.0\n0.000472\n0.017987\n-0.123558\n-0.008709\n-0.000278\n0.010901\n0.051361\n\n\n2014\n252.0\n0.001447\n0.013643\n-0.079928\n-0.005763\n0.001042\n0.009585\n0.081982\n\n\n2015\n252.0\n0.000020\n0.016843\n-0.061163\n-0.008595\n-0.000630\n0.009395\n0.057355\n\n\n2016\n252.0\n0.000575\n0.014702\n-0.065706\n-0.005742\n0.000873\n0.007724\n0.064963\n\n\n2017\n251.0\n0.001637\n0.011091\n-0.038777\n-0.003720\n0.000667\n0.006761\n0.060981\n\n\n2018\n251.0\n-0.000057\n0.018106\n-0.066331\n-0.008958\n0.000524\n0.009429\n0.070421\n\n\n2019\n252.0\n0.002665\n0.016466\n-0.099607\n-0.004797\n0.002753\n0.011551\n0.068335\n\n\n2020\n253.0\n0.002807\n0.029387\n-0.128647\n-0.010259\n0.001749\n0.017396\n0.119808\n\n\n2021\n251.0\n0.001325\n0.015841\n-0.041674\n-0.007655\n0.001474\n0.012455\n0.053851"
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "href": "python/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of stock symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets as you like. The following code takes any vector of symbols, e.g., symbol = [\"AAPL\", \"MMM\", \"BA\"], and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for an arbitrary number of assets. We perform the analysis with data from all current constituents of the Dow Jones Industrial Average index. \nWe first download a table with DOW Jones constituents from an external website.\n\nurl = (\"https://www.ssga.com/us/en/institutional/etfs/library-content/\"\n       \"products/fund-data/etfs/us/holdings-daily-us-en-dia.xlsx\")\n\nsymbols = (pd.read_excel(url, skiprows=4, nrows=30)\n  .get(\"Ticker\")\n  .tolist()\n)\n\nNext, we can use yf.download() to download prices for all stock symbols in the above list.\n\nindex_prices = (yf.download(tickers=symbols, \n                            start=\"2000-01-01\",\n                            end=\"2021-12-31\",\n                            progress=False)\n  .melt(ignore_index=False, \n        var_name=[\"variable\", \"symbol\"])\n  .reset_index()\n  .pivot(index=[\"Date\", \"symbol\"], \n         columns=\"variable\", \n         values=\"value\")\n  .reset_index()\n  .rename(columns = {\"Date\": \"date\", \n                     \"Open\": \"open\", \n                     \"High\": \"high\", \n                     \"Low\": \"low\",\n                     \"Close\": \"close\", \n                     \"Adj Close\": \"adjusted\", \n                     \"Volume\": \"volume\"\n                    })\n)\n\nThe resulting data frame contains {python} len(index_prices) daily observations for {python} len(index_prices[\"symbol\"].unique()) different stock symbols. Figure 3 illustrates the time series of downloaded adjusted prices for each of the constituents of the Dow Jones index. Make sure you understand every single line of code! (What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nindex_prices_figure = (ggplot(index_prices, \n                              aes(y=\"adjusted\", \n                                  x=\"date\", \n                                  color=\"symbol\"))\n + geom_line()\n + labs(x=\"\", y=\"\", color=\"\",\n        title=\"Stock prices of DOW index constituents\")\n + theme(legend_position=\"none\")\n )\nindex_prices_figure.draw()\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\nDo you notice the small differences relative to the code we used before? yf.download(symbols) returns a data frame for several symbols as well. All we need to do to illustrate all symbols simultaneously is to include color = symbol in the ggplot2 aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates the point well.\nThe same holds for stock returns. Before computing the returns, we use groupby(\"symbol\") such that the assign() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: groupby(\"symbol\") is the key to aggregating the time series into symbol-specific variables of interest.\n\nall_returns = (index_prices\n  .assign(ret = lambda x: x.groupby(\"symbol\")[\"adjusted\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n  .dropna(subset=\"ret\")\n)\n\nall_returns.groupby(\"symbol\")[\"ret\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nAAPL\n5534.0\n0.001297\n0.025237\n-0.518692\n-0.010100\n0.000941\n0.013111\n0.139050\n\n\nAMGN\n5534.0\n0.000475\n0.019886\n-0.134124\n-0.008830\n0.000077\n0.009601\n0.151021\n\n\nAXP\n5534.0\n0.000547\n0.022962\n-0.175950\n-0.008379\n0.000396\n0.009700\n0.218822\n\n\nBA\n5534.0\n0.000614\n0.021992\n-0.238484\n-0.009625\n0.000574\n0.010878\n0.243186\n\n\nCAT\n5534.0\n0.000699\n0.020360\n-0.145175\n-0.009527\n0.000487\n0.010996\n0.147230\n\n\nCRM\n4412.0\n0.001283\n0.026786\n-0.271482\n-0.011358\n0.000566\n0.013425\n0.260449\n\n\nCSCO\n5534.0\n0.000370\n0.023922\n-0.162107\n-0.009009\n0.000490\n0.010301\n0.243884\n\n\nCVX\n5534.0\n0.000486\n0.017494\n-0.221248\n-0.007891\n0.000753\n0.008919\n0.227407\n\n\nDIS\n5534.0\n0.000531\n0.019293\n-0.183630\n-0.008368\n0.000366\n0.008986\n0.159722\n\n\nDOW\n702.0\n0.000799\n0.028071\n-0.216577\n-0.012512\n0.000571\n0.015287\n0.209091\n\n\nGS\n5534.0\n0.000584\n0.023326\n-0.189596\n-0.010114\n0.000350\n0.011161\n0.264678\n\n\nHD\n5534.0\n0.000601\n0.019350\n-0.287356\n-0.008041\n0.000544\n0.009190\n0.140665\n\n\nHON\n5534.0\n0.000523\n0.019516\n-0.173669\n-0.008009\n0.000560\n0.008897\n0.282230\n\n\nIBM\n5534.0\n0.000262\n0.016600\n-0.155420\n-0.007070\n0.000287\n0.007666\n0.120233\n\n\nINTC\n5534.0\n0.000400\n0.023587\n-0.220330\n-0.010305\n0.000466\n0.011279\n0.201229\n\n\nJNJ\n5534.0\n0.000415\n0.012260\n-0.158456\n-0.005089\n0.000309\n0.006093\n0.122292\n\n\nJPM\n5534.0\n0.000625\n0.024423\n-0.207275\n-0.008938\n0.000232\n0.009873\n0.250968\n\n\nKO\n5534.0\n0.000329\n0.013261\n-0.100609\n-0.005468\n0.000431\n0.006345\n0.138796\n\n\nMCD\n5534.0\n0.000553\n0.014825\n-0.158754\n-0.006189\n0.000742\n0.007322\n0.181255\n\n\nMMM\n5534.0\n0.000452\n0.014939\n-0.129450\n-0.006285\n0.000540\n0.007535\n0.125986\n\n\nMRK\n5534.0\n0.000325\n0.016950\n-0.267806\n-0.007439\n0.000245\n0.008444\n0.130329\n\n\nMSFT\n5534.0\n0.000587\n0.019251\n-0.155978\n-0.008024\n0.000359\n0.009221\n0.195652\n\n\nNKE\n5534.0\n0.000824\n0.019003\n-0.198127\n-0.007891\n0.000592\n0.009230\n0.155314\n\n\nPG\n5534.0\n0.000398\n0.013377\n-0.302358\n-0.005203\n0.000371\n0.006140\n0.120090\n\n\nTRV\n5534.0\n0.000554\n0.018488\n-0.208004\n-0.007306\n0.000630\n0.008269\n0.255555\n\n\nUNH\n5534.0\n0.001012\n0.019964\n-0.186362\n-0.008072\n0.000810\n0.009909\n0.347550\n\n\nV\n3471.0\n0.000995\n0.018948\n-0.136434\n-0.007636\n0.001223\n0.009523\n0.149974\n\n\nVZ\n5534.0\n0.000287\n0.015149\n-0.118462\n-0.007086\n0.000212\n0.007449\n0.146324\n\n\nWBA\n5534.0\n0.000340\n0.018121\n-0.149873\n-0.008526\n0.000000\n0.009254\n0.166355\n\n\nWMT\n5534.0\n0.000321\n0.014925\n-0.101833\n-0.006663\n0.000276\n0.006919\n0.117085"
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "href": "python/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "title": "Introduction to Tidy Finance",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol can also make sense. For instance, suppose you are interested in answering the question: are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily adjusted closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e9 (Python can handle scientific notation) denotes daily trading volume in billion USD.\n\ntrading_volume = (index_prices\n  .assign(trading_volume = lambda x: (x[\"volume\"] * x[\"adjusted\"]) / 1e9)\n  .groupby(\"date\")[\"trading_volume\"]\n  .sum()\n  .reset_index()\n  .assign(trading_volume_lag = lambda x: x[\"trading_volume\"].shift())\n)\n\ntrading_volume_figure = (ggplot(trading_volume, \n                                aes(x=\"date\", \n                                    y=\"trading_volume\"))\n + geom_line()\n + labs(x=\"\", \n        y=\"\",\n        title=\"Aggregate daily trading volume of \"\n              \"DOW index constituents in billion USD\")\n)\ntrading_volume_figure.draw()\n\n\n\n\nFigure 4: Total daily trading volume in billion USD.\n\n\n\n\nFigure 4 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 5, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\ntrading_volume_figure = (ggplot(trading_volume, \n                                aes(x=\"trading_volume_lag\", \n                                    y=\"trading_volume\"))\n + geom_point()\n + geom_abline(aes(intercept=0, slope=1), \n               linetype=\"dashed\")\n + labs(x=\"Previous day aggregate trading volume\",\n        y=\"Aggregate trading volume\",\n        title=\"Persistence in daily trading volume of DOW constituents \"\n              \"in billion USD\")\n)\ntrading_volume_figure.draw()\n\n\n\n\nFigure 5: Total daily trading volume in billion USD."
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "href": "python/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio Choice Problems",
    "text": "Portfolio Choice Problems\nIn the previous part, we show how to download stock market data and inspect it with graphs and summary statistics. Now, we move to a typical question in Finance: how to allocate wealth across different assets optimally. The standard framework for optimal portfolio selection considers investors that prefer higher future returns but dislike future return volatility (defined as the square root of the return variance): the mean-variance investor (Markowitz 1952).\n An essential tool to evaluate portfolios in the mean-variance context is the efficient frontier, the set of portfolios which satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility (the square root of the variance, i.e., the risk), see, e.g., Merton (1972). We compute and visualize the efficient frontier for several stocks. First, we extract each asset’s monthly returns. In order to keep things simple, we work with a balanced panel and exclude DOW constituents for which we do not observe a price on every single trading day since the year 2000.\n\nprices = (index_prices\n  .groupby(\"symbol\")\n  .apply(lambda x: x.assign(counts = x[\"adjusted\"].dropna().count()))\n  .reset_index(drop=True)\n  .query(\"counts == counts.max()\")\n)\n\nNext, we transform the returns from a tidy data frame into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) symbols and one row for each of the \\(T\\) trading days to compute the sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] where \\(r_t\\) is the \\(N\\) vector of returns on date \\(t\\) and the sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'.\\] We achieve this by using pivot() with the new column names from the column symbol and setting the values to adjusted. We compute the vector of sample average returns and the sample variance-covariance matrix, which we consider as proxies for the parameters of the distribution of future stock returns. Thus, for simplicity, we refer to \\(\\Sigma\\) and \\(\\mu\\) instead of explicitly highlighting that the sample moments are estimates. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account.\n\nreturns_matrix = (prices\n  .pivot(columns=\"symbol\", \n         values=\"adjusted\", \n         index=\"date\")\n  .resample(\"m\")\n  .last()\n  .pct_change()\n  .dropna()\n)\nlength_year = 12\nmu = np.array(returns_matrix.mean()).T * length_year\nsigma = np.array(returns_matrix.cov()) * length_year\n\nThen, we compute the minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) as well as the expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) and volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\sum\\limits_{i=1}^N\\omega_i = 1.\\] The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\), where \\(\\iota\\) is a vector of ones and \\(\\Sigma^{-1}\\) is the inverse of \\(\\Sigma\\).\n\nN = returns_matrix.shape[1]\niota = np.ones(N)\nsigma_inv = np.linalg.inv(sigma) \n\nmvp_weights = sigma_inv @ iota\nmvp_weights /= mvp_weights.sum()\nmvp_return = (mu.T @ mvp_weights)\nmvp_volatility = np.sqrt(mvp_weights.T @ sigma @ mvp_weights)\nmvp_moments = pd.DataFrame([mvp_return, mvp_volatility],\n                          index=[\"average_ret\", \"volatility\"]).T\n\nThe command np.linalg.inv() returns the solution of a system of equations \\(Ax = b\\). If b is not provided, as in the example above, it defaults to the identity matrix such that np.linalg.inv(sigma) delivers \\(\\Sigma^{-1}\\) (if a unique solution exists).\nNote that the monthly volatility of the minimum variance portfolio is of the same order of magnitude as the daily standard deviation of the individual components. Thus, the diversification benefits in terms of risk reduction are tremendous!\nNext, we set out to find the weights for a portfolio that achieves, as an example, three times the expected return of the minimum variance portfolio. However, mean-variance investors are not interested in any portfolio that achieves the required return but rather in the efficient portfolio, i.e., the portfolio with the lowest standard deviation. If you wonder where the solution \\(\\omega_\\text{eff}\\) comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance given a minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, their objective function is to choose \\(\\omega_\\text{eff}\\) as the solution to \\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\]\nThe code below implements the analytic solution to this optimization problem for a benchmark return \\(\\bar\\mu\\), which we set to 3 times the expected return of the minimum variance portfolio. We encourage you to verify that it is correct.\n\nbenchmark_multiple = 3\nmu_bar = benchmark_multiple * mvp_return\nC = iota.T @ sigma_inv @ iota\nD = iota.T @ sigma_inv @ mu\nE = mu.T @ sigma_inv @ mu\nlambda_tilde = 2 * (mu_bar - D / C) / (E - D ** 2 / C)\n\nefp_weights = (mvp_weights + lambda_tilde / 2 *\n               (sigma_inv @ mu - D * mvp_weights))"
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#the-efficient-frontier",
    "href": "python/introduction-to-tidy-finance.html#the-efficient-frontier",
    "title": "Introduction to Tidy Finance",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. The code below implements the construction of the efficient frontier, which characterizes the highest expected return achievable at each level of risk. To understand the code better, make sure to familiarize yourself with the inner workings of the for loop.\n\na = np.arange(-0.4, 2.0, 0.01)\nres = pd.DataFrame(columns=[\"mu\", \"sd\"], index=a).astype(float)\n\nfor i in a:\n    w = (1 - i) * mvp_weights + i * efp_weights\n    res.loc[i, \"mu\"] = (w.T @ mu)\n    res.loc[i, \"sd\"] = np.sqrt(w.T @ sigma @ w)\n\nThe code above proceeds in two steps: First, we compute a vector of combination weights \\(a\\) and then we evaluate the resulting linear combination with \\(a\\in\\mathbb{python}\\):\n\\[\\omega^* = a\\omega_\\text{eff}(\\bar\\mu) + (1-a)\\omega_\\text{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] with \\(\\lambda^* = 2\\frac{a\\bar\\mu + (1-a)\\tilde\\mu - D/C}{E-D^2/C}\\) where \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). Finally, it is simple to visualize the efficient frontier alongside the two efficient portfolios within one powerful figure using ggplot2 (see Figure 6). We also add the individual stocks in the same call. We compute annualized returns based on the simple assumption that monthly returns are independent and identically distributed. Thus, the average annualized return is just 12 times the expected monthly return.\n\nmvp_return = (mu.T @ mvp_weights)\nmvp_volatility = np.sqrt(mvp_weights.T @ sigma @ mvp_weights)\nefp_return = mu_bar\nefp_volatility = np.sqrt(efp_weights.T @ sigma @ efp_weights)\n\nres_figure = (ggplot(res, aes(x=\"sd\", \n                              y=\"mu\"))\n + geom_point()\n + geom_point(pd.DataFrame({\"mu\": [mvp_return, efp_return],\n                            \"sd\": [mvp_volatility, efp_volatility]}),\n              size=4)\n + geom_point(pd.DataFrame({\"mu\": mu, \n                            \"sd\": np.sqrt(np.diag(sigma))}))\n + labs(x=\"Annualized standard deviation\",\n        y=\"Annualized expected return\",\n        title=\"Efficient frontier for DOW index constituents\")\n + scale_x_continuous(labels=percent_format())\n + scale_y_continuous(labels=percent_format())\n)\nres_figure.draw()\n\n\n\n\nFigure 6: The big dots indicate the location of the minimum variance and the efficient portfolio which delivers 3 times the expected return of the minimum variance portfolio, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\nThe line in Figure 6 indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters \\(\\Sigma\\) and \\(\\mu\\) as given)."
  },
  {
    "objectID": "python/introduction-to-tidy-finance.html#exercises",
    "href": "python/introduction-to-tidy-finance.html#exercises",
    "title": "Introduction to Tidy Finance",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market symbols of your choice from Yahoo!Finance with yf.download() from the yfinance package. Plot two time series of the symbol’s un-adjusted and adjusted closing prices. Explain the differences.\nCompute daily net returns for the asset and visualize the distribution of daily returns in a histogram. Also, use geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns within the histogram. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns\nTake your code from before and generalize it such that you can perform all the computations for an arbitrary vector of symbols (e.g., symbol &lt;- [\"AAPL\", \"MMM\", \"BA\"]). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nConsider the research question: Are days with high aggregate trading volume often also days with large absolute price changes? Find an appropriate visualization to analyze the question.\nCompute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.\nIn the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?\nThe efficient frontier characterizes the portfolios with the highest expected return for different levels of risk, i.e., standard deviation. Identify the portfolio with the highest expected return per standard deviation. Hint: the ratio of expected return to standard deviation is an important concept in Finance."
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html",
    "href": "python/size-sorts-and-p-hacking.html",
    "title": "Size Sorts and p-Hacking",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums Walter, Weber, and Weiss (2022). Exploiting these ideas to generate favorable results is called p-hacking. There is arguably a thin line between p-hacking and conducting robustness tests. Our purpose here is to illustrate the substantial variation that can arise along the evidence-generating process.\nThe chapter relies on the following set of packages:\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\n\nimport itertools\nfrom multiprocessing import Pool, cpu_count"
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#data-preparation",
    "href": "python/size-sorts-and-p-hacking.html#data-preparation",
    "title": "Size Sorts and p-Hacking",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we retrieve the relevant data from our SQLite-database introduced in Chapters 2-4. Firm size is defined as market equity in most asset pricing applications that we retrieve from CRSP. We further use the Fama-French factor returns for performance evaluation.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM crsp_monthly\", \n    con=tidy_finance, \n    parse_dates={\"month\": {\"unit\":\"D\", \"origin\":\"unix\"}}\n  )\n  .dropna()\n)\n\nfactors_ff_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff_monthly\", \n    con=tidy_finance, \n    parse_dates={\"month\": {\"unit\":\"D\", \"origin\":\"unix\"}}\n  )\n  .dropna()\n)"
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#size-distribution",
    "href": "python/size-sorts-and-p-hacking.html#size-distribution",
    "title": "Size Sorts and p-Hacking",
    "section": "Size Distribution",
    "text": "Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 8.1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the CRSP universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\nmarket_cap_concentration = (crsp_monthly\n  .groupby(\"month\", group_keys = False)\n  .apply(lambda x: x.assign(top01=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.99)),\n                            top05=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.95)),\n                            top10=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.90)),\n                            top25=(x[\"mktcap\"] &gt;= np.quantile(x[\"mktcap\"], 0.75))))\n  .reset_index(drop=True)\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series({\"Largest 1% of stocks\": (x[\"mktcap\"][x[\"top01\"]].sum()) / x[\"mktcap\"].sum(),\n                              \"Largest 5% of stocks\": (x[\"mktcap\"][x[\"top05\"]].sum()) / x[\"mktcap\"].sum(),\n                              \"Largest 10% of stocks\": (x[\"mktcap\"][x[\"top10\"]].sum()) / x[\"mktcap\"].sum(),\n                              \"Largest 25% of stocks\": (x[\"mktcap\"][x[\"top25\"]].sum()) / x[\"mktcap\"].sum() }))\n  .reset_index()\n  .melt(id_vars=\"month\", \n        var_name=\"name\", \n        value_name=\"value\")\n)\n\nplot_market_cap_concentration = (ggplot(market_cap_concentration, \n    aes(x=\"month\", y=\"value\", color=\"name\", linetype=\"name\"))\n  + geom_line()\n  + scale_y_continuous(labels=percent_format())\n  + scale_x_date(name=\"\", date_labels=\"%Y\")\n  + labs(x=\"\", y=\"\", color=\"\", linetype=\"\", \n         title=\"Percentage of total market capitalization in largest stocks\")\n  + theme(subplots_adjust={\"bottom\": 0.12}, legend_position=(.5, 0), \n          legend_direction=\"horizontal\", legend_title=element_blank())\n)\nplot_market_cap_concentration.draw()\n\nC:\\Users\\christoph.scheuch\\.conda\\envs\\tidy-finance\\lib\\site-packages\\plotnine\\themes\\themeable.py:1902: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n\n\n\n\n\nFigure 1: We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\nNext, firm sizes also differ across listing exchanges. Stocks’ primary listings were important in the past and are potentially still relevant today. Figure 2 shows that the New York Stock Exchange (NYSE) was and still is the largest listing exchange in terms of market capitalization. More recently, NASDAQ has gained relevance as a listing exchange. Do you know what the small peak in NASDAQ’s market cap around the year 2000 was?\n\nmarket_cap_share = (crsp_monthly\n  .groupby([\"month\", \"exchange\"])\n  .apply(lambda x: pd.DataFrame({\"mktcap\": x[\"mktcap\"].sum()}, index=[0]))\n  .reset_index(drop=False)\n  .assign(\n    total_market_cap=lambda x: x.groupby(\"month\")[\"mktcap\"].transform(\"sum\"),\n    share=lambda x: x[\"mktcap\"] / x[\"total_market_cap\"]\n  )\n)\n\nplot_market_cap_share = (ggplot(\n  market_cap_share, aes(x=\"month\", y=\"share\", fill=\"exchange\", color=\"exchange\")) \n  + geom_area(position=\"stack\", stat=\"identity\", alpha=0.5) \n  + geom_line(position=\"stack\") \n  + scale_y_continuous(labels=percent_format())\n  + scale_x_date(name=\"\", date_labels=\"%Y\")\n  + labs(x=\"\", y=\"\", fill=\"\", color=\"\",\n         title=\"Share of total market capitalization per listing exchange\")\n  + theme(subplots_adjust={\"bottom\": 0.12}, legend_position=(.5, 0), \n          legend_direction=\"horizontal\", legend_title=element_blank())\n)\nplot_market_cap_share.draw()\n\nC:\\Users\\christoph.scheuch\\.conda\\envs\\tidy-finance\\lib\\site-packages\\plotnine\\themes\\themeable.py:1902: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n\n\n\n\n\nFigure 2: Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\n\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function describe() does not include all statistics we are interested in, which is why we create the function create_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each listing exchange. We also add a row with add_row() with the overall summary statistics.\nThe resulting table shows that firms listed on NYSE in December 2021 are significantly larger on average than firms listed on the other exchanges. Moreover, NASDAQ lists the largest number of firms. This discrepancy between firm sizes across listing exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. In the following, we use this distinction to update our portfolio sort procedure.\n\ndef get_summary_statistics(data, variable, filter_variable, percentiles):\n   summary_df = (data\n                 .get([filter_variable, variable])\n                 .groupby(filter_variable)\n                 .describe(percentiles=percentiles)\n                 ) \n   summary_df.columns = summary_df.columns.droplevel(0)\n   summary_overall_df = (data\n                         .get(variable)\n                         .describe(percentiles=percentiles)\n                         ) \n   summary_df.loc[\"Overall\",:] = summary_overall_df\n   return summary_df\n\nget_summary_statistics(\n  crsp_monthly[crsp_monthly[\"month\"] == crsp_monthly[\"month\"].max()],\n   \"mktcap\",\n   \"exchange\",\n   [0.05, 0.5, 0.95]\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n5%\n50%\n95%\nmax\n\n\nexchange\n\n\n\n\n\n\n\n\n\n\n\n\nAMEX\n144.0\n417.493685\n2188.003634\n7.565570\n12.468110\n76.662897\n1220.035556\n2.571889e+04\n\n\nNASDAQ\n2751.0\n8390.963149\n88820.083400\n7.005530\n29.298895\n431.973496\n18783.135368\n2.902368e+06\n\n\nNYSE\n1369.0\n17939.275120\n48521.280506\n23.913000\n203.815939\n3456.685274\n80805.912673\n4.729411e+05\n\n\nOther\n1.0\n13906.246549\nNaN\n13906.246549\n13906.246549\n13906.246549\n13906.246549\n1.390625e+04\n\n\nOverall\n4265.0\n11187.909167\n76596.194034\n7.005530\n34.353498\n802.433816\n40692.376048\n2.902368e+06"
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "python/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "Size Sorts and p-Hacking",
    "section": "Univariate Size Portfolios with Flexible Breakpoints",
    "text": "Univariate Size Portfolios with Flexible Breakpoints\nIn Chapter 7, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges. In published asset pricing articles, many scholars compute sorting breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks.\nTo replicate the NYSE-centered sorting procedure, we introduce exchanges as an argument in our assign_portfolio() function from Univariate Portfolio Sorts. The exchange-specific argument then enters in the filter data[\"exchanges\"].isin(exchanges). For example, if exchanges = 'NYSE' is specified, only stocks listed on NYSE are used to compute the breakpoints. Alternatively, you could specify exchanges = [\"NYSE\", \"NASDAQ\", \"AMEX\"], which keeps all stocks listed on either of these exchanges.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    data_filtered = data[data[\"exchange\"].isin(exchanges)]\n    breakpoints = np.quantile(data_filtered[sorting_variable].dropna(), \n                              np.linspace(0, 1, n_portfolios + 1), \n                              method = \"linear\")\n    assigned_portfolios = pd.cut(data_filtered[sorting_variable],\n                                 bins=breakpoints,\n                                 labels=range(1, breakpoints.size),\n                                 include_lowest=True)\n    return assigned_portfolios"
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "href": "python/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "title": "Size Sorts and p-Hacking",
    "section": "Weighting Schemes for Portfolios",
    "text": "Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. The statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns if value_weighted = True. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Chapter 7.\n\ndef compute_portfolio_returns(n_portfolios=10, \n                              exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"],\n                              value_weighted=True, \n                              data=crsp_monthly):\n    \n    def calculate_returns(data, value_weighted):\n        if value_weighted:\n            return np.average(data[\"ret_excess\"], weights=data[\"mktcap_lag\"])\n        else:\n            return data[\"ret_excess\"].mean()\n    \n    returns = (data\n      .groupby(\"month\")\n      .apply(lambda x: x.assign(portfolio = assign_portfolio(x, exchanges, \"mktcap_lag\", n_portfolios)))\n      .reset_index(drop=True)\n      .groupby([\"portfolio\", \"month\"])\n      .apply(lambda x: x.assign(ret = calculate_returns(x, value_weighted)))\n      .reset_index(drop=True)\n      .groupby(\"month\")\n      .apply(lambda x: x.assign(size_premium = x.loc[x[\"portfolio\"] == x[\"portfolio\"].min(), \"ret\"].mean() - x.loc[x[\"portfolio\"] == x[\"portfolio\"].max(), \"ret\"].mean()))\n      .reset_index(drop=True)\n      .aggregate({\"size_premium\": \"mean\"})\n    )\n    return returns\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from NYSE-listed stocks.\n\nret_all = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"NYSE\", \"NASDAQ\", \"AMEX\"],\n  value_weighted=True,\n  data=crsp_monthly\n)\n\nret_nyse = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"NYSE\"],\n  value_weighted=True,\n  data=crsp_monthly\n)\n\ndata = pd.DataFrame([ret_all * 100, ret_nyse * 100], \n                    index =[\"NYSE, NASDAQ & AMEX\", \"NYSE\"])\ndata.columns = [\"Premium\"]\ndata\n\n\n\n\n\n\n\n\nPremium\n\n\n\n\nNYSE, NASDAQ & AMEX\n0.030927\n\n\nNYSE\n0.330102\n\n\n\n\n\n\n\nThe table shows that the size premium is more than 60 percent larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50 percent of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible."
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "href": "python/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "title": "Size Sorts and p-Hacking",
    "section": "P-Hacking and Non-standard Errors",
    "text": "P-Hacking and Non-standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong, the aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al. 2021). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large scale study, Menkveld et al. (2021) find that the magnitude of non-standard errors are similar than the estimation uncertainty based on a chosen model which shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. \nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference due to multiple testing invalid (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow we conduct a series of robustness tests which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function expand_grid() produces a table of all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\nn_portfolios = [2, 5, 10]\nexchanges = [[\"NYSE\"], [\"NYSE\", \"NASDAQ\", \"AMEX\"]]\nvalue_weighted = [True, False]\ndata = [crsp_monthly,\n        crsp_monthly[crsp_monthly[\"industry\"] != \"Finance\"],\n        crsp_monthly[crsp_monthly[\"month\"] &lt; \"1990-06-01\"],\n        crsp_monthly[crsp_monthly[\"month\"] &gt;= \"1990-06-01\"],\n        ]\np_hacking_setup = list(itertools.product(n_portfolios, exchanges, value_weighted, data))\n\nTo speed the computation up we parallelize the (many) different sorting procedures, as in the beta estimation of Chapter 6. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\np_hacking_results = pd.DataFrame(p_hacking_setup)\np_hacking_results.columns = [\"n_portfolios\", \"exchanges\", \"value_weighted\", \"data\"]\nall_size_premia = pd.DataFrame()\nfor i in p_hacking_setup:\n    tmp_ret = compute_portfolio_returns(i[0], i[1], i[2], i[3])\n    all_size_premia = pd.concat([all_size_premia, tmp_ret])\n\np_hacking_results[\"size_premium\"] = all_size_premia.values\np_hacking_results\n\n\n\n\n\n\n\n\nn_portfolios\nexchanges\nvalue_weighted\ndata\nsize_premium\n\n\n\n\n0\n2\n[NYSE]\nTrue\npermno date month ret ...\n0.003301\n\n\n1\n2\n[NYSE]\nTrue\npermno date month ret ...\n0.003478\n\n\n2\n2\n[NYSE]\nTrue\npermno date month ret ...\n0.003703\n\n\n3\n2\n[NYSE]\nTrue\npermno date month ret ...\n0.002979\n\n\n4\n2\n[NYSE]\nFalse\npermno date month ret ...\n0.002466\n\n\n5\n2\n[NYSE]\nFalse\npermno date month ret ...\n0.002789\n\n\n6\n2\n[NYSE]\nFalse\npermno date month ret ...\n0.002716\n\n\n7\n2\n[NYSE]\nFalse\npermno date month ret ...\n0.002266\n\n\n8\n2\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.000309\n\n\n9\n2\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.000177\n\n\n10\n2\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.000098\n\n\n11\n2\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.000444\n\n\n12\n2\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.002787\n\n\n13\n2\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.003206\n\n\n14\n2\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.002015\n\n\n15\n2\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.003280\n\n\n16\n5\n[NYSE]\nTrue\npermno date month ret ...\n0.003995\n\n\n17\n5\n[NYSE]\nTrue\npermno date month ret ...\n0.004559\n\n\n18\n5\n[NYSE]\nTrue\npermno date month ret ...\n0.004900\n\n\n19\n5\n[NYSE]\nTrue\npermno date month ret ...\n0.003271\n\n\n20\n5\n[NYSE]\nFalse\npermno date month ret ...\n0.004141\n\n\n21\n5\n[NYSE]\nFalse\npermno date month ret ...\n0.004849\n\n\n22\n5\n[NYSE]\nFalse\npermno date month ret ...\n0.005077\n\n\n23\n5\n[NYSE]\nFalse\npermno date month ret ...\n0.003392\n\n\n24\n5\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.002971\n\n\n25\n5\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.003616\n\n\n26\n5\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.003443\n\n\n27\n5\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.002669\n\n\n28\n5\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.008714\n\n\n29\n5\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.009993\n\n\n30\n5\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.007239\n\n\n31\n5\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.009655\n\n\n32\n10\n[NYSE]\nTrue\npermno date month ret ...\n0.004637\n\n\n33\n10\n[NYSE]\nTrue\npermno date month ret ...\n0.005299\n\n\n34\n10\n[NYSE]\nTrue\npermno date month ret ...\n0.006095\n\n\n35\n10\n[NYSE]\nTrue\npermno date month ret ...\n0.003470\n\n\n36\n10\n[NYSE]\nFalse\npermno date month ret ...\n0.005819\n\n\n37\n10\n[NYSE]\nFalse\npermno date month ret ...\n0.006692\n\n\n38\n10\n[NYSE]\nFalse\npermno date month ret ...\n0.006765\n\n\n39\n10\n[NYSE]\nFalse\npermno date month ret ...\n0.005062\n\n\n40\n10\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.010768\n\n\n41\n10\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.012280\n\n\n42\n10\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.011432\n\n\n43\n10\n[NYSE, NASDAQ, AMEX]\nTrue\npermno date month ret ...\n0.010345\n\n\n44\n10\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.018257\n\n\n45\n10\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.020259\n\n\n46\n10\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.015492\n\n\n47\n10\n[NYSE, NASDAQ, AMEX]\nFalse\npermno date month ret ...\n0.020021"
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#the-size-premium-variation",
    "href": "python/size-sorts-and-p-hacking.html#the-size-premium-variation",
    "title": "Size Sorts and p-Hacking",
    "section": "The Size-Premium Variation",
    "text": "The Size-Premium Variation\nWe provide a graph in Figure 3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature which we include as a dotted vertical line.\n\np_hacking_results_figure = (ggplot(\n  p_hacking_results, aes(x=\"size_premium\")) \n  + geom_histogram(bins=len(p_hacking_results))\n  + scale_x_continuous(labels=percent_format())\n  + labs(x=None, y=None, fill=None, color=None, \n         title=\"Distribution of size premiums for different sorting choices\")\n  + geom_vline(aes(xintercept=factors_ff_monthly[\"smb\"].mean()), linetype=\"dashed\")\n)\np_hacking_results_figure.draw()\n\n\n\n\nFigure 3: The dashed vertical line indicates the average Fama-French SMB premium."
  },
  {
    "objectID": "python/size-sorts-and-p-hacking.html#exercises",
    "href": "python/size-sorts-and-p-hacking.html#exercises",
    "title": "Size Sorts and p-Hacking",
    "section": "Exercises",
    "text": "Exercises\n\nWe gained several insights on the size distribution above. However, we did not analyze the average size across listing exchanges and industries. Which listing exchanges/industries have the largest firms? Plot the average firm size for the three listing exchanges over time. What do you conclude?\nWe compute breakpoints but do not take a look at them in the exposition above. This might cover potential data errors. Plot the breakpoints for ten size portfolios over time. Then, take the difference between the two extreme portfolios and plot it. Describe your results.\nThe returns that we analyse above do not account for differences in the exposure to market risk, i.e., the CAPM beta. Change the function compute_portfolio_returns() to output the CAPM alpha or beta instead of the average excess return.\nWhile you saw the spread in returns from the p-hacking exercise, we did not show which choices led to the largest effects. Find a way to investigate which choice variable has the largest impact on the estimated size premium.\nWe computed several size premiums, but they do not follow the definition of Fama and French (1993). Which of our approaches comes closest to their SMB premium?"
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html",
    "href": "python/value-and-bivariate-sorts.html",
    "title": "Value and Bivariate Sorts",
    "section": "",
    "text": "Note\n\n\n\nYou are reading the work-in-progress edition of Tidy Finance with Python. Code chunks and text might change over the next couple of months. We are always looking for feedback via contact@tidy-finance.org. Meanwhile, you can find the complete R version here.\nIn this chapter, we extend univariate portfolio analysis to bivariate sorts, which means we assign stocks to portfolios based on two characteristics. Bivariate sorts are regularly used in the academic asset pricing literature and are the basis for the Fama and French three factors. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. To calculate book-to-market ratios, accounting data is required, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nThe current chapter relies on this set of packages.\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport sqlite3"
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#data-preparation",
    "href": "python/value-and-bivariate-sorts.html#data-preparation",
    "title": "Value and Bivariate Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we load the necessary data from our SQLite-database introduced in Chapters 2-4. We conduct portfolio sorts based on the CRSP sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Chapter 8.\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n\ncrsp_monthly = (pd.read_sql_query(\n    sql=\"SELECT permno, gvkey, month, ret_excess, mktcap, mktcap_lag, exchange FROM crsp_monthly\",\n    con=tidy_finance,\n    dtype={\"permno\": np.int32},\n    parse_dates={\"month\": {\"unit\": \"D\",\"origin\": \"unix\"}})\n  .dropna()\n)\n\nFurther, we utilize accounting data. The most common source of accounting data is Compustat. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date.\n\nbook_equity = (pd.read_sql_query(\n    sql=\"SELECT gvkey, datadate, be from compustat\",\n    con=tidy_finance, \n    parse_dates={\"datadate\": {\"unit\":\"D\", \"origin\":\"unix\"}})\n  .dropna()\n  .assign(month = lambda x: (pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()))\n)"
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#book-to-market-ratio",
    "href": "python/value-and-bivariate-sorts.html#book-to-market-ratio",
    "title": "Value and Bivariate Sorts",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias - we must not include data in forming a portfolio that is not public knowledge at the time. Of course, researchers have more information when looking into the past than agents had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nWe continue to lag market capitalization and firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.1\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fillna()-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by .groupby() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nme = (crsp_monthly\n  .assign(sorting_date = lambda x: x[\"month\"] + pd.DateOffset(months=1))\n  .rename(columns={\"mktcap\": \"me\"})\n  .get([\"permno\", \"sorting_date\", \"me\"])\n)\n\nbm = (book_equity\n  .merge(crsp_monthly, how=\"inner\", on=[\"gvkey\", \"month\"])\n  .assign(bm = lambda x: x[\"be\"] / x[\"mktcap\"],\n          sorting_date = lambda x: x[\"month\"] + pd.DateOffset(months=6))\n  .assign(comp_date = lambda x: x[\"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"sorting_date\", \"comp_date\", \"bm\"])\n)\n\ndata_for_sorts = (crsp_monthly\n  .merge(bm, \n         how=\"left\", \n         left_on=[\"permno\", \"gvkey\", \"month\"], \n         right_on=[\"permno\", \"gvkey\", \"sorting_date\"])\n  .merge(me, \n         how=\"left\", \n         left_on=[\"permno\", \"month\"], \n         right_on=[\"permno\", \"sorting_date\"])\n  .get([\"permno\", \"gvkey\", \"month\", \"ret_excess\", \n        \"mktcap_lag\", \"me\", \"bm\", \"exchange\", \"comp_date\"])\n)\n\n# TODO: only fill bm and comp_date, but keep all columns\ndata_for_sorts = (data_for_sorts\n  .sort_values(by=[\"permno\", \"gvkey\", \"month\"])\n  .groupby([\"permno\", \"gvkey\"])\n  .apply(lambda x: x.fillna(method=\"ffill\"))\n  .reset_index(drop = True)\n  .assign(threshold_date = lambda x: x[\"month\"] - pd.DateOffset(months=12))\n  .query(\"comp_date &gt; threshold_date\")\n  .drop(columns=[\"comp_date\", \"threshold_date\"])\n  .dropna()\n)\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function allowing for the specification of exchanges to use for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios, exchanges):\n    breakpoints = (data\n      .query(\"exchange in @exchanges\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    assigned_portfolios = pd.cut(data[sorting_variable],\n                                 bins=breakpoints,\n                                 labels=pd.Series(range(1, breakpoints.size)),\n                                 include_lowest=True,\n                                 duplicates=\"drop\")\n    return assigned_portfolios\n\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis."
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#independent-sorts",
    "href": "python/value-and-bivariate-sorts.html#independent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Independent Sorts",
    "text": "Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_me, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"month\", group_keys=False)\n  .apply(lambda x: x.assign(\n         portfolio_bm = assign_portfolio(data=x,\n                                         sorting_variable=\"bm\",\n                                         n_portfolios=5,\n                                         exchanges=[\"NYSE\"]),\n         portfolio_me = assign_portfolio(data=x,\n                                         sorting_variable=\"me\",\n                                         n_portfolios=5,\n                                         exchanges=[\"NYSE\"])))\n  .groupby([\"month\", \"portfolio_bm\", \"portfolio_me\"])\n  .apply(lambda x: pd.Series(\n         {\"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])}))\n  .reset_index()\n)\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium = (value_portfolios\n  .groupby([\"month\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series(\n         {\"value_premium\": (x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() - x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean())}))\n  .reset_index()\n  .aggregate({\"value_premium\": \"mean\"})\n)\nprint(value_premium * 100)\n\nvalue_premium    0.381508\ndtype: float64\n\n\nThe resulting annualized value premium is 4.608 percent."
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#dependent-sorts",
    "href": "python/value-and-bivariate-sorts.html#dependent-sorts",
    "title": "Value and Bivariate Sorts",
    "section": "Dependent Sorts",
    "text": "Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts ensure an equal amount of stocks within each portfolio.\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable = \"me\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"month\", group_keys=False)\n  .apply(lambda x: x.assign(\n         portfolio_me = assign_portfolio(data=x,\n                                         sorting_variable=\"me\",\n                                         n_portfolios=5,\n                                         exchanges=[\"NYSE\"])))\n  .groupby([\"month\", \"portfolio_me\"], group_keys=False)\n  .apply(lambda x: x.assign(\n         portfolio_bm = assign_portfolio(data=x,\n                                         sorting_variable=\"bm\",\n                                         n_portfolios=5,\n                                         exchanges=[\"NYSE\"])))\n  .groupby([\"month\", \"portfolio_bm\", \"portfolio_me\"])\n  .apply(lambda x: pd.Series(\n         {\"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])}))\n  .reset_index()\n)\n\nvalue_premium = (value_portfolios\n  .groupby([\"month\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"month\")\n  .apply(lambda x: pd.Series(\n         {\"value_premium\": x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() - x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()}))\n  .reset_index()\n  .aggregate({\"value_premium\": \"mean\"})\n)\n\nprint(value_premium * 100)\n\nvalue_premium    0.315498\ndtype: float64\n\n\nThe value premium from dependent sorts is 3.948 percent per year.\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Chapter 8, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices."
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#exercises",
    "href": "python/value-and-bivariate-sorts.html#exercises",
    "title": "Value and Bivariate Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nIn Chapter 8, we examine the distribution of market equity. Repeat this analysis for book equity and the book-to-market ratio (alongside a plot of the breakpoints, i.e., deciles).\nWhen we investigate the portfolios, we focus on the returns exclusively. However, it is also of interest to understand the characteristics of the portfolios. Write a function to compute the average characteristics for size and book-to-market across the 25 independently and dependently sorted portfolios.\nAs for the size premium, also the value premium constructed here does not follow Fama and French (1993). Implement a p-hacking setup as in Chapter 8 to find a premium that comes closest to their HML premium."
  },
  {
    "objectID": "python/value-and-bivariate-sorts.html#footnotes",
    "href": "python/value-and-bivariate-sorts.html#footnotes",
    "title": "Value and Bivariate Sorts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe definition of a time lag is another choice a researcher has to make, similar to breakpoint choices as we describe in the Chapter 8 on p-hacking.↩︎"
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html",
    "href": "r/accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "In this chapter, we suggest a way to organize your financial data. Everybody, who has experience with data, is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nlibrary(tidyverse)\nlibrary(scales)\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2021-12-31\")"
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#fama-french-data",
    "href": "r/accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, there is a neat package by Nelson Areal that allows us to access the data easily: the frenchdata package provides functions to download and read data sets from Prof. Kenneth French finance data library (Areal 2021). \n\nlibrary(frenchdata)\n\nWe can use the main function of the package to download monthly Fama-French factors. The set 3 Factors includes the return time series of the market, size, and value factors alongside the risk-free rates. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the site, check the raw data files to appreciate the time you can save thanks to frenchdata.\n\nfactors_ff_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff_monthly &lt;- factors_ff_monthly_raw$subsets$data[[1]] |&gt;\n  transmute(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff_daily_raw &lt;- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff_daily &lt;- factors_ff_daily_raw$subsets$data[[1]] |&gt;\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date)\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw &lt;- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly &lt;- industries_ff_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |&gt;\n  mutate(across(where(is.numeric), ~ . / 100)) |&gt;\n  select(month, everything(), -date) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling get_french_data_list(). For an alternative to download Fama-French data, check out the FFdownload package by Sebastian Stöckl."
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#q-factors",
    "href": "r/accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link &lt;-\n  \"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2021.csv\"\n\nfactors_q_monthly &lt;- read_csv(factors_q_monthly_link) |&gt;\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |&gt;\n  select(-R_F, -R_MKT, -year) |&gt;\n  rename_with(~ str_remove(., \"R_\")) |&gt;\n  rename_with(~ str_to_lower(.)) |&gt;\n  mutate(across(-month, ~ . / 100)) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date)"
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "r/accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2021 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google drive location, we need additional packages to access the data directly from our R session. Therefore, we load readxl to read the XLSX-file (Wickham and Bryan 2022) and googledrive for the Google drive connection (D’Agostino McGowan and Bryan 2021).\n\nlibrary(readxl)\nlibrary(googledrive)\n\nUsually, you need to authenticate if you interact with Google drive directly in R. Since the data is stored via a public link, we can proceed without any authentication.\n\ndrive_deauth()\n\nThe drive_download() function from the googledrive package allows us to download the data and store it locally.\n\nmacro_predictors_link &lt;-\n  \"https://docs.google.com/spreadsheets/d/1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2\"\n\ndrive_download(\n  macro_predictors_link,\n  path = \"macro_predictors.xlsx\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997)\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors &lt;- read_xlsx(\n  \"macro_predictors.xlsx\",\n  sheet = \"Monthly\"\n) |&gt;\n  mutate(month = ym(yyyymm)) |&gt;\n  mutate(across(where(is.character), as.numeric)) |&gt;\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |&gt;\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |&gt;\n  filter(month &gt;= start_date & month &lt;= end_date) |&gt;\n  drop_na()\n\nFinally, after reading in the macro predictors to our memory, we remove the raw data file from our temporary storage.\n\nfile.remove(\"macro_predictors.xlsx\")\n\n[1] TRUE"
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "r/accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar tidyquant package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\nlibrary(tidyquant)\n\ncpi_monthly &lt;- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date,\n  to = end_date\n) |&gt;\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)]\n  )\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. The tidyquant package provides access to around 10,000 time series of the FRED database. If your desired time series is not included, we recommend working with the fredr package (Boysel and Vaughan 2021). Note that you need to get an API key to use its functionality. We refer to the package documentation for details."
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "r/accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the dplyr functions. We refer to this tutorial for more information on SQL.\nThere are two packages that make working with SQLite in R very simple: RSQLite (Müller et al. 2022) embeds the SQLite database engine in R, and dbplyr (Wickham, Girlich, and Ruiz 2022) is the database back-end for dplyr. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting dplyr into SQL. Check out the RSQLite and dbplyr vignettes for more information.\n\nlibrary(RSQLite)\nlibrary(dbplyr)\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Note that we use the extended_types=TRUE option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance.sqlite in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function dbWriteTable(), which copies the data to our SQLite-database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_monthly\",\n    value = factors_ff_monthly,\n    overwrite = TRUE\n  )\n\nWe can use the remote table as an in-memory data frame by building a connection via tbl().\n\nfactors_ff_monthly_db &lt;- tbl(tidy_finance, \"factors_ff_monthly\")\n\nAll dplyr calls are evaluated lazily, i.e., the data is not in our R session’s memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.\n\nfactors_ff_monthly_db |&gt;\n  select(month, rf)\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.41.2 [data/tidy_finance.sqlite]\n  month          rf\n  &lt;date&gt;      &lt;dbl&gt;\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# ℹ more rows\n\n\nIf we want to have the whole table in memory, we need to collect() it. You will see that we regularly load the data into the memory in the next chapters.\n\nfactors_ff_monthly_db |&gt;\n  select(month, rf) |&gt;\n  collect()\n\n# A tibble: 744 × 2\n  month          rf\n  &lt;date&gt;      &lt;dbl&gt;\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# ℹ 739 more rows\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_daily\",\n    value = factors_ff_daily,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"industries_ff_monthly\",\n    value = industries_ff_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"factors_q_monthly\",\n    value = factors_q_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"macro_predictors\",\n    value = macro_predictors,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"cpi_monthly\",\n    value = cpi_monthly,\n    overwrite = TRUE\n  )\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly &lt;- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly &lt;- factors_q_monthly |&gt; collect()"
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "r/accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the dbSendQuery() function.\n\nres &lt;- dbSendQuery(tidy_finance, \"VACUUM\")\nres\n\n&lt;SQLiteResult&gt;\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\n\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read up in this tutorial. \nWe store the result of the above query in res because the database keeps the result set open. To close open results and avoid warnings going forward, we can use dbClearResult().\n\ndbClearResult(res)\n\nApart from cleaning up, you might be interested in listing all the tables that are currently in your database. You can do this via the dbListTables() function.\n\ndbListTables(tidy_finance)\n\n [1] \"beta\"                  \"compustat\"            \n [3] \"cpi_monthly\"           \"crsp_daily\"           \n [5] \"crsp_monthly\"          \"factors_ff_daily\"     \n [7] \"factors_ff_monthly\"    \"factors_q_monthly\"    \n [9] \"industries_ff_monthly\" \"macro_predictors\"     \n[11] \"mergent\"               \"trace_enhanced\"       \n\n\nThis function comes in handy if you are unsure about the correct naming of the tables in your database."
  },
  {
    "objectID": "r/accessing-and-managing-financial-data.html#exercises",
    "href": "r/accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via read_csv(). Validate that you get the same data as via the frenchdata package.\nDownload the Fama-French 5 factors using the frenchdata package. Use get_french_data_list() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the resulting rf, mkt_excess, smb, and hml columns to factors_ff_monthly. Explain any differences you might find."
  },
  {
    "objectID": "r/changelog.html",
    "href": "r/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "June 15, 2023, Commit 47dbb30: We moved the first usage of broom:tidy() from Fama-Macbeth Regressions to Univariate Portfolio Sorts to clean up the CAPM estimation.\nJune 12, 2023, Commit e008622: We fixed some inconsencies in notation of portfolio weights. Now, we refer to portfolio weights with \\(\\omega\\) throughout the complete book.\nJune 12, 2023, Commit 186ec7b2: We fixed a typo in the discussion of the elastic net in Chapter Factor Selection via Machine Learning.\nMay 23, 2023, Commit d5e355c: We update the workflow to collect() tables from tidy_finance.sqlite: To make variable selection more obvious, we now explicitly select() columns before collecting. As part of the pull request Commit 91d3077, we now select excess returns instead of net returns in the Chapter Fama-MacBeth Regressions.\nMay 20, 2023, Commit be0f0b4: We include NA-observations in the Mergent filters in Chapter TRACE and FISD.\nMay 17, 2023, Commit 2209bb1: We changed the assign_portfolio()-functions in Chapters Univariate Portfolio Sorts, Size Sorts and p-Hacking, Value and Bivariate Sorts, and Replicating Fama and French Factors. Additionally, we added a small explanation to potential issues with the function for clustered sorting variables in Chapter Univariate Portfolio Sorts.\nMay 12, 2023, Commit 54b76d7: We removed magic numbers in Chapter Introduction to Tidy Finance and introduced the scales packages already in the introduction chapter to reduce scaling issues in figures.\nMar. 30, 2023, Issue 29: We upgraded to tidyverse 2.0.0 and R 4.2.3 and removed all explicit loads of lubridate.\nFeb. 15, 2023, Commit bfda6af: We corrected an error in the calculation of the annualized average return volatility in the Chapter Introduction to Tidy Finance.\nMar. 06, 2023, Commit 857f0f5: We corrected an error in the label of Figure 6, which wrongly claimed to show the efficient tangency portfolio.\nMar. 09, 2023, Commit fae4ac3: We corrected a typo in the definition of the power utility function in Chapter Portfolio Performance. The utility function implemented in the code is now consistent with the text."
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html",
    "href": "r/constrained-optimization-and-backtesting.html",
    "title": "Constrained Optimization and Backtesting",
    "section": "",
    "text": "In this chapter, we conduct portfolio backtesting in a realistic setting by including transaction costs and investment constraints such as no-short-selling rules. We start with standard mean-variance efficient portfolios and introduce constraints in a step-by-step manner. To do so, we rely on numerical optimization procedures in R. We conclude the chapter by providing an out-of-sample backtesting procedure for the different strategies that we introduce in this chapter.\nThroughout this chapter, we use the following packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(quadprog)\nlibrary(alabama)\nCompared to previous chapters, we introduce the quadprog package (Turlach, Weingessel, and Moler 2019) to perform numerical constrained optimization for quadratic objective functions and alabama (Varadhan 2022) for more general non-linear objective functions and constraints."
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#data-preparation",
    "href": "r/constrained-optimization-and-backtesting.html#data-preparation",
    "title": "Constrained Optimization and Backtesting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by loading the required data from our SQLite-database introduced in Chapters 2-4. For simplicity, we restrict our investment universe to the monthly Fama-French industry portfolio returns in the following application. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nindustry_returns &lt;- tbl(tidy_finance, \"industries_ff_monthly\") |&gt;\n  select(-month) |&gt;\n  collect()\n\nindustry_returns\n\n# A tibble: 744 × 10\n    NoDur   Durbl   Manuf   Enrgy   HiTec   Telcm   Shops    Hlth\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 -0.0335 -0.114  -0.0841 -0.0749 -0.0983  0.0095 -0.057  -0.0635\n2  0.0293 -0.0207  0.0049 -0.0343  0.0539  0.0836  0.0406  0.0027\n3 -0.0141 -0.0526 -0.0355 -0.008   0.0158  0.0014  0.0016  0.0161\n4  0.0154 -0.0209 -0.0353 -0.0426  0.0056 -0.0105 -0.0021  0.0168\n5  0.0652 -0.0051  0.0386 -0.0346  0.0957  0.0332  0.0329  0.138 \n# ℹ 739 more rows\n# ℹ 2 more variables: Utils &lt;dbl&gt;, Other &lt;dbl&gt;"
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "href": "r/constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Recap of Portfolio Choice",
    "text": "Recap of Portfolio Choice\nA common objective for portfolio optimization is to find mean-variance efficient portfolio weights, i.e., the allocation which delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case, where the investor is only concerned about portfolio variance, she may choose to implement the minimum variance portfolio (MVP) weights which are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1\\] where \\(\\Sigma\\) is the \\((N \\times N)\\) covariance matrix of the returns. The optimal weights \\(\\omega_\\text{mvp}\\) can be found analytically and are \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). In terms of code, the math is equivalent to the following chunk. \n\nn_industries &lt;- ncol(industry_returns)\n\nSigma &lt;- cov(industry_returns)\nw_mvp &lt;- solve(Sigma) %*% rep(1, n_industries)\nw_mvp &lt;- as.vector(w_mvp / sum(w_mvp))\n\nmu &lt;- colMeans(industry_returns)\n\nNext, consider an investor who aims to achieve minimum variance given a required expected portfolio return \\(\\bar{\\mu}\\) such that she chooses \\[\\omega_\\text{eff}({\\bar{\\mu}}) =\\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] We leave it as an exercise below to show that the portfolio choice problem can equivalently be formulated for an investor with mean-variance preferences and risk aversion factor \\(\\gamma\\). That means the investor aims to choose portfolio weights as the solution to \\[ \\omega^*_\\gamma = \\arg\\max \\omega' \\mu - \\frac{\\gamma}{2}\\omega'\\Sigma \\omega\\quad \\text{ s.t. } \\omega'\\iota = 1.\\] The solution to the optimal portfolio choice problem is: \\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\] Empirically, this classical solution imposes many problems. In particular, the estimates of \\(\\mu\\) are noisy over short horizons, the (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements and thus, estimation error is huge. Seminal papers on the effect of ignoring estimation uncertainty, among others, are Brown (1976), Jobson and Korkie (1980), Jorion (1986), and Chopra and Ziemba (1993).\nEven worse, if the asset universe contains more assets than available time periods \\((N &gt; T)\\), the sample covariance matrix is no longer positive definite such that the inverse \\(\\Sigma^{-1}\\) does not exist anymore. To address estimation issues for vast-dimensional covariance matrices, regularization techniques are a popular tool (see, e.g., Ledoit and Wolf 2003, 2004, 2012; Fan, Fan, and Lv 2008).\nWhile the uncertainty associated with estimated parameters is challenging, the data-generating process is also unknown to the investor. In other words, model uncertainty reflects that it is ex-ante not even clear which parameters require estimation (for instance, if returns are driven by a factor model, selecting the universe of relevant factors imposes model uncertainty). Wang (2005) and Garlappi, Uppal, and Wang (2007) provide theoretical analysis on optimal portfolio choice under model and estimation uncertainty. In the most extreme case, Pflug, Pichler, and Wozabal (2012) shows that the naive portfolio which allocates equal wealth to all assets is the optimal choice for an investor averse to model uncertainty.\nOn top of the estimation uncertainty, transaction costs are a major concern. Rebalancing portfolios is costly, and, therefore, the optimal choice should depend on the investor’s current holdings. In the presence of transaction costs, the benefits of reallocating wealth may be smaller than the costs associated with turnover. This aspect has been investigated theoretically, among others, for one risky asset by Magill and Constantinides (1976) and Davis and Norman (1990). Subsequent extensions to the case with multiple assets have been proposed by Balduzzi and Lynch (1999) and Balduzzi and Lynch (2000). More recent papers on empirical approaches which explicitly account for transaction costs include Gârleanu and Pedersen (2013), and DeMiguel, Nogales, and Uppal (2014), and DeMiguel, Martín-Utrera, and Nogales (2015)."
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "href": "r/constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "title": "Constrained Optimization and Backtesting",
    "section": "Estimation Uncertainty and Transaction Costs",
    "text": "Estimation Uncertainty and Transaction Costs\nThe empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plug in some sample estimates \\(\\hat \\mu\\) and \\(\\hat \\Sigma\\) can be summarized rather briefly: mean-variance optimization performs poorly! The literature discusses many proposals to overcome these empirical issues. For instance, one may impose some form of regularization of \\(\\Sigma\\), rely on Bayesian priors inspired by theoretical asset pricing models (Kan and Zhou 2007) or use high-frequency data to improve forecasting (Hautsch, Kyj, and Malec 2015). One unifying framework that works easily, effectively (even for large dimensions), and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs (Hautsch and Voigt 2019).\nAssume that returns are from a multivariate normal distribution with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, we assume quadratic transaction costs which penalize rebalancing such that \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\] with cost parameter \\(\\beta&gt;0\\) and \\(\\omega_{t^+} = {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). \\(\\omega_{t^+}\\) denotes the portfolio weights just before rebalancing. Note that \\(\\omega_{t^+}\\) differs mechanically from \\(\\omega_t\\) due to the returns in the past period. Intuitively, transaction costs penalize portfolio performance when the portfolio is shifted from the current holdings \\(\\omega_{t^+}\\) to a new allocation \\(\\omega_{t+1}\\). In this setup, transaction costs do not increase linearly. Instead, larger rebalancing is penalized more heavily than small adjustments. Then, the optimal portfolio choice for an investor with mean variance preferences is \\[\\begin{aligned}\\omega_{t+1} ^* &=  \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned}\\] where \\[\\mu^*=\\mu+\\beta \\omega_{t^+} \\quad  \\text{and} \\quad \\Sigma^*=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\\] As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters \\(\\Sigma^*\\) and \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]\nAn alternative formulation of the optimal portfolio can be derived as follows: \\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1.\\] The optimal weights correspond to a mean-variance portfolio, where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering a higher expected return."
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "href": "r/constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\nThe function below implements the efficient portfolio weight in its general form, allowing for transaction costs (conditional on the holdings before reallocation). For \\(\\beta=0\\), the computation resembles the standard mean-variance efficient framework. gamma denotes the coefficient of risk aversion \\(\\gamma\\), beta is the transaction cost parameter \\(\\beta\\) and w_prev are the weights before rebalancing \\(\\omega_{t^+}\\).\n\ncompute_efficient_weight &lt;- function(Sigma,\n                                     mu,\n                                     gamma = 2,\n                                     beta = 0, # transaction costs\n                                     w_prev = rep(\n                                       1 / ncol(Sigma),\n                                       ncol(Sigma)\n                                     )) {\n  iota &lt;- rep(1, ncol(Sigma))\n  Sigma_processed &lt;- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed &lt;- mu + beta * w_prev\n\n  Sigma_inverse &lt;- solve(Sigma_processed)\n\n  w_mvp &lt;- Sigma_inverse %*% iota\n  w_mvp &lt;- as.vector(w_mvp / sum(w_mvp))\n  w_opt &lt;- w_mvp + 1 / gamma *\n    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%\n      mu_processed\n  return(as.vector(w_opt))\n}\n\ncompute_efficient_weight(Sigma, mu)\n\n [1]  1.395  0.293 -1.391  0.477  0.363 -0.320  0.545  0.446 -0.132\n[10] -0.675\n\n\nThe portfolio weights above indicate the efficient portfolio for an investor with risk aversion coefficient \\(\\gamma=2\\) in absence of transaction costs. Some of the positions are negative which implies short-selling, most of the positions are rather extreme. For instance, a position of \\(-1\\) implies that the investor takes a short position worth her entire wealth to lever long positions in other assets. What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyze the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter \\(\\beta\\) and risk aversion \\(\\gamma\\).\n\ntransaction_costs &lt;- expand_grid(\n  gamma = c(2, 4, 8, 20),\n  beta = 20 * qexp((1:99) / 100)\n) |&gt;\n  mutate(\n    weights = map2(\n      .x = gamma,\n      .y = beta,\n      ~ compute_efficient_weight(Sigma,\n        mu,\n        gamma = .x,\n        beta = .y / 10000,\n        w_prev = w_mvp\n      )\n    ),\n    concentration = map_dbl(weights, ~ sum(abs(. - w_mvp)))\n  )\n\nThe code chunk above computes the optimal weight in presence of transaction cost for different values of \\(\\beta\\) and \\(\\gamma\\) but with the same initial allocation, the theoretical optimal minimum variance portfolio. Starting from the initial allocation, the investor chooses her optimal allocation along the efficient frontier to reflect her own risk preferences. If transaction costs would be absent, the investor would simply implement the mean-variance efficient allocation. If transaction costs make it costly to rebalance, her optimal portfolio choice reflects a shift toward the efficient portfolio, whereas her current portfolio anchors her investment.\n\ntransaction_costs |&gt;\n  mutate(risk_aversion = as_factor(gamma)) |&gt;\n  ggplot(aes(\n    x = beta,\n    y = concentration,\n    color = risk_aversion,\n    linetype = risk_aversion\n  )) +\n  geom_line() +\n  guides(linetype = \"none\") + \n  labs(\n    x = \"Transaction cost parameter\",\n    y = \"Distance from MVP\",\n    color = \"Risk aversion\",\n    title = \"Portfolio weights for different risk aversion and transaction cost\"\n  )\n\n\n\n\nFigure 1: The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\n\n\n\n\nFigure 1 shows rebalancing from the initial portfolio (which we always set to the minimum variance portfolio weights in this example). The higher the transaction costs parameter \\(\\beta\\), the smaller is the rebalancing from the initial portfolio. In addition, if risk aversion \\(\\gamma\\) increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings."
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#constrained-optimization",
    "href": "r/constrained-optimization-and-backtesting.html#constrained-optimization",
    "title": "Constrained Optimization and Backtesting",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\nNext, we introduce constraints to the above optimization procedure. Very often, typical constraints such as short-selling restrictions prevent analytical solutions for optimal portfolio weights (short-selling restrictions simply imply that negative weights are not allowed such that we require that \\(w_i \\geq 0\\quad \\forall i\\)). However, numerical optimization allows computing the solutions to such constrained problems. For the purpose of mean-variance optimization, we rely on the solve.QP() function from the package quadprog.\nThe function solve.QP() delivers numerical solutions to quadratic programming problems of the form \\[\\min(-\\mu \\omega + 1/2 \\omega' \\Sigma \\omega) \\text{ s.t. } A' \\omega &gt;= b_0.\\] The function takes one argument (meq) for the number of equality constraints. Therefore, the above matrix \\(A\\) is simply a vector of ones to ensure that the weights sum up to one. In the case of short-selling constraints, the matrix \\(A\\) is of the form \\[A' = \\begin{pmatrix}1 & 1& \\ldots&1 \\\\1 & 0 &\\ldots&0\\\\0 & 1 &\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}'\\qquad b_0 = \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}.\\]\nBefore we dive into constrained optimization, we revisit the unconstrained problem and replicate the analytical solutions for the minimum variance and efficient portfolio weights from above. We verify that the output is equal to the above solution. Note that near() is a safe way to compare two vectors for pairwise equality. The alternative == is sensitive to small differences that may occur due to the representation of floating points on a computer, while near() has a built-in tolerance. As just discussed, we set Amat to a matrix with a column of ones and bvec to 1 to enforce the constraint that weights must sum up to one. meq=1 means that one (out of one) constraints must be satisfied with equality.\n\nw_mvp_numerical &lt;- solve.QP(\n  Dmat = Sigma,\n  dvec = rep(0, n_industries),\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(w_mvp, w_mvp_numerical$solution))\n\n[1] TRUE\n\nw_efficient_numerical &lt;- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(compute_efficient_weight(Sigma, mu), w_efficient_numerical$solution))\n\n[1] TRUE\n\n\nThe result above shows that indeed the numerical procedure recovered the optimal weights for a scenario, where we already know the analytic solution. For more complex optimization routines, R’s optimization task view provides an overview of the vast optimization landscape. \nNext, we approach problems where no analytical solutions exist. First, we additionally impose short-sale constraints, which implies \\(N\\) inequality constraints of the form \\(\\omega_i &gt;=0\\).\n\nw_no_short_sale &lt;- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(1, diag(n_industries)),\n  bvec = c(1, rep(0, n_industries)),\n  meq = 1\n)\nw_no_short_sale$solution\n\n [1] 5.17e-01 3.06e-18 2.05e-16 7.90e-02 0.00e+00 2.65e-17 1.48e-01\n [8] 2.56e-01 8.74e-18 0.00e+00\n\n\nAs expected, the resulting portfolio weights are all positive (up to numerical precision). Typically, the holdings in the presence of short-sale constraints are concentrated among way fewer assets than for the unrestricted case. You can verify that sum(w_no_short_sale$solution) returns 1. In other words: solve.QP() provides the numerical solution to a portfolio choice problem for a mean-variance investor with risk aversion gamma = 2, where negative holdings are forbidden.\nsolve.QP() is fast because it benefits from a very clear problem structure with a quadratic objective and linear constraints. However, optimization often requires more flexibility. As an example, we show how to compute optimal weights, subject to the so-called Regulation T-constraint, which requires that the sum of all absolute portfolio weights is smaller than 1.5, that is \\(\\sum_{i=1}^N |\\omega_i| \\leq 1.5\\). The constraint enforces that a maximum of 50 percent of the allocated wealth can be allocated to short positions, thus implying an initial margin requirement of 50 percent. Imposing such a margin requirement reduces portfolio risks because extreme portfolio weights are not attainable anymore. The implementation of Regulation-T rules is numerically interesting because the margin constraints imply a non-linear constraint on the portfolio weights. Thus, we can no longer rely on solve.QP(), which is defined as solving quadratic programming problems with linear constraints. Instead, we rely on the package alabama, which requires a separate definition of objective and constraint functions.\n\ninitial_weights &lt;- rep(\n  1 / n_industries,\n  n_industries\n)\n\nobjective &lt;- function(w, gamma = 2) {\n  -t(w) %*% (1 + mu) +\n    gamma / 2 * t(w) %*% Sigma %*% w\n}\n\ninequality_constraints &lt;- function(w, reg_t = 1.5) {\n  reg_t - sum(abs(w))\n}\n\nequality_constraints &lt;- function(w) {\n  sum(w) - 1\n}\n\nw_reg_t &lt;- constrOptim.nl(\n  par = initial_weights,\n  hin = inequality_constraints,\n  fn = objective,\n  heq = equality_constraints,\n  control.outer = list(trace = FALSE)\n)\nw_reg_t$par\n\n [1]  3.41e-01 -1.29e-05 -1.08e-01  1.40e-01  7.32e-02 -1.13e-02\n [7]  2.47e-01  3.14e-01  1.33e-01 -1.29e-01\n\n\nNote that the function constrOptim.nl() requires a starting vector of parameter values, i.e., an initial portfolio. Under the hood, alamaba performs numerical optimization by searching for a local minimum of the function objective() (subject to the equality constraints equality_constraints() and the inequality constraints inequality_constraints()). Note that the starting point should not matter if the algorithm identifies a global minimum.\nFigure 2 shows the optimal allocation weights across all 10 industries for the four different strategies considered so far: minimum variance, efficient portfolio with \\(\\gamma\\) = 2, efficient portfolio with short-sale constraints, and the Regulation-T constrained portfolio.\n\ntibble(\n  `No short-sale` = w_no_short_sale$solution,\n  `Minimum Variance` = w_mvp,\n  `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n  `Regulation-T` = w_reg_t$par,\n  Industry = colnames(industry_returns)\n) |&gt;\n  pivot_longer(-Industry,\n    names_to = \"Strategy\",\n    values_to = \"weights\"\n  ) |&gt;\n  ggplot(aes(\n    fill = Strategy,\n    y = weights,\n    x = Industry\n  )) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() +\n  labs(\n    y = \"Allocation weight\", fill = NULL,\n    title = \"Optimal allocations for different strategies\"\n  ) +\n  scale_y_continuous(labels = percent)\n\n\n\n\nFigure 2: Optimal allocation weights for the 10 industry portfolios and the 4 different allocation strategies.\n\n\n\n\nThe results clearly indicate the effect of imposing additional constraints: the extreme holdings the investor implements if she follows the (theoretically optimal) efficient portfolio vanish under, e.g., the Regulation-T constraint. You may wonder why an investor would deviate from what is theoretically the optimal portfolio by imposing potentially arbitrary constraints. The short answer is: the efficient portfolio is only efficient if the true parameters of the data generating process correspond to the estimated parameters \\(\\hat\\Sigma\\) and \\(\\hat\\mu\\). Estimation uncertainty may thus lead to inefficient allocations. By imposing restrictions, we implicitly shrink the set of possible weights and prevent extreme allocations, which could result from error-maximization due to estimation uncertainty (Jagannathan and Ma 2003).\nBefore we move on, we want to propose a final allocation strategy, which reflects a somewhat more realistic structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for transaction costs of the form \\(\\beta\\sum_{i=1}^N |(\\omega_{i, t+1} - \\omega_{i, t^+})|\\). No closed-form solution exists, and we rely on non-linear optimization procedures.\n\ncompute_efficient_weight_L1_TC &lt;- function(mu,\n                                           Sigma,\n                                           gamma = 2,\n                                           beta = 0,\n                                           initial_weights = rep(\n                                             1 / ncol(Sigma),\n                                             ncol(Sigma)\n                                           )) {\n  objective &lt;- function(w) {\n    -t(w) %*% mu +\n      gamma / 2 * t(w) %*% Sigma %*% w +\n      (beta / 10000) / 2 * sum(abs(w - initial_weights))\n  }\n\n  w_optimal &lt;- constrOptim.nl(\n    par = initial_weights,\n    fn = objective,\n    heq = function(w) {\n      sum(w) - 1\n    },\n    control.outer = list(trace = FALSE)\n  )\n\n  return(w_optimal$par)\n}"
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "href": "r/constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "title": "Constrained Optimization and Backtesting",
    "section": "Out-of-Sample Backtesting",
    "text": "Out-of-Sample Backtesting\nFor the sake of simplicity, we committed one fundamental error in computing portfolio weights above: we used the full sample of the data to determine the optimal allocation (Arnott, Harvey, and Markowitz 2019). To implement this strategy at the beginning of the 2000s, you will need to know how the returns will evolve until 2021. While interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We do so next in a backtesting application for three strategies. For the backtest, we recompute optimal weights just based on past available data.\nThe few lines below define the general setup. We consider 120 periods from the past to update the parameter estimates before recomputing portfolio weights. Then, we update portfolio weights which is costly and affects the performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, the mean-variance efficient portfolio with ex-ante adjustment for transaction costs, and the naive portfolio, which allocates wealth equally across the different assets.\n\nwindow_length &lt;- 120\nperiods &lt;- nrow(industry_returns) - window_length\n\nbeta &lt;- 50\ngamma &lt;- 2\n\nperformance_values &lt;- matrix(NA,\n  nrow = periods,\n  ncol = 3\n)\ncolnames(performance_values) &lt;- c(\"raw_return\", \"turnover\", \"net_return\")\n\nperformance_values &lt;- list(\n  \"MV (TC)\" = performance_values,\n  \"Naive\" = performance_values,\n  \"MV\" = performance_values\n)\n\nw_prev_1 &lt;- w_prev_2 &lt;- w_prev_3 &lt;- rep(\n  1 / n_industries,\n  n_industries\n)\n\nWe also define two helper functions: one to adjust the weights due to returns and one for performance evaluation, where we compute realized returns net of transaction costs.\n\nadjust_weights &lt;- function(w, next_return) {\n  w_prev &lt;- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance &lt;- function(w, w_previous, next_return, beta = 50) {\n  raw_return &lt;- as.matrix(next_return) %*% w\n  turnover &lt;- sum(abs(w - w_previous))\n  net_return &lt;- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\n\nThe following code chunk performs a rolling-window estimation, which we implement in a loop. In each period, the estimation window contains the returns available up to the current period. Note that we use the sample variance-covariance matrix and ignore the estimation of \\(\\hat\\mu\\) entirely, but you might use more advanced estimators in practice.\n\nfor (p in 1:periods) {\n  returns_window &lt;- industry_returns[p:(p + window_length - 1), ]\n  next_return &lt;- industry_returns[p + window_length, ] |&gt; as.matrix()\n\n  Sigma &lt;- cov(returns_window)\n  mu &lt;- 0 * colMeans(returns_window)\n\n  # Transaction-cost adjusted portfolio\n  w_1 &lt;- compute_efficient_weight_L1_TC(\n    mu = mu,\n    Sigma = Sigma,\n    beta = beta,\n    gamma = gamma,\n    initial_weights = w_prev_1\n  )\n\n  performance_values[[1]][p, ] &lt;- evaluate_performance(w_1,\n    w_prev_1,\n    next_return,\n    beta = beta\n  )\n\n  w_prev_1 &lt;- adjust_weights(w_1, next_return)\n\n  # Naive portfolio\n  w_2 &lt;- rep(1 / n_industries, n_industries)\n\n  performance_values[[2]][p, ] &lt;- evaluate_performance(\n    w_2,\n    w_prev_2,\n    next_return\n  )\n\n  w_prev_2 &lt;- adjust_weights(w_2, next_return)\n\n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 &lt;- compute_efficient_weight(\n    Sigma = Sigma,\n    mu = mu,\n    gamma = gamma\n  )\n\n  performance_values[[3]][p, ] &lt;- evaluate_performance(\n    w_3,\n    w_prev_3,\n    next_return\n  )\n\n  w_prev_3 &lt;- adjust_weights(w_3, next_return)\n}\n\nFinally, we get to the evaluation of the portfolio strategies net-of-transaction costs. Note that we compute annualized returns and standard deviations. \n\nperformance &lt;- lapply(\n  performance_values,\n  as_tibble\n) |&gt;\n  bind_rows(.id = \"strategy\")\n\nperformance |&gt;\n  group_by(strategy) |&gt;\n  summarize(\n    Mean = 12 * mean(100 * net_return),\n    SD = sqrt(12) * sd(100 * net_return),\n    `Sharpe ratio` = if_else(Mean &gt; 0,\n      Mean / SD,\n      NA_real_\n    ),\n    Turnover = 100 * mean(turnover)\n  )\n\n# A tibble: 3 × 5\n  strategy   Mean    SD `Sharpe ratio` Turnover\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 MV       -0.635  12.5         NA     213.    \n2 MV (TC)  12.3    15.0          0.820   0.0296\n3 Naive    12.3    15.0          0.818   0.230 \n\n\nThe results clearly speak against mean-variance optimization. Turnover is huge when the investor only considers her portfolio’s expected return and variance. Effectively, the mean-variance portfolio generates a negative annualized return after adjusting for transaction costs. At the same time, the naive portfolio turns out to perform very well. In fact, the performance gains of the transaction-cost adjusted mean-variance portfolio are small. The out-of-sample Sharpe ratio is slightly higher than for the naive portfolio. Note the extreme effect of turnover penalization on turnover: MV (TC) effectively resembles a buy-and-hold strategy which only updates the portfolio once the estimated parameters \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\)indicate that the current allocation is too far away from the optimal theoretical portfolio."
  },
  {
    "objectID": "r/constrained-optimization-and-backtesting.html#exercises",
    "href": "r/constrained-optimization-and-backtesting.html#exercises",
    "title": "Constrained Optimization and Backtesting",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the portfolio choice problem for transaction-cost adjusted certainty equivalent maximization with risk aversion parameter \\(\\gamma\\) \\[\\omega_{t+1} ^* =  \\arg\\max_{\\omega \\in \\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\] where \\(\\Sigma\\) and \\(\\mu\\) are (estimators of) the variance-covariance matrix of the returns and the vector of expected returns. Assume for now that transaction costs are quadratic in rebalancing and proportional to stock illiquidity such that \\[\\nu_t\\left(\\omega, B\\right) = \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] where \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) is a diagonal matrix, where \\(ill_1, \\ldots, ill_N\\). Derive a closed-form solution for the mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based on the transaction cost specification above. Discuss the effect of illiquidity \\(ill_i\\) on the individual portfolio weights relative to an investor that myopically ignores transaction costs in her decision.\nUse the solution from the previous exercise to update the function compute_efficient_weight() such that you can compute optimal weights conditional on a matrix \\(B\\) with illiquidity measures.\nIllustrate the evolution of the optimal weights from the naive portfolio to the efficient portfolio in the mean-standard deviation diagram.\nIs it always optimal to choose the same \\(\\beta\\) in the optimization problem than the value used in evaluating the portfolio performance? In other words: can it be optimal to choose theoretically sub-optimal portfolios based on transaction cost considerations that do not reflect the actual incurred costs? Evaluate the out-of-sample Sharpe ratio after transaction costs for a range of different values of imposed \\(\\beta\\) values."
  },
  {
    "objectID": "r/difference-in-differences.html",
    "href": "r/difference-in-differences.html",
    "title": "Difference in Differences",
    "section": "",
    "text": "In this chapter, we illustrate the concept of difference in differences (DD) estimators by evaluating the effects of climate change regulation on the pricing of bonds across firms. DD estimators are typically used to recover the treatment effects of natural or quasi-natural experiments that trigger sharp changes in the environment of a specific group. Instead of looking at differences in just one group (e.g., the effect in the treated group), DD investigates the treatment effects by looking at the difference between differences in two groups. Such experiments are usually exploited to address endogeneity concerns (e.g., Roberts and Whited 2013). The identifying assumption is that the outcome variable would change equally in both groups without the treatment. This assumption is also often referred to as the assumption of parallel trends. Moreover, we would ideally also want a random assignment to the treatment and control groups. Due to lobbying or other activities, this randomness is often violated in (financial) economics.\nIn the context of our setting, we investigate the impact of the Paris Agreement (PA), signed on December 12, 2015, on the bond yields of polluting firms. We first estimate the treatment effect of the agreement using panel regression techniques that we discuss in the Chapter 12. We then present two methods to illustrate the treatment effect over time graphically. Although we demonstrate that the treatment effect of the agreement is anticipated by bond market participants well in advance, the techniques we present below can also be applied to many other settings.\nThe approach we use here replicates the results of Seltzer, Starks, and Zhu (2022) partly. Specifically, we borrow their industry definitions for grouping firms into green and brown types. Overall, the literature on ESG effects in corporate bond markets is already large but continues to grow (for recent examples, see, e.g., Halling, Yu, and Zechner (2021), Handler, Jankowitsch, and Pasler (2022), Huynh and Xia (2021), among many others).\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(fixest)\nlibrary(broom)"
  },
  {
    "objectID": "r/difference-in-differences.html#data-preparation",
    "href": "r/difference-in-differences.html#data-preparation",
    "title": "Difference in Differences",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use TRACE and Mergent FISD as data sources from our SQLite-database introduced in Chapters 2-4. \n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nmergent &lt;- tbl(tidy_finance, \"mergent\") |&gt;\n  select(complete_cusip, maturity, offering_amt, sic_code) |&gt;\n  collect()\n\ntrace_enhanced &lt;- tbl(tidy_finance, \"trace_enhanced\") |&gt;\n  select(cusip_id, trd_exctn_dt, rptd_pr, entrd_vol_qt, yld_pt)|&gt;\n  collect()\n\nWe start our analysis by preparing the sample of bonds. We only consider bonds with a time to maturity of more than one year to the signing of the PA, so that we have sufficient data to analyze the yield behavior after the treatment date. This restriction also excludes all bonds issued after the agreement. We also consider only the first two digits of the SIC industry code to identify the polluting industries (in line with Seltzer, Starks, and Zhu 2022).\n\ntreatment_date &lt;- ymd(\"2015-12-12\")\n\nbonds &lt;- mergent |&gt;\n  mutate(\n    time_to_maturity = as.numeric(maturity - treatment_date),\n    time_to_maturity = time_to_maturity / 365,\n    sic_code = as.integer(substr(sic_code, 1, 2)),\n    log_offering_amt = log(offering_amt)\n  ) |&gt;\n  filter(time_to_maturity &gt;= 1) |&gt;\n  select(\n    cusip_id = complete_cusip,\n    time_to_maturity, log_offering_amt, sic_code\n  )\n\npolluting_industries &lt;- c(\n  49, 13, 45, 29, 28, 33, 40, 20,\n  26, 42, 10, 53, 32, 99, 37\n)\n\nbonds &lt;- bonds |&gt;\n  mutate(polluter = sic_code %in% polluting_industries)\n\nNext, we aggregate the individual transactions as reported in TRACE to a monthly panel of bond yields. We consider bond yields for a bond’s last trading day in a month. Therefore, we first aggregate bond data to daily frequency and apply common restrictions from the literature (see, e.g., Bessembinder et al. 2008). We weigh each transaction by volume to reflect a trade’s relative importance and avoid emphasizing small trades. Moreover, we only consider transactions with reported prices rptd_pr larger than 25 (to exclude bonds that are close to default) and only bond-day observations with more than five trades on a corresponding day (to exclude prices based on too few, potentially non-representative transactions). \n\ntrace_aggregated &lt;- trace_enhanced |&gt;\n  filter(rptd_pr &gt; 25) |&gt;\n  group_by(cusip_id, trd_exctn_dt) |&gt;\n  summarize(\n    avg_yield = weighted.mean(yld_pt, entrd_vol_qt * rptd_pr),\n    trades = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  drop_na(avg_yield) |&gt;\n  filter(trades &gt;= 5) |&gt;\n  mutate(month = floor_date(trd_exctn_dt, \"months\")) |&gt;\n  group_by(cusip_id, month) |&gt;\n  slice_max(trd_exctn_dt) |&gt;\n  ungroup() |&gt;\n  select(cusip_id, month, avg_yield)\n\nBy combining the bond-specific information from Mergent FISD for our bond sample with the aggregated TRACE data, we arrive at the main sample for our analysis.\n\nbonds_panel &lt;- bonds |&gt;\n  inner_join(trace_aggregated, by = \"cusip_id\", multiple = \"all\") |&gt;\n  drop_na()\n\nBefore we can run the first regression, we need to define the treated indicator, which is the product of the post_period (i.e., all months after the signing of the PA) and the polluter indicator defined above.\n\nbonds_panel &lt;- bonds_panel |&gt;\n  mutate(post_period = month &gt;= floor_date(treatment_date, \"months\"))\n\nbonds_panel &lt;- bonds_panel |&gt;\n  mutate(treated = polluter & post_period)\n\nAs usual, we tabulate summary statistics of the variables that enter the regression to check the validity of our variable definitions.\n\nbonds_panel |&gt;\n  pivot_longer(\n    cols = c(avg_yield, time_to_maturity, log_offering_amt),\n    names_to = \"measure\"\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 9\n  measure           mean    sd    min   q05   q50   q95   max      n\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n1 avg_yield         4.08 4.21  0.0595  1.27  3.37  8.07 128.  127428\n2 log_offering_amt 13.3  0.823 4.64   12.2  13.2  14.5   16.5 127428\n3 time_to_maturity  8.54 8.41  1.01    1.50  5.81 27.4  101.  127428"
  },
  {
    "objectID": "r/difference-in-differences.html#panel-regressions",
    "href": "r/difference-in-differences.html#panel-regressions",
    "title": "Difference in Differences",
    "section": "Panel Regressions",
    "text": "Panel Regressions\nThe PA is a legally binding international treaty on climate change. It was adopted by 196 Parties at COP 21 in Paris on 12 December 2015 and entered into force on 4 November 2016. The PA obliges developed countries to support efforts to build clean, climate-resilient futures. One may thus hypothesize that adopting climate-related policies may affect financial markets. To measure the magnitude of this effect, we first run an OLS regression without fixed effects where we include the treated, post_period, and polluter dummies, as well as the bond-specific characteristics log_offering_amt and time_to_maturity. This simple model assumes that there are essentially two periods (before and after the PA) and two groups (polluters and non-polluters). Nonetheless, it should indicate whether polluters have higher yields following the PA compared to non-polluters.\nThe second model follows the typical DD regression approach by including individual (cusip_id) and time (month) fixed effects. In this model, we do not include any other variables from the simple model because the fixed effects subsume them, and we observe the coefficient of our main variable of interest: treated.\n\nmodel_without_fe &lt;- feols(\n  fml = avg_yield ~ treated + post_period + polluter +\n    log_offering_amt + time_to_maturity,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\nmodel_with_fe &lt;- feols(\n  fml = avg_yield ~ treated | cusip_id + month,\n  vcov = \"iid\",\n  data = bonds_panel\n)\n\netable(model_without_fe, model_with_fe, coefstat = \"tstat\")\n\n                    model_without_fe     model_with_fe\nDependent Var.:            avg_yield         avg_yield\n                                                      \nConstant            10.66*** (56.60)                  \ntreatedTRUE        0.4610*** (9.284) 0.9807*** (29.43)\npost_periodTRUE  -0.1747*** (-5.940)                  \npolluterTRUE       0.4745*** (15.05)                  \nlog_offering_amt -0.5451*** (-38.55)                  \ntime_to_maturity   0.0573*** (41.29)                  \nFixed-Effects:   ------------------- -----------------\ncusip_id                          No               Yes\nmonth                             No               Yes\n________________ ___________________ _________________\nVCOV type                        IID               IID\nObservations                 127,428           127,428\nR2                           0.03151           0.64689\nWithin R2                         --           0.00713\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth models indicate that polluters have significantly higher yields after the PA than non-polluting firms. Note that the magnitude of the treated coefficient varies considerably across models."
  },
  {
    "objectID": "r/difference-in-differences.html#visualizing-parallel-trends",
    "href": "r/difference-in-differences.html#visualizing-parallel-trends",
    "title": "Difference in Differences",
    "section": "Visualizing Parallel Trends",
    "text": "Visualizing Parallel Trends\nEven though the regressions above indicate that there is an impact of the PA on bond yields of polluters, the tables do not tell us anything about the dynamics of the treatment effect. In particular, the models provide no indication about whether the crucial parallel trends assumption is valid. This assumption requires that in the absence of treatment, the difference between the two groups is constant over time. Although there is no well-defined statistical test for this assumption, visual inspection typically provides a good indication.\nTo provide such visual evidence, we revisit the simple OLS model and replace the treated and post_period indicators with month dummies for each group. This approach estimates the average yield change of both groups for each period and provides corresponding confidence intervals. Plotting the coefficient estimates for both groups around the treatment date shows us the dynamics of our panel data.\n\nmodel_without_fe_time &lt;- feols(\n  fml = avg_yield ~ polluter + month:polluter +\n    time_to_maturity + log_offering_amt,\n  vcov = \"iid\",\n  data = bonds_panel |&gt;\n    mutate(month = factor(month))\n)\n\nmodel_without_fe_coefs &lt;- tidy(model_without_fe_time) |&gt;\n  filter(str_detect(term, \"month\")) |&gt;\n  mutate(\n    month = ymd(substr(term, nchar(term) - 9, nchar(term))),\n    treatment = str_detect(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  )\n\nmodel_without_fe_coefs |&gt;\n  ggplot(aes(\n    month, \n    color = treatment,\n    linetype = treatment,\n    shape = treatment\n    )) +\n  geom_vline(aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  guides(linetype = \"none\") + \n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    shape = \"Polluter?\",\n    color = \"Polluter?\",\n    title = \"Polluters respond stronger to Paris Agreement than green firms\"\n  )\n\n\n\n\nFigure 1: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Climate Agreement on bond yields (in percent) for polluters and non-polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\nFigure 1 shows that throughout most of 2014, the yields of the two groups changed in unison. However, starting at the end of 2014, the yields start to diverge, reaching the highest difference around the signing of the PA. Afterward, the yields for both groups fall again, and the polluters arrive at the same level as at the beginning of 2014. The non-polluters, on the other hand, even experience significantly lower yields than polluters after the signing of the agreement.\nInstead of plotting both groups using the simple model approach, we can also use the fixed-effects model and focus on the polluter’s yield response to the signing relative to the non-polluters. To perform this estimation, we need to replace the treated indicator with separate time dummies for the polluters, each marking a one-month period relative to the treatment date. We then regress the monthly yields on the set of time dummies and cusip_id and month fixed effects.\n\nbonds_panel_alt &lt;- bonds_panel |&gt;\n  mutate(\n    diff_to_treatment = interval(\n      floor_date(treatment_date, \"month\"), month\n    ) %/% months(1)\n  )\n\nvariables &lt;- bonds_panel_alt |&gt;\n  distinct(diff_to_treatment, month) |&gt;\n  arrange(month) |&gt;\n  mutate(variable_name = as.character(NA))\n\nformula &lt;- \"avg_yield ~ \"\n\nfor (j in 1:nrow(variables)) {\n  if (variables$diff_to_treatment[j] != 0) {\n    old_names &lt;- names(bonds_panel_alt)\n    bonds_panel_alt &lt;- bonds_panel_alt |&gt;\n      mutate(new_var = diff_to_treatment == variables$diff_to_treatment[j] & \n               polluter)\n    new_var_name &lt;- ifelse(variables$diff_to_treatment[j] &lt; 0,\n      str_c(\"lag\", abs(variables$diff_to_treatment[j])),\n      str_c(\"lead\", variables$diff_to_treatment[j])\n    )\n    variables$variable_name[j] &lt;- new_var_name\n    names(bonds_panel_alt) &lt;- c(old_names, new_var_name)\n    formula &lt;- str_c(\n      formula,\n      ifelse(j == 1,\n        new_var_name,\n        str_c(\"+\", new_var_name)\n      )\n    )\n  }\n}\nformula &lt;- str_c(formula, \"| cusip_id + month\")\n\nmodel_with_fe_time &lt;- feols(\n  fml = as.formula(formula),\n  vcov = \"iid\",\n  data = bonds_panel_alt\n)\n\nmodel_with_fe_time_coefs &lt;- tidy(model_with_fe_time) |&gt;\n  mutate(\n    term = str_remove(term, \"TRUE\"),\n    ci_up = estimate + qnorm(0.975) * std.error,\n    ci_low = estimate + qnorm(0.025) * std.error\n  ) |&gt;\n  left_join(\n    variables,\n    by = c(\"term\" = \"variable_name\")\n  ) |&gt;\n  bind_rows(tibble(\n    term = \"lag0\",\n    estimate = 0,\n    ci_up = 0,\n    ci_low = 0,\n    month = floor_date(treatment_date, \"month\")\n  ))\n\nmodel_with_fe_time_coefs |&gt;\n  ggplot(aes(x = month, y = estimate)) +\n  geom_vline(aes(xintercept = floor_date(treatment_date, \"month\")),\n    linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = 0),\n    linetype = \"dashed\"\n  ) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_up),\n    alpha = 0.5\n  ) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = NULL,\n    y = \"Yield\",\n    title = \"Polluters' yield patterns around Paris Agreement signing\"\n  )\n\n\n\n\nFigure 2: The figure shows the coefficient estimates and 95 percent confidence intervals for OLS regressions estimating the treatment effect of the Paris Climate Agreement on bond yields (in percent) for polluters. The horizontal line represents the benchmark yield of polluters before the Paris Agreement. The vertical line indicates the date of the agreement (December 12, 2015).\n\n\n\n\n The resulting graph shown in Figure 2 confirms the main conclusion of the previous image: polluters’ yield patterns show a considerable anticipation effect starting toward the end of 2014. Yields only marginally increase after the signing of the agreement. However, as opposed to the simple model, we do not see a complete reversal back to the pre-agreement level. Yields of polluters stay at a significantly higher level even one year after the signing.\nNotice that during the year after the PA was signed, the 45th President of the United States was elected on November 8, 2016. During his campaign there were some indications of intentions to withdraw the US from the PA, which ultimately happened on November 4, 2020. Hence, reversal effects are potentially driven by these actions."
  },
  {
    "objectID": "r/difference-in-differences.html#exercises",
    "href": "r/difference-in-differences.html#exercises",
    "title": "Difference in Differences",
    "section": "Exercises",
    "text": "Exercises\n\nThe 46th President of the US rejoined the Paris Agreement in February 2021. Repeat the difference in differences analysis for the day of his election victory. Note that you will also have to download new TRACE data. How did polluters’ yields react to this action?\nBased on the exercise on ratings in Chapter 4, include ratings as a control variable in the analysis above. Do the results change?"
  },
  {
    "objectID": "r/fama-macbeth-regressions.html",
    "href": "r/fama-macbeth-regressions.html",
    "title": "Fama-MacBeth Regressions",
    "section": "",
    "text": "In this chapter, we present a simple implementation of Fama and MacBeth (1973), a regression approach commonly called Fama-MacBeth regressions. Fama-MacBeth regressions are widely used in empirical asset pricing studies. We use individual stocks as test assets to estimate the risk premium associated with the three factors included in Fama and French (1993).\nResearchers use the two-stage regression approach to estimate risk premiums in various markets, but predominately in the stock market. Essentially, the two-step Fama-MacBeth regressions exploit a linear relationship between expected returns and exposure to (priced) risk factors. The basic idea of the regression approach is to project asset returns on factor exposures or characteristics that resemble exposure to a risk factor in the cross-section in each time period. Then, in the second step, the estimates are aggregated across time to test if a risk factor is priced. In principle, Fama-MacBeth regressions can be used in the same way as portfolio sorts introduced in previous chapters.\nThe Fama-MacBeth procedure is a simple two-step approach: The first step uses the exposures (characteristics) as explanatory variables in \\(T\\) cross-sectional regressions. For example, if \\(r_{i,t+1}\\) denote the excess returns of asset \\(i\\) in month \\(t+1\\), then the famous Fama-French three factor model implies the following return generating process (see also Campbell et al. 1998): \\[\\begin{aligned}r_{i,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{i,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{i,t} + \\lambda^{HML}_t \\beta^{HML}_{i,t} + \\epsilon_{i,t}.\\end{aligned}\\] Here, we are interested in the compensation \\(\\lambda^{f}_t\\) for the exposure to each risk factor \\(\\beta^{f}_{i,t}\\) at each time point, i.e., the risk premium. Note the terminology: \\(\\beta^{f}_{i,t}\\) is a asset-specific characteristic, e.g., a factor exposure or an accounting variable. If there is a linear relationship between expected returns and the characteristic in a given month, we expect the regression coefficient to reflect the relationship, i.e., \\(\\lambda_t^{f}\\neq0\\).\nIn the second step, the time-series average \\(\\frac{1}{T}\\sum_{t=1}^T \\hat\\lambda^{f}_t\\) of the estimates \\(\\hat\\lambda^{f}_t\\) can then be interpreted as the risk premium for the specific risk factor \\(f\\). We follow Zaffaroni and Zhou (2022) and consider the standard cross-sectional regression to predict future returns. If the characteristics are replaced with time \\(t+1\\) variables, then the regression approach captures risk attributes rather than risk premiums.\nBefore we move to the implementation, we want to highlight that the characteristics, e.g., \\(\\hat\\beta^{f}_{i}\\), are often estimated in a separate step before applying the actual Fama-MacBeth methodology. You can think of this as a step 0. You might thus worry that the errors of \\(\\hat\\beta^{f}_{i}\\) impact the risk premiums’ standard errors. Measurement error in \\(\\hat\\beta^{f}_{i}\\) indeed affects the risk premium estimates, i.e., they lead to biased estimates. The literature provides adjustments for this bias (see, e.g., Shanken 1992; Kim 1995; Chen, Lee, and Lee 2015, among others) but also shows that the bias goes to zero as \\(T \\to \\infty\\). We refer to Gagliardini, Ossola, and Scaillet (2016) for an in-depth discussion also covering the case of time-varying betas. Moreover, if you plan to use Fama-MacBeth regressions with individual stocks: Hou, Xue, and Zhang (2020) advocates using weighed-least squares to estimate the coefficients such that they are not biased toward small firms. Without this adjustment, the high number of small firms would drive the coefficient estimates.\nThe current chapter relies on this set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(sandwich)\nlibrary(broom)"
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#data-preparation",
    "href": "r/fama-macbeth-regressions.html#data-preparation",
    "title": "Fama-MacBeth Regressions",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe illustrate Fama and MacBeth (1973) with the monthly CRSP sample and use three characteristics to explain the cross-section of returns: market capitalization, the book-to-market ratio, and the CAPM beta (i.e., the covariance of the excess stock returns with the market excess returns). We collect the data from our SQLite-database introduced in Chapters 2-4.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, gvkey, month, ret_excess, mktcap) |&gt;\n  collect()\n\ncompustat &lt;- tbl(tidy_finance, \"compustat\") |&gt;\n  select(datadate, gvkey, be) |&gt;\n  collect()\n\nbeta &lt;- tbl(tidy_finance, \"beta\") |&gt;\n  select(month, permno, beta_monthly) |&gt;\n  collect()\n\nWe use the Compustat and CRSP data to compute the book-to-market ratio and the (log) market capitalization. Furthermore, we also use the CAPM betas based on monthly returns we computed in the previous chapters.\n\ncharacteristics &lt;- compustat |&gt;\n  mutate(month = floor_date(ymd(datadate), \"month\")) |&gt;\n  left_join(crsp_monthly, by = c(\"gvkey\", \"month\")) |&gt;\n  left_join(beta, by = c(\"permno\", \"month\")) |&gt;\n  transmute(gvkey,\n    bm = be / mktcap,\n    log_mktcap = log(mktcap),\n    beta = beta_monthly,\n    sorting_date = month %m+% months(6)\n  )\n\ndata_fama_macbeth &lt;- crsp_monthly |&gt;\n  left_join(characteristics, by = c(\"gvkey\", \"month\" = \"sorting_date\")) |&gt;\n  group_by(permno) |&gt;\n  arrange(month) |&gt;\n  fill(c(beta, bm, log_mktcap), .direction = \"down\") |&gt;\n  ungroup() |&gt;\n  left_join(crsp_monthly |&gt;\n    select(permno, month, ret_excess_lead = ret_excess) |&gt;\n    mutate(month = month %m-% months(1)),\n  by = c(\"permno\", \"month\")\n  ) |&gt;\n  select(permno, month, ret_excess_lead, beta, log_mktcap, bm) |&gt;\n  drop_na()"
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#cross-sectional-regression",
    "href": "r/fama-macbeth-regressions.html#cross-sectional-regression",
    "title": "Fama-MacBeth Regressions",
    "section": "Cross-sectional Regression",
    "text": "Cross-sectional Regression\nNext, we run the cross-sectional regressions with the characteristics as explanatory variables for each month. We regress the returns of the test assets at a particular time point on the characteristics of each asset. By doing so, we get an estimate of the risk premiums \\(\\hat\\lambda^{f}_t\\) for each point in time. \n\nrisk_premiums &lt;- data_fama_macbeth |&gt;\n  nest(data = c(ret_excess_lead, beta, log_mktcap, bm, permno)) |&gt;\n  mutate(estimates = map(\n    data,\n    ~ tidy(lm(ret_excess_lead ~ beta + log_mktcap + bm, data = .x))\n  )) |&gt;\n  unnest(estimates)"
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#time-series-aggregation",
    "href": "r/fama-macbeth-regressions.html#time-series-aggregation",
    "title": "Fama-MacBeth Regressions",
    "section": "Time-Series Aggregation",
    "text": "Time-Series Aggregation\nNow that we have the risk premiums’ estimates for each period, we can average across the time-series dimension to get the expected risk premium for each characteristic. Similarly, we manually create the \\(t\\)-test statistics for each regressor, which we can then compare to usual critical values of 1.96 or 2.576 for two-tailed significance tests.\n\nprice_of_risk &lt;- risk_premiums |&gt;\n  group_by(factor = term) |&gt;\n  summarize(\n    risk_premium = mean(estimate) * 100,\n    t_statistic = mean(estimate) / sd(estimate) * sqrt(n())\n  )\n\nIt is common to adjust for autocorrelation when reporting standard errors of risk premiums. As in Chapter 7, the typical procedure for this is computing Whitney K. Newey and West (1987) standard errors. We again recommend the data-driven approach of Whitney K. Newey and West (1994) using the NeweyWest() function, but note that you can enforce the typical 6 lag settings via NeweyWest(., lag = 6, prewhite = FALSE).\n\nregressions_for_newey_west &lt;- risk_premiums |&gt;\n  select(month, factor = term, estimate) |&gt;\n  nest(data = c(month, estimate)) |&gt;\n  mutate(\n    model = map(data, ~ lm(estimate ~ 1, .)),\n    mean = map(model, tidy)\n  )\n\nprice_of_risk_newey_west &lt;- regressions_for_newey_west |&gt;\n  mutate(newey_west_se = map_dbl(model, ~ sqrt(NeweyWest(.)))) |&gt;\n  unnest(mean) |&gt;\n  mutate(t_statistic_newey_west = estimate / newey_west_se) |&gt;\n  select(factor,\n    risk_premium = estimate,\n    t_statistic_newey_west\n  )\n\nleft_join(price_of_risk,\n  price_of_risk_newey_west |&gt;\n    select(factor, t_statistic_newey_west),\n  by = \"factor\"\n)\n\n# A tibble: 4 × 4\n  factor      risk_premium t_statistic t_statistic_newey_west\n  &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;                  &lt;dbl&gt;\n1 (Intercept)      1.33         5.21                   4.47  \n2 beta             0.00840      0.0804                 0.0717\n3 bm               0.138        2.97                   2.54  \n4 log_mktcap      -0.114       -3.18                  -2.93  \n\n\nFinally, let us interpret the results. Stocks with higher book-to-market ratios earn higher expected future returns, which is in line with the value premium. The negative value for log market capitalization reflects the size premium for smaller stocks. Consistent with results from earlier chapters, we detect no relation between beta and future stock returns."
  },
  {
    "objectID": "r/fama-macbeth-regressions.html#exercises",
    "href": "r/fama-macbeth-regressions.html#exercises",
    "title": "Fama-MacBeth Regressions",
    "section": "Exercises",
    "text": "Exercises\n\nDownload a sample of test assets from Kenneth French’s homepage and reevaluate the risk premiums for industry portfolios instead of individual stocks.\nUse individual stocks with weighted-least squares based on a firm’s size as suggested by Hou, Xue, and Zhang (2020). Then, repeat the Fama-MacBeth regressions without the weighting scheme adjustment but drop the smallest 20 percent of firms each month. Compare the results of the three approaches.\nImplement a rolling-window regression for the time-series estimation of the factor exposure. Skip one month after each rolling period before including the exposures in the cross-sectional regression to avoid a look-ahead bias. Then, adapt the cross-sectional regression and compute the average risk premiums."
  },
  {
    "objectID": "r/hex-sticker.html",
    "href": "r/hex-sticker.html",
    "title": "Hex Sticker",
    "section": "",
    "text": "library(hexSticker)\n\nsticker(\"images/logo-website.png\", \n        package = \"Tidy Finance\", \n        p_size = 20, p_color = \"black\",\n        s_x = 1, s_y = 0.75, s_width = 0.7, s_height = 0.7, asp = 0.9,\n        h_color = \"#3b9ab2\",\n        h_fill = \"white\",\n        url = \"tidy-finance.org\",\n        filename = \"images/hex-sticker.png\")\n\n\n\n\nTidy Finance HEX Sticker"
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html",
    "href": "r/introduction-to-tidy-finance.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "The main aim of this chapter is to familiarize yourself with the tidyverse. We start by downloading and visualizing stock data from Yahoo!Finance. Then we move to a simple portfolio choice problem and construct the efficient frontier. These examples introduce you to our approach of Tidy Finance."
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "href": "r/introduction-to-tidy-finance.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with Stock Market Data",
    "text": "Working with Stock Market Data\nAt the start of each session, we load the required packages. Throughout the entire book, we always use the tidyverse (Wickham et al. 2019). In this chapter, we also load the convenient tidyquant package (Dancho and Vaughan 2022) to download price data. This package provides a convenient wrapper for various quantitative functions compatible with the tidyverse. Finally, the package scales (Wickham and Seidel 2022) provides useful scale functions for visualizations.\nYou typically have to install a package once before you can load it. In case you have not done this yet, call install.packages(\"tidyquant\"). If you have trouble using tidyquant, check out the corresponding documentation.\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(scales)\n\nWe first download daily prices for one stock symbol, e.g., the Apple stock, AAPL, directly from the data provider Yahoo!Finance. To download the data, you can use the command tq_get. If you do not know how to use it, make sure you read the help file by calling ?tq_get. We especially recommend taking a look at the examples section of the documentation. We request daily data for a period of more than 20 years.\n\nprices &lt;- tq_get(\"AAPL\",\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\nprices\n\n# A tibble: 5,535 × 8\n  symbol date        open  high   low close    volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800    0.849\n2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600    0.778\n3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600    0.789\n4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800    0.721\n5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400    0.755\n# ℹ 5,530 more rows\n\n\n tq_get downloads stock market data from Yahoo!Finance if you do not specify another data source. The function returns a tibble with eight quite self-explanatory columns: symbol, date, the market prices at the open, high, low, and close, the daily volume (in the number of traded shares), and the adjusted price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the ggplot2 package (Wickham 2016) to visualize the time series of adjusted prices in Figure 1 . This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012).\n\nprices |&gt;\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Apple stock prices between beginning of 2000 and end of 2021\"\n  )\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n Instead of analyzing prices, we compute daily net returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted day \\(t\\) price. In that context, the function lag() is helpful, which returns the previous value in a vector.\n\nreturns &lt;- prices |&gt;\n  arrange(date) |&gt;\n  mutate(ret = adjusted / lag(adjusted) - 1) |&gt;\n  select(symbol, date, ret)\nreturns\n\n# A tibble: 5,535 × 3\n  symbol date           ret\n  &lt;chr&gt;  &lt;date&gt;       &lt;dbl&gt;\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# ℹ 5,530 more rows\n\n\nThe resulting tibble contains three columns, where the last contains the daily returns (ret). Note that the first entry naturally contains a missing value (NA) because there is no previous price. Obviously, the use of lag() would be meaningless if the time series is not ordered by ascending dates. The command arrange() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use arrange(desc(date)).\nFor the upcoming examples, we remove missing values as these would require separate treatment when computing, e.g., sample averages. In general, however, make sure you understand why NA values occur and carefully examine if you can simply get rid of these observations.\n\nreturns &lt;- returns |&gt;\n  drop_na(ret)\n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. Additionally, we add a dashed line that indicates the 5 percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most 5 percent. The 5 percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to stylized facts of returns.\n\nquantile_05 &lt;- quantile(returns |&gt; pull(ret), probs = 0.05)\nreturns |&gt;\n  ggplot(aes(x = ret)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical 5 percent quantile.\n\n\n\n\nHere, bins = 100 determines the number of bins used in the illustration and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns. A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.\n\nreturns |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))\n\n# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.00130       0.0252        -0.519         0.139\n\n\nWe see that the maximum daily return was 13.905 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing group_by(year = year(date)), where the call year(date) returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the return distribution.\n\nreturns |&gt;\n  group_by(year = year(date)) |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |&gt;\n  print(n = Inf)\n\n# A tibble: 22 × 5\n    year daily_mean daily_sd daily_min daily_max\n   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2000 -0.00346     0.0549   -0.519     0.137 \n 2  2001  0.00233     0.0393   -0.172     0.129 \n 3  2002 -0.00121     0.0305   -0.150     0.0846\n 4  2003  0.00186     0.0234   -0.0814    0.113 \n 5  2004  0.00470     0.0255   -0.0558    0.132 \n 6  2005  0.00349     0.0245   -0.0921    0.0912\n 7  2006  0.000949    0.0243   -0.0633    0.118 \n 8  2007  0.00366     0.0238   -0.0702    0.105 \n 9  2008 -0.00265     0.0367   -0.179     0.139 \n10  2009  0.00382     0.0214   -0.0502    0.0676\n11  2010  0.00183     0.0169   -0.0496    0.0769\n12  2011  0.00104     0.0165   -0.0559    0.0589\n13  2012  0.00130     0.0186   -0.0644    0.0887\n14  2013  0.000472    0.0180   -0.124     0.0514\n15  2014  0.00145     0.0136   -0.0799    0.0820\n16  2015  0.0000199   0.0168   -0.0612    0.0574\n17  2016  0.000575    0.0147   -0.0657    0.0650\n18  2017  0.00164     0.0111   -0.0388    0.0610\n19  2018 -0.0000573   0.0181   -0.0663    0.0704\n20  2019  0.00266     0.0165   -0.0996    0.0683\n21  2020  0.00281     0.0294   -0.129     0.120 \n22  2021  0.00133     0.0158   -0.0417    0.0539\n\n\n\nIn case you wonder: the additional argument .names = \"{.fn}\" in across() determines how to name the output columns. The specification is rather flexible and allows almost arbitrary column names, which can be useful for reporting. The print() function simply controls the output options for the R console."
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "href": "r/introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of symbols (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the tidyverse magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets as you like. The following code takes any vector of symbols, e.g., symbol &lt;- c(\"AAPL\", \"MMM\", \"BA\"), and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for an arbitrary number of assets. We perform the analysis with data from all current constituents of the Dow Jones Industrial Average index. \n\nsymbols &lt;- tq_index(\"DOW\") |&gt; \n  filter(company != \"US DOLLAR\")\nsymbols\n\n# A tibble: 30 × 8\n  symbol company           identifier sedol weight sector shares_held\n  &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 UNH    UNITEDHEALTH GRO… 91324P102  2917… 0.0918 -          5652402\n2 MSFT   MICROSOFT CORP    594918104  2588… 0.0646 -          5652402\n3 GS     GOLDMAN SACHS GR… 38141G104  2407… 0.0623 -          5652402\n4 HD     HOME DEPOT INC    437076102  2434… 0.0593 -          5652402\n5 MCD    MCDONALD S CORP   580135101  2550… 0.0567 -          5652402\n# ℹ 25 more rows\n# ℹ 1 more variable: local_currency &lt;chr&gt;\n\n\nConveniently, tidyquant provides a function to get all stocks in a stock index with a single call (similarly, tq_exchange(\"NASDAQ\") delivers all stocks currently listed on the NASDAQ exchange). \n\nindex_prices &lt;- tq_get(symbols,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\n\nThe resulting tibble contains 158033 daily observations for 30 different corporations. Figure 3 illustrates the time series of downloaded adjusted prices for each of the constituents of the Dow Jones index. Make sure you understand every single line of code! (What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nindex_prices |&gt;\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of DOW index constituents\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\nDo you notice the small differences relative to the code we used before? tq_get(symbols) returns a tibble for several symbols as well. All we need to do to illustrate all stock symbols simultaneously is to include color = symbol in the ggplot2 aesthetics. In this way, we generate a separate line for each symbol. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates the point well.\nThe same holds for stock returns. Before computing the returns, we use group_by(symbol) such that the mutate() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: group_by(symbol) is the key to aggregating the time series into symbol-specific variables of interest.\n\nall_returns &lt;- index_prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret = adjusted / lag(adjusted) - 1) |&gt;\n  select(symbol, date, ret) |&gt;\n  drop_na(ret)\n\nall_returns |&gt;\n  group_by(symbol) |&gt;\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |&gt;\n  print(n = Inf)\n\n# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 AAPL     0.00130    0.0252    -0.519     0.139\n 2 AMGN     0.000475   0.0199    -0.134     0.151\n 3 AXP      0.000547   0.0230    -0.176     0.219\n 4 BA       0.000614   0.0220    -0.238     0.243\n 5 CAT      0.000699   0.0204    -0.145     0.147\n 6 CRM      0.00128    0.0268    -0.271     0.260\n 7 CSCO     0.000370   0.0239    -0.162     0.244\n 8 CVX      0.000486   0.0175    -0.221     0.227\n 9 DIS      0.000531   0.0193    -0.184     0.160\n10 DOW      0.000799   0.0281    -0.217     0.209\n11 GS       0.000584   0.0233    -0.190     0.265\n12 HD       0.000601   0.0194    -0.287     0.141\n13 HON      0.000523   0.0195    -0.174     0.282\n14 IBM      0.000262   0.0166    -0.155     0.120\n15 INTC     0.000400   0.0236    -0.220     0.201\n16 JNJ      0.000415   0.0123    -0.158     0.122\n17 JPM      0.000625   0.0244    -0.207     0.251\n18 KO       0.000329   0.0133    -0.101     0.139\n19 MCD      0.000553   0.0148    -0.159     0.181\n20 MMM      0.000452   0.0149    -0.129     0.126\n21 MRK      0.000325   0.0170    -0.268     0.130\n22 MSFT     0.000587   0.0193    -0.156     0.196\n23 NKE      0.000824   0.0190    -0.198     0.155\n24 PG       0.000398   0.0134    -0.302     0.120\n25 TRV      0.000554   0.0185    -0.208     0.256\n26 UNH      0.00101    0.0200    -0.186     0.348\n27 V        0.000995   0.0189    -0.136     0.150\n28 VZ       0.000287   0.0151    -0.118     0.146\n29 WBA      0.000340   0.0181    -0.150     0.166\n30 WMT      0.000321   0.0149    -0.102     0.117\n\n\n\nNote that you are now also equipped with all tools to download price data for each symbol listed in the S&P 500 index with the same number of lines of code. Just use symbol &lt;- tq_index(\"SP500\"), which provides you with a tibble that contains each symbol that is (currently) part of the S&P 500. However, don’t try this if you are not prepared to wait for a couple of minutes because this is quite some data to download!"
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "href": "r/introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "title": "Introduction to Tidy Finance",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol can also make sense. For instance, suppose you are interested in answering the question: are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e9 (R can handle scientific notation) denotes daily trading volume in billion USD.\n\ntrading_volume &lt;- index_prices |&gt;\n  group_by(date) |&gt;\n  summarize(trading_volume = sum(volume * adjusted))\n\ntrading_volume |&gt;\n  ggplot(aes(x = date, y = trading_volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of DOW index constitutens\"\n  ) +\n    scale_y_continuous(labels = unit_format(unit = \"B\", scale = 1e-9))\n\n\n\n\nFigure 4: Total daily trading volume in billion USD.\n\n\n\n\nFigure 4 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 5, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\ntrading_volume |&gt;\n  ggplot(aes(x = lag(trading_volume), y = trading_volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of DOW index constituents\"\n  ) + \n  scale_x_continuous(labels = unit_format(unit = \"B\", scale = 1e-9)) +\n  scale_y_continuous(labels = unit_format(unit = \"B\", scale = 1e-9))\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 5: Total daily trading volume in billion USD.\n\n\n\n\nDo you understand where the warning ## Warning: Removed 1 rows containing missing values (geom_point). comes from and what it means? Purely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days."
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "href": "r/introduction-to-tidy-finance.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio Choice Problems",
    "text": "Portfolio Choice Problems\nIn the previous part, we show how to download stock market data and inspect it with graphs and summary statistics. Now, we move to a typical question in Finance: how to allocate wealth across different assets optimally. The standard framework for optimal portfolio selection considers investors that prefer higher future returns but dislike future return volatility (defined as the square root of the return variance): the mean-variance investor (Markowitz 1952).\n An essential tool to evaluate portfolios in the mean-variance context is the efficient frontier, the set of portfolios which satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility (the square root of the variance, i.e., the risk), see, e.g., Merton (1972). We compute and visualize the efficient frontier for several stocks. First, we extract each asset’s monthly returns. In order to keep things simple, we work with a balanced panel and exclude DOW constituents for which we do not observe a price on every single trading day since the year 2000.\n\nindex_prices &lt;- index_prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  filter(n == max(n)) |&gt;\n  select(-n)\nreturns &lt;- index_prices |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(symbol, month) |&gt;\n  summarize(price = last(adjusted), .groups = \"drop_last\") |&gt;\n  mutate(ret = price / lag(price) - 1) |&gt;\n  drop_na(ret) |&gt;\n  select(-price)\n\nHere, floor_date() is a function from the lubridate package (Grolemund and Wickham 2011), which provides useful functions to work with dates and times.\nNext, we transform the returns from a tidy tibble into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) symbols and one row for each of the \\(T\\) trading days to compute the sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] where \\(r_t\\) is the \\(N\\) vector of returns on date \\(t\\) and the sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'.\\] We achieve this by using pivot_wider() with the new column names from the column symbol and setting the values to ret. We compute the vector of sample average returns and the sample variance-covariance matrix, which we consider as proxies for the parameters of the distribution of future stock returns. Thus, for simplicity, we refer to \\(\\Sigma\\) and \\(\\mu\\) instead of explicitly highlighting that the sample moments are estimates. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account.\n\nreturns_matrix &lt;- returns |&gt;\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |&gt;\n  select(-month)\nsigma &lt;- cov(returns_matrix)\nmu &lt;- colMeans(returns_matrix)\n\nThen, we compute the minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) as well as the expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) and volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\sum\\limits_{i=1}^N\\omega_i = 1.\\] The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\), where \\(\\iota\\) is a vector of ones and \\(\\Sigma^{-1}\\) is the inverse of \\(\\Sigma\\).\n\nN &lt;- ncol(returns_matrix)\niota &lt;- rep(1, N)\nsigma_inv &lt;- solve(sigma)\nmvp_weights &lt;- sigma_inv %*% iota\nmvp_weights &lt;- mvp_weights / sum(mvp_weights)\ntibble(\n  average_ret = as.numeric(t(mvp_weights) %*% mu),\n  volatility = as.numeric(sqrt(t(mvp_weights) %*% sigma %*% mvp_weights))\n)\n\n# A tibble: 1 × 2\n  average_ret volatility\n        &lt;dbl&gt;      &lt;dbl&gt;\n1     0.00857     0.0314\n\n\nThe command solve(A, b) returns the solution of a system of equations \\(Ax = b\\). If b is not provided, as in the example above, it defaults to the identity matrix such that solve(sigma) delivers \\(\\Sigma^{-1}\\) (if a unique solution exists).\nNote that the monthly volatility of the minimum variance portfolio is of the same order of magnitude as the daily standard deviation of the individual components. Thus, the diversification benefits in terms of risk reduction are tremendous!\nNext, we set out to find the weights for a portfolio that achieves, as an example, three times the expected return of the minimum variance portfolio. However, mean-variance investors are not interested in any portfolio that achieves the required return but rather in the efficient portfolio, i.e., the portfolio with the lowest standard deviation. If you wonder where the solution \\(\\omega_\\text{eff}\\) comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance given a minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, their objective function is to choose \\(\\omega_\\text{eff}\\) as the solution to \\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\]\nThe code below implements the analytic solution to this optimization problem for a benchmark return \\(\\bar\\mu\\), which we set to 3 times the expected return of the minimum variance portfolio. We encourage you to verify that it is correct.\n\nbenchmark_multiple &lt;- 3\nmu_bar &lt;- benchmark_multiple * t(mvp_weights) %*% mu\nC &lt;- as.numeric(t(iota) %*% sigma_inv %*% iota)\nD &lt;- as.numeric(t(iota) %*% sigma_inv %*% mu)\nE &lt;- as.numeric(t(mu) %*% sigma_inv %*% mu)\nlambda_tilde &lt;- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights &lt;- mvp_weights +\n  lambda_tilde / 2 * (sigma_inv %*% mu - D * mvp_weights)"
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#the-efficient-frontier",
    "href": "r/introduction-to-tidy-finance.html#the-efficient-frontier",
    "title": "Introduction to Tidy Finance",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. The code below implements the construction of the efficient frontier, which characterizes the highest expected return achievable at each level of risk. To understand the code better, make sure to familiarize yourself with the inner workings of the for loop.\n\nlength_year &lt;- 12\na &lt;- seq(from = -0.4, to = 1.9, by = 0.01)\nres &lt;- tibble(\n  a = a,\n  mu = NA,\n  sd = NA\n)\nfor (i in seq_along(a)) {\n  w &lt;- (1 - a[i]) * mvp_weights + (a[i]) * efp_weights\n  res$mu[i] &lt;- length_year * t(w) %*% mu   \n  res$sd[i] &lt;- sqrt(length_year) * sqrt(t(w) %*% sigma %*% w)\n}\n\nThe code above proceeds in two steps: First, we compute a vector of combination weights \\(a\\) and then we evaluate the resulting linear combination with \\(a\\in\\mathbb{R}\\):\n\\[\\omega^* = a\\omega_\\text{eff}(\\bar\\mu) + (1-a)\\omega_\\text{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] with \\(\\lambda^* = 2\\frac{a\\bar\\mu + (1-a)\\tilde\\mu - D/C}{E-D^2/C}\\) where \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). Finally, it is simple to visualize the efficient frontier alongside the two efficient portfolios within one powerful figure using ggplot2 (see Figure 6). We also add the individual stocks in the same call. We compute annualized returns based on the simple assumption that monthly returns are independent and identically distributed. Thus, the average annualized return is just 12 times the expected monthly return.\n\nres |&gt;\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point(\n    data = res |&gt; filter(a %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point(\n    data = tibble(\n      mu = length_year * mu,       \n      sd = sqrt(length_year) * sqrt(diag(sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation\",\n    y = \"Annualized expected return\",\n    title = \"Efficient frontier for DOW index constituents\"\n  ) +\n  scale_x_continuous(labels = percent) +\n  scale_y_continuous(labels = percent)\n\n\n\n\nFigure 6: The big dots indicate the location of the minimum variance and the efficient portfolio which delivers 3 times the expected return of the minimum variance portfolio, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\nThe line in Figure 6 indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters \\(\\Sigma\\) and \\(\\mu\\) as given)."
  },
  {
    "objectID": "r/introduction-to-tidy-finance.html#exercises",
    "href": "r/introduction-to-tidy-finance.html#exercises",
    "title": "Introduction to Tidy Finance",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market symbol of your choice from Yahoo!Finance with tq_get() from the tidyquant package. Plot two time series of the symbol’s un-adjusted and adjusted closing prices. Explain the differences.\nCompute daily net returns for the asset and visualize the distribution of daily returns in a histogram. Also, use geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns within the histogram. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns\nTake your code from before and generalize it such that you can perform all the computations for an arbitrary vector of symbols (e.g., symbol &lt;- c(\"AAPL\", \"MMM\", \"BA\")). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nConsider the research question: Are days with high aggregate trading volume often also days with large absolute price changes? Find an appropriate visualization to analyze the question.\nCompute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.\nIn the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?\nThe efficient frontier characterizes the portfolios with the highest expected return for different levels of risk, i.e., standard deviation. Identify the portfolio with the highest expected return per standard deviation. Hint: the ratio of expected return to standard deviation is an important concept in Finance."
  },
  {
    "objectID": "r/other-data-providers.html",
    "href": "r/other-data-providers.html",
    "title": "Other Data Providers",
    "section": "",
    "text": "In the previous chapters, we introduced many ways to get financial data that researchers regularly use. We showed how to load data into R from Yahoo!Finance and commonly used file types, such as comma-separated or Excel files. Then, we introduced remotely connecting to WRDS and downloading data from there. However, this is only a subset of the vast amounts of data available these days.\nIn this short chapter, we aim to provide an overview of common alternative data providers for which direct access via R packages exists. Such a list requires constant adjustments because both data providers and access methods change. However, we want to emphasize two main insights: First, the number of R packages that provide access to (financial) data is large. Too large actually to survey here exhaustively. Instead, we can only cover the tip of the iceberg. Second, R provides the functionalities to access basically any form of files or data available online. Thus, even if a desired data source does not come with a well-established R package, chances are high that data can be retrieved by establishing your own API connection or by scrapping the content.\nIn our non-exhaustive list below, we restrict ourselves to listing data sources accessed through easy-to-use R packages. For further inspiration on potential data sources, we recommend reading the R task view empirical finance. Further inspiration (on more general social sciences) can be found here."
  },
  {
    "objectID": "r/other-data-providers.html#exercises",
    "href": "r/other-data-providers.html#exercises",
    "title": "Other Data Providers",
    "section": "Exercises",
    "text": "Exercises\n\nSelect one of the data sources in the table above and retrieve some data: Browse the homepage of the data provider or the package documentation to find inspiration on which type of data is available to you and how to download the data into your R session.\nGenerate summary statistics of the data you retrieved and provide some useful visualization. The possibilities are endless: Maybe there is some interesting economic event you want to analyze, such as stock market responses to Twitter activity.\nSimfin provides excellent data coverage. Use their API to find out if the information Simfin provides overlaps with the CRSP/Compustat dataset in the tidy_finance.sqlite database introduced in Chapters 2-4."
  },
  {
    "objectID": "r/proofs.html",
    "href": "r/proofs.html",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines her optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return she wants to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plug-in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\)."
  },
  {
    "objectID": "r/proofs.html#optimal-portfolio-choice",
    "href": "r/proofs.html#optimal-portfolio-choice",
    "title": "Proofs",
    "section": "",
    "text": "The minimum variance portfolio weights are given by the solution to \\[\\omega_\\text{mvp} = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega= 1,\\] where \\(\\iota\\) is an \\((N \\times 1)\\) vector of ones. The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1).\\] We can solve the first-order conditions of the Lagrangian equation: \\[\n\\begin{aligned}\n& \\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega} = 0 \\Leftrightarrow 2\\Sigma \\omega = \\lambda\\iota \\Rightarrow \\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota \\\\ \\end{aligned}\n\\] Next, the constraint that weights have to sum up to one delivers: \\(1 = \\iota'\\omega = \\frac{\\lambda}{2}\\iota'\\Sigma^{-1}\\iota \\Rightarrow \\lambda = \\frac{2}{\\iota'\\Sigma^{-1}\\iota}.\\) Finally, plug-in the derived value of \\(\\lambda\\) to get \\[\n\\begin{aligned}\n\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}.\n\\end{aligned}\n\\]\n\n\n\nConsider an investor who aims to achieve minimum variance given a desired expected return \\(\\bar{\\mu}\\), that is: \\[\\omega_\\text{eff}\\left(\\bar{\\mu}\\right) = \\arg\\min \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] The Lagrangian reads \\[ \\mathcal{L}(\\omega) = \\omega'\\Sigma \\omega - \\lambda(\\omega'\\iota - 1) - \\tilde{\\lambda}(\\omega'\\mu - \\bar{\\mu}). \\] We can solve the first-order conditions to get \\[\n\\begin{aligned}\n2\\Sigma \\omega &= \\lambda\\iota + \\tilde\\lambda \\mu\\\\\n\\Rightarrow\\omega &= \\frac{\\lambda}{2}\\Sigma^{-1}\\iota + \\frac{\\tilde\\lambda}{2}\\Sigma^{-1}\\mu.\n\\end{aligned}\n\\]\nNext, the two constraints (\\(w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}\\)) imply \\[\n\\begin{aligned}\n1 &= \\iota'\\omega = \\frac{\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\iota}_{C} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\iota'\\Sigma^{-1}\\mu}_D\\\\\n\\Rightarrow \\lambda&= \\frac{2 - \\tilde\\lambda D}{C}\\\\\n\\bar\\mu &= \\mu'\\omega = \\frac{\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\iota}_{D} + \\frac{\\tilde\\lambda}{2}\\underbrace{\\mu'\\Sigma^{-1}\\mu}_E = \\frac{1}{2}\\left(\\frac{2 - \\tilde\\lambda D}{C}\\right)D+\\frac{\\tilde\\lambda}{2}E  \\\\&=\\frac{D}{C}+\\frac{\\tilde\\lambda}{2}\\left(E - \\frac{D^2}{C}\\right)\\\\\n\\Rightarrow \\tilde\\lambda &= 2\\frac{\\bar\\mu - D/C}{E-D^2/C}.\n\\end{aligned}\n\\] As a result, the efficient portfolio weight takes the form (for \\(\\bar{\\mu} \\geq D/C = \\mu'\\omega_\\text{mvp}\\)) \\[\\omega_\\text{eff}\\left(\\bar\\mu\\right) = \\omega_\\text{mvp} + \\frac{\\tilde\\lambda}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right).\\] Thus, the efficient portfolio allocates wealth in the minimum variance portfolio \\(\\omega_\\text{mvp}\\) and a levered (self-financing) portfolio to increase the expected return.\nNote that the portfolio weights sum up to one as \\[\\iota'\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right) = D - D = 0\\text{ so }\\iota'\\omega_\\text{eff} = \\iota'\\omega_\\text{mvp} = 1.\\] Finally, the expected return of the efficient portfolio is \\[\\mu'\\omega_\\text{eff} = \\frac{D}{C} + \\bar\\mu - \\frac{D}{C} = \\bar\\mu.\\]\n\n\n\nWe argue that an investor with a quadratic utility function with certainty equivalent \\[\\max_\\omega CE(\\omega) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen with the aim to minimize volatility given a pre-specified level or expected returns \\[\\min_\\omega \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1.\\] Note the difference: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines her optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return she wants to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk averse investors (higher \\(\\gamma\\)) will chose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nTo proof this equivalence, consider first the optimal portfolio weights for a certainty equivalent maximizing investor. The first order condition reads \\[\n\\begin{aligned}\n\\mu - \\lambda \\iota &= \\gamma \\Sigma \\omega \\\\\n\\Leftrightarrow \\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\lambda\\iota\\right)\n\\end{aligned}\n\\] Next, we make use of the constraint \\(\\iota'\\omega = 1\\). \\[\n\\begin{aligned}\n\\iota'\\omega &= 1 = \\frac{1}{\\gamma}\\left(\\iota'\\Sigma^{-1}\\mu - \\lambda\\iota'\\Sigma^{-1}\\iota\\right)\\\\\n\\Rightarrow \\lambda &= \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right).\n\\end{aligned}\n\\] Plug-in the value of \\(\\lambda\\) reveals the desired portfolio for an investor with risk aversion \\(\\gamma\\). \\[\n\\begin{aligned}\n\\omega &= \\frac{1}{\\gamma}\\Sigma^{-1}\\left(\\mu - \\frac{1}{\\iota'\\Sigma^{-1}\\iota}\\left(\\iota'\\Sigma^{-1}\\mu - \\gamma \\right)\\right) \\\\\n\\Rightarrow \\omega &= \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\iota'\\Sigma^{-1}\\right)\\mu\\\\\n&= \\omega_\\text{mvp} + \\frac{1}{\\gamma}\\left(\\Sigma^{-1}\\mu - \\frac{\\iota'\\Sigma^{-1}\\mu}{\\iota'\\Sigma^{-1}\\iota}\\Sigma^{-1}\\iota\\right).\n\\end{aligned}\n\\] The resulting weights correspond to the efficient portfolio with desired return \\(\\bar r\\) such that (in the notation of book) \\[\\frac{1}{\\gamma} = \\frac{\\tilde\\lambda}{2} = \\frac{\\bar\\mu - D/C}{E - D^2/C}\\] which implies that the desired return is just \\[\\bar\\mu = \\frac{D}{C} + \\frac{1}{\\gamma}\\left({E - D^2/C}\\right)\\] which is \\(\\frac{D}{C} = \\mu'\\omega_\\text{mvp}\\) for \\(\\gamma\\rightarrow \\infty\\) as expected. For instance, letting \\(\\gamma \\rightarrow \\infty\\) implies \\(\\bar\\mu = \\frac{D}{C} = \\omega_\\text{mvp}'\\mu\\)."
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html",
    "href": "r/size-sorts-and-p-hacking.html",
    "title": "Size Sorts and p-Hacking",
    "section": "",
    "text": "In this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums Walter, Weber, and Weiss (2022). Exploiting these ideas to generate favorable results is called p-hacking. There is arguably a thin line between p-hacking and conducting robustness tests. Our purpose here is to illustrate the substantial variation that can arise along the evidence-generating process.\nThe chapter relies on the following set of packages:\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(furrr)\nlibrary(rlang)\nCompared to previous chapters, we introduce the rlang package (Henry and Wickham 2022) for more advanced parsing of functional expressions."
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#data-preparation",
    "href": "r/size-sorts-and-p-hacking.html#data-preparation",
    "title": "Size Sorts and p-Hacking",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we retrieve the relevant data from our SQLite-database introduced in Chapters 2-4. Firm size is defined as market equity in most asset pricing applications that we retrieve from CRSP. We further use the Fama-French factor returns for performance evaluation.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  collect()\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  select(smb) |&gt;\n  collect()"
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#size-distribution",
    "href": "r/size-sorts-and-p-hacking.html#size-distribution",
    "title": "Size Sorts and p-Hacking",
    "section": "Size Distribution",
    "text": "Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 8.1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the CRSP universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\ncrsp_monthly |&gt;\n  group_by(month) |&gt;\n  mutate(\n    top01 = if_else(mktcap &gt;= quantile(mktcap, 0.99), 1, 0),\n    top05 = if_else(mktcap &gt;= quantile(mktcap, 0.95), 1, 0),\n    top10 = if_else(mktcap &gt;= quantile(mktcap, 0.90), 1, 0),\n    top25 = if_else(mktcap &gt;= quantile(mktcap, 0.75), 1, 0)\n  ) |&gt;\n  summarize(\n    total_market_cap =  sum(mktcap),\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap,\n    .groups = \"drop\"\n  ) |&gt;\n  select(-total_market_cap) |&gt; \n  pivot_longer(cols = -month) |&gt;\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |&gt;\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name,\n    linetype = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\n\n\n\n\nFigure 1: We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\nNext, firm sizes also differ across listing exchanges. Stocks’ primary listings were important in the past and are potentially still relevant today. Figure 2 shows that the New York Stock Exchange (NYSE) was and still is the largest listing exchange in terms of market capitalization. More recently, NASDAQ has gained relevance as a listing exchange. Do you know what the small peak in NASDAQ’s market cap around the year 2000 was?\n\ncrsp_monthly |&gt;\n  group_by(month, exchange) |&gt;\n  summarize(mktcap = sum(mktcap),\n            .groups = \"drop_last\") |&gt;\n  mutate(share = mktcap / sum(mktcap)) |&gt;\n  ggplot(aes(\n    x = month, \n    y = share, \n    fill = exchange, \n    color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\n\n\n\n\nFigure 2: Years are on the horizontal axis and the corresponding share of total market capitalization per listing exchange on the vertical axis.\n\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function summary() does not include all statistics we are interested in, which is why we create the function create_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each listing exchange. We also add a row with add_row() with the overall summary statistics.\nThe resulting table shows that firms listed on NYSE in December 2021 are significantly larger on average than firms listed on the other exchanges. Moreover, NASDAQ lists the largest number of firms. This discrepancy between firm sizes across listing exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. In the following, we use this distinction to update our portfolio sort procedure.\n\ncreate_summary &lt;- function(data, column_name) {\n  data |&gt;\n    select(value = {{ column_name }}) |&gt;\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q50 = quantile(value, 0.50),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |&gt;\n  filter(month == max(month)) |&gt;\n  group_by(exchange) |&gt;\n  create_summary(mktcap) |&gt;\n  add_row(crsp_monthly |&gt;\n            filter(month == max(month)) |&gt;\n            create_summary(mktcap) |&gt;\n            mutate(exchange = \"Overall\"))\n\n# A tibble: 5 × 9\n  exchange   mean     sd      min     q05     q50    q95    max     n\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 AMEX       415.  2181.     7.57    12.6    75.8  1218. 2.57e4   145\n2 NASDAQ    8651. 90038.     7.01    29.3   429.  18781. 2.90e6  2779\n3 NYSE     17858. 48619.    23.9    195.   3434.  80748. 4.73e5  1395\n4 Other    13906.    NA  13906.   13906.  13906.  13906. 1.39e4     1\n5 Overall  11349. 77458.     7.01    34.3   796.  40647. 2.90e6  4320"
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "r/size-sorts-and-p-hacking.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "Size Sorts and p-Hacking",
    "section": "Univariate Size Portfolios with Flexible Breakpoints",
    "text": "Univariate Size Portfolios with Flexible Breakpoints\nIn Chapter 7, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges. In published asset pricing articles, many scholars compute sorting breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks.\nTo replicate the NYSE-centered sorting procedure, we introduce exchanges as an argument in our assign_portfolio() function. The exchange-specific argument then enters in the filter filter(exchange %in% exchanges). For example, if exchanges = 'NYSE' is specified, only stocks listed on NYSE are used to compute the breakpoints. Alternatively, you could specify exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"), which keeps all stocks listed on either of these exchanges. Overall, regular expressions are a powerful tool, and we only touch on a specific case here.\n\nassign_portfolio &lt;- function(n_portfolios,\n                             exchanges,\n                             data) {\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    filter(exchange %in% exchanges) |&gt;\n    pull(mktcap_lag) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(mktcap_lag,\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  # Output\n  return(assigned_portfolios)\n}"
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "href": "r/size-sorts-and-p-hacking.html#weighting-schemes-for-portfolios",
    "title": "Size Sorts and p-Hacking",
    "section": "Weighting Schemes for Portfolios",
    "text": "Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. The statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns if value_weighted = TRUE. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Chapter 7.\n\ncompute_portfolio_returns &lt;- function(n_portfolios = 10,\n                                      exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data |&gt;\n    group_by(month) |&gt;\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = pick(everything())\n    )) |&gt;\n    group_by(month, portfolio) |&gt;\n    summarize(\n      ret = if_else(value_weighted,\n        weighted.mean(ret_excess, mktcap_lag),\n        mean(ret_excess)\n      ),\n      .groups = \"drop_last\"\n    ) |&gt;\n    summarize(size_premium = ret[portfolio == min(portfolio)] -\n      ret[portfolio == max(portfolio)]) |&gt;\n    summarize(size_premium = mean(size_premium))\n}\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from NYSE-listed stocks.\n\nret_all &lt;- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse &lt;- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(\n  Exchanges = c(\"NYSE, NASDAQ & AMEX\", \"NYSE\"),\n  Premium = as.numeric(c(ret_all, ret_nyse)) * 100\n)\n\n# A tibble: 2 × 2\n  Exchanges           Premium\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 NYSE, NASDAQ & AMEX  0.0975\n2 NYSE                 0.166 \n\n\nThe table shows that the size premium is more than 60 percent larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50 percent of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible."
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "href": "r/size-sorts-and-p-hacking.html#p-hacking-and-non-standard-errors",
    "title": "Size Sorts and p-Hacking",
    "section": "P-Hacking and Non-standard Errors",
    "text": "P-Hacking and Non-standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong, the aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al. 2021). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large scale study, Menkveld et al. (2021) find that the magnitude of non-standard errors are similar than the estimation uncertainty based on a chosen model which shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. \nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference due to multiple testing invalid (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow we conduct a series of robustness tests which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function expand_grid() produces a table of all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\np_hacking_setup &lt;- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n  value_weighted = c(TRUE, FALSE),\n  data = parse_exprs(\n    'crsp_monthly; \n     crsp_monthly |&gt; filter(industry != \"Finance\");\n     crsp_monthly |&gt; filter(month &lt; \"1990-06-01\");\n     crsp_monthly |&gt; filter(month &gt;=\"1990-06-01\")'\n  )\n)\n\nTo speed the computation up we parallelize the (many) different sorting procedures, as in the beta estimation of Chapter 6. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\nplan(multisession, workers = availableCores())\n\np_hacking_setup &lt;- p_hacking_setup |&gt;\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = eval_tidy(..4)\n    )\n  ))\n\np_hacking_results &lt;- p_hacking_setup |&gt;\n  mutate(data = map_chr(data, deparse)) |&gt;\n  unnest(size_premium) |&gt;\n  arrange(desc(size_premium))\np_hacking_results\n\n# A tibble: 48 × 5\n  n_portfolios exchanges value_weighted data             size_premium\n         &lt;dbl&gt; &lt;list&gt;    &lt;lgl&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n1           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0186\n2           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0182\n3           10 &lt;chr [3]&gt; FALSE          \"crsp_monthly\"         0.0163\n4           10 &lt;chr [3]&gt; FALSE          \"filter(crsp_mo…       0.0139\n5           10 &lt;chr [3]&gt; TRUE           \"filter(crsp_mo…       0.0115\n# ℹ 43 more rows"
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#the-size-premium-variation",
    "href": "r/size-sorts-and-p-hacking.html#the-size-premium-variation",
    "title": "Size Sorts and p-Hacking",
    "section": "The Size-Premium Variation",
    "text": "The Size-Premium Variation\nWe provide a graph in Figure 3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature which we include as a dotted vertical line.\n\np_hacking_results |&gt;\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of size premiums for different sorting choices\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb)),\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\nFigure 3: The dashed vertical line indicates the average Fama-French SMB premium."
  },
  {
    "objectID": "r/size-sorts-and-p-hacking.html#exercises",
    "href": "r/size-sorts-and-p-hacking.html#exercises",
    "title": "Size Sorts and p-Hacking",
    "section": "Exercises",
    "text": "Exercises\n\nWe gained several insights on the size distribution above. However, we did not analyze the average size across listing exchanges and industries. Which listing exchanges/industries have the largest firms? Plot the average firm size for the three listing exchanges over time. What do you conclude?\nWe compute breakpoints but do not take a look at them in the exposition above. This might cover potential data errors. Plot the breakpoints for ten size portfolios over time. Then, take the difference between the two extreme portfolios and plot it. Describe your results.\nThe returns that we analyse above do not account for differences in the exposure to market risk, i.e., the CAPM beta. Change the function compute_portfolio_returns() to output the CAPM alpha or beta instead of the average excess return.\nWhile you saw the spread in returns from the p-hacking exercise, we did not show which choices led to the largest effects. Find a way to investigate which choice variable has the largest impact on the estimated size premium.\nWe computed several size premiums, but they do not follow the definition of Fama and French (1993). Which of our approaches comes closest to their SMB premium?"
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html",
    "href": "r/univariate-portfolio-sorts.html",
    "title": "Univariate Portfolio Sorts",
    "section": "",
    "text": "In this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Chapter 9.\nA univariate portfolio sort considers only one sorting variable \\(x_{t-1,i}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\).\nThe objective is to assess the cross-sectional relation between \\(x_{t-1,i}\\) and, typically, stock excess returns \\(r_{t,i}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nThe current chapter relies on the following set of packages.\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(scales)\nlibrary(lmtest)\nlibrary(broom)\nlibrary(sandwich)\nCompared to previous chapters, we introduce lmtest (Zeileis and Hothorn 2002) for inference for estimated coefficients, broom package (Robinson, Hayes, and Couch 2022) to tidy the estimation output of many estimated linear models, and sandwich (Zeileis 2006) for different covariance matrix estimators"
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#data-preparation",
    "href": "r/univariate-portfolio-sorts.html#data-preparation",
    "title": "Univariate Portfolio Sorts",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start with loading the required data from our SQLite-database introduced in Chapters 2-4. In particular, we use the monthly CRSP sample as our asset universe. Once we form our portfolios, we use the Fama-French market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the tibble with market betas computed in the previous chapter.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(permno, month, ret_excess, mktcap_lag) |&gt;\n  collect()\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  select(month, mkt_excess) |&gt;\n  collect()\n\nbeta &lt;- tbl(tidy_finance, \"beta\") |&gt;\n  select(permno, month, beta_monthly) |&gt;\n  collect()"
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "href": "r/univariate-portfolio-sorts.html#sorting-by-market-beta",
    "title": "Univariate Portfolio Sorts",
    "section": "Sorting by Market Beta",
    "text": "Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as crsp_monthly |&gt; group_by(permno) |&gt; mutate(beta_lag = lag(beta))) instead. This procedure, however, does not work correctly if there are non-explicit missing values in the time series.\n\nbeta_lag &lt;- beta |&gt;\n  mutate(month = month %m+% months(1)) |&gt;\n  select(permno, month, beta_lag = beta_monthly) |&gt;\n  drop_na()\n\ndata_for_sorts &lt;- crsp_monthly |&gt;\n  inner_join(beta_lag, by = c(\"permno\", \"month\"))\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in weighted.mean().\n\nbeta_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(\n    breakpoint = median(beta_lag),\n    portfolio = case_when(\n      beta_lag &lt;= breakpoint ~ \"low\",\n      beta_lag &gt; breakpoint ~ \"high\"\n    )\n  ) |&gt;\n  group_by(month, portfolio) |&gt;\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), \n            .groups = \"drop\")"
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#performance-evaluation",
    "href": "r/univariate-portfolio-sorts.html#performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero, i.e., you do not need to invest money to realize this strategy in the absence of frictions.\n\nbeta_longshort &lt;- beta_portfolios |&gt;\n  pivot_wider(id_cols = month, names_from = portfolio, values_from = ret) |&gt;\n  mutate(long_short = high - low)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Whitney K. Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. While it seems that researchers often default on choosing a pre-specified lag length of 6 months, we instead recommend a data-driven approach. This automatic selection is advocated by Whitney K. Newey and West (1994) and available in the sandwich package. To implement this test, we compute the average return via lm() and then employ the coeftest() function. If you want to implement the typical 6-lag default setting, you can enforce it by passing the arguments lag = 6, prewhite = FALSE to the coeftest() function in the code below and it passes them on to NeweyWest().\n\nmodel_fit &lt;- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 0.000287   0.001311    0.22     0.83\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint hence does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high beta stocks by shorting low beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate."
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "r/univariate-portfolio-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "Univariate Portfolio Sorts",
    "section": "Functional Programming for Portfolio Sorts",
    "text": "Functional Programming for Portfolio Sorts\nNow we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we employ the curly-curly-operator to give us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the findInterval() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases, where the number of portfolio constituents differs substantially due to the distribution of the characteristics, require careful consideration and, depending on the application, might require customized sorting approaches.\n\nassign_portfolio &lt;- function(data, \n                             sorting_variable, \n                             n_portfolios) {\n  # Compute breakpoints\n  breakpoints &lt;- data |&gt;\n    pull({{ sorting_variable }}) |&gt;\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  assigned_portfolios &lt;- data |&gt;\n    mutate(portfolio = findInterval(\n      pick(everything()) |&gt;\n        pull({{ sorting_variable }}),\n      breakpoints,\n      all.inside = TRUE\n    )) |&gt;\n    pull(portfolio)\n  \n  # Output\n  return(assigned_portfolios)\n}\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios &lt;- data_for_sorts |&gt;\n  group_by(month) |&gt;\n  mutate(\n    portfolio = assign_portfolio(\n      data = pick(everything()),\n      sorting_variable = beta_lag,\n      n_portfolios = 10\n    ),\n    portfolio = as.factor(portfolio)\n  ) |&gt;\n  group_by(portfolio, month) |&gt;\n  summarize(\n    ret_excess = weighted.mean(ret_excess, mktcap_lag),\n    .groups = \"drop\"\n  )|&gt;\n  left_join(factors_ff_monthly, by = \"month\")"
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#more-performance-evaluation",
    "href": "r/univariate-portfolio-sorts.html#more-performance-evaluation",
    "title": "Univariate Portfolio Sorts",
    "section": "More Performance Evaluation",
    "text": "More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary &lt;- beta_portfolios |&gt;\n  nest(data = c(month, ret_excess, mkt_excess)) |&gt;\n  mutate(estimates = map(\n    data, ~ tidy(lm(ret_excess ~ 1 + mkt_excess, data = .x))\n  )) |&gt;\n  unnest(estimates) |&gt; \n  select(portfolio, term, estimate) |&gt; \n  pivot_wider(names_from = term, values_from = estimate) |&gt; \n  rename(alpha = `(Intercept)`, beta = mkt_excess) |&gt; \n  left_join(\n    beta_portfolios |&gt; \n      group_by(portfolio) |&gt; \n      summarize(ret_excess = mean(ret_excess),\n                .groups = \"drop\"), by = \"portfolio\"\n  )\n\nFigure 1 illustrates the CAPM alphas of beta-sorted portfolios. It shows that low beta portfolios tend to exhibit positive alphas, while high beta portfolios exhibit negative alphas.\n\nbeta_portfolios_summary |&gt;\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"CAPM alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")\n\n\n\n\nFigure 1: Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period.\n\n\n\n\nThese results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero."
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "href": "r/univariate-portfolio-sorts.html#the-security-market-line-and-beta-portfolios",
    "title": "Univariate Portfolio Sorts",
    "section": "The Security Market Line and Beta Portfolios",
    "text": "The Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 2 illustrates the security market line: We see that (not surprisingly) the high beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm &lt;- lm(ret_excess ~ 1 + beta, data = beta_portfolios_summary)$coefficients\n\nbeta_portfolios_summary |&gt;\n  ggplot(aes(\n    x = beta, \n    y = ret_excess, \n    color = portfolio\n  )) +\n  geom_point() +\n  geom_abline(\n    intercept = 0,\n    slope = mean(factors_ff_monthly$mkt_excess),\n    linetype = \"solid\"\n  ) +\n  geom_abline(\n    intercept = sml_capm[1],\n    slope = sml_capm[2],\n    linetype = \"dashed\"\n  ) +\n  scale_y_continuous(\n    labels = percent,\n    limit = c(0, mean(factors_ff_monthly$mkt_excess) * 2)\n  ) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\n\n\n\n\nFigure 2: Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort &lt;- beta_portfolios |&gt;\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) |&gt;\n  filter(portfolio %in% c(\"low\", \"high\")) |&gt;\n  pivot_wider(id_cols = month, \n              names_from = portfolio, \n              values_from = ret_excess) |&gt;\n  mutate(long_short = high - low) |&gt;\n  left_join(factors_ff_monthly, by = \"month\")\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\ncoeftest(lm(long_short ~ 1, data = beta_longshort),\n  vcov = NeweyWest\n)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.00233    0.00322    0.72     0.47\n\n\nHowever, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha, although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting against beta factor (Frazzini and Pedersen 2014). Betting against beta corresponds to a strategy that shorts high beta stocks and takes a (levered) long position in low beta stocks. If borrowing constraints prevent investors from taking positions on the SML they are instead incentivized to buy high beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital constraint investors with lower risk aversion.\n\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort),\n  vcov = NeweyWest\n)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.00446    0.00256   -1.74    0.082 .  \nmkt_excess   1.16562    0.09562   12.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFigure 3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates no consistent striking patterns over the last years - each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort |&gt;\n  group_by(year = year(month)) |&gt;\n  summarize(\n    low = prod(1 + low),\n    high = prod(1 + high),\n    long_short = prod(1 + long_short)\n  ) |&gt;\n  pivot_longer(cols = -year) |&gt;\n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )\n\n\n\n\nFigure 3: We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\nOverall, this chapter shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next chapter, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis."
  },
  {
    "objectID": "r/univariate-portfolio-sorts.html#exercises",
    "href": "r/univariate-portfolio-sorts.html#exercises",
    "title": "Univariate Portfolio Sorts",
    "section": "Exercises",
    "text": "Exercises\n\nTake the two long-short beta strategies based on different numbers of portfolios and compare the returns. Is there a significant difference in returns? How do the Sharpe ratios compare between the strategies? Find one additional portfolio evaluation statistic and compute it.\nWe plotted the alphas of the ten beta portfolios above. Write a function that tests these estimates for significance. Which portfolios have significant alphas?\nThe analysis here is based on betas from monthly returns. However, we also computed betas from daily returns. Re-run the analysis and point out differences in the results.\nGiven the results in this chapter, can you define a long-short strategy that yields positive abnormal returns (i.e., alphas)? Plot the cumulative excess return of your strategy and the market excess return for comparison."
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html",
    "href": "r/wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "This chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the SQLite-database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nFirst, we load the packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(RSQLite)\nlibrary(dbplyr)\nWe use the same date range as in the previous chapter to ensure consistency.\nstart_date &lt;- ymd(\"1960-01-01\")\nend_date &lt;- ymd(\"2021-12-31\")"
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "r/wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with R as SQLite. We use the RPostgres package to establish a connection to the WRDS database (Wickham, Ooms, and Müller 2022). Note that you could also use the odbc package to connect to a PostgreSQL database, but then you need to install the appropriate drivers yourself. RPostgres already contains a suitable driver.\n\nlibrary(RPostgres)\n\nTo establish a connection, you use the function dbConnect() with the following arguments. Note that you need to replace the user and password arguments with your own credentials. We defined system variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world (these system variables are stored in an .Renviron-file and loaded with the Sys.getenv() function).\nAdditionally, you have to use multi-factor (i.e., two-factor) authentication since May 2023 when establishing a PostgreSQL or other remote connections. You have two choices to provide the additional identification. First, if you have Duo Push enabled for your WRDS account, you will receive a push notification on your mobile phone when trying to establish a connection with the code below. Upon accepting the notification, you can continue your work. Second, you can log in to a WRDS website that requires multi-factor authentication with your username and the same IP address. Once you have successfully identified yourself on the website, your username-IP combination will be remembered for 30 days, and you can comfortably use the remote connection below.\n\nwrds &lt;- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition). Alternatively, you can also query the data structure with the function dbSendQuery(). If you are interested, there is an exercise below that is based on WRDS’ tutorial on “Querying WRDS Data using R”. Furthermore, the penultimate section of this chapter shows how to investigate the structure of databases."
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "r/wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds connection object that we just created to first access monthly CRSP return data. Actually, we need three tables to get the desired data: (i) the CRSP monthly security file,\n\nmsf_db &lt;- tbl(wrds, in_schema(\"crsp\", \"msf\"))\n\n\nthe identifying information,\n\n\nmsenames_db &lt;- tbl(wrds, in_schema(\"crsp\", \"msenames\"))\n\nand (iii) the delisting information.\n\nmsedelist_db &lt;- tbl(wrds, in_schema(\"crsp\", \"msedelist\"))\n\nWe use the three remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we keep only data in the time windows of interest, (ii) we keep only US-listed stocks as identified via share codes shrcd 10 and 11, and (iii) we keep only months within permno-specific start dates namedt and end dates nameendt. In addition, we add delisting codes and returns. You can read up in the great textbook of Bali, Engle, and Murray (2016) for an extensive discussion on the filters we apply in the code below.\n\ncrsp_monthly &lt;- msf_db |&gt;\n  filter(date &gt;= start_date & date &lt;= end_date) |&gt;\n  inner_join(\n    msenames_db |&gt;\n      filter(shrcd %in% c(10, 11)) |&gt;\n      select(permno, exchcd, siccd, namedt, nameendt),\n    by = c(\"permno\")\n  ) |&gt;\n  filter(date &gt;= namedt & date &lt;= nameendt) |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  left_join(\n    msedelist_db |&gt;\n      select(permno, dlstdt, dlret, dlstcd) |&gt;\n      mutate(month = floor_date(dlstdt, \"month\")),\n    by = c(\"permno\", \"month\")\n  ) |&gt;\n  select(\n    permno, # Security identifier\n    date, # Date of the observation\n    month, # Month of the observation\n    ret, # Return\n    shrout, # Shares outstanding (in thousands)\n    altprc, # Last traded price in a month\n    exchcd, # Exchange code\n    siccd, # Industry code\n    dlret, # Delisting return\n    dlstcd # Delisting code\n  ) |&gt;\n  collect() |&gt;\n  mutate(\n    month = ymd(month),\n    shrout = shrout * 1000\n  )\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares shrout and the last traded price in a month altprc. Note that in contrast to returns ret, these two variables are not adjusted ex-post for any corporate actions like stock splits. Moreover, the altprc is negative whenever the last traded price does not exist, and CRSP decides to report the mid-quote of the last available order book instead. Hence, we take the absolute value of the market cap. We also keep the market cap in millions of USD just for convenience as we do not want to print huge numbers in our figures and tables. In addition, we set zero market cap to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(\n    mktcap = abs(shrout * altprc) / 1000000,\n    mktcap = na_if(mktcap, 0)\n  )\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag &lt;- crsp_monthly |&gt;\n  mutate(month = month %m+% months(1)) |&gt;\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(mktcap_lag, by = c(\"permno\", \"month\"))\n\nIf you wonder why we do not use the lag() function, e.g., via crsp_monthly |&gt; group_by(permno) |&gt; mutate(mktcap_lag = lag(mktcap)), take a look at the exercises.\nNext, we follow Bali, Engle, and Murray (2016) in transforming listing exchange codes to explicit exchange names.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(exchange = case_when(\n    exchcd %in% c(1, 31) ~ \"NYSE\",\n    exchcd %in% c(2, 32) ~ \"AMEX\",\n    exchcd %in% c(3, 33) ~ \"NASDAQ\",\n    .default = \"Other\"\n  ))\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(industry = case_when(\n    siccd &gt;= 1 & siccd &lt;= 999 ~ \"Agriculture\",\n    siccd &gt;= 1000 & siccd &lt;= 1499 ~ \"Mining\",\n    siccd &gt;= 1500 & siccd &lt;= 1799 ~ \"Construction\",\n    siccd &gt;= 2000 & siccd &lt;= 3999 ~ \"Manufacturing\",\n    siccd &gt;= 4000 & siccd &lt;= 4899 ~ \"Transportation\",\n    siccd &gt;= 4900 & siccd &lt;= 4999 ~ \"Utilities\",\n    siccd &gt;= 5000 & siccd &lt;= 5199 ~ \"Wholesale\",\n    siccd &gt;= 5200 & siccd &lt;= 5999 ~ \"Retail\",\n    siccd &gt;= 6000 & siccd &lt;= 6799 ~ \"Finance\",\n    siccd &gt;= 7000 & siccd &lt;= 8999 ~ \"Services\",\n    siccd &gt;= 9000 & siccd &lt;= 9999 ~ \"Public\",\n    TRUE ~ \"Missing\"\n  ))\n\nWe also construct returns adjusted for delistings as described by Bali, Engle, and Murray (2016). The delisting of a security usually results when a company ceases operations, declares bankruptcy, merges, does not meet listing requirements, or seeks to become private. The adjustment tries to reflect the returns of investors who bought the stock in the month before the delisting and held it until the delisting date. After this transformation, we can drop the delisting returns and codes.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  mutate(ret_adj = case_when(\n    is.na(dlstcd) ~ ret,\n    !is.na(dlstcd) & !is.na(dlret) ~ dlret,\n    dlstcd %in% c(500, 520, 580, 584) |\n      (dlstcd &gt;= 551 & dlstcd &lt;= 574) ~ -0.30,\n    dlstcd == 100 ~ ret,\n    TRUE ~ -1\n  )) |&gt;\n  select(-c(dlret, dlstcd))\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop adjusted returns and the risk-free rate from our tibble. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the tibble factors_ff_monthly.\n\ntidy_finance &lt;- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff_monthly\") |&gt;\n  select(month, rf) |&gt;\n  collect()\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(factors_ff_monthly,\n    by = \"month\"\n  ) |&gt;\n  mutate(\n    ret_excess = ret_adj - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) |&gt;\n  select(-ret_adj, -rf)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  drop_na(ret_excess, mktcap, mktcap_lag)\n\nFinally, we store the monthly CRSP file in our database.\n\n  dbWriteTable(tidy_finance,\n    \"crsp_monthly\",\n    value = crsp_monthly,\n    overwrite = TRUE\n  )"
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "r/wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2021, there were 2,779 stocks with a primary listing on NASDAQ, 1,395 on NYSE, 145 on AMEX, and only one belonged to the other category.\n\ncrsp_monthly |&gt;\n  count(exchange, date) |&gt;\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 1: Number of stocks in the CRSP sample listed at each of the US exchanges.\n\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange and plotting it just as if they were in memory. All values in Figure 2 are at the end of 2021 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\ntbl(tidy_finance, \"crsp_monthly\") |&gt;\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), by = \"month\") |&gt;\n  group_by(month, exchange) |&gt;\n  summarize(\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi,\n    .groups = \"drop\"\n  ) |&gt;\n  collect() |&gt;\n  mutate(month = ymd(month)) |&gt;\n  ggplot(aes(\n    x = month, y = mktcap / 1000,\n    color = exchange, linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 2: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2021.\n\n\n\n\nOf course, performing the computation in the database is not really meaningful because we can easily pull all the required data into our memory. The code chunk above is slower than performing the same steps on tables that are already in memory. However, we just want to illustrate that you can perform many things in the database before loading the data into your memory. Before we proceed, we load the monthly CPI data.\n\ncpi_monthly &lt;- tbl(tidy_finance, \"cpi_monthly\") |&gt;\n  collect()\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\ncrsp_monthly_industry &lt;- crsp_monthly |&gt;\n  left_join(cpi_monthly, by = \"month\") |&gt;\n  group_by(month, industry) |&gt;\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry |&gt;\n  ggplot(aes(\n    x = month,\n    y = securities,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 3: Number of stocks in the CRSP sample associated with different industries.\n\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2021 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\ncrsp_monthly_industry |&gt;\n  ggplot(aes(\n    x = month,\n    y = mktcap / 1000,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market cap by industry in billions as of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 4: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2021."
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "r/wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data. While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire data set to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easy to handle. That is, instead of downloading data about many stocks all at once, download the data in small batches for each stock consecutively. Such operations can be implemented in for()-loops, where we download, prepare, and store the data for a single stock in each iteration. This operation might nonetheless take a couple of hours, so you have to be patient either way (we often run such code overnight). To keep track of the progress, you can use txtProgressBar(). Eventually, we end up with more than 68 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno, date, and month alongside the excess returns. We thus ensure that our local database contains only the data we actually use and that we can load the full daily data into our memory later. Notice that we also use the function dbWriteTable() here with the option to append the new data to an existing table, when we process the second and all following batches.\n\ndsf_db &lt;- tbl(wrds, in_schema(\"crsp\", \"dsf\"))\n\nfactors_ff_daily &lt;- tbl(tidy_finance, \"factors_ff_daily\") |&gt;\n  collect()\n\npermnos &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  distinct(permno) |&gt;\n  pull()\n\nprogress &lt;- txtProgressBar(\n  min = 0,\n  max = length(permnos),\n  initial = 0,\n  style = 3\n)\n\nfor (j in 1:length(permnos)) {\n  permno_sub &lt;- permnos[j]\n  crsp_daily_sub &lt;- dsf_db |&gt;\n    filter(permno == permno_sub &\n      date &gt;= start_date & date &lt;= end_date) |&gt;\n    select(permno, date, ret) |&gt;\n    collect() |&gt;\n    drop_na()\n\n  if (nrow(crsp_daily_sub) &gt; 0) {\n    crsp_daily_sub &lt;- crsp_daily_sub |&gt;\n      mutate(month = floor_date(date, \"month\")) |&gt;\n      left_join(factors_ff_daily |&gt;\n        select(date, rf), by = \"date\") |&gt;\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) |&gt;\n      select(permno, date, month, ret_excess)\n\n    dbWriteTable(tidy_finance,\n        \"crsp_daily\",\n        value = crsp_daily_sub,\n        overwrite = ifelse(j == 1, TRUE, FALSE),\n        append = ifelse(j != 1, TRUE, FALSE)\n      )\n  }\n  setTxtProgressBar(progress, j)\n}\n\nclose(progress)\n\ncrsp_daily_db &lt;- tbl(tidy_finance, \"crsp_daily\")\n\nTo the best of our knowledge, the daily CRSP data does not require any adjustments like the monthly data. The adjustment of the monthly data comes from the fact that CRSP aggregates daily data into monthly observations and has to decide which prices and returns to record if a stock gets delisted. In the daily data, there is simply no price or return after delisting, so there is also no aggregation problem."
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "r/wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat data",
    "text": "Preparing Compustat data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies.\n\nfunda_db &lt;- tbl(wrds, in_schema(\"comp\", \"funda\"))\n\nWe follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, (ii) in the standard format (i.e., consolidated information in standard presentation), and (iii) only data in the desired time window.\n\ncompustat &lt;- funda_db |&gt;\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" &\n      consol == \"C\" &\n      datadate &gt;= start_date & datadate &lt;= end_date\n  ) |&gt;\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk, # Preferred stock par value\n    capx, # Capital investment\n    oancf # Operating cash flow\n  ) |&gt;\n  collect()\n\nNext, we calculate the book value of preferred stock and equity inspired by the variable definition in Ken French’s data library. Note that we set negative or zero equity to missing which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat &lt;- compustat |&gt;\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be &lt;= 0, as.numeric(NA), be)\n  )\n\nWe keep only the last available information for each firm-year group. Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2021). Therefore, datadate is not the date when data was made available to the public. Check out the exercises for more insights into the peculiarities of datadate.\n\ncompustat &lt;- compustat |&gt;\n  mutate(year = year(datadate)) |&gt;\n  group_by(gvkey, year) |&gt;\n  filter(datadate == max(datadate)) |&gt;\n  ungroup()\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n  dbWriteTable(tidy_finance,\n    \"compustat\",\n    value = compustat,\n    overwrite = TRUE\n  )"
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "r/wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP).\n\nccmxpf_linktable_db &lt;- tbl(\n  wrds,\n  in_schema(\"crsp\", \"ccmxpf_linktable\")\n)\n\nThe linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via today().\n\nccmxpf_linktable &lt;- ccmxpf_linktable_db |&gt;\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) |&gt;\n  select(permno = lpermno, gvkey, linkdt, linkenddt) |&gt;\n  collect() |&gt;\n  mutate(linkenddt = replace_na(linkenddt, today()))\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links &lt;- crsp_monthly |&gt;\n  inner_join(ccmxpf_linktable, by = \"permno\", relationship = \"many-to-many\") |&gt;\n  filter(!is.na(gvkey) & (date &gt;= linkdt & date &lt;= linkenddt)) |&gt;\n  select(permno, gvkey, date)\n\ncrsp_monthly &lt;- crsp_monthly |&gt;\n  left_join(ccm_links, by = c(\"permno\", \"date\"))\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\n  dbWriteTable(tidy_finance,\n    \"crsp_monthly\",\n    value = crsp_monthly,\n    overwrite = TRUE\n  )\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 60s but hovers around 80% for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\ncrsp_monthly |&gt;\n  group_by(permno, year = year(month)) |&gt;\n  filter(date == max(date)) |&gt;\n  ungroup() |&gt;\n  left_join(compustat, by = c(\"gvkey\", \"year\")) |&gt;\n  group_by(exchange, year) |&gt;\n  summarize(\n    share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(\n    x = year, \n    y = share, \n    color = exchange,\n    linetype = exchange\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\nFigure 5: End-of-year share of securities with book equity values by listing exchange."
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "href": "r/wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Some Tricks for PostgreSQL Databases",
    "text": "Some Tricks for PostgreSQL Databases\nAs we mentioned above, the WRDS database runs on PostgreSQL rather than SQLite. Finding the right tables for your data needs can be tricky in the WRDS PostgreSQL instance, as the tables are organized in schemas. If you wonder what the purpose of schemas is, check out this documetation. For instance, if you want to find all tables that live in the crsp schema, you run\n\ndbListObjects(wrds, Id(schema = \"crsp\"))\n\nThis operation returns a list of all tables that belong to the crsp family on WRDS, e.g., &lt;Id&gt; schema = crsp, table = msenames. Similarly, you can fetch a list of all tables that belong to the comp family via\n\ndbListObjects(wrds, Id(schema = \"comp\"))\n\nIf you want to get all schemas, then run\n\ndbListObjects(wrds)"
  },
  {
    "objectID": "r/wrds-crsp-and-compustat.html#exercises",
    "href": "r/wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCheck out the structure of the WRDS database by sending queries in the spirit of “Querying WRDS Data using R” and verify the output with dbListObjects(). How many tables are associated with CRSP? Can you identify what is stored within msp500?\nCompute mkt_cap_lag using lag(mktcap) rather than joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are they different?\nIn the main part, we look at the distribution of market capitalization across exchanges and industries. Now, plot the average market capitalization of firms for each exchange and industry. What do you find?\ndatadate refers to the date to which the fiscal year of a corresponding firm refers to. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat data in funda_db and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?"
  }
]