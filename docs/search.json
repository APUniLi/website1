[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"website online version Tidy Finance R, book currently development intended eventual print release. book result joint effort Christoph Scheuch, Stefan Voigt, Patrick Weiss.grateful kind feedback every aspect book. please get touch us via contact@tidy-finance.org spot typos, discover issues deserve attention, suggestions additional chapters sections. Additionally, let us know found text helpful. look forward hearing !","code":""},{"path":"index.html","id":"why-does-this-book-exist","chapter":"Preface","heading":"Why does this book exist?","text":"Financial economics vibrant area research, central part businesses activities, least implicitly relevant everyday life. Despite relevance society vast number empirical studies financial phenomenons, one quickly learns actual implementation typically rather opaque.\ngraduate students, particularly surprised lack public code seminal papers even textbooks key concepts financial economics. lack transparent code leads numerous replication efforts (failures), also constitutes waste resources problems already solved countless others secrecy.book aims lift curtain reproducible finance providing fully transparent code base many common financial applications. hope inspire others share code publicly take part journey towards reproducible research future.","code":""},{"path":"index.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who should read this book?","text":"write book three audiences:Students want acquire basic tools required conduct financial research ranging undergrad graduate level. book’s structure simple enough material sufficient self-study purposes.Instructors look materials teach empirical finance courses. provide plenty examples (hopefully) intuitive explanations can easily adjusted expanded.Data analysts statisticians work issues pertaining financial data need practical tools .","code":""},{"path":"index.html","id":"what-will-you-learn","chapter":"Preface","heading":"What will you learn?","text":"book currently divided 5 parts:Chapter 1 introduces important concepts around approach Tidy Finance revolves.Chapter 2 provides tools organize data prepare common data sets used financial research: CRSP Compustat. reuse data chapter following chapters.Chapters 3-7 deal key concepts empirical asset pricing beta estimation, portfolio sorts, performance analysis.Chapters 8-9 apply machine learning methods problems factor selection option pricing.Chapters 10-11 provide approaches parametric, constrained portfolio optimization, backtesting procedures.chapter self-contained can read individually. Yet data chapter provides important background necessary data management subsequent chapters. number chapters covered content subject change introduce additional material near future.","code":""},{"path":"index.html","id":"what-wont-you-learn","chapter":"Preface","heading":"What won’t you learn?","text":"book empirical work. assume basic knowledge statistics econometrics, provide detailed treatments underlying theoretical models methods applied book. Instead, find references seminal academic work journal articles detailed treatments.\nbelieve comparative advantage provide thorough implementation portfolio sorts, backtesting procedures, machine learning methods, related topics empirical finance enrich implementations discussions needy-greedy choices face conducting empirical analyses. hence refrain deriving theoretical models discussing statistical properties well-established tools.","code":""},{"path":"index.html","id":"why-r","chapter":"Preface","heading":"Why R?","text":"believe R among best choices programming language area finance. favorite features include:R free open-source can use academic professional contexts.diverse active online community works broad range tools.massive set actively maintained packages kinds applications exists, e.g., data manipulation, visualization, machine learning, etc.Powerful tools communication, e.g., Rmarkdown shiny, readily available.RStudio one best development environments interactive data analysis.Strong foundations functional programming provided.Smooth integration programming languages, e.g., SQL, Python, C, C++, Fortran, etc.information, refer Wickham et al. (2019).","code":""},{"path":"index.html","id":"why-tidy","chapter":"Preface","heading":"Why tidy?","text":"start working data, quickly realize spend lot time reading, cleaning, transforming data. fact, often said 80% data analysis spent preparing data. tidying data, want structure data sets facilitate analyses. Wickham (2014) puts :[T]idy datasets alike, every messy dataset messy way. Tidy datasets provide standardized way link structure dataset (physical layout) semantics (meaning).essence, tidy data follows three principles:Every column variable.Every row observation.Every cell single value.Throughout book, try follow principles best can. want learn tidy data principles informal manner, refer vignette.addition data layer, also tidy coding principles outlined tidy tools manifesto try follow:Reuse existing data structures.Compose simple functions pipe.Embrace functional programming.Design humans.particular, heavily draw set packages called tidyverse (Wickham et al. 2019). tidyverse consistent set packages data analysis tasks, ranging importing wrangling visualizing modeling data grammar. addition explicit tidy principles, tidyverse benefits: () master one package, easier master others, (ii) core packages developed maintained Public Benefit Company RStudio, Inc. ","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"continue, make sure software need book:Install R RStudio. get walk-installation every major operating system, follow steps outlined summary. whole process done clicks. wonder difference: R open-source language environment statistical computing graphics, free download use. R runs computations, RStudio integrated development environment provides interface adding many convenient features tools. suggest coding RStudio.Open RStudio install tidyverse. sure works? find helpful information install packages brief summary.new R, recommend starting following sources:gentle good introduction workings R can found form weighted dice project. done setting R machine, try follow instructions project.main book tidyverse available online free: R Data Science Hadley Wickham Garrett Grolemund explains majority tools use book.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"Preface","heading":"About the authors","text":"met Vienna Graduate School Finance us graduated different focus shared passion: coding R. continue sharpen R skills part current occupations:Christoph Scheuch Director Product social trading platform wikifolio.com responsible product planning, execution, monitoring. also manages team data scientists analyze user behavior develop new products.Stefan Voigt Assistant Professor Finance Department Economics University Copenhagen research fellow Danish Finance Institute. research focuses blockchain technology, high-frequency trading, financial econometrics. Stefan teaches parts book courses empirical finance.Patrick Weiss Post-Doc Vienna University Economics Business. research centers around intersection asset pricing corporate finance.","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC -NC-SA 4.0.code samples book licensed Creative Commons CC0 1.0 Universal (CC0 1.0), .e., public domain.","code":""},{"path":"index.html","id":"colophon","chapter":"Preface","heading":"Colophon","text":"book written RStudio using bookdown. website hosted github pages automatically updated every commit. complete source available GitHub. generated plots book using ggplot2 classic dark--light theme (theme_bw()).version book built R version 4.2.0 (2022-04-22) following packages:","code":""},{"path":"introduction-to-tidy-finance.html","id":"introduction-to-tidy-finance","chapter":"1 Introduction to Tidy Finance","heading":"1 Introduction to Tidy Finance","text":"main aim chapter familiarize tidyverse. start downloading visualizing stock data moving simple portfolio choice problem. examples introduce approach Tidy Finance.","code":""},{"path":"introduction-to-tidy-finance.html","id":"working-with-stock-market-data","chapter":"1 Introduction to Tidy Finance","heading":"1.1 Working with stock market data","text":"start session, load required packages.\nThroughout entire book, always use package tidyverse.\nchapter, also load convenient tidyquant package download price data.\ntypically install package can load . case done yet, call install.packages(\"tidyquant\").\ntrouble using tidyquant, check documentation.first download daily prices one stock market ticker, e.g., AAPL, directly data provider Yahoo!Finance.\ndownload data, can use command tq_get. know use , make sure read help file calling ?tq_get.\nespecially recommend taking look documentation’s examples section.tq_get downloads stock market data Yahoo!Finance specify another data source. function returns tibble eight quite self-explanatory columns: symbol, date, market prices open, high, low close, daily volume (number traded shares), adjusted price USD. adjusted prices corrected anything might affect stock price market closes, e.g., stock splits dividends. actions affect quoted prices, direct impact investors hold stock.Next, use ggplot2 visualize time series adjusted prices.Instead analyzing prices, compute daily returns defined \\((p_t - p_{t-1}) / p_{t-1}\\) \\(p_t\\) adjusted day \\(t\\) price. function lag computes previous value vector.resulting tibble contains three columns last contains daily returns. Note first entry naturally contains NA previous price. Additionally, computations require time series ordered date.\nOtherwise, lag meaningless.upcoming examples, remove missing values require separate treatment computing, e.g., sample averages. general, however, make sure understand NA values occur carefully examine can simply get rid observations.Next, visualize distribution daily returns histogram. convenience, multiply returns 100 get returns percent visualizations. Additionally, also add dashed red line indicates 5% quantile daily returns histogram, (crude) proxy worst return stock probability least 5%., bins = 100 determines number bins hence implicitly width bins.\nproceeding, make sure understand use geom geom_vline() add dotted red line indicates 5% quantile daily returns.\ntypical task proceeding data compute summary statistics main variables interest.see maximum daily return around 13.905 percent.\ncan also compute summary statistics year imposing group_by(year = year(date)), call year(date) computes year.case wonder: additional argument .names = \"{.fn}\" across() determines name output columns. specification rather flexible allows almost arbitrary column names, can useful reporting.","code":"\nlibrary(tidyverse)\nlibrary(tidyquant)\nprices <- tq_get(\"AAPL\", \n                 get = \"stock.prices\", \n                 from = \"2000-01-01\", to = \"2022-03-30\")\nprices## # A tibble: 5,596 × 8\n##   symbol date        open  high   low close    volume\n##   <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>\n## 1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800\n## 2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600\n## 3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600\n## 4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800\n## 5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400\n## # … with 5,591 more rows, and 1 more variable:\n## #   adjusted <dbl>\nprices %>%\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL, \n    y = NULL,\n    title = \"AAPL stock prices\",\n    subtitle = \"Prices in USD, adjusted for dividend payments and stock splits\"\n  )\nreturns <- prices %>%\n  arrange(date) %>%\n  mutate(ret = (adjusted - lag(adjusted)) / lag(adjusted)) %>%\n  select(symbol, date, ret)\nreturns## # A tibble: 5,596 × 3\n##   symbol date           ret\n##   <chr>  <date>       <dbl>\n## 1 AAPL   2000-01-03 NA     \n## 2 AAPL   2000-01-04 -0.0843\n## 3 AAPL   2000-01-05  0.0146\n## 4 AAPL   2000-01-06 -0.0865\n## 5 AAPL   2000-01-07  0.0474\n## # … with 5,591 more rows\nreturns <- returns %>%\n  drop_na(ret)\nquantile_05 <- quantile(returns %>% pull(ret) * 100, 0.05)\n\nreturns %>%\n  ggplot(aes(x = ret * 100)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL, \n    y = NULL,\n    title = \"Distribution of daily AAPL returns (in percent)\",\n    subtitle = \"The dotted vertical line indicates the historical 5% quantile\"\n  )\nreturns %>%\n  mutate(ret = ret * 100) %>%\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))## # A tibble: 1 × 4\n##   ret_daily_mean ret_daily_sd ret_daily_min\n##            <dbl>        <dbl>         <dbl>\n## 1          0.129         2.52         -51.9\n## # … with 1 more variable: ret_daily_max <dbl>\nreturns %>%\n  mutate(ret = ret * 100) %>%\n  group_by(year = year(date)) %>%\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  ))## # A tibble: 23 × 5\n##    year daily_mean daily_sd daily_min daily_max\n##   <dbl>      <dbl>    <dbl>     <dbl>     <dbl>\n## 1  2000     -0.346     5.49    -51.9      13.7 \n## 2  2001      0.233     3.93    -17.2      12.9 \n## 3  2002     -0.121     3.05    -15.0       8.46\n## 4  2003      0.186     2.34     -8.14     11.3 \n## 5  2004      0.470     2.55     -5.58     13.2 \n## # … with 18 more rows"},{"path":"introduction-to-tidy-finance.html","id":"scaling-up-the-analysis","chapter":"1 Introduction to Tidy Finance","heading":"1.2 Scaling up the analysis","text":"next step, generalize code computations can handle arbitrary vector tickers (e.g., constituents index). Following tidy principles, quite easy download data, plot price time series, tabulate summary statistics arbitrary number assets.tidyverse magic starts: tidy data makes extremely easy generalize computations many assets like. following code takes vector tickers, e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\"), automates download well plot price time series. end, create table summary statistics arbitrary number assets. perform analysis data current constituents Dow Jones Industrial Average index.resulting file contains 159863 daily observations total 30 different corporations. figure illustrates time series downloaded adjusted prices constituents Dow Jones index. Make sure understand every single line code! (arguments aes()? alternative geoms use visualize time series? Hint: know answers try change code see difference intervention causes).notice small differences relative code used ? tq_get(ticker) returns tibble several symbols well. need illustrate tickers simultaneously include color = symbol ggplot2 aesthetics. way, generate separate line ticker. course, simply many lines graph properly identify individual stocks, illustrates point well.holds stock returns. computing returns, use group_by(symbol) mutate command performed symbol individually. logic applies computation summary statistics: group_by(symbol) key aggregating time series ticker-specific variables interest.Note now also equipped tools download price data ticker listed S&P 500 index number lines code. Just use ticker <- tq_index(\"SP500\"), provides tibble contains symbol (currently) part S&P 500. However, don’t try prepared wait couple minutes quite data download!","code":"\nticker <- tq_index(\"DOW\") \nindex_prices <- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2022-03-30\")\nindex_prices %>%\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"DOW index stock prices\",\n    subtitle = \"Prices in USD, adjusted for dividend payments and stock splits\"\n  ) +\n  theme(legend.position = \"none\")\nall_returns <- index_prices %>%\n  group_by(symbol) %>%\n  mutate(ret = adjusted / lag(adjusted) - 1) %>%\n  select(symbol, date, ret) %>%\n  drop_na(ret)\n\nall_returns %>%\n  mutate(ret = ret * 100) %>%\n  group_by(symbol) %>%\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n        .names = \"{.fn}\"\n  ))## # A tibble: 30 × 5\n##   symbol daily_mean daily_sd daily_min daily_max\n##   <chr>       <dbl>    <dbl>     <dbl>     <dbl>\n## 1 AMGN       0.0484     1.98     -13.4      15.1\n## 2 AXP        0.0572     2.30     -17.6      21.9\n## 3 BA         0.0603     2.20     -23.8      24.3\n## 4 CAT        0.0707     2.03     -14.5      14.7\n## 5 CRM        0.124      2.68     -27.1      26.0\n## # … with 25 more rows"},{"path":"introduction-to-tidy-finance.html","id":"other-forms-of-data-aggregation","chapter":"1 Introduction to Tidy Finance","heading":"1.3 Other forms of data aggregation","text":"course, aggregation across variables symbol can make sense well. instance, suppose interested answering question: days high aggregate trading volume likely followed days high aggregate trading volume? provide initial analysis question, take downloaded prices compute aggregate daily trading volume Dow Jones constituents USD. Recall column volume denoted number traded shares. Thus, multiply trading volume daily closing price get proxy aggregate trading volume USD. Scaling 1e9 denotes daily trading volume billion USD.One way illustrate persistence trading volume plot volume day \\(t\\) volume day \\(t-1\\) example . add 45°-line indicate hypothetical one--one relation geom_abline, addressing potential differences axes’ scales.understand warning ## Warning: Removed 1 rows containing missing values (geom_point). comes means? Purely eye-balling reveals days high trading volume often followed similarly high trading volume days.","code":"\nvolume <- index_prices %>%\n  mutate(volume_usd = volume * close / 1e9) %>%\n  group_by(date) %>%\n  summarize(volume = sum(volume_usd))\n\nvolume %>%\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume (billion USD)\"\n  ) \nvolume %>%\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    color = \"red\",\n    linetype = \"dotted\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume (billion USD)\",\n    y = \"Aggregate trading volume (billion USD)\",\n    title = \"Persistence of trading volume\"\n  )## Warning: Removed 1 rows containing missing values\n## (geom_point)."},{"path":"introduction-to-tidy-finance.html","id":"portfolio-choice-problems","chapter":"1 Introduction to Tidy Finance","heading":"1.4 Portfolio choice problems","text":"previous part, show download stock market data inspect graphs summary statistics. Now, move typical question Finance, namely, optimally allocate wealth across different assets. standard framework optimal portfolio selection considers investors prefer higher future returns dislike future return volatility (defined square root return variance): mean-variance investor.essential tool evaluate portfolios mean-variance context efficient frontier, set portfolios satisfy condition portfolio exists higher expected return volatility (.e., risk). compute visualize efficient frontier several stocks.\nFirst, extract asset’s monthly returns. order keep things simple, work balanced panel exclude tickers observe price every single trading day since 2000.Next, transform returns tidy tibble \\((T \\times N)\\) matrix one column \\(N\\) tickers compute covariance matrix \\(\\Sigma\\) also expected return vector \\(\\mu\\). achieve using pivot_wider() new column names column symbol setting values ret.\ncompute vector sample average returns sample variance-covariance matrix, consider proxies parameters future returns., compute minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) well expected return \\(\\omega_\\text{mvp}'\\mu\\) volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) portfolio. Recall minimum variance portfolio vector portfolio weights solution \n\\[\\omega_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } \\sum\\limits_{=1}^Nw_i = 1.\\]\neasy show analytically, \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\) \\(\\iota\\) vector ones.Note monthly volatility minimum variance portfolio order magnitude daily standard deviation individual components. Thus, diversification benefits terms risk reduction tremendous!Next, set find weights portfolio achieves three times expected return minimum variance portfolio. However, mean-variance investors interested portfolio achieves required return rather efficient portfolio, .e., portfolio lowest standard deviation.\nwonder solution \\(\\omega_\\text{eff}\\) comes : efficient portfolio chosen investor aims achieve minimum variance given minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, objective function choose \\(\\omega_\\text{eff}\\) solution \n\\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\ncode implements analytic solution optimization problem benchmark return \\(\\bar\\mu\\) set 3 times expected return minimum variance portfolio. encourage verify correct.","code":"\nindex_prices <- index_prices %>%\n  group_by(symbol) %>%\n  mutate(n = n()) %>%\n  ungroup() %>%\n  filter(n == max(n)) %>%\n  select(-n)\n\nreturns <- index_prices %>%\n  mutate(month = floor_date(date, \"month\")) %>%\n  group_by(symbol, month) %>%\n  summarize(price = last(adjusted), .groups = \"drop_last\") %>%\n  mutate(ret = price / lag(price) - 1) %>%\n  drop_na(ret) %>%\n  select(-price)\nreturns_matrix <- returns %>%\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) %>%\n  select(-month)\n\nsigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\n\ntibble(expected_ret = t(mvp_weights) %*% mu, \n       volatility = sqrt(t(mvp_weights) %*% sigma %*% mvp_weights))## # A tibble: 1 × 2\n##   expected_ret[,1] volatility[,1]\n##              <dbl>          <dbl>\n## 1          0.00838         0.0314\nmu_bar <- 3 * t(mvp_weights) %*% mu\n\nC <- as.numeric(t(iota) %*% solve(sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(sigma) %*% mu)\n\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights + lambda_tilde / 2 * (solve(sigma) %*% mu - D / C * solve(sigma) %*% iota)"},{"path":"introduction-to-tidy-finance.html","id":"the-efficient-frontier","chapter":"1 Introduction to Tidy Finance","heading":"1.5 The efficient frontier","text":"two mutual fund separation theorem states soon two efficient portfolios (minimum variance portfolio efficient portfolio another required level expected returns like ), can characterize entire efficient frontier combining two portfolios. code implements construction efficient frontier, characterizes highest expected return achievable level risk. understand code better, make sure familiarize inner workings loop.Finally, simple visualize efficient frontier alongside two efficient portfolios within one, powerful figure using ggplot2. also add individual stocks call.\nblack line indicates efficient frontier: set portfolios mean-variance efficient investor choose . Compare performance relative individual assets (blue dots) - become clear diversifying yields massive performance gains (least long take parameters \\(\\Sigma\\) \\(\\mu\\) given).","code":"\nc <- seq(from = -0.4, to = 1.9, by = 0.01)\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\n\nfor (i in seq_along(c)) {\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights\n  res$mu[i] <- 12 * 100 * t(w) %*% mu\n  res$sd[i] <- 12 * sqrt(100) * sqrt(t(w) %*% sigma %*% w)\n}\nres %>%\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point( # locate the minimum variance and efficient portfolio\n    data = res %>% filter(c %in% c(0, 1)),\n    color = \"red\",\n    size = 4\n  ) +\n  geom_point( # locate the individual assets\n    data = tibble(mu = 12 * 100 * mu, sd = 12 * 10 * sqrt(diag(sigma))),\n    aes(y = mu, x = sd), color = \"blue\", size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Dow Jones asset returns and efficient frontier\",\n    subtitle = \"Red dots indicate the location of the minimum variance and efficient tangency portfolio\"\n  )"},{"path":"introduction-to-tidy-finance.html","id":"exercises","chapter":"1 Introduction to Tidy Finance","heading":"1.6 Exercises","text":"Download daily prices another stock market ticker choice Yahoo!Finance tq_get tidyquant package. Plot two time series ticker’s un-adjusted adjusted closing prices. Explain differences.Compute daily net returns asset visualize distribution daily returns histogram. Also, use geom_vline() add dashed red line indicates 5% quantile daily returns within histogram. Compute summary statistics (mean, standard deviation, minimum maximum) daily returnsTake code generalize can perform computations arbitrary vector tickers (e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\")). Automate download, plot price time series, create table return summary statistics arbitrary number assets.Consider research question: days high aggregate trading volume often also days large absolute price changes? Find appropriate visualization analyze question.Compute monthly returns downloaded stock market prices. Compute vector historical average returns sample variance-covariance matrix. compute minimum variance portfolio weights portfolio volatility average returns, visualize mean-variance efficient frontier. Choose one assets identify portfolio yields historical volatility achieves highest possible average return.portfolio choice analysis, restricted sample assets trading every single day since 2000. decision problem want infer future expected portfolio performance results?efficient frontier characterizes portfolios highest expected return different levels risk, .e., standard deviation. Identify portfolio highest expected return per standard deviation. Hint: ratio expected return standard deviation important concept Finance.","code":""},{"path":"accessing-managing-financial-data.html","id":"accessing-managing-financial-data","chapter":"2 Accessing & managing financial data","heading":"2 Accessing & managing financial data","text":"chapter, propose way organize financial data. Everybody, experience data, also familiar storing data various formats like CSV, XLS, XLSX, delimited value stores. Reading saving data can become cumbersome case using different data formats, across different projects, well across different programming languages. Moreover, storing data delimited files often leads problems respect column type consistency. instance, date-type columns frequently lead inconsistencies across different data formats programming languages.chapter shows import different data sets. Specifically, data comes application programming interface (API) Yahoo!Finance, downloaded standard CSV files, XLSX file stored public Google drive repositories, SQL database connection. store data single database, serves source data subsequent chapters.First, load global packages use throughout chapter. Later , load packages sections need .Moreover, initially define date range fetch store financial data, making future data updates tractable. case need another time frame, need adjust dates. data starts 1960 since asset pricing studies use data 1962 .","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nstart_date <- as.Date(\"1960-01-01\")\nend_date <- as.Date(\"2020-12-31\")"},{"path":"accessing-managing-financial-data.html","id":"fama-french-data","chapter":"2 Accessing & managing financial data","heading":"2.1 Fama-French data","text":"start downloading famous Fama-French factors (e.g., (Eugene F. Fama French 1993)) portfolio returns commonly used empirical asset pricing. Fortunately, neat package Nelson Areal allows us easily access data: frenchdata package provides functions download read data sets Prof. Kenneth French finance data library.can use main function package download monthly Fama-French factors. set 3 Factors includes return time series market, size, value factors alongside risk-free rates. Note manual work correctly parse columns scale appropriately raw Fama-French data comes unpractical data format. precise descriptions variables, suggest consulting Prof. Kenneth French finance data library directly. site, check raw data files appreciate time saved frenchdata.straightforward download corresponding daily Fama-French factors function.subsequent chapter, also use 49 monthly industry portfolios, let us fetch data, .worth taking look available portfolio return time series Kenneth French’s homepage. check sets calling frenchdata::get_french_data_list().","code":"\nlibrary(frenchdata)\nfactors_ff_monthly <- download_french_data(\"Fama/French 3 Factors\")$subsets$data[[1]] %>%\n  transmute(\n    month = floor_date(ymd(paste0(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) %>%\n  filter(month >= start_date & month <= end_date)\nfactors_ff_daily <- download_french_data(\"Fama/French 3 Factors [Daily]\")$subsets$data[[1]] %>%\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) %>%\n  filter(date >= start_date & date <= end_date)\nindustries_ff_monthly <- download_french_data(\"49 Industry Portfolios\")$subsets$data[[1]] %>%\n  mutate(month = floor_date(ymd(paste0(date, \"01\")), \"month\")) %>%\n  mutate(across(where(is.numeric), ~ . / 100)) %>%\n  select(month, everything(), -date) %>%\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"q-factors","chapter":"2 Accessing & managing financial data","heading":"2.2 q-factors","text":"recent years, academic discourse experienced rise alternative factor models, e.g., form (Hou, Xue, Zhang 2014) q-factor model. refer extended background information provided original authors information. q factors can downloaded directly authors’ homepage within read_csv().also need adjust data. First, discard information use . , rename columns “R_”-prescript using regular expressions write column names lower case. can try sticking consistent style naming objects, try illustrate - emphasis try. can check style guides available online, e.g., Hadley Wickham’s tidyverse style guide.","code":"\nfactors_q_monthly <- read_csv(\"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2020.csv\") %>%\n  mutate(month = as.Date(paste(year, month, \"01\", sep = \"-\"))) %>%\n  select(-R_F, -R_MKT, -year) %>%\n  rename_with(~ gsub(\"R_\", \"\", .)) %>%\n  rename_with(~ str_to_lower(.)) %>%\n  mutate(across(-month, ~ . / 100)) %>%\n  filter(month >= start_date & month <= end_date)"},{"path":"accessing-managing-financial-data.html","id":"macroeconomic-predictors","chapter":"2 Accessing & managing financial data","heading":"2.3 Macroeconomic predictors","text":"next data source set macroeconomic variables often used predictors equity premium. (Welch Goyal 2008) comprehensively reexamine performance variables suggested academic literature good predictors equity premium. authors host data updated 2020 Amit Goyal’s website. Since data .xlsx-file stored public Google drive location, need additional packages access data directly R session. Therefore, load readxl read .xlsx-file googledrive Google drive connection.Usually, need authenticate interact Google drive directly R. Since data stored via public link, can proceed without authentication.drive_download() function googledrive package allows us download data store locally.Next, read new data transform columns variables later use. can consult material Amit Goyal’s website definitions variables transformations.Finally, reading macro predictors memory, remove raw data file temporary storage.","code":"\nlibrary(readxl)\nlibrary(googledrive)\ndrive_deauth()\ndrive_download(\"https://drive.google.com/file/d/1ACbhdnIy0VbCWgsnXkjcddiV8HF4feWv/view\",\n  path = \"data/macro_predictors.xlsx\"\n)\nmacro_predictors <- read_xlsx(\"data/macro_predictors.xlsx\", \n                              sheet = \"Monthly\") %>%\n  mutate(month = ym(yyyymm)) %>%\n  filter(month >= start_date & month <= end_date) %>%\n  mutate(across(where(is.character), as.numeric)) %>%\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) %>%\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) %>%\n  drop_na()\nfile.remove(\"data/macro_predictors.xlsx\")## [1] TRUE"},{"path":"accessing-managing-financial-data.html","id":"setting-up-a-database","chapter":"2 Accessing & managing financial data","heading":"2.4 Setting up a database","text":"Now downloaded data web memory R session, let us set database store information future use. use data stored database throughout following chapters, alternatively implement different strategy replace respective code.many ways set organize database, depending use case. purpose, efficient way use SQLite database, C-language library implements small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note SQL (Structured Query Language) standard language accessing manipulating databases, heavily inspired dplyr functions. refer tutorial information SQL.two packages make working SQLite R simple: RSQLite embeds SQLite database engine R dbplyr database back-end dplyr. packages allow set database remotely store tables use remote database tables -memory data frames automatically converting dplyr SQL. Check RSQLite dbplyr vignettes information.SQLite database easily created - code really . Note use extended_types option enable date types storing fetching data, otherwise date columns stored integer values.Next, create remote table monthly Fama-French factor data.can use remote table -memory data frame building connection via tbl().dplyr calls evaluated lazily, .e., data memory R session, actually, database work. can see noticing output show number rows. fact, following code chunk fetches top 10 rows database printing.want whole table memory, need collect() . see regularly load data memory next chapters.last couple code chunks really organize simple database! can also share SQLite database across devices programming languages.move next data source, let us also store four tables new SQLite database.now , need access data stored database follow three steps: () Establish connection SQLite database, (ii) call table want extract, (iii) collect data. convenience, following steps show need compact fashion.","code":"\nlibrary(RSQLite)\nlibrary(dbplyr)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", \n                          extended_types = TRUE)\nfactors_ff_monthly %>%\n  dbWriteTable(tidy_finance, \"factors_ff_monthly\", ., overwrite = TRUE)\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\nfactors_ff_monthly_db %>%\n  select(month, rf)## # Source:   SQL [?? x 2]\n## # Database: sqlite 3.38.5 [C:\\Users\\ncj140\\Dropbox\\Projects\\tidy_finance\\data\\tidy_finance.sqlite]\n##   month          rf\n##   <date>      <dbl>\n## 1 1960-01-01 0.0033\n## 2 1960-02-01 0.0029\n## 3 1960-03-01 0.0035\n## 4 1960-04-01 0.0019\n## 5 1960-05-01 0.0027\n## # … with more rows\nfactors_ff_monthly_db %>%\n  select(month, rf) %>%\n  collect()## # A tibble: 732 × 2\n##   month          rf\n##   <date>      <dbl>\n## 1 1960-01-01 0.0033\n## 2 1960-02-01 0.0029\n## 3 1960-03-01 0.0035\n## 4 1960-04-01 0.0019\n## 5 1960-05-01 0.0027\n## # … with 727 more rows\nfactors_ff_daily %>%\n  dbWriteTable(tidy_finance, \"factors_ff_daily\", ., overwrite = TRUE)\n\nindustries_ff_monthly %>%\n  dbWriteTable(tidy_finance, \"industries_ff_monthly\", ., overwrite = TRUE)\n\nfactors_q_monthly %>%\n  dbWriteTable(tidy_finance, \"factors_q_monthly\", ., overwrite = TRUE)\n\nmacro_predictors %>%\n  dbWriteTable(tidy_finance, \"macro_predictors\", ., overwrite = TRUE)\nlibrary(tidyverse)\nlibrary(RSQLite)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", \n                          extended_types = TRUE)\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly <- factors_q_monthly %>% collect()"},{"path":"accessing-managing-financial-data.html","id":"accessing-wrds","chapter":"2 Accessing & managing financial data","heading":"2.5 Accessing WRDS","text":"Wharton Research Data Services (WRDS) widely used source asset firm-specific financial data used academic settings. WRDS data platform provides data validation, flexible delivery options, access many different data sources. data WRDS also organized SQL database, although use PostgreSQL engine. database engine just easy handle R SQLite. use RPostgres package establish connection WRDS database. Note also use odbc package connect PostgreSQL database, need install appropriate drivers . RPostgres already contains suitable driver.establish connection, use function dbConnect() following arguments. Note need replace user password fields credentials. defined system variables purpose book obviously want share credentials rest world.remote connection WRDS useful. Yet, database contains many different databases tables. can check WRDS homepage identify table’s name looking (go beyond exposition). Alternatively, can also query data structure function dbSendQuery(). interested, exercise based WRDS’ tutorial “Querying WRDS Data using R”. Furthermore, penultimate section chapter shows investigate structure databases.","code":"\nlibrary(RPostgres)\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)"},{"path":"accessing-managing-financial-data.html","id":"downloading-and-preparing-crsp","chapter":"2 Accessing & managing financial data","heading":"2.6 Downloading and preparing CRSP","text":"Center Research Security Prices (CRSP) provides widely used data US stocks. use wrds connection object just created first access monthly CRSP return data. Actually, need three tables get desired data: () CRSP monthly security file,identifying information,(iii) delisting information.use three remote tables fetch data want put local database. Just , idea let WRDS database work just download data actually need. apply common filters data selection criteria narrow data interest: () keep data time windows interest, (ii) keep US-listed stocks identified via share codes 10 11, (iii) keep months valid permno-specific information msenames. addition, add delisting reasons returns. can read great textbook (Bali, Engle, Murray 2016) (BEM) extensive discussion filters apply code .Now, relevant monthly return data memory proceed preparing data future analyses. perform preparation step current stage since want avoid executing mutations every time use data subsequent chapters.first additional variable create market capitalization (mktcap). Note keep market cap millions US dollars just convenience (want print huge numbers figures tables). Moreover, set zero market cap missing makes conceptually little sense (.e., firm bankrupt).next variable frequently use one-month lagged market capitalization. Lagged market capitalization typically used compute value-weighted portfolio returns, demonstrate later chapter. simple consistent way add column lagged market cap values add one month observation join information monthly CRSP data.wonder use lag() function, e.g., via crsp_monthly %>% group_by(permno) %>% mutate(mktcap_lag = lag(mktcap)), take look exercises.Next, follow BEM transforming listing exchange codes explicit exchange names.Similarly, transform industry codes industry descriptions following BEM. Notice also categorizations industries (e.g., Eugene Fama Kenneth French) commonly used.also construct returns adjusted delistings described BEM. transformation, can drop delisting returns codes.Next, compute excess returns subtracting monthly risk-free rate provided Fama-French data. base analyses excess returns, can drop adjusted returns risk-free rate tibble. Note ensure excess returns bounded -1 less -100% return make conceptually sense.Since excess returns market capitalization crucial analyses, can safely exclude observations missing returns market capitalization.Finally, store monthly CRSP file database.","code":"\nmsf_db <- tbl(wrds, in_schema(\"crsp\", \"msf\"))\nmsf_db## # Source:   table<\"crsp\".\"msf\"> [?? x 21]\n## # Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n##   cusip    permno permco issuno hexcd hsiccd date      \n##   <chr>     <dbl>  <dbl>  <dbl> <dbl>  <dbl> <date>    \n## 1 68391610  10000   7952  10396     3   3990 1985-12-31\n## 2 68391610  10000   7952  10396     3   3990 1986-01-31\n## 3 68391610  10000   7952  10396     3   3990 1986-02-28\n## 4 68391610  10000   7952  10396     3   3990 1986-03-31\n## 5 68391610  10000   7952  10396     3   3990 1986-04-30\n## # … with more rows, and 14 more variables:\n## #   bidlo <dbl>, askhi <dbl>, prc <dbl>, vol <dbl>,\n## #   ret <dbl>, bid <dbl>, ask <dbl>, shrout <dbl>,\n## #   cfacpr <dbl>, cfacshr <dbl>, altprc <dbl>,\n## #   spread <dbl>, altprcdt <date>, retx <dbl>\nmsenames_db <- tbl(wrds, in_schema(\"crsp\", \"msenames\"))\nmsenames_db## # Source:   table<\"crsp\".\"msenames\"> [?? x 21]\n## # Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n##   permno namedt     nameendt   shrcd exchcd siccd\n##    <dbl> <date>     <date>     <dbl>  <dbl> <dbl>\n## 1  10000 1986-01-07 1986-12-03    10      3  3990\n## 2  10000 1986-12-04 1987-03-09    10      3  3990\n## 3  10000 1987-03-10 1987-06-11    10      3  3990\n## 4  10001 1986-01-09 1993-11-21    11      3  4920\n## 5  10001 1993-11-22 2004-06-09    11      3  4920\n## # … with more rows, and 15 more variables:\n## #   ncusip <chr>, ticker <chr>, comnam <chr>,\n## #   shrcls <chr>, tsymbol <chr>, naics <chr>,\n## #   primexch <chr>, trdstat <chr>, secstat <chr>,\n## #   permco <dbl>, compno <dbl>, issuno <dbl>,\n## #   hexcd <dbl>, hsiccd <dbl>, cusip <chr>\nmsedelist_db <- tbl(wrds, in_schema(\"crsp\", \"msedelist\"))\nmsedelist_db## # Source:   table<\"crsp\".\"msedelist\"> [?? x 19]\n## # Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n##   permno dlstdt     dlstcd nwperm nwcomp nextdt    \n##    <dbl> <date>      <dbl>  <dbl>  <dbl> <date>    \n## 1  10000 1987-06-11    560      0      0 1987-06-12\n## 2  10001 2017-08-03    233      0      0 NA        \n## 3  10002 2013-02-15    231  35263   1658 NA        \n## 4  10003 1995-12-15    231  10569   8477 NA        \n## 5  10005 1991-07-11    560      0      0 1991-07-12\n## # … with more rows, and 13 more variables:\n## #   dlamt <dbl>, dlretx <dbl>, dlprc <dbl>,\n## #   dlpdt <date>, dlret <dbl>, permco <dbl>,\n## #   compno <dbl>, issuno <dbl>, hexcd <dbl>,\n## #   hsiccd <dbl>, cusip <chr>, acperm <dbl>,\n## #   accomp <dbl>\ncrsp_monthly <- msf_db %>%\n  filter(date >= start_date & date <= end_date) %>%\n  inner_join(msenames_db %>%\n    filter(shrcd %in% c(10, 11)) %>%\n    select(permno, exchcd, siccd, namedt, nameendt), by = c(\"permno\")) %>%\n  filter(date >= namedt & date <= nameendt) %>%\n  mutate(month = floor_date(date, \"month\")) %>%\n  left_join(msedelist_db %>%\n    select(permno, dlstdt, dlret, dlstcd) %>%\n    mutate(month = floor_date(dlstdt, \"month\")), by = c(\"permno\", \"month\")) %>%\n  select(\n    permno, # Security identifier\n    date, # Date of the observation\n    month, # Month of the observation\n    ret, # Return\n    shrout, # Shares outstanding (in thousands)\n    altprc, # Last traded price in a month\n    exchcd, # Exchange code\n    siccd, # Industry code\n    dlret, # Delisting return\n    dlstcd # Delisting code\n  ) %>%\n  mutate(\n    month = as.Date(month),\n    shrout = shrout * 1000\n  ) %>%\n  collect()\ncrsp_monthly <- crsp_monthly %>%\n  mutate(\n    mktcap = abs(shrout * altprc) / 1000000,\n    mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap)\n  )\nmktcap_lag <- crsp_monthly %>%\n  mutate(month = month %m+% months(1)) %>%\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(mktcap_lag, by = c(\"permno\", \"month\"))\ncrsp_monthly <- crsp_monthly %>%\n  mutate(exchange = case_when(\n    exchcd %in% c(1, 31) ~ \"NYSE\",\n    exchcd %in% c(2, 32) ~ \"AMEX\",\n    exchcd %in% c(3, 33) ~ \"NASDAQ\",\n    TRUE ~ \"Other\"\n  ))\ncrsp_monthly <- crsp_monthly %>%\n  mutate(industry = case_when(\n    siccd >= 1 & siccd <= 999 ~ \"Agriculture\",\n    siccd >= 1000 & siccd <= 1499 ~ \"Mining\",\n    siccd >= 1500 & siccd <= 1799 ~ \"Construction\",\n    siccd >= 2000 & siccd <= 3999 ~ \"Manufacturing\",\n    siccd >= 4000 & siccd <= 4899 ~ \"Transportation\",\n    siccd >= 4900 & siccd <= 4999 ~ \"Utilities\",\n    siccd >= 5000 & siccd <= 5199 ~ \"Wholesale\",\n    siccd >= 5200 & siccd <= 5999 ~ \"Retail\",\n    siccd >= 6000 & siccd <= 6799 ~ \"Finance\",\n    siccd >= 7000 & siccd <= 8999 ~ \"Services\",\n    siccd >= 9000 & siccd <= 9999 ~ \"Public\",\n    TRUE ~ \"Missing\"\n  ))\ncrsp_monthly <- crsp_monthly %>%\n  mutate(ret_adj = case_when(\n    is.na(dlstcd) ~ ret,\n    !is.na(dlstcd) & !is.na(dlret) ~ dlret,\n    dlstcd %in% c(500, 520, 580, 584) |\n      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,\n    dlstcd == 100 ~ ret,\n    TRUE ~ -1\n  )) %>%\n  select(-c(dlret, dlstcd))\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly %>% select(month, rf), by = \"month\") %>%\n  mutate(\n    ret_excess = ret_adj - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) %>%\n  select(-ret_adj, -rf)\ncrsp_monthly <- crsp_monthly %>%\n  drop_na(ret_excess, mktcap, mktcap_lag)\ncrsp_monthly %>%\n  dbWriteTable(tidy_finance, \"crsp_monthly\", ., overwrite = TRUE)"},{"path":"accessing-managing-financial-data.html","id":"first-glimpse-of-the-crsp-sample","chapter":"2 Accessing & managing financial data","heading":"2.7 First glimpse of the CRSP sample","text":"move data sources, let us look descriptive statistics CRSP sample, main source stock returns.figure shows monthly number securities listing exchange time. NYSE longest history data, NASDAQ exhibits considerable large number stocks. number stocks AMEX decreasing steadily last couple decades. end 2020, 2300 stocks NASDAQ, 1244 NYSE, 147 AMEX 1 belongs category.Next, look aggregate market capitalization respective listing exchanges. ensure look meaningful data comparable time, adjust nominal values inflation. use familiar tidyquant package fetch consumer price index (CPI) data Federal Reserve Economic Data (FRED).CPI data might come handy point, also put local database.fact, can use tables database calculate aggregate market caps listing exchange plotting just memory. values end year(end_date) dollars ensure inter-temporal comparability. NYSE-listed stocks far largest market capitalization, followed NASDAQ-listed stocks.course, performing computation database really meaningful already required data memory. code chunk slower performing steps tables already memory. However, just want illustrate can perform many things database loading data memory.Next, look descriptive statistics industry. figure plots number stocks sample SIC industry classifiers. sample period, largest share stocks apparently Manufacturing, albeit number peaked somewhere 90s. number firms associated public administration seems category rise recent years, even surpassing Manufacturing end sample period.also compute market value stocks belonging respective industries. values terms billions end 2020 dollars. points time, manufacturing firms comprise largest portion market capitalization. Towards end sample, however, financial firms services begin make substantial portion market value.","code":"\ncrsp_monthly %>%\n  count(exchange, date) %>%\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date, to = end_date\n) %>%\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(crsp_monthly$month)]\n  )\ncpi_monthly %>%\n  dbWriteTable(tidy_finance, \"cpi_monthly\", ., overwrite = TRUE)\ntbl(tidy_finance, \"crsp_monthly\") %>%\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), by = \"month\") %>%\n  group_by(month, exchange) %>%\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi\n  ) %>%\n  collect() %>%\n  mutate(month = as.Date(month)) %>%\n  ggplot(aes(x = month, y = mktcap / 1000, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market value (billions of Dec 2020 Dollars) by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ncrsp_monthly_industry <- crsp_monthly %>%\n  left_join(cpi_monthly, by = \"month\") %>%\n  group_by(month, industry) %>%\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry %>%\n  ggplot(aes(x = month, y = securities, color = industry, linetype = industry)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\ncrsp_monthly_industry %>%\n  ggplot(aes(x = month, y = mktcap / 1000, color = industry, linetype = industry)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market value (billions of Dec 2020 Dollars) by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)"},{"path":"accessing-managing-financial-data.html","id":"daily-crsp-data","chapter":"2 Accessing & managing financial data","heading":"2.8 Daily CRSP data","text":"turn accounting data, also want provide proposal downloading daily CRSP data. monthly data typically fit memory can downloaded meaningful amount time, usually true daily return data. daily CRSP data file substantially larger monthly data can exceed 20GB. two important implications: hold daily return data memory (hence possible copy entire data set local database), experience, download usually crashes (never stops) much data WRDS cloud prepare send R session.solution challenge. many ‘big data’ problems, can split big task several smaller tasks easy handle. , instead downloading data many stocks , download data small batches stock consecutively. operations can implemented loops, download, prepare, store data single stock iteration. operation might nonetheless take couple hours, patient either way (often run code overnight). keep track progress, can use txtProgressBar(). Eventually, end 68 million rows daily return data. Note store identifying information actually need, namely permno, date, month alongside excess returns. thus ensure local database contains data actually use can load full daily data memory later.","code":"\ndsf_db <- tbl(wrds, in_schema(\"crsp\", \"dsf\"))\npermnos <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  distinct(permno) %>%\n  pull()\n\nprogress <- txtProgressBar(min = 0, max = length(permnos), initial = 0, style = 3)\nfor (j in 1:length(permnos)) {\n  permno_sub <- permnos[j]\n  crsp_daily_sub <- dsf_db %>%\n    filter(permno == permno_sub &\n      date >= start_date & date <= end_date) %>%\n    select(permno, date, ret) %>%\n    collect() %>%\n    drop_na()\n\n  if (nrow(crsp_daily_sub)) {\n    crsp_daily_sub <- crsp_daily_sub %>%\n      mutate(month = floor_date(date, \"month\")) %>%\n      left_join(factors_ff_daily %>%\n        select(date, rf), by = \"date\") %>%\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) %>%\n      select(permno, date, month, ret_excess)\n\n    if (j == 1) {\n      overwrite <- TRUE\n      append <- FALSE\n    } else {\n      overwrite <- FALSE\n      append <- TRUE\n    }\n\n    crsp_daily_sub %>%\n      dbWriteTable(tidy_finance, \"crsp_daily\", ., overwrite = overwrite, append = append)\n  }\n  setTxtProgressBar(progress, j)\n}\nclose(progress)\n\ncrsp_daily_db <- tbl(tidy_finance, \"crsp_daily\")"},{"path":"accessing-managing-financial-data.html","id":"preparing-compustat-data","chapter":"2 Accessing & managing financial data","heading":"2.9 Preparing Compustat data","text":"Firm accounting data important source information use portfolio analyses subsequent chapters. commonly used source firm financial information Compustat provided S&P Global Market Intelligence, global data vendor provides financial, statistical, market information active inactive companies throughout world. US Canadian companies, annual history available back 1950 quarterly well monthly histories date back 1962.access Compustat data, can tap WRDS, hosts funda table contains annual firm-level information North American companies.follow typical filter conventions pull data actually need: () get industrial fundamental data (.e., ignore financial services) (ii) standard format (.e., consolidated information standard presentation), (iii) data desired time window.Next, calculate book value preferred stock equity inspired variable definition Ken French’s data library. Note set negative zero equity missing makes conceptually little sense (.e., firm bankrupt).keep last available information firm-year group. Note datadate defines time corresponding financial data refers (e.g., annual report December 31, 2020). Therefore, datadate date data made available public. Check exercises insights peculiarities datadate.last step, already done preparing firm fundamentals. Thus, can store local database.","code":"\nfunda_db <- tbl(wrds, in_schema(\"comp\", \"funda\"))\ncompustat <- funda_db %>%\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" &\n      consol == \"C\" &\n      datadate >= start_date & datadate <= end_date\n  ) %>%\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk # Preferred stock par value\n  ) %>%\n  collect()\ncompustat <- compustat %>%\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be <= 0, as.numeric(NA), be)\n  )\ncompustat <- compustat %>%\n  mutate(year = year(datadate)) %>%\n  group_by(gvkey, year) %>%\n  filter(datadate == max(datadate)) %>%\n  ungroup()\ncompustat %>%\n  dbWriteTable(tidy_finance, \"compustat\", ., overwrite = TRUE)"},{"path":"accessing-managing-financial-data.html","id":"merging-crsp-with-compustat","chapter":"2 Accessing & managing financial data","heading":"2.10 Merging CRSP with Compustat","text":"Unfortunately, CRSP Compustat use different keys identify stocks firms. CRSP uses permno stocks, Compustat uses gvkey identify firms. Fortunately, curated matching table WRDS allows us merge CRSP Compustat, create connection CRSP-Compustat Merged table (provided CRSP).linking table contains links CRSP Compustat identifiers various approaches. However, need make sure keep relevant correct links, following description outlined BEM. Note also currently active links end date, just enter current date via Sys.Date().use links create new table mapping stock identifier, firm identifier, month. add links Compustat gvkey monthly stock data.last step, update previously prepared monthly CRSP file linking information local database.close chapter, let us look interesting descriptive statistic data. book value equity plays crucial role many asset pricing applications, interesting know many stocks information available. Hence, figure plots share securities book equity values exchange. turns coverage pretty bad AMEX- NYSE-listed stocks 60s hovers around 80% periods thereafter. can ignore erratic coverage securities belong category since handful anyway sample.","code":"\nccmxpf_linktable_db <- tbl(wrds, in_schema(\"crsp\", \"ccmxpf_linktable\"))\nccmxpf_linktable <- ccmxpf_linktable_db %>%\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) %>%\n  select(permno = lpermno, gvkey, linkdt, linkenddt) %>%\n  collect() %>%\n  mutate(linkenddt = replace_na(linkenddt, Sys.Date()))\nccmxpf_linktable## # A tibble: 31,770 × 4\n##   permno gvkey  linkdt     linkenddt \n##    <dbl> <chr>  <date>     <date>    \n## 1  25881 001000 1970-11-13 1978-06-30\n## 2  10015 001001 1983-09-20 1986-07-31\n## 3  10023 001002 1972-12-14 1973-06-05\n## 4  10031 001003 1983-12-07 1989-08-16\n## 5  54594 001004 1972-04-24 2022-07-07\n## # … with 31,765 more rows\nccm_links <- crsp_monthly %>%\n  inner_join(ccmxpf_linktable, by = \"permno\") %>%\n  filter(!is.na(gvkey) & (date >= linkdt & date <= linkenddt)) %>%\n  select(permno, gvkey, date)\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(ccm_links, by = c(\"permno\", \"date\"))\ncrsp_monthly %>%\n  dbWriteTable(tidy_finance, \"crsp_monthly\", ., overwrite = TRUE)\ncrsp_monthly %>%\n  group_by(permno, year = year(month)) %>%\n  filter(date == max(date)) %>%\n  ungroup() %>%\n  left_join(compustat, by = c(\"gvkey\", \"year\")) %>%\n  group_by(exchange, year) %>%\n  summarize(share = n_distinct(permno[!is.na(be)]) / n_distinct(permno)) %>%\n  ggplot(aes(x = year, y = share, color = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"End-of-year share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) + \n  coord_cartesian(ylim = c(0, 1))"},{"path":"accessing-managing-financial-data.html","id":"managing-sqlite-databases","chapter":"2 Accessing & managing financial data","heading":"2.11 Managing SQLite databases","text":"Finally, end data chapter, revisit SQLite database . drop database objects tables delete data tables, database file size remains unchanged SQLite just marks deleted objects free reserves space future uses. result, database file always grows size.optimize database file, can run VACUUM command database, rebuilds database frees unused space. can execute command database using dbSendQuery() function.VACUUM command actually performs couple additional cleaning steps, can read tutorial.Apart cleaning , might interested listing tables currently database. can via dbListTables() function.function comes handy unsure correct naming tables database.","code":"\ndbSendQuery(tidy_finance, \"VACUUM\")## <SQLiteResult>\n##   SQL  VACUUM\n##   ROWS Fetched: 0 [complete]\n##        Changed: 0\ndbListTables(tidy_finance)## Warning: Closing open result set, pending rows##  [1] \"beta\"                  \"compustat\"            \n##  [3] \"cpi_monthly\"           \"crsp_daily\"           \n##  [5] \"crsp_monthly\"          \"factors_ff_daily\"     \n##  [7] \"factors_ff_monthly\"    \"factors_q_monthly\"    \n##  [9] \"industries_ff_monthly\" \"macro_predictors\""},{"path":"accessing-managing-financial-data.html","id":"some-tricks-for-postgresql-databases","chapter":"2 Accessing & managing financial data","heading":"2.12 Some tricks for PostgreSQL databases","text":"mentioned , WRDS database runs PostgreSQL rather SQLite. Finding right tables data needs can tricky WRDS PostgreSQL instance, tables organized schemas. wonder purpose schemas , check documetation. instance, want find tables live crsp schema, runThis operation returns list tables belong crsp family WRSD, e.g. <Id> schema = crsp, table = msenames. Similarly, can fetch list tables belong comp family viaIf want get schemas, run","code":"\ndbListObjects(wrds, Id(schema = \"crsp\"))\ndbListObjects(wrds, Id(schema = \"comp\"))\ndbListObjects(wrds)"},{"path":"accessing-managing-financial-data.html","id":"exercises-1","chapter":"2 Accessing & managing financial data","heading":"2.13 Exercises","text":"Download monthly Fama-French factors manually Ken French’s data library read via read_csv(). Validate get data via frenchdata package.Check structure WRDS database sending queries spirit “Querying WRDS Data using R” verify output dbListObjects(). many tables associated CRSP? Can identify stored within msp500?Compute mkt_cap_lag using lag(mktcap) rather joins . Filter rows lag-based market capitalization measure different one computed . different?main part, look distribution market capitalization across exchanges industries. Now, plot average market capitalization firms exchange industry. find?datadate refers date fiscal year corresponding firm refers . Count number observations Compustat month date variable. find? finding suggest pooling observations fiscal year?Go back original Compustat data funda_db extract rows firm multiple rows fiscal year. reason observations?Repeat analysis market capitalization book equity, computed Compustat data. , used matched sample plot book equity market capitalization. two variables related?","code":""},{"path":"beta-estimation.html","id":"beta-estimation","chapter":"3 Beta estimation","heading":"3 Beta estimation","text":"chapter, introduce important concept financial economics: exposure individual stock changes market portfolio. According Capital Asset Pricing Model (CAPM), cross-sectional variation expected asset returns function covariance excess return asset excess return market portfolio. regression coefficient market returns excess returns usually called market beta.\nchapter, show estimation procedure market betas. go details foundations market beta simply refer treatment CAPM information. Instead, provide details functions use compute results. particular, leverage useful computational concepts: rolling-window estimation parallelization.use following packages throughout chapter:","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(slider)\nlibrary(scales)\nlibrary(furrr)"},{"path":"beta-estimation.html","id":"estimating-beta-using-monthly-returns","chapter":"3 Beta estimation","heading":"3.1 Estimating beta using monthly returns","text":"estimation procedure based rolling-window estimation may use either monthly daily returns different window lengths. First, let us start loading monthly data prepared previous chapter SQLite-database introduced chapter “Accessing & managing financial data”.estimate CAPM equation\n\\[\nr_{, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{, t}\n\\]\nregress excess stock returns ret_excess excess returns market portfolio mkt_excess.\nR provides simple solution estimate (linear) models function lm(). lm() requires formula input specified compact symbolic form. expression form y ~ model interpreted specification response y modeled linear predictor specified symbolically model. model consists series terms separated + operators. addition standard linear models, lm() provides lot flexibility. check documentation information. start, restrict data time series observations CRSP correspond Apple’s stock (.e., permno 14593 Apple) compute \\(\\alpha_i\\) well \\(\\beta_i\\).lm() returns object class lm contains information usually care linear models. summary() returns overview estimated parameters. coefficients(fit) return estimated coefficients. output indicates Apple moves excessively market estimated \\(\\beta_i\\) one (\\(\\hat\\beta_i\\) = 1.4).","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  select(permno, month, industry, ret_excess, mkt_excess)\nfit <- lm(ret_excess ~ mkt_excess,\n  data = crsp_monthly %>%\n    filter(permno == \"14593\")\n)\n\nsummary(fit)## \n## Call:\n## lm(formula = ret_excess ~ mkt_excess, data = crsp_monthly %>% \n##     filter(permno == \"14593\"))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5167 -0.0610  0.0009  0.0643  0.3940 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.01051    0.00532    1.98    0.049 *  \n## mkt_excess   1.40081    0.11748   11.92   <2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.115 on 478 degrees of freedom\n## Multiple R-squared:  0.229,  Adjusted R-squared:  0.228 \n## F-statistic:  142 on 1 and 478 DF,  p-value: <2e-16"},{"path":"beta-estimation.html","id":"rolling-window-estimation","chapter":"3 Beta estimation","heading":"3.2 Rolling-window estimation","text":"estimated regression coefficients example, scale estimation \\(\\beta_i\\) whole different level perform rolling-window estimations entire CRSP sample. following function implements CAPM regression data frame (part thereof) containing least min_obs observations avoid huge fluctuations time series short. condition violated, function returns missing value.Next, define function rolling estimation. perform rolling-window estimation, use slider package Davis Vaughan. slide_period function able handle months window input straightforward manner. thus avoid using time-series package (e.g., zoo) converting data fit package functions, rather stay world tibbles.following function takes input data slides across month vector, considering total months months. function essentially performs three steps: () combine rows single data frame (comes handy case daily data), (ii) compute betas sliding across months, (iii) return tibble months corresponding beta estimates (particularly useful case daily data).\ndemonstrate , can also apply function daily returns data.attack whole CRSP sample, let us focus couple examples well-known firms.want estimate rolling betas Apple, can use mutate().\ntake total 5 years data require least 48 months return data compute betas.\nCheck exercises want ot compute beta different time periods.actually quite simple perform rolling-window estimation arbitrary number stocks, visualize following code chunk.","code":"\nestimate_capm <- function(data, min_obs = 1) {\n  if (nrow(data) < min_obs) {\n    beta <- as.numeric(NA)\n  } else {\n    fit <- lm(ret_excess ~ mkt_excess, data = data)\n    beta <- as.numeric(fit$coefficients[2])\n  }\n  return(beta)\n}\nroll_capm_estimation <- function(data, months, min_obs) {\n  data <- bind_rows(data) %>%\n    arrange(month)\n\n  betas <- slide_period_vec(\n    .x = data,\n    .i = data$month,\n    .period = \"month\",\n    .f = ~ estimate_capm(., min_obs),\n    .before = months - 1,\n    .complete = FALSE\n  )\n\n  tibble(\n    month = unique(data$month),\n    beta = betas\n  )\n}\nexamples <- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n)\nbeta_example <- crsp_monthly %>%\n  filter(permno == examples$permno[1]) %>%\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) %>%\n  drop_na()\nbeta_example## # A tibble: 433 × 6\n##   permno month      industry      ret_excess mkt_excess\n##    <dbl> <date>     <chr>              <dbl>      <dbl>\n## 1  14593 1984-12-01 Manufacturing     0.170      0.0184\n## 2  14593 1985-01-01 Manufacturing    -0.0108     0.0799\n## 3  14593 1985-02-01 Manufacturing    -0.152      0.0122\n## 4  14593 1985-03-01 Manufacturing    -0.112     -0.0084\n## 5  14593 1985-04-01 Manufacturing    -0.0467    -0.0096\n## # … with 428 more rows, and 1 more variable:\n## #   beta <dbl>\nbeta_examples <- crsp_monthly %>%\n  inner_join(examples, by = \"permno\") %>%\n  group_by(permno) %>%\n  mutate(roll_capm_estimation(cur_data(), months = 60, min_obs = 48)) %>%\n  ungroup() %>%\n  select(permno, company, month, beta_monthly = beta) %>%\n  drop_na()\n\nbeta_examples %>%\n  ggplot(aes(x = month, y = beta_monthly, color = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of monthly data\"\n  )"},{"path":"beta-estimation.html","id":"parallelized-rolling-window-estimation","chapter":"3 Beta estimation","heading":"3.3 Parallelized rolling-window estimation","text":"Even though now just apply function using group_by() whole CRSP sample, advise computationally quite expensive.\nRemember perform rolling-window estimations across stocks time periods.\nHowever, estimation problem ideal scenario employ power parallelization.\nParallelization means split tasks perform rolling-window estimations across different workers (cores local machine).First, nest() data permno. Nested data means now list permno corresponding time series data.Next, ant apply roll_capm_estimation() function stock. situation ideal use case map(), takes list vector input returns object length input. case, map() returns single data frame time series beta estimates stock. Therefore, use unnest() transform list outputs tidy data frame.However, instead, want perform estimations rolling betas different stocks parallel. can use flexibility future package, use define want perform parallel estimation. Windows machine, makes sense define multisession, means separate R processes running background machine perform individual jobs. check documentation plan(), can also see ways resolve parallelization.Using eight cores, estimation sample around 25k stocks takes around 20 minutes. course, can speed things considerably cores available share workload powerful cores. Notice difference code ? need replace map() future_map().","code":"\ncrsp_monthly_nested <- crsp_monthly %>%\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_monthly_nested## # A tibble: 29,203 × 3\n##   permno industry      data              \n##    <dbl> <chr>         <list>            \n## 1  10000 Manufacturing <tibble [16 × 3]> \n## 2  10001 Utilities     <tibble [378 × 3]>\n## 3  10002 Finance       <tibble [324 × 3]>\n## 4  10003 Finance       <tibble [118 × 3]>\n## 5  10005 Mining        <tibble [65 × 3]> \n## # … with 29,198 more rows\ncrsp_monthly_nested %>%\n  inner_join(examples, by = \"permno\") %>%\n  mutate(beta = map(data, ~ roll_capm_estimation(., months = 60, min_obs = 48))) %>%\n  unnest(c(beta)) %>%\n  select(permno, month, beta_monthly = beta) %>%\n  drop_na()## # A tibble: 1,362 × 3\n##   permno month      beta_monthly\n##    <dbl> <date>            <dbl>\n## 1  10107 1990-03-01         1.39\n## 2  10107 1990-04-01         1.38\n## 3  10107 1990-05-01         1.43\n## 4  10107 1990-06-01         1.43\n## 5  10107 1990-07-01         1.45\n## # … with 1,357 more rows\nplan(multisession, workers = availableCores())\nbeta_monthly <- crsp_monthly_nested %>%\n  mutate(beta = future_map(data, ~ roll_capm_estimation(., months = 60, min_obs = 48))) %>%\n  unnest(c(beta)) %>%\n  select(permno, month, beta_monthly = beta) %>%\n  drop_na()"},{"path":"beta-estimation.html","id":"estimating-beta-using-daily-returns","chapter":"3 Beta estimation","heading":"3.4 Estimating beta using daily returns","text":"provide descriptive statistics beta estimates, implement estimation daily CRSP sample well.\nDepending application, might either use longer horizon beta estimates based monthly data shorter horizon estimates based daily returns.First, load daily CRSP data.\nNote sample large compared monthly data, make sure enough memory available.also need daily Fama-French market excess returns.make sure keep relevant data save memory space.\nHowever, note machine might enough memory read whole daily CRSP sample. case, refer exercises try working loops chapter 2.Just like , nest data permno parallelization.estimation looks like couple examples using map().\ndaily data, use function take 3 months data require least 50 daily return observations months.\nrestrictions help us retrieve somehow smooth coefficient estimates.sake completeness, tell session use multiple workers parallelization.code chunk beta estimation using daily returns now looks similar one monthly data. whole estimation takes around 30 minutes using eight cores 32gb memory.","code":"\ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\") %>%\n  collect()\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\") %>%\n  collect()\ncrsp_daily <- crsp_daily %>%\n  inner_join(factors_ff_daily, by = \"date\") %>%\n  select(permno, month, ret_excess, mkt_excess)\ncrsp_daily_nested <- crsp_daily %>%\n  nest(data = c(month, ret_excess, mkt_excess))\ncrsp_daily_nested %>%\n  inner_join(examples, by = \"permno\") %>%\n  mutate(beta_daily = map(data, ~ roll_capm_estimation(., months = 3, min_obs = 50))) %>%\n  unnest(c(beta_daily)) %>%\n  select(permno, month, beta_daily = beta) %>%\n  drop_na()## # A tibble: 1,543 × 3\n##   permno month      beta_daily\n##    <dbl> <date>          <dbl>\n## 1  10107 1986-05-01      0.898\n## 2  10107 1986-06-01      0.906\n## 3  10107 1986-07-01      0.822\n## 4  10107 1986-08-01      0.900\n## 5  10107 1986-09-01      1.01 \n## # … with 1,538 more rows\nplan(multisession, workers = availableCores())\nbeta_daily <- crsp_daily_nested %>%\n  mutate(beta_daily = future_map(data, ~ roll_capm_estimation(., months = 3, min_obs = 50))) %>%\n  unnest(c(beta_daily)) %>%\n  select(permno, month, beta_daily = beta) %>%\n  drop_na()"},{"path":"beta-estimation.html","id":"comparing-beta-estimates","chapter":"3 Beta estimation","heading":"3.5 Comparing beta estimates","text":"typical value stock betas? get feeling, illustrate dispersion estimated \\(\\hat\\beta_i\\) across different industries across time . first figure shows typical business models across industries imply different exposure general market economy. However, barely firms exhibit negative exposure market factor.Next, illustrate time-variation cross-section estimated betas. figure shows monthly deciles estimated betas (based monthly data) indicates interesting pattern: First, betas seem vary time sense periods, clear trend across deciles. Second, sample exhibits periods dispersion across stocks increases sense lower decile decreases upper decile increases, indicates stocks correlation market increases others decreases. Note also : stocks negative betas extremely rare exception.compare difference daily monthly data, combine beta estimates single table. , use table plot comparison beta estimates example stocks.estimates look expected. can see, really depends estimation window data frequency beta estimates turn .Finally, write estimates database can use later chapters.Whenever perform kind estimation, also makes sense rough plausibility tests. possible check plot share stocks beta estimates time.\ndescriptive helps us discover potential errors data preparation estimation procedure.\ninstance, suppose gap output betas.\ncase, go back check previous steps.figure indicate troubles, let us move next check.also encourage everyone always look distributional summary statistics variables. can easily spot outliers weird distributions looking tables.Finally, since two different estimators theoretical object, expect estimators least positively correlated (although perfectly estimators based different sample periods).Indeed, find positive correlation beta estimates. subsequent chapters, mainly use estimates based monthly data readers able replicate due potential memory limitations might arise daily data.","code":"\ncrsp_monthly %>%\n  left_join(beta_monthly, by = c(\"permno\", \"month\")) %>%\n  drop_na(beta_monthly) %>%\n  group_by(industry, permno) %>%\n  summarize(beta = mean(beta_monthly)) %>%\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Average beta estimates by industry\"\n  )\nbeta_monthly %>%\n  drop_na(beta_monthly) %>%\n  group_by(month) %>%\n  summarize(\n    x = quantile(beta_monthly, seq(0.1, 0.9, 0.1)),\n    quantile = 100 * seq(0.1, 0.9, 0.1),\n    .groups = \"drop\"\n  ) %>%\n  ggplot(aes(x = month, y = x, color = as_factor(quantile))) +\n  geom_line() +\n  labs(\n    x = NULL, y = \"Beta\", color = NULL,\n    title = \"Distribution of estimated betas\",\n    subtitle = \"Monthly deciles for CRSP cross-section\"\n  )\nbeta <- beta_monthly %>%\n  full_join(beta_daily, by = c(\"permno\", \"month\")) %>%\n  arrange(permno, month)\n\nbeta %>%\n  inner_join(examples, by = \"permno\") %>%\n  pivot_longer(cols = c(beta_monthly, beta_daily)) %>%\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  facet_wrap(~company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Comparison of beta estimates using 5 years of monthly and 3 months of daily data\"\n  )## Warning: Removed 46 row(s) containing missing values\n## (geom_path).\nbeta %>%\n  dbWriteTable(tidy_finance, \"beta\", ., overwrite = TRUE)\nbeta_long <- crsp_monthly %>%\n  left_join(beta, by = c(\"permno\", \"month\")) %>%\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long %>%\n  group_by(month, name) %>%\n  summarize(share = sum(!is.na(value)) / n()) %>%\n  ggplot(aes(x = month, y = share, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\nbeta_long %>%\n  select(name, value) %>%\n  drop_na() %>%\n  group_by(name) %>%\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q25 = quantile(value, 0.25),\n    q50 = quantile(value, 0.50),\n    q75 = quantile(value, 0.75),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  )## # A tibble: 2 × 11\n##   name        mean    sd   min    q05   q25   q50   q75\n##   <chr>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n## 1 beta_daily 0.743 0.925 -43.7 -0.452 0.203 0.679  1.22\n## 2 beta_mont… 1.10  0.711 -13.0  0.123 0.631 1.03   1.47\n## # … with 3 more variables: q95 <dbl>, max <dbl>,\n## #   n <int>\nbeta %>%\n  select(beta_daily, beta_monthly) %>%\n  cor(., use = \"complete.obs\")##              beta_daily beta_monthly\n## beta_daily        1.000        0.322\n## beta_monthly      0.322        1.000"},{"path":"beta-estimation.html","id":"exercises-2","chapter":"3 Beta estimation","heading":"3.6 Exercises","text":"Compute beta estimates based monthly data using 1, 3, 5 years data impose minimum number observations 10, 28, 48 months return data, respectively. strongly correlated estimated betas?Compute beta estimates based monthly data using 5 years data impose different numbers minimum observations. share permno-month observations successful beta estimates vary across different requirements? find high correlation across estimated betas?Instead using future_map(), perform beta estimation loop (using either monthly daily data) subset 100 permnos choice. Verify get results parallelized code .Filter stocks negative betas. stocks frequently exhibit negative betas, resemble estimation errors?","code":""},{"path":"univariate-portfolio-sorts.html","id":"univariate-portfolio-sorts","chapter":"4 Univariate portfolio sorts","heading":"4 Univariate portfolio sorts","text":"chapter, dive portfolio sorts, one widely used statistical methodologies empirical asset pricing. key application portfolio sorts examine whether one variables can predict future excess returns. general, idea sort individual stocks portfolios, stocks within portfolio similar respect sorting variable, firm size. different portfolios represent well-diversified investments differ level sorting variable. can attribute differences return distribution impact sorting variable.\nstart introducing univariate portfolio sorts (sort based one characteristic). later chapter, tackle bivariate sorting.univariate portfolio sort considers one sorting variable \\(x_{t-1,}\\).\n, \\(\\) denotes stock \\(t-1\\) indicates characteristic observable investors time \\(t\\).\nobjective assess cross-sectional relation \\(x_{t-1,}\\) , typically, stock excess returns \\(r_{t,}\\) time \\(t\\) outcome variable.\nillustrate portfolio sorts work, use estimates market betas previous chapter sorting variable.current chapter relies following set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(scales)"},{"path":"univariate-portfolio-sorts.html","id":"data-preparation","chapter":"4 Univariate portfolio sorts","heading":"4.1 Data preparation","text":"start loading required data SQLite-database introduced chapter “Accessing & managing financial data”. particular, use monthly CRSP sample asset universe.\nform portfolios, use Fama-French factor returns compute risk-adjusted performance (.e., alpha).\nbeta tibble market betas computed previous chapter.keep relevant data CRSP sample.","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") %>%\n  collect()\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  select(permno, month, ret_excess, mkt_excess, mktcap_lag)\ncrsp_monthly## # A tibble: 3,225,079 × 5\n##   permno month      ret_excess mkt_excess mktcap_lag\n##    <dbl> <date>          <dbl>      <dbl>      <dbl>\n## 1  10000 1986-02-01    -0.262      0.0713       16.1\n## 2  10000 1986-03-01     0.359      0.0488       12.0\n## 3  10000 1986-04-01    -0.104     -0.0131       16.3\n## 4  10000 1986-05-01    -0.228      0.0462       15.2\n## 5  10000 1986-06-01    -0.0102     0.0103       11.8\n## # … with 3,225,074 more rows"},{"path":"univariate-portfolio-sorts.html","id":"sorting-by-market-beta","chapter":"4 Univariate portfolio sorts","heading":"4.2 Sorting by market beta","text":"Next, merge sorting variable return data. use one-month lagged betas sorting variable ensure sorts rely information available create portfolios.\nlag stock beta one month, add one month current date join resulting information return data.\nprocedure ensures month \\(t\\) information available month \\(t+1\\).\nmay tempted simply use call crsp_monthly %>% group_by(permno) %>% mutate(beta_lag = lag(beta))) instead.\nprocedure, however, work non-explicit missing values time series.first step conduct portfolio sorts calculate periodic breakpoints can use group stocks portfolios.\nsimplicity, start median single breakpoint.\ncompute value-weighted returns two resulting portfolios, means lagged market capitalization determines weight weighted.mean().","code":"\nbeta_lag <- beta %>%\n  mutate(month = month %m+% months(1)) %>%\n  select(permno, month, beta_lag = beta_daily) %>%\n  drop_na()\n\ndata_for_sorts <- crsp_monthly %>%\n  inner_join(beta_lag, by = c(\"permno\", \"month\"))\nbeta_portfolios <- data_for_sorts %>%\n  group_by(month) %>%\n  mutate(\n    breakpoint = median(beta_lag),\n    portfolio = case_when(\n      beta_lag <= breakpoint ~ \"low\",\n      beta_lag > breakpoint ~ \"high\"\n    )\n  ) %>%\n  group_by(month, portfolio) %>%\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\")"},{"path":"univariate-portfolio-sorts.html","id":"performance-evaluation","chapter":"4 Univariate portfolio sorts","heading":"4.3 Performance evaluation","text":"following figure shows monthly excess returns two portfolios.can construct long-short strategy based two portfolios: buy high-beta portfolio , time, short low-beta portfolio. Thereby, overall position market net-zero, .e., need invest money realize strategy absence frictions.compute average return corresponding standard error test whether long-short portfolio yields average positive negative excess returns. asset pricing literature, one typically uses Newey West (1987) \\(t\\)-statistics test null hypothesis average portfolio excess returns equal zero. implement test, compute average return via lm() employ coeftest function.results indicate reject null hypothesis average returns equal zero. portfolio strategy using median breakpoint hence yield abnormal returns. finding surprising reconsider CAPM? certainly . CAPM yields high beta stocks yield higher expected returns. portfolio sort implicitly mimics investment strategy finances high beta stocks shorting low beta stocks. Therefore, one expect average excess returns yield return risk-free rate.","code":"\nbeta_portfolios %>%\n  ggplot(aes(x = month, y = ret, fill = portfolio)) +\n  geom_col() +\n  facet_wrap(~portfolio, ncol = 1) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Monthly beta portfolio excess returns using median as breakpoint\"\n  ) +\n  theme(legend.position = \"none\")\nbeta_longshort <- beta_portfolios %>%\n  pivot_wider(month, names_from = portfolio, values_from = ret) %>%\n  mutate(long_short = high - low) %>%\n  left_join(factors_ff_monthly, by = \"month\")\nmodel_fit <- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest, lag = 6)## \n## t test of coefficients:\n## \n##              Estimate Std. Error t value Pr(>|t|)\n## (Intercept) -0.000171   0.001005   -0.17     0.86"},{"path":"univariate-portfolio-sorts.html","id":"functional-programming-for-portfolio-sorts","chapter":"4 Univariate portfolio sorts","heading":"4.4 Functional programming for portfolio sorts","text":"Now take portfolio sorts next level. want able sort stocks arbitrary number portfolios. case, functional programming handy: employ curly-curly-operator give us flexibility concerning variable use sorting, denoted var. use quantile() compute breakpoints n_portfolios. , assign portfolios stocks using findInterval() function. output following function new column contains number portfolio stock belongs.can use function sort stocks ten portfolios month using lagged betas compute value-weighted returns portfolio.\nNote transform portfolio column factor variable provides convenience figure construction .","code":"\nassign_portfolio <- function(data, var, n_portfolios) {\n  breakpoints <- data %>%\n    summarize(breakpoint = quantile({{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) %>%\n    pull(breakpoint) %>%\n    as.numeric()\n\n  data %>%\n    mutate(portfolio = findInterval({{ var }},\n      breakpoints,\n      all.inside = TRUE\n    )) %>%\n    pull(portfolio)\n}\nbeta_portfolios <- data_for_sorts %>%\n  group_by(month) %>%\n  mutate(\n    portfolio = assign_portfolio(\n      data = cur_data(),\n      var = beta_lag,\n      n_portfolios = 10\n    ),\n    portfolio = as.factor(portfolio)\n  ) %>%\n  group_by(portfolio, month) %>%\n  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\")"},{"path":"univariate-portfolio-sorts.html","id":"more-performance-evaluation","chapter":"4 Univariate portfolio sorts","heading":"4.5 More performance evaluation","text":"next step, compute summary statistics beta portfolio. Namely, compute CAPM-adjusted alphas, beta beta portfolio, average returns.figure illustrates CAPM alphas beta-sorted portfolios. shows low beta portfolios tend exhibit positive alphas, high beta portfolios exhibit negative alphas.results suggest negative relation beta future stock returns, contradicts predictions CAPM. According CAPM, returns increase beta across portfolios risk-adjusted returns statistically indistinguishable zero.","code":"\nbeta_portfolios_summary <- beta_portfolios %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  group_by(portfolio) %>%\n  summarize(\n    alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),\n    beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),\n    ret = mean(ret)\n  )\nbeta_portfolios_summary %>%\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")"},{"path":"univariate-portfolio-sorts.html","id":"the-security-market-line-and-beta-portfolios","chapter":"4 Univariate portfolio sorts","heading":"4.6 The security market line and beta portfolios","text":"CAPM predicts portfolios lie security market line (SML). slope SML equal market risk premium reflects risk-return trade-given time.provide evidence CAPM predictions, form long-short strategy buys high-beta portfolio shorts low-beta portfolio., resulting long-short strategy exhibit statistically significant returns.However, long-short portfolio yields statistically significant negative CAPM-adjusted alpha, although, controlling effect beta, average excess stock returns zero according CAPM. results thus provide evidence support CAPM. negative value documented -called betting beta factor (Frazzini Pedersen (2014)). Betting beta corresponds strategy shorts high beta stocks takes (levered) long position low beta stocks. borrowing constraints prevent investors taking positions SML instead incentivized buy high beta stocks, leads relatively higher price (therefore lower expected returns implied CAPM) high beta stocks. result, betting--beta strategy earns providing liquidity capital constraint investors lower risk aversion.plot shows annual returns extreme beta portfolios mainly interested . figure illustrates consistent striking patterns last years - portfolio exhibits periods positive negative annual returns.Overall, chapter shows functional programming can leveraged form arbitrary number portfolios using sorting variable evaluate performance resulting portfolios. next chapter, dive deeper many degrees freedom arise context portfolio analysis.","code":"\nsml_capm <- lm(ret ~ 1 + beta, data = beta_portfolios_summary)$coefficients\n\nbeta_portfolios_summary %>%\n  ggplot(aes(x = beta, y = ret, color = portfolio)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = mean(factors_ff_monthly$mkt_excess)) +\n  geom_abline(intercept = sml_capm[1], slope = sml_capm[2], color = \"green\") +\n  scale_y_continuous(labels = percent, limit = c(0, mean(factors_ff_monthly$mkt_excess) * 2)) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\nbeta_longshort <- beta_portfolios %>%\n  ungroup() %>%\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) %>%\n  filter(portfolio %in% c(\"low\", \"high\")) %>%\n  pivot_wider(month, names_from = portfolio, values_from = ret) %>%\n  mutate(long_short = high - low) %>%\n  left_join(factors_ff_monthly, by = \"month\")\ncoeftest(lm(long_short ~ 1, data = beta_longshort), vcov = NeweyWest)## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept) 0.000734   0.002483     0.3     0.77\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort), vcov = NeweyWest)## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.00441    0.00262   -1.69    0.092 .  \n## mkt_excess   0.89461    0.10214    8.76   <2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nbeta_longshort %>%\n  group_by(year = year(month)) %>%\n  summarize(\n    low = prod(1 + low),\n    high = prod(1 + high),\n    long_short = prod(1 + long_short)\n  ) %>%\n  pivot_longer(cols = -year) %>%\n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )"},{"path":"univariate-portfolio-sorts.html","id":"exercises-3","chapter":"4 Univariate portfolio sorts","heading":"4.7 Exercises","text":"Take two long-short beta strategies based different numbers portfolios compare returns. significant difference returns? Sharpe ratios compare strategies? Find one additional portfolio evaluation statistic compute .plotted alphas ten beta portfolios . Write function tests estimates significance. portfolios significant alphas?analysis based betas daily returns. However, also computed betas monthly returns. Re-run analysis point differences results.Given results chapter, can define long-short strategy yields positive abnormal returns (.e., alphas)? Plot cumulative excess return strategy market excess return comparison.","code":""},{"path":"size-sorts-and-p-hacking.html","id":"size-sorts-and-p-hacking","chapter":"5 Size sorts and p-hacking","heading":"5 Size sorts and p-hacking","text":"chapter, continue portfolio sorts univariate setting. Yet, consider firm size sorting variable, gives rise well-known return factor: size premium. size premium arises buying small stocks selling large stocks. Prominently, Eugene F. Fama French (1993) include factor three-factor model. Apart , asset managers commonly include size key firm characteristic making investment decisions.also introduce new choices formation portfolios. particular, discuss listing exchanges, industries, weighting regimes, periods. choices matter portfolio returns result different size premiums. Exploiting ideas generate favorable results called p-hacking.\narguably thin line p-hacking conducting robustness tests, purpose simply illustrate substantial variation can arise along evidence generating process.","code":""},{"path":"size-sorts-and-p-hacking.html","id":"data-preparation-1","chapter":"5 Size sorts and p-hacking","heading":"5.1 Data preparation","text":"chapter relies following set packages.First, retrieve relevant data SQLite-database introduced chapter “Accessing & managing financial data”. Firm size defined market equity asset pricing applications retrieve CRSP. use Fama-French factor returns performance evaluation.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(scales)\nlibrary(furrr)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()"},{"path":"size-sorts-and-p-hacking.html","id":"size-distribution","chapter":"5 Size sorts and p-hacking","heading":"5.2 Size distribution","text":"build size portfolios, investigate distribution variable firm size. Visualizing data valuable starting point understand input analysis. figure shows fraction total market capitalization concentrated largest firm. produce graph, create monthly indicators track whether stock belongs largest x% firms.\n, aggregate firms within bucket compute buckets’ share total market capitalization.figure shows largest 1% firms cover 50% total market capitalization, holding just 25% largest firms CRSP universe essentially replicates market portfolio. distribution firm size thus implies largest firms market dominate many small firms whenever use value-weighted benchmarks.Next, firm sizes also differ across listing exchanges. Stocks’ primary listings important past potentially still relevant today. graph shows New York Stock Exchange (NYSE) still largest listing exchange terms market capitalization. recently, NASDAQ gained relevance listing exchange. know small peak NASDAQ’s market cap around year 2000 ?Finally, consider distribution firm size across listing exchanges create summary statistics. pre-build function summary() include statistics interested , create function create_summary() adds standard deviation number observations. , apply current month CRSP data listing exchange. also add row add_row() overall summary statistics.resulting table shows firms listed NYSE significantly larger average firms listed exchanges. Moreover, NASDAQ lists largest number firms. discrepancy firm sizes across listing exchanges motivated researchers form breakpoints exclusively NYSE sample apply breakpoints stocks. following, use distinction update portfolio sort procedure.","code":"\ncrsp_monthly %>%\n  group_by(month) %>%\n  mutate(\n    top01 = if_else(mktcap >= quantile(mktcap, 0.99), 1L, 0L),\n    top05 = if_else(mktcap >= quantile(mktcap, 0.95), 1L, 0L),\n    top10 = if_else(mktcap >= quantile(mktcap, 0.90), 1L, 0L),\n    top25 = if_else(mktcap >= quantile(mktcap, 0.75), 1L, 0L),\n    total_market_cap = sum(mktcap)\n  ) %>%\n  summarize(\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap\n  ) %>%\n  pivot_longer(cols = -month) %>%\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) %>%\n  ggplot(aes(x = month, y = value, color = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\ncrsp_monthly %>%\n  group_by(month, exchange) %>%\n  summarize(mktcap = sum(mktcap)) %>%\n  mutate(share = mktcap / sum(mktcap)) %>%\n  ggplot(aes(x = month, y = share, fill = exchange, color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\ncreate_summary <- function(data, column_name) {\n  data %>%\n    select(value = {{ column_name }}) %>%\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q25 = quantile(value, 0.25),\n      q50 = quantile(value, 0.50),\n      q75 = quantile(value, 0.75),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly %>%\n  filter(month == max(month)) %>%\n  group_by(exchange) %>%\n  create_summary(mktcap) %>%\n  add_row(crsp_monthly %>%\n    filter(month == max(month)) %>%\n    create_summary(mktcap) %>%\n    mutate(exchange = \"Overall\"))## # A tibble: 5 × 11\n##   exchange   mean     sd      min     q05    q25    q50\n##   <chr>     <dbl>  <dbl>    <dbl>   <dbl>  <dbl>  <dbl>\n## 1 AMEX       283.  1298.     6.04    10.1 3.07e1 6.59e1\n## 2 NASDAQ    8041. 74386.     4.65    27.3 1.34e2 4.85e2\n## 3 NYSE     16427. 43130.     5.35   154.  9.16e2 3.34e3\n## 4 Other    10061.    NA  10061.   10061.  1.01e4 1.01e4\n## 5 Overall  10558. 63975.     4.65    31.0 1.85e2 8.72e2\n## # … with 4 more variables: q75 <dbl>, q95 <dbl>,\n## #   max <dbl>, n <int>"},{"path":"size-sorts-and-p-hacking.html","id":"univariate-size-portfolios-with-flexible-breakpoints","chapter":"5 Size sorts and p-hacking","heading":"5.3 Univariate size portfolios with flexible breakpoints","text":"previous chapter, construct portfolios varying number portfolios different sorting variables. , extend framework compute breakpoints subset data, instance, based selected listing exchanges. published asset pricing articles, many scholars compute sorting breakpoints NYSE-listed stocks. NYSE-specific breakpoints applied entire universe stocks.replicate NYSE-centered sorting procedure, introduce exchanges argument assign_portfolio() function. exchange-specific argument enters filter filter(grepl(exchanges, exchange)). function grepl() part family functions regular expressions, provide various functionalities work manipulate character strings. , replace character string stored column exchange binary variable indicates string matches pattern specified argument exchanges. example, exchanges = 'NYSE' specified, stocks NYSE used compute breakpoints. Alternatively, specify exchanges = 'NYSE|NASDAQ|AMEX', keeps stocks listed either exchanges. Overall, regular expressions powerful tool, touch specific case .","code":"\nassign_portfolio <- function(n_portfolios,\n                             exchanges,\n                             data) {\n  breakpoints <- data %>%\n    filter(grepl(exchanges, exchange)) %>%\n    summarize(breakpoint = quantile(\n      mktcap_lag,\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) %>%\n    pull(breakpoint) %>%\n    as.numeric()\n\n  data %>%\n    mutate(portfolio = findInterval(mktcap_lag, breakpoints, all.inside = TRUE)) %>%\n    pull(portfolio)\n}"},{"path":"size-sorts-and-p-hacking.html","id":"weighting-schemes-for-portfolios","chapter":"5 Size sorts and p-hacking","heading":"5.4 Weighting schemes for portfolios","text":"Apart computing breakpoints different samples, researchers often use different portfolio weighting schemes. far, weighted portfolio constituent relative market equity previous period. protocol called value-weighting. alternative protocol equal-weighting, assigns stock’s return weight, .e., simple average constituents’ returns. Notice equal-weighting difficult practice portfolio manager needs rebalance portfolio monthly value-weighting truly passive investment.implement two weighting schemes function compute_portfolio_returns() takes logical argument weight returns firm value. statement if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)) generates value-weighted returns value_weighted = TRUE. Additionally, long-short portfolio long smallest firms short largest firms, consistent research showing small firms outperform larger counterparts. Apart two changes, function similar procedure previous chapter.see function compute_portfolio_returns() works, consider simple median breakpoint example value-weighted returns. interested effect restricting listing exchanges estimation size premium. first function call, compute returns based breakpoints listing exchanges. , computed returns based breakpoints NYSE-listed stocks.table shows size premium 60% larger consider stocks NYSE form breakpoint month. NYSE-specific breakpoints larger, 50% stocks entire universe resulting small portfolio NYSE firms larger average. impact choice negligible.","code":"\ncompute_portfolio_returns <- function(n_portfolios = 10,\n                                      exchanges = \"NYSE|NASDAQ|AMEX\",\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data %>%\n    group_by(month) %>%\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = cur_data()\n    )) %>%\n    group_by(month, portfolio) %>%\n    summarize(\n      ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),\n      .groups = \"drop_last\"\n    ) %>%\n    summarize(size_premium = ret[portfolio == min(portfolio)] - ret[portfolio == max(portfolio)]) %>%\n    summarize(size_premium = mean(size_premium))\n}\nret_all <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE|NASDAQ|AMEX\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(Exchanges = c(\"all\", \"NYSE\"), Premium = as.numeric(c(ret_all, ret_nyse)) * 100)## # A tibble: 2 × 2\n##   Exchanges Premium\n##   <chr>       <dbl>\n## 1 all         0.110\n## 2 NYSE        0.181"},{"path":"size-sorts-and-p-hacking.html","id":"p-hacking-and-non-standard-errors","chapter":"5 Size sorts and p-hacking","heading":"5.5 P-hacking and non-standard errors","text":"Since choice exchange significant impact, next step investigate effect data processing decisions researchers make along way. particular, portfolio sort analysis decide least number portfolios, listing exchanges form breakpoints, equal- value-weighting. , one may exclude firms active finance industry restrict analysis parts time series. variations choices discuss part scholarly articles published top finance journals.\nintention application show different ways form portfolios result different estimated size premia. Despite effects multitude choices, correct way. also noted none procedures wrong, aim simply illustrate changes can arise due variation evidence-generating process (Menkveld et al. 2021).\nmalicious perspective, modeling choices give researcher multiple chances find statistically significant results. Yet considered p-hacking renders statistical inference due multiple testing invalid (Harvey, Liu, Zhu 2016).Nevertheless, multitude options creates problem since single correct way sorting portfolios. researcher convince reader results come p-hacking exercise? circumvent dilemma, academics encouraged present evidence different sorting schemes robustness tests report multiple approaches show result depend single choice. Thus, robustness premiums key feature.conduct series robustness tests also interpreted p-hacking exercise. , examine size premium different specifications presented table p_hacking_setup. function expand_grid() produces table possible permutations arguments. Notice use argument data exclude financial firms truncate time series.speed computation parallelize (many) different sorting procedures, Chapter 3. Finally, report resulting size premiums descending order. indeed substantial size premia possible data, particular use equal-weighted portfolios.","code":"\np_hacking_setup <- expand_grid(\n  n_portfolios = c(2, 5, 10),\n  exchanges = c(\"NYSE\", \"NYSE|NASDAQ|AMEX\"),\n  value_weighted = c(TRUE, FALSE),\n  data = rlang::parse_exprs('crsp_monthly; crsp_monthly %>% filter(industry != \"Finance\");\n                             crsp_monthly %>% filter(month < \"1990-06-01\");\n                             crsp_monthly %>% filter(month >=\"1990-06-01\")')\n)\np_hacking_setup## # A tibble: 48 × 4\n##   n_portfolios exchanges value_weighted data      \n##          <dbl> <chr>     <lgl>          <list>    \n## 1            2 NYSE      TRUE           <sym>     \n## 2            2 NYSE      TRUE           <language>\n## 3            2 NYSE      TRUE           <language>\n## 4            2 NYSE      TRUE           <language>\n## 5            2 NYSE      FALSE          <sym>     \n## # … with 43 more rows\nplan(multisession, workers = availableCores())\n\np_hacking_setup <- p_hacking_setup %>%\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = rlang::eval_tidy(..4)\n    )\n  ))\n\np_hacking_results <- p_hacking_setup %>%\n  mutate(data = map_chr(data, deparse)) %>%\n  unnest(size_premium) %>%\n  mutate(data = str_remove(data, \"crsp_monthly %>% \")) %>%\n  arrange(desc(size_premium))\np_hacking_results## # A tibble: 48 × 5\n##   n_portfolios exchanges        value_weighted data    \n##          <dbl> <chr>            <lgl>          <chr>   \n## 1           10 NYSE|NASDAQ|AMEX FALSE          \"filter…\n## 2           10 NYSE|NASDAQ|AMEX FALSE          \"filter…\n## 3           10 NYSE|NASDAQ|AMEX FALSE          \"crsp_m…\n## 4           10 NYSE|NASDAQ|AMEX FALSE          \"filter…\n## 5           10 NYSE|NASDAQ|AMEX TRUE           \"filter…\n## # … with 43 more rows, and 1 more variable:\n## #   size_premium <dbl>"},{"path":"size-sorts-and-p-hacking.html","id":"the-size-premium-variation","chapter":"5 Size sorts and p-hacking","heading":"5.6 The size-premium variation","text":"provide graph shows different premiums. plot also shows relation average Fama-French SMB (small minus big) premium used literature include dotted vertical line.","code":"\np_hacking_results %>%\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Size premium over different sorting choices\",\n    subtitle = \"The dotted vertical line indicates the average Fama-French SMB permium\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb)),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)"},{"path":"size-sorts-and-p-hacking.html","id":"exercises-4","chapter":"5 Size sorts and p-hacking","heading":"5.7 Exercises","text":"gained several insights size distribution . However, analyse average size across exchanges industries. exchanges/industries largest firms? Plot average firm size three exchanges time. see?compute breakpoints take look exposition . might cover potential data errors. Plot breakpoints ten size portfolios time. , take difference two extreme portfolios plot . Describe results.returns analyse account differences exposure market risk, .e., CAPM beta. Change function compute_portfolio_returns() output CAPM alpha beta instead average excess return.saw spread returns p-hacking exercise, show choices led largest effects. Find way investigate choice variable largest impact estimated size premium.computed several size premiums, follow definition Eugene F. Fama French (1993). approaches comes closest SMB premium?","code":""},{"path":"value-and-bivariate-sorts.html","id":"value-and-bivariate-sorts","chapter":"6 Value and bivariate sorts","heading":"6 Value and bivariate sorts","text":"chapter extends univariate portfolio analysis bivariate sorts means assign stocks portfolios based two characteristics. Bivariate sorts regularly used academic asset pricing literature. Yet, scholars also use sorts three grouping variables. Conceptually, portfolio sorts easily applicable higher dimensions.form portfolios firm size book--market ratio. calculate book--market ratios, accounting data required necessitates additional steps portfolio formation. end, demonstrate form portfolios two sorting variables using -called independent dependent portfolio sorts.current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(scales)"},{"path":"value-and-bivariate-sorts.html","id":"data-preparation-2","chapter":"6 Value and bivariate sorts","heading":"6.1 Data preparation","text":"First, load necessary data SQLite-database introduced chapter “Accessing & managing financial data”. conduct portfolio sorts based CRSP sample keep necessary columns memory. use data sources firm size previous chapter., utilize accounting data. common source accounting data Compustat. need book equity data application, select database. Additionally, convert variable datadate monthly value, consider monthly returns need account exact date. achieve , use function floor_date().","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", \n                          extended_types = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\n\ncrsp_monthly <- crsp_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  select(permno, gvkey, month, ret_excess, mkt_excess, \n         mktcap, mktcap_lag, exchange) %>%\n  drop_na()\ncompustat <- tbl(tidy_finance, \"compustat\") %>%\n  collect()\n\nbe <- compustat %>%\n  select(gvkey, datadate, be) %>%\n  drop_na() %>%\n  mutate(month = floor_date(ymd(datadate), \"month\"))"},{"path":"value-and-bivariate-sorts.html","id":"book-to-market-ratio","chapter":"6 Value and bivariate sorts","heading":"6.2 Book-to-market ratio","text":"fundamental problem handling accounting data look-ahead bias - must include data forming portfolio public knowledge time. course, researchers information looking past agents moment. However, abnormal excess returns trading strategy rely information advantage differential result informed agents’ trades. Hence, lag accounting information.continue lag market capitalization firm size one month. , compute book--market ratio, relates firm’s book equity market equity. Firms high (low) book--market called value (growth) firms. matching accounting market equity information month, lag book--market six months. sufficiently conservative approach accounting information usually released well six months pass. However, asset pricing literature, even longer lags used well.1Having variables, .e., firm size lagged one month book--market lagged six months, merge sorting variables returns using sorting_date-column created purpose. final step data preparation deals differences frequency variables. Returns firm size recorded monthly. Yet accounting information released annual basis. Hence, match book--market one month per year eleven empty observations. solve frequency issue, carry latest book--market ratio firm subsequent months, .e., fill missing observations current report. done via fill()-function sorting date firm (identify permno gvkey) firm basis (group_by() usual). last step, remove rows missing entries returns matched annual report.last step preparation portfolio sorts computation breakpoints. continue use function allowing specification exchanges use breakpoints. Additionally, reintroduce argument var function defining different sorting variables via curly-curly.data preparation steps, present bivariate portfolio sorts independent dependent basis.","code":"\nme <- crsp_monthly %>%\n  mutate(sorting_date = month %m+% months(1)) %>%\n  select(permno, sorting_date, me = mktcap)\n\nbm <- be %>%\n  inner_join(crsp_monthly %>%\n    select(month, permno, gvkey, mktcap), by = c(\"gvkey\", \"month\")) %>%\n  mutate(\n    bm = be / mktcap,\n    sorting_date = month %m+% months(6)\n  ) %>%\n  select(permno, gvkey, sorting_date, bm) %>%\n  arrange(permno, gvkey, sorting_date)\n\ndata_for_sorts <- crsp_monthly %>%\n  left_join(bm, by = c(\"permno\", \"gvkey\", \"month\" = \"sorting_date\")) %>%\n  left_join(me, by = c(\"permno\", \"month\" = \"sorting_date\")) %>%\n  select(permno, gvkey, month, ret_excess, mktcap_lag, me, bm, exchange)\n\ndata_for_sorts <- data_for_sorts %>%\n  arrange(permno, gvkey, month) %>%\n  group_by(permno, gvkey) %>%\n  fill(bm) %>%\n  drop_na()\nassign_portfolio <- function(data, var, n_portfolios, exchanges) {\n  breakpoints <- data %>%\n    filter(exchange %in% exchanges) %>%\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) %>%\n    pull(breakpoint) %>%\n    as.numeric()\n\n  data %>%\n    mutate(portfolio = findInterval({{ var }}, breakpoints, all.inside = TRUE)) %>%\n    pull(portfolio)\n}"},{"path":"value-and-bivariate-sorts.html","id":"independent-sorts","chapter":"6 Value and bivariate sorts","heading":"6.3 Independent sorts","text":"Bivariate sorts create portfolios within two-dimensional space spanned two sorting variables. possible assess return impact either sorting variable return differential trading strategy invests portfolios either end respective variables spectrum. create five--five matrix using book--market firm size sorting variables example . end 25 portfolios. Since interested value premium (.e., return differential high low book--market firms), go long five portfolios highest book--market firms short five portfolios lowest book--market firms. five portfolios end due size splits employed alongside book--market splits.implement independent bivariate portfolio sort, assign monthly portfolios sorting variables separately create variables portfolio_bm portfolio_bm, respectively. , separate portfolios combined final sort stored portfolio_combined. assigning portfolios, compute average return within portfolio month. Additionally, keep book--market portfolio makes computation value premium easier. alternative disaggregate combined portfolio separate step. Notice weigh stocks within portfolio market capitalization, .e., decide value-weight returns.Equipped monthly portfolio returns, ready compute value premium. However, still decide invest five high five low book--market portfolios. common approach weigh portfolios equally, yet another researcher’s choice. , compute return differential high low book--market portfolios show average value premium.resulting annualized value premium 3.936 percent.","code":"\nvalue_portfolios <- data_for_sorts %>%\n  group_by(month) %>%\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = paste0(portfolio_bm, portfolio_me)\n  ) %>%\n  group_by(month, portfolio_combined) %>%\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\nvalue_premium <- value_portfolios %>%\n  group_by(month, portfolio_bm) %>%\n  summarize(ret = mean(ret), .groups = \"drop_last\") %>%\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] - \n              ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)## [1] 0.328"},{"path":"value-and-bivariate-sorts.html","id":"dependent-sorts","chapter":"6 Value and bivariate sorts","heading":"6.4 Dependent sorts","text":"previous exercise, assigned portfolios without considering second variable assignment. protocol called independent portfolio sorts. alternative, .e., dependent sorts, creates portfolios second sorting variable within bucket first sorting variable. example , sort firms five size buckets, within buckets, assign firms five book--market portfolios. Hence, monthly breakpoints specific size group. decision independent dependent portfolio sorts another choice researcher. Notice dependent sorts ensure equal amount stocks within portfolio.implement dependent sorts, first create size portfolios calling assign_portfolio() var = . , group data month size portfolio assigning book--market portfolio. rest implementation . Finally, compute value premium.value premium dependent sorts 3.18 percent per year.Overall, show conduct bivariate portfolio sorts chapter. one case, sort portfolios independently . Yet also discuss create dependent portfolio sorts. Along line previous chapter, see many choices researcher make implement portfolio sorts, bivariate sorts increase number choices.","code":"\nvalue_portfolios <- data_for_sorts %>%\n  group_by(month) %>%\n  mutate(portfolio_me = assign_portfolio(\n    data = cur_data(),\n    var = me,\n    n_portfolios = 5,\n    exchanges = c(\"NYSE\")\n  )) %>%\n  group_by(month, portfolio_me) %>%\n  mutate(\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm,\n      n_portfolios = 5,\n      exchanges = c(\"NYSE\")\n    ),\n    portfolio_combined = paste0(portfolio_bm, portfolio_me)\n  ) %>%\n  group_by(month, portfolio_combined) %>%\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag),\n    portfolio_bm = unique(portfolio_bm),\n    .groups = \"drop\"\n  )\n\nvalue_premium <- value_portfolios %>%\n  group_by(month, portfolio_bm) %>%\n  summarize(ret = mean(ret), .groups = \"drop_last\") %>%\n  summarize(value_premium = ret[portfolio_bm == max(portfolio_bm)] - \n              ret[portfolio_bm == min(portfolio_bm)])\n\nmean(value_premium$value_premium * 100)## [1] 0.265"},{"path":"value-and-bivariate-sorts.html","id":"exercises-5","chapter":"6 Value and bivariate sorts","heading":"6.5 Exercises","text":"previous chapter, examined distribution market equity. Repeat analysis book equity book--market ratio (alongside plot breakpoints, .e., deciles).investigate portfolios, focus returns exclusively. However, also interest understand characteristics portfolios. Write function compute average characteristics size book--market across 25 independently dependently sorted portfolios.size premium, also value premium constructed follow Eugene F. Fama French (1993). Implement p-hacking setup previous chapter find premium comes closest HML premium.","code":""},{"path":"replicating-fama-french-factors.html","id":"replicating-fama-french-factors","chapter":"7 Replicating Fama & French factors","heading":"7 Replicating Fama & French factors","text":"Fama French three-factor model (see Eugene F. Fama French (1993)) cornerstone asset pricing. top market factor represented traditional CAPM beta, model includes size value factors. introduce factors previous chapter, definition remains . Size SMB factor (small-minus-big) long small firms short large firms. value factor HML (high-minus-low) long high book--market firms short low book--market counterparts. chapter, also want show main idea replicate significant factors.current chapter relies set packages.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)"},{"path":"replicating-fama-french-factors.html","id":"databases","chapter":"7 Replicating Fama & French factors","heading":"7.1 Databases","text":"use CRSP Compustat data sources, need exactly variables compute size value factors way Fama French . Hence, nothing new load data SQLite-database introduced chapter “Accessing & managing financial data”..","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") %>%\n  collect()\n\ndata_ff <- crsp_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  select(\n    permno, gvkey, month, ret_excess, mkt_excess,\n    mktcap, mktcap_lag, exchange\n  ) %>%\n  drop_na()\n\nbe <- compustat %>%\n  select(gvkey, datadate, be) %>%\n  drop_na()"},{"path":"replicating-fama-french-factors.html","id":"data-preparation-3","chapter":"7 Replicating Fama & French factors","heading":"7.2 Data preparation","text":"Yet start merging data set computing premiums, differences previous chapter. First, Fama French form portfolios June year \\(t\\), whereby returns July first monthly return respective portfolio. firm size, consequently use market capitalization recorded June. held constant June year \\(t+1\\).Second, Fama French also different protocol computing book--market ratio. use market equity end year \\(t - 1\\) book equity reported year \\(t-1\\), .e., datadate within last year. Hence, book--market ratio can based accounting information 18 months old. Market equity also necessarily reflect time point book equity.implement time lags, employ temporary sorting_date-column. Notice combine information, want single observation per year stock since interested computing breakpoints held constant entire year. ensure call distinct() end chunk .","code":"\nme_ff <- data_ff %>%\n  filter(month(month) == 6) %>%\n  mutate(sorting_date = month %m+% months(1)) %>%\n  select(permno, sorting_date, me_ff = mktcap)\n\nme_ff_dec <- data_ff %>%\n  filter(month(month) == 12) %>%\n  mutate(sorting_date = ymd(paste0(year(month) + 1, \"0701)\"))) %>%\n  select(permno, gvkey, sorting_date, bm_me = mktcap)\n\nbm_ff <- be %>%\n  mutate(sorting_date = ymd(paste0(year(datadate) + 1, \"0701\"))) %>%\n  select(gvkey, sorting_date, bm_be = be) %>%\n  drop_na() %>%\n  inner_join(me_ff_dec, by = c(\"gvkey\", \"sorting_date\")) %>%\n  mutate(bm_ff = bm_be / bm_me) %>%\n  select(permno, sorting_date, bm_ff)\n\nvariables_ff <- me_ff %>%\n  inner_join(bm_ff, by = c(\"permno\", \"sorting_date\")) %>%\n  drop_na() %>%\n  distinct(permno, sorting_date, .keep_all = TRUE)"},{"path":"replicating-fama-french-factors.html","id":"portfolio-sorts","chapter":"7 Replicating Fama & French factors","heading":"7.3 Portfolio sorts","text":"Next, construct portfolios adjusted assign_portfolio() function. Fama French rely NYSE-specific breakpoints, form two portfolios size dimension median three portfolios dimension book--market 30%- 70%-percentiles, use independent sorts. sorts book--market require adjustment previous function seq() produce produce right breakpoints. Instead n_portfolios, now specify percentiles, take breakpoint-sequence object specified function’s call. Specifically, give percentiles = c(0, 0.3, 0.7, 1) function. Additionally, perform inner_join() return data ensure use traded stocks computing breakpoints first step.Next, merge portfolios return data rest year. implement step, create new column sorting_date return data setting date sort July \\(t-1\\) month June (year \\(t\\)) earlier July year \\(t\\) month July later.","code":"\nassign_portfolio <- function(data, var, percentiles) {\n  breakpoints <- data %>%\n    filter(exchange == \"NYSE\") %>%\n    summarize(breakpoint = quantile(\n      {{ var }},\n      probs = {{ percentiles }},\n      na.rm = TRUE\n    )) %>%\n    pull(breakpoint) %>%\n    as.numeric()\n\n  data %>%\n    mutate(portfolio = findInterval({{ var }}, breakpoints, all.inside = TRUE)) %>%\n    pull(portfolio)\n}\n\nportfolios_ff <- variables_ff %>%\n  inner_join(data_ff, by = c(\"permno\" = \"permno\", \"sorting_date\" = \"month\")) %>%\n  group_by(sorting_date) %>%\n  mutate(\n    portfolio_me = assign_portfolio(\n      data = cur_data(),\n      var = me_ff,\n      percentiles = c(0, 0.5, 1)\n    ),\n    portfolio_bm = assign_portfolio(\n      data = cur_data(),\n      var = bm_ff,\n      percentiles = c(0, 0.3, 0.7, 1)\n    )\n  ) %>%\n  select(permno, sorting_date, portfolio_me, portfolio_bm)\nportfolios_ff <- data_ff %>%\n  mutate(sorting_date = case_when(\n    month(month) <= 6 ~ ymd(paste0(year(month) - 1, \"0701\")),\n    month(month) >= 7 ~ ymd(paste0(year(month), \"0701\"))\n  )) %>%\n  inner_join(portfolios_ff, by = c(\"permno\", \"sorting_date\"))"},{"path":"replicating-fama-french-factors.html","id":"fama-and-french-factor-returns","chapter":"7 Replicating Fama & French factors","heading":"7.4 Fama and French factor returns","text":"Equipped return data assigned portfolios, can now compute value-weighted average return six portfolios. , form Fama French factors. size factor (.e., SMB), go long three small portfolios short three large portfolios taking average across either group. value factor (.e., HML), go long two high book--market portfolios short two low book--market portfolios, weighting equally.","code":"\nfactors_ff_monthly_replicated <- portfolios_ff %>%\n  mutate(portfolio = paste0(portfolio_me, portfolio_bm)) %>%\n  group_by(portfolio, month) %>%\n  summarize(\n    ret = weighted.mean(ret_excess, mktcap_lag), .groups = \"drop\",\n    portfolio_me = unique(portfolio_me),\n    portfolio_bm = unique(portfolio_bm)\n  ) %>%\n  group_by(month) %>%\n  summarize(\n    smb_replicated = mean(ret[portfolio_me == 1]) - mean(ret[portfolio_me == 2]),\n    hml_replicated = mean(ret[portfolio_bm == 3]) - mean(ret[portfolio_bm == 1])\n  )"},{"path":"replicating-fama-french-factors.html","id":"replication-evaluation","chapter":"7 Replicating Fama & French factors","heading":"7.5 Replication evaluation","text":"previous section, replicated size value premiums following procedure outlined Fama French. However, follow procedure strictly. final question : close get? answer question looking two time-series estimates regression analysis using lm(). good job, see non-significant intercept (rejecting notion systematic error), coefficient close 1 (indicating high correlation), adjusted R-squared close 1 (indicating high proportion explained variance).results SMB factor quite convincing three criteria outlined met coefficient R-squared 99%.replication HML factor also success, although slightly lower level coefficient R-squared around 95%.evidence hence allows us conclude relatively good job replicating original Fama-French premiums, although see underlying code.\nperspective, perfect match possible additional information maintainers original data.","code":"\ntest <- factors_ff_monthly %>%\n  inner_join(factors_ff_monthly_replicated, by = \"month\") %>%\n  mutate(\n    smb_replicated = round(smb_replicated, 4),\n    hml_replicated = round(hml_replicated, 4)\n  )\nsummary(lm(smb ~ smb_replicated, data = test))## \n## Call:\n## lm(formula = smb ~ smb_replicated, data = test)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.020320 -0.001501  0.000027  0.001519  0.014615 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)\n## (Intercept)    -0.000143   0.000133   -1.07     0.28\n## smb_replicated  0.996413   0.004418  225.55   <2e-16\n##                   \n## (Intercept)       \n## smb_replicated ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.00355 on 712 degrees of freedom\n## Multiple R-squared:  0.986,  Adjusted R-squared:  0.986 \n## F-statistic: 5.09e+04 on 1 and 712 DF,  p-value: <2e-16\nsummary(lm(hml ~ hml_replicated, data = test))## \n## Call:\n## lm(formula = hml ~ hml_replicated, data = test)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.022250 -0.002933 -0.000101  0.002366  0.027475 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)\n## (Intercept)    0.000294   0.000214    1.38     0.17\n## hml_replicated 0.958849   0.007376  130.00   <2e-16\n##                   \n## (Intercept)       \n## hml_replicated ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.0057 on 712 degrees of freedom\n## Multiple R-squared:  0.96,   Adjusted R-squared:  0.96 \n## F-statistic: 1.69e+04 on 1 and 712 DF,  p-value: <2e-16"},{"path":"fama-macbeth-regressions.html","id":"fama-macbeth-regressions","chapter":"8 Fama-MacBeth Regressions","heading":"8 Fama-MacBeth Regressions","text":"regression approach Eugene F. Fama MacBeth (1973) widely used empirical asset pricing studies.\nResearchers use two-stage regression approach estimate risk premiums various markets, predominately stock market.\nEssentially, two-step Fama-MacBeth regressions exploit linear relationship expected returns exposure (priced) risk factors.\nbasic idea regression approach project asset returns factor exposures characteristics resemble exposure risk factor cross-section time period.\n, second step, estimates aggregated across time test risk factor priced.\nprinciple, Fama-MacBeth regressions can used way portfolio sorts introduced previous chapters.\nchapter, present simple implementation Eugene F. Fama MacBeth (1973) introduce concept regressions. use individual stocks test assets estimate risk premium associated three factors included Eugene F. Fama French (1993).Fama-MacBeth procedure simple two-step approach:\nfirst step uses exposures (characteristics) explanatory variables \\(T\\) cross-sectional regressions, .e.,\n\\[\\begin{aligned}r_{,t+1} = \\alpha_i + \\lambda^{M}_t \\beta^M_{,t}  + \\lambda^{SMB}_t \\beta^{SMB}_{,t} + \\lambda^{HML}_t \\beta^{HML}_{,t} + \\epsilon_{,t}\\text{, t}.\\end{aligned}\\]\n, interested compensation \\(\\lambda^{f}_t\\) exposure risk factor \\(\\beta^{f}_{,t}\\) time point, .e., risk premium. Note terminology: \\(\\beta^{f}_{,t}\\) asset-specific characteristic, e.g., factor exposure accounting variable. linear relationship expected returns characteristic given month, expect regression coefficient reflect relationship, .e., \\(\\lambda_t^{f}\\neq0\\).second step, time-series average \\(\\frac{1}{T}\\sum\\limits_{t=1}^T \\hat\\lambda^{f}_t\\) estimates \\(\\hat\\lambda^{f}_t\\) can interpreted risk premium specific risk factor \\(f\\). follow Zaffaroni Zhou (2022) consider standard cross-sectional regression predict future returns. characteristics replaced time \\(t+1\\) variables, regression approach rather captures risk attributes.move implementation, want highlight characteristics, e.g., \\(\\hat\\beta^{f}_{}\\), typically estimated separate step applying actual Fama-MacBeth methodology. can think step 0. might thus worry errors \\(\\hat\\beta^{f}_{}\\) impact risk premiums’ standard errors. Measurement error \\(\\hat\\beta^{f}_{}\\) indeed affects risk premium estimates, .e., lead biased estimates. literature provides adjustments bias (see, e.g., Shanken (1992), Kim (1995), Chen, Lee, Lee (2015), among others) also shows bias goes zero \\(T \\\\infty\\). refer Gagliardini, Ossola, Scaillet (2016) -depth discussion also covering case time-varying betas. Moreover, plan use Fama-MacBeth regressions individual stocks: Hou, Xue, Zhang (2020) advocates using weighed-least squares estimate coefficients biased towards small firms. Without adjustment, high number small firms drive coefficient estimates.","code":""},{"path":"fama-macbeth-regressions.html","id":"data-preparation-4","chapter":"8 Fama-MacBeth Regressions","heading":"8.1 Data preparation","text":"current chapter relies set packages.illustrate Eugene F. Fama MacBeth (1973) monthly CRSP sample use three characteristics explain cross-section returns: market capitalization, book--market ratio, CAPM beta (.e., covariance excess stock returns market excess returns). collect data SQLite-database introduced chapter “Accessing & managing financial data”.use Compustat CRSP data compute book--market ratio (log) market capitalization.\nFurthermore, also use CAPM betas based daily returns computed previous chapters.","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(sandwich)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\n\ncompustat <- tbl(tidy_finance, \"compustat\") %>%\n  collect()\n\nbeta <- tbl(tidy_finance, \"beta\") %>%\n  collect()\nbm <- compustat %>%\n  mutate(month = floor_date(ymd(datadate), \"month\")) %>%\n  left_join(crsp_monthly, by = c(\"gvkey\", \"month\")) %>%\n  left_join(beta, by = c(\"permno\", \"month\")) %>%\n  transmute(gvkey,\n            bm = be / mktcap,\n            log_mktcap = log(mktcap),\n            beta = beta_daily,\n            sorting_date = month %m+% months(6))\n\ndata_fama_macbeth <- crsp_monthly %>%\n  left_join(bm, by = c(\"gvkey\", \"month\" = \"sorting_date\")) %>%\n  group_by(permno) %>%\n  arrange(month) %>%\n  fill(c(beta, bm, log_mktcap), .direction = \"down\") %>%\n  ungroup() %>%\n  left_join(crsp_monthly %>%\n              select(permno, month, ret_excess_lead = ret) %>%\n              mutate(month = month %m-% months(1)),\n            by = c(\"permno\", \"month\")) %>%\n  select(permno, month, ret_excess_lead, beta, log_mktcap, bm) %>%\n  drop_na()"},{"path":"fama-macbeth-regressions.html","id":"cross-sectional-regression","chapter":"8 Fama-MacBeth Regressions","heading":"8.2 Cross-sectional regression","text":"Next, run cross-sectional regressions characteristics explanatory variables month.\nregress returns test assets particular time point asset’s characteristics.\n, get estimate risk premiums \\(\\hat\\lambda^{F_f}_t\\) point time.","code":"\nrisk_premiums <- data_fama_macbeth %>%\n  nest(data = -month) %>% \n  mutate(estimates = map(.x = data, \n                         ~tidy(lm(ret_excess_lead ~ . - permno, data = .x)))) %>% \n  unnest(estimates)"},{"path":"fama-macbeth-regressions.html","id":"time-series-aggregation","chapter":"8 Fama-MacBeth Regressions","heading":"8.3 Time-series aggregation","text":"Now risk premiums’ estimates period, can average across time-series dimension get expected risk premium characteristic. Similarly, manually create t-test statistics regressor, can compare usual critical values 1.96 2.576 two-tailed significance tests.final note: common adjust autocorrelation reporting standard errors risk premiums. typical procedure computing Newey West (1987) standard errors. One necessary input Newey-West standard errors chosen bandwidth based number lags employed estimation. seems researchers often default choosing pre-specified lag length 6 months, instead recommend data-driven approach. automatic selection advocated Newey West (1994) available sandwich package thanks Zeileis (2004). want implement apparent default, can enforce NeweyWest(., lag = 6, prewhite = FALSE) code .Finally, let us interpret results. Stocks higher book--market ratios earn higher expected future returns, line value premium. negative value log market capitalization reflects size premium smaller stocks. Lastly, negative value CAPM betas characteristics line well-established betting beta anomalies, indicating investors borrowing constraints tilt portfolio towards high beta stocks replicate levered market portfolio (Frazzini Pedersen 2014).","code":"\nprice_of_risk <- risk_premiums %>%\n  group_by(factor = term) %>%\n  summarise(\n    risk_premium = mean(estimate)*100,\n    t_statistic = mean(estimate) / sd(estimate) * sqrt(n())\n  )\nregressions_for_newey_west <- risk_premiums %>%\n  select(month, factor = term, estimate) %>%\n  nest(data = c(month, estimate)) %>%\n  mutate(\n    model = map(data, ~ lm(estimate ~ 1, .)),\n    mean = map(model, tidy)\n  )\n\nprice_of_risk_newey_west <- regressions_for_newey_west %>%\n  mutate(newey_west_se = map_dbl(model, ~ sqrt(NeweyWest(.)))) %>%\n  unnest(mean) %>%\n  mutate(t_statistic_newey_west = estimate / newey_west_se) %>%\n  select(factor,\n    risk_premium = estimate,\n    t_statistic_newey_west\n  )\n\nleft_join(price_of_risk,\n          price_of_risk_newey_west %>% select(factor, t_statistic_newey_west),\n          by = \"factor\")## # A tibble: 4 × 4\n##   factor      risk_premium t_statistic t_statistic_new…\n##   <chr>              <dbl>       <dbl>            <dbl>\n## 1 (Intercept)       1.62         5.09             4.07 \n## 2 beta             -0.0587      -0.790           -0.792\n## 3 bm                0.177        3.48             2.95 \n## 4 log_mktcap       -0.114       -3.00            -2.51"},{"path":"fama-macbeth-regressions.html","id":"exercises-6","chapter":"8 Fama-MacBeth Regressions","heading":"8.4 Exercises","text":"Download sample test assets Kenneth French’s homepage reevaluate risk premiums industry portfolios instead individual stocks.Use individual stocks weighted-least squares based firm’s size suggested Hou, Xue, Zhang (2020). , repeat Fama-MacBeth regressions without weighting scheme adjustment drop smallest 20% firms month. Compare results three approaches.Implement rolling-window regression time-series estimation factor exposure. Skip one month rolling period including exposures cross-sectional regression avoid look-ahead bias. , adapt cross-sectional regression compute average risk premiums.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"factor-selection-via-machine-learning","chapter":"9 Factor selection via machine learning","heading":"9 Factor selection via machine learning","text":"aim chapter twofold. data science perspective, introduce tidymodels, collection packages modeling machine learning (ML) using tidyverse principles. tidymodels comes handy workflow sorts typical prediction tasks. finance perspective, address factor zoo (John H. Cochrane 2011). previous chapters, illustrate stock characteristics size provide valuable pricing information addition market beta.\nfindings question usefulness Capital Asset Pricing Model.\nfact, last decades, financial economists “discovered” plethora additional factors may correlated marginal utility consumption (thus deserve prominent role pricing applications). search factors explain cross section expected stock returns produced hundreds potential candidates, noted recently Harvey, Liu, Zhu (2016), (McLean2016?) Hou, Xue, Zhang (2020).\nTherefore, given multitude proposed risk factors, challenge days rather : believe relevance 300+ risk factors?. recent years, promising methods vast field machine learning (ML) got applied common finance applications. refer Mullainathan Spiess (2017) treatment ML perseptive econometrician, Nagel (2021) excellent review ML practices asset pricing, Easley et al. (2020) ML applications (high-frequency) market microstructure Dixon, Halperin, Bilokon (2020) detailed treatment methodological aspects.introduce Lasso Ridge regression special case penalized regression models. , explain concept cross-validation model tuning Elastic Net regularization popular example. implement showcase entire cycle model specification, training, forecast evaluation within tidymodels universe. tools can generally applied abundance interesting asset pricing problems, apply penalized regressions identifying macro-economic variables asset pricing factors help explain cross-section industry portfolios.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"brief-theoretical-background","chapter":"9 Factor selection via machine learning","heading":"9.1 Brief theoretical background","text":"book empirical work tidy manner, refer many excellent textbook treatments ML methods especially penalized regressions deeper discussion De Prado (2018). Instead, briefly summarize idea Lasso Ridge regressions well general Elastic Net. , turn fascinating question implement, tune, use models tidymodels workflow.set stage, start definition linear model: suppose data \\((y_t, x_t), t = 1,\\ldots, T\\) \\(x_t\\) \\((K \\times 1)\\) vector regressors \\(y_t\\) response observation \\(t\\).\nlinear model takes form \\(y_t = \\beta' x_t + \\varepsilon_t\\) error term \\(\\varepsilon_t\\) studied abundance. well-known ordinary-least square (OLS) estimator \\((K \\times 1)\\) vector \\(\\beta\\) minimizes sum squared residuals \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\]\noften interested estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML predictive performance time. new observation \\(\\tilde{x}_t\\), linear model generates predictions \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\]\nbest can ?\nreally: Instead minimizing sum squared residuals, penalized linear models can improve predictive performance choosing estimators \\(\\hat{\\beta}\\) lower variance estimator \\(\\hat\\beta^\\text{ols}\\).\ntime, seems appealing restrict set regressors meaningful ones possible. words, \\(K\\) large (number proposed factors asset pricing literature), may desirable feature select reasonable factors set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) redundant factors.clear promised benefits penalized regressions (reducing mean squared error) come cost. cases, reducing variance estimator introduces bias \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). effect bias-variance trade-? understand implications, assume following data-generating process \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] properties \\(\\hat\\beta^\\text{ols}\\) unbiased estimator may desirable circumstances, certainly consider predictive accuracy. instance, mean-squared error (MSE) depends model choice follow: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(\\textbf{x}))^2)=E((f(\\textbf{x})+\\epsilon-\\hat{f}(\\textbf{x}))^2)\\\\\n&= \\underbrace{E((f(\\textbf{x})-\\hat{f}(\\textbf{x}))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(\\textbf{x})^2\\right)+E\\left(f(\\textbf{x})^2\\right)-2E\\left(f(\\textbf{x})\\hat{f}(\\textbf{x})\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(\\textbf{x})^2\\right)+f(\\textbf{x})^2-2f(\\textbf{x})E\\left(\\hat{f}(\\textbf{x})\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(\\textbf{x})\\right)}_{\\text{variance model}}+ \\underbrace{E\\left((f(\\textbf{x})-\\hat{f}(\\textbf{x}))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] model can reduce \\(\\sigma_\\varepsilon^2\\), biased estimator small variance may lower mean squared error unbiased estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"ridge-regression","chapter":"9 Factor selection via machine learning","heading":"9.1.1 Ridge regression","text":"One biased estimator known Ridge regression. Hoerl Kennard (1970) propose minimize sum squared errors simultaneously imposing penalty \\(L_2\\) norm parameters \\(\\hat\\beta\\). Formally, means penalty factor \\(\\lambda\\geq 0\\) minimization problem takes form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). , \\(X = \\left(x_1 \\ldots x_T\\right)'\\) \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). closed-form solution resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda \\right)^{-1}X'y.\\] couple observations worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) \\(\\lambda = 0\\) \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) \\(\\lambda\\rightarrow \\infty\\). Also \\(\\lambda > 0\\), \\(\\left(X'X + \\lambda \\right)\\) non-singular even \\(X'X\\) means \\(\\hat\\beta^\\text{ridge}\\) exists even \\(\\hat\\beta\\) defined. However, note also Ridge estimator requires careful choice hyperparameter \\(\\lambda\\) controls amount regularization: larger value \\(\\lambda\\) implies shrinkage regression coefficient towards 0, smaller value \\(\\lambda\\) reduces bias resulting estimator.Usually, \\(X\\) contains intercept column ones. general rule, associated intercept coefficient penalized. practice, often implies \\(y\\) simply demeaned computing \\(\\hat\\beta^\\text{ridge}\\).statistical properties Ridge estimator? First, bad news \\(\\hat\\beta^\\text{ridge}\\) biased estimator \\(\\beta\\). However, good news (homoscedastic error terms) variance Ridge estimator guaranteed smaller variance ordinary least square estimator. encourage verify two statements exercises. result, face trade-: Ridge regression sacrifices bias achieve smaller variance OLS estimator.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"lasso","chapter":"9 Factor selection via machine learning","heading":"9.1.2 Lasso","text":"alternative Ridge regression Lasso (least absolute shrinkage selection operator). Similar Ridge regression, Lasso (Tibshirani 1996) penalized biased estimator.\nmain difference Ridge regression Lasso shrink coefficients effectively selects variables setting coefficients irrelevant variables zero. Lasso implements \\(L_1\\) penalization parameters : \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| < c.\\] closed form solution \\(\\hat\\beta^\\text{Lasso}\\) maximization problem efficient algorithms exist (e.g., R package glmnet). Like Ridge regression, hyperparameter \\(\\lambda\\) specified beforehand.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"elastic-net","chapter":"9 Factor selection via machine learning","heading":"9.1.3 Elastic Net","text":"Elastic Net (Zou Hastie 2005) combines \\(L_1\\) \\(L_2\\) penalization encourages grouping effect strongly correlated predictors tend model together. general framework considers following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, chose two hyperparameters: shrinkage factor \\(\\lambda\\) weighting parameter \\(\\rho\\). Elastic Net resembles Lasso \\(\\rho = 1\\) Ridge regression \\(\\rho = 0\\). R package glmnet provides efficient algorithms compute coefficients penalized regressions, good exercise implement Ridge Lasso estimation use glmnet package tidymodels back-end.","code":""},{"path":"factor-selection-via-machine-learning.html","id":"data-preparation-5","chapter":"9 Factor selection via machine learning","heading":"9.2 Data preparation","text":"get started, load required packages data. main focus workflow behind amazing tidymodels package collection. Kuhn Silge (2018) provide thorough introduction tidymodels components.analysis, use four different data sources load SQLite-database introduced chapter “Accessing & managing financial data”. start two different sets factor portfolio returns suggested representing practical risk factor exposure thus relevant comes asset pricing applications.standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, high-minus-low book--market valuation sorts) defined Eugene F. Fama French (1992) Eugene F. Fama French (1993)Monthly q-factor returns Hou, Xue, Zhang (2014). factors contain size factor, investment factor, return--equity factor, expected growth factorNext, include macroeconomic predictors may predict general stock market economy. Macroeconomic variables effectively serve conditioning information inclusion hints relevance conditional models instead unconditional asset pricing. refer interested reader John H. Cochrane (2009) role conditioning information.set macroeconomic predictors comes paper “Comprehensive Look Empirical Performance Equity Premium Prediction” (Welch Goyal 2008). data updated authors 2020 contains monthly variables suggested good predictors equity premium. variables Dividend Price Ratio, Earnings Price Ratio, Stock Variance, Net Equity Expansion, Treasury Bill rate, inflationFinally, need set test assets. aim understand plenty factors macroeconomic variable combinations prove helpful explaining test assets’ cross-section returns.line many existing papers, use monthly portfolio returns 49 different industries according definition Kenneth French’s homepage test assets.combine monthly observations one data frame.data contains 22 columns regressors 13 macro variables 8 factor returns month.\nfigure provides summary statistics 49 monthly industry excess returns percent.","code":"\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(tidymodels) \nlibrary(furrr) \nlibrary(glmnet)\nlibrary(broom)\nlibrary(timetk)\nlibrary(scales)\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect() %>%\n  rename_with(~ paste0(\"factor_ff_\", .), -month)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\") %>%\n  collect() %>%\n  rename_with(~ paste0(\"factor_q_\", .), -month)\n\nmacro_predictors <- tbl(tidy_finance, \"macro_predictors\") %>%\n  collect() %>%\n  rename_with(~ paste0(\"macro_\", .), -month) %>%\n  select(-macro_rp_div)\n\nindustries_ff_monthly <- tbl(tidy_finance, \"industries_ff_monthly\") %>%\n  collect() %>%\n  pivot_longer(-month, \n               names_to = \"industry\", values_to = \"ret\") %>%\n  mutate(industry = as_factor(industry))\ndata <- industries_ff_monthly %>%\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  left_join(factors_q_monthly, by = \"month\") %>%\n  left_join(macro_predictors, by = \"month\") %>%\n  mutate(\n    ret = ret - factor_ff_rf\n  ) %>% \n  select(month, industry, ret, everything()) %>%\n  drop_na()\ndata %>%\n  group_by(industry) %>%\n  mutate(ret = 100 * ret) %>%\n  ggplot(aes(x = industry, y = ret)) + \n  geom_boxplot() + coord_flip() +\n  labs(x = NULL, y = NULL,\n       title = \"Industry excess return distribtion (in %)\")"},{"path":"factor-selection-via-machine-learning.html","id":"the-tidymodels-workflow","chapter":"9 Factor selection via machine learning","heading":"9.3 The tidymodels workflow","text":"illustrate penalized linear regressions, employ tidymodels collection packages modeling ML using tidyverse principles. can simply use install.packages(\"tidymodels\") get access related packages. recommend checking work Max Kuhn Julia Silge: continuously write great book ‘Tidy Modeling R’ using tidy principles.tidymodels workflow encompasses main stages modeling process: pre-processing data, model fitting, post-processing results. demonstrate , tidymodels provides efficient workflows can update low effort.Using ideas Ridge Lasso regressions, following example guides () pre-processing data (data split variable mutation), (ii) building models, (iii) fitting models, (iv) tuning models create “best” possible predictions.start, restrict analysis just one industry: Agriculture. first split sample training test set.\npurpose, tidymodels provides function initial_time_split rsample package.\nsplit takes last 20% data test set, used model tuning.\nuse test set evaluate predictive accuracy --sample scenario.object split simply keeps track observations training test set.\ncan call training set training(split), can extract test set testing(split).","code":"\nsplit <- initial_time_split(\n  data %>%\n    filter(industry == \"Agric\") %>%\n    select(-industry),\n  prop = 4 / 5\n)\nsplit## <Training/Testing/Total>\n## <517/130/647>"},{"path":"factor-selection-via-machine-learning.html","id":"pre-process-data","chapter":"9 Factor selection via machine learning","heading":"9.3.1 Pre-process data","text":"Recipes help pre-process data training model. Recipes series pre-processing steps variable selection, transformation, conversion qualitative predictors indicator variables. recipe starts formula defines general structure dataset role variable (regressor dependent variable). dataset, recipe contains following steps fit model:formula defines want explain excess returns available predictors.exclude column month analysis.include interaction terms factors macroeconomic predictors.demean scale regressor standard deviation one.table available recipe steps can found . 2022, 150 different processing steps available! One important point: definition recipe trigger calculations yet rather provides description tasks applied. result, easy reuse recipes different models thus make sure outcomes comparable based input.\nexample , make difference whether use input data = training(split) data = testing(split).\nmatters early stage column names types.can apply recipe data suitable structure. code combines two different functions: prep estimates required parameters training set can applied data sets later. bake applies processed computations new data.Note resulting data contains 130 observations test set 126 columns. many? Recall recipe states compute every possible interaction term factors predictors, increases dimension data matrix substantially.may ask stage: use recipe instead simply using data wrangling commands mutate select? tidymodels beauty lot happening hood. Recall, simple scaling step, actually compute standard deviation column, store value, apply identical transformation different dataset, e.g., testing(split). prepped recipe stores values hands bake novel dataset. Easy pie tidymodels, isn’t ?","code":"\nrec <- recipe(ret ~ ., data = training(split)) %>%\n  step_rm(month) %>% \n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) %>% \n  step_normalize(all_predictors()) %>%\n  step_center(ret, skip = TRUE)\ntmp_data <- bake(prep(rec, training(split)), new_data = testing(split))\ntmp_data## # A tibble: 130 × 126\n##   factor_ff_rf factor_ff_mkt_excess factor_ff_smb\n##          <dbl>                <dbl>         <dbl>\n## 1        -1.92                0.644        0.298 \n## 2        -1.88                1.27         0.387 \n## 3        -1.88                0.341        1.43  \n## 4        -1.88               -1.80        -0.0411\n## 5        -1.88               -1.29        -0.627 \n## # … with 125 more rows, and 123 more variables:\n## #   factor_ff_hml <dbl>, factor_q_me <dbl>,\n## #   factor_q_ia <dbl>, factor_q_roe <dbl>,\n## #   factor_q_eg <dbl>, macro_dp <dbl>, macro_dy <dbl>,\n## #   macro_ep <dbl>, macro_de <dbl>, macro_svar <dbl>,\n## #   macro_bm <dbl>, macro_ntis <dbl>, macro_tbl <dbl>,\n## #   macro_lty <dbl>, macro_ltr <dbl>, …"},{"path":"factor-selection-via-machine-learning.html","id":"build-a-model","chapter":"9 Factor selection via machine learning","heading":"9.3.2 Build a model","text":"Next, can build actual model based pre-processed data. line definition , estimate regression coefficients Lasso regression get\n\\[\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\] want emphasize tidymodels workflow model similar, irrespective specific model. see , straightforward fit Ridge regression coefficients - later - Neural networks Random forests basically code. structure always follows: create -called workflow use fit function. table available model APIs available .\nnow, start linear regression model given value penalty factor \\(\\lambda\\). setup , mixture denotes value \\(\\rho\\), hence setting mixture = 1 implies Lasso.’s - done! object lm_model contains definition model required information. Note set_engine(\"glmnet\") indicates API character tidymodels workflow: hood, package glmnet heavy lifting, linear_reg provides unified framework collect inputs. workflow ends combining everything necessary (serious) data science workflow, namely, recipe model.","code":"\nlm_model <- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) %>%\n  set_engine(\"glmnet\", intercept = FALSE)\nlm_fit <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(lm_model)\nlm_fit## ══ Workflow ═══════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ───────────────────────────────────────\n## 4 Recipe Steps\n## \n## • step_rm()\n## • step_interact()\n## • step_normalize()\n## • step_center()\n## \n## ── Model ──────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = 1e-04\n##   mixture = 1\n## \n## Engine-Specific Arguments:\n##   intercept = FALSE\n## \n## Computational engine: glmnet"},{"path":"factor-selection-via-machine-learning.html","id":"fit-a-model","chapter":"9 Factor selection via machine learning","heading":"9.3.3 Fit a model","text":"workflow , ready use fit. Typically, use training data fit model.\ntraining data pre-processed according recipe steps, Lasso regression coefficients computed.\nFirst, focus predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) figure illustrates projections entire time series Agricultur industry portfolio returns. grey area indicates --sample period, use fit model.estimated coefficients look like? analyze values illustrate difference tidymodels workflow underlying glmnet package, worth computing coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. code estimates coefficients Lasso Ridge regression processed training data sample. Note glmnet actually takes vector y matrix regressors \\(X\\) input. Moreover, glmnet requires choosing penalty parameter \\(\\alpha\\), corresponds \\(\\rho\\) notation . using tidymodels model API, details need consideration.objects fit_lasso fit_ridge contain entire sequence estimated coefficients multiple values penalty factor \\(\\lambda\\). figure illustrates trajectories regression coefficients function penalty factor. Lasso Ridge coefficients converge zero penalty factor increases.One word caution: package glmnet computes estimates coefficients \\(\\hat\\beta\\) based numerical optimization procedures.\nresult, estimated coefficients special case regularization (\\(\\lambda = 0\\)) can deviate standard OLS estimates.","code":"\npredicted_values <- lm_fit %>%\n  fit(data = training(split)) %>%\n  predict(data %>% filter(industry == \"Agric\")) %>%\n  bind_cols(data %>% filter(industry == \"Agric\")) %>%\n  select(month, \n         \"Fitted value\"= .pred, \n         \"Realization\" = ret) %>%\n  pivot_longer(-month, names_to = \"Variable\")\n\npredicted_values %>%\n  ggplot(aes(x = month, y = value, color = Variable)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Monthly realized and fitted agricultural industry risk premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) seq.Date(from = min(x), to = max(x), by = \"5 years\"),\n    minor_breaks = function(x) seq.Date(from = min(x), to = max(x), by = \"1 years\"),\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) + \n  annotate(\"rect\", \n           xmin = testing(split) %>% pull(month) %>% min(), \n           xmax = testing(split) %>% pull(month) %>% max(), \n           ymin = -Inf, ymax = Inf, \n           alpha = 0.5, fill=\"grey70\")\nx <- tmp_data %>%\n  select(-ret) %>%\n  as.matrix()\ny <- tmp_data %>% pull(ret)\n\nfit_lasso <- glmnet(\n  x = x,\n  y = y,\n  alpha = 1, intercept = FALSE, standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge <- glmnet(\n  x = x,\n  y = y,\n  alpha = 0, intercept = FALSE, standardize = FALSE,\n  lambda.min.ratio = 0\n)\nbind_rows(\n  tidy(fit_lasso) %>% mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) %>% mutate(Model = \"Ridge\")\n) %>%\n  rename(\"Variable\" = term) %>%\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Lambda\", y = NULL,\n    title = \"Estimated Coefficients paths as a function of the penalty factor\"\n  ) +\n  theme(legend.position = \"none\")"},{"path":"factor-selection-via-machine-learning.html","id":"tune-a-model","chapter":"9 Factor selection via machine learning","heading":"9.3.4 Tune a model","text":"compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , simply imposed value penalty hyperparameter \\(\\lambda\\). Model tuning process optimally selecting hyperparameters. tidymodels provides extensive tuning options based -called cross-validation. , refer treatment cross-validation get detailed discussion statistical underpinnings. focus general idea implementation tidymodels.goal choosing \\(\\lambda\\) (hyperparameter, e.g., \\(\\rho\\)) find way produce predictors \\(\\hat{Y}\\) outcome \\(Y\\) minimizes mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, MSPE directly observable. can compute estimate data random observe entire population.Obviously, train algorithm data use compute error, estimate \\(\\hat{\\text{MSPE}}\\) indicate way better predictive accuracy can expect real --sample data. result called overfitting.Cross-validation technique allows us alleviate problem. approximate true MSPE average many mean squared prediction errors obtained creating predictions \\(K\\) new random samples data, none used train algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). practice, done carving piece data pretending independent sample. divide data training set test set. MSPE test set measure actual predictive ability, use training set fit models aim find optimal hyperparameter values. , divide training sample (several) subsets, fit model grid potential hyperparameter values (e.g., \\(\\lambda\\)), evaluate predictive accuracy independent sample. works follows:Specify grid hyperparameters.Obtain predictors \\(\\hat{y}_i(\\lambda)\\) denote predictors used parameters \\(\\lambda\\).Compute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] K-fold cross-validation, computation \\(K\\) times. Simply pick validation set \\(M=T/K\\) observations random think random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), \\(k=1\\).pick \\(K\\)? Large values \\(K\\) preferable training data better imitates original data. However, larger values \\(K\\) much higher computation time.\ntidymodels provides required tools conduct \\(K\\)-fold cross-validation. just update model specification let tidymodels know parameters tune. case, specify penalty factor \\(\\lambda\\) well mixing factor \\(\\rho\\) free parameters. Note simple change existing workflow update_model.sample, consider time-series cross-validation sample. means tune models 20 random samples length five years validation period four years. grid possible hyperparameters, fit model fold evaluate \\(\\hat{\\text{MSPE}}\\) corresponding validation set. Finally, select model specification lowest MSPE validation set. First, define cross-validation folds based training data ., evaluate performance grid different penalty values. tidymodels provides functionalities construct suitable grid hyperparameters grid_regular. code chunk creates \\(10 \\times 3\\) hyperparameters grid. , function tune_grid evaluates models fold.tuning process, collect evaluation metrics (root mean-squared error example) identify optimal model. figure illustrates average validation set’s root mean-squared error value \\(\\lambda\\) \\(\\rho\\).figure shows cross-validated mean squared prediction error drops Lasso Elastic Net spikes afterward. Ridge regression, MSPE increases certain threshold. Recall larger regularization, restricted model becomes. Thus, choose model lowest MSPE, exhibits intermediate level regularization.","code":"\nlm_model <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit %>%\n  update_model(lm_model)\ndata_folds <- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\nlm_tune <- lm_fit %>%\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),\n    metrics = metric_set(rmse)\n  )\nautoplot(lm_tune) +\n  labs(y = \"Root mean-squared prediction error\",\n       title = \"MSPE for Agricultur excess returns\",\n       subtitle = \"Lasso (1.0), Ridge (0.0), and Elastic Net (0.5) with different levels of regularization.\")"},{"path":"factor-selection-via-machine-learning.html","id":"parallelized-workflow","chapter":"9 Factor selection via machine learning","heading":"9.3.5 Parallelized workflow","text":"starting point question: factors determine industry returns? illustrate entire workflow, now run penalized regressions ten industries. want identify relevant variables fitting Lasso models industry returns time series. specifically, perform cross-validation industry identify optimal penalty factor \\(\\lambda\\). , use set finalize_*-functions take list tibble tuning parameter values update objects values. determining best model, compute final fit entire training set analyze estimated coefficients.First, define Lasso model one tuning parameter.following task can easily parallelized reduce computing time substantially. use parallelization capabilities furrr. Note can also just recycle steps collect function.just happened? principle, exactly instead computing Lasso coefficients one industry, ten parallel. final option seed = TRUE required make cross-validation process reproducible.\nNow, just housekeeping keep variables Lasso set zero. illustrate results heat map.heat map conveys two main insights. First, see lot white, means many factors, macroeconomic variables, interaction terms relevant explaining cross-section returns across industry portfolios. fact, market factor return--equity factor play role several industries. Second, seems quite heterogeneity across different industries. even market factor selected Lasso Utilities (means proposed model essentially just contains intercept), many factors selected , e.g., High-Tech Energy, coincide . words, seems clear picture need many factors, Lasso provide conses across industries comes pricing abilities.","code":"\nlasso_model <- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) %>%\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit %>%\n  update_model(lasso_model)\nselect_variables <- function(input) {\n  # Split into training and testing data\n  split <- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds <- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune <- lm_fit %>%\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Finalizing: Identify the best model and fit with the training data\n  lasso_lowest_rmse <- lm_tune %>% select_by_one_std_err(\"rmse\")\n  lasso_final <- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit <- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  lasso_final_fit %>%\n    extract_fit_parsnip() %>%\n    tidy() %>%\n    mutate(\n      term = gsub(\"factor_|macro_|industry_\", \"\", term)\n    )\n}\n\n# Parallelization\nplan(multisession, workers = availableCores()) \n\n# Computation by industry\nselected_factors <- data %>%\n  nest(data = -industry) %>% \n  mutate(selected_variables = future_map(data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\nselected_factors %>%\n  unnest(selected_variables) %>%\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) %>%\n  add_count(term) %>%\n  mutate(\n    term = gsub(\"NA|ff_|q_\", \"\", term),\n    term = gsub(\"_x_\", \" \", term),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) %>%\n  filter(term != \"Other\") %>%\n  mutate(term = fct_drop(term)) %>%\n  complete(industry, term, fill = list(selected = 0)) %>%\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"cornflowerblue\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )"},{"path":"factor-selection-via-machine-learning.html","id":"exercises-7","chapter":"9 Factor selection via machine learning","heading":"9.4 Exercises","text":"Write function requires three inputs, namely, y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), lambda returns Ridge estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function indicate whether \\(X\\) contains vector ones first column, exempt \\(L_2\\) penalty.Compute \\(L_2\\) norm (\\(\\beta'\\beta\\)) regression coefficients based predictive regression previous exercise range \\(\\lambda\\)’s illustrate effect penalization suitable figure.Now, write function requires three inputs, namely,y (\\(T\\) vector), X (\\((T \\times K)\\) matrix), ’lambda` returns Lasso estimator (\\(K\\) vector) given penalization parameter \\(\\lambda\\). Recall intercept penalized. Therefore, function indicate whether \\(X\\) contains vector ones first column, exempt \\(L_1\\) penalty.understand Ridge Lasso regressions , familiarize glmnet() package’s documentation. thoroughly tested well-established package provides efficient code compute penalized regression coefficients Ridge Lasso combinations, commonly called Elastic Nets.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"option-pricing-via-machine-learning","chapter":"10 Option pricing via machine learning","heading":"10 Option pricing via machine learning","text":"Machine learning (ML) seen part artificial intelligence.\nML algorithms build model based training data order make predictions decisions without explicitly programmed .\nML can specified along vast array different branches, chapter focuses -called supervised learning regressions. basic idea supervised learning algorithms build mathematical model data contains inputs desired outputs. chapter, apply well-known methods random forests neural networks simple application option pricing. specifically, going create artificial dataset option prices different values based Black-Scholes pricing equation call options. , train different models learn price call options without prior knowledge theoretical underpinnings famous option pricing equation.roadmap follows: First, briefly introduce regression trees, random forests, neural networks. focus implementation, leave thorough treatment statistical underpinnings textbooks authors real comparative advantage issues.\nshow implement random forests deep neural networks tidy principles using tidymodels tensorflow complicated network structures.order replicate analysis regarding neural networks chapter, install TensorFlow system, requires administrator rights machine. Parts can done within R. Just follow quick-start instructions.Throughout chapter, need following packages.","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(keras)\nlibrary(hardhat)"},{"path":"option-pricing-via-machine-learning.html","id":"regression-trees-and-random-forests","chapter":"10 Option pricing via machine learning","heading":"10.1 Regression trees and random forests","text":"Regression trees popular ML approach incorporating multiway predictor interactions. Trees fully nonparametric possess logic departs markedly traditional regressions. Trees designed find groups observations behave similarly . tree “grows” sequence steps. step, new “branch” sorts data leftover preceding step bins based one predictor variables. sequential branching slices space predictors rectangular partitions approximates unknown function \\(f(x)\\) average value outcome variable within partition.partition predictor space \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). predictor \\(x\\) falls within region \\(R_j\\) estimate \\(f(x)\\) average training observations, \\(\\hat y_i\\), associated predictor \\(x_i\\) also \\(R_j\\). select partition \\(\\mathbf{x}\\) split order create new partitions, find predictor \\(j\\) value \\(s\\) define two new partitions, called \\(R_1(j,s)\\) \\(R_2(j,s)\\), split observations current partition asking \\(x_j\\) bigger \\(s\\):\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j < s\\} \\mbox{   } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\npick \\(j\\) \\(s\\), find pair minimizes residual sum square (RSS):\n\\[\n\\sum_{:\\, x_i \\R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{:\\, x_i \\R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nNote: Unlike case sample variance, scale number elements \\(R_k(j, s)\\)! chapter penalized regressions, first relevant question : hyperparameter decisions? Instead regularization parameter, trees fully determined number branches used generate partition (sometimes one specifies minimum number observations final branch instead maximum number branches).Models single tree suffer high variance. Random forests address shortcomings decision trees. goal improve predictive performance reduce instability averaging multiple decision trees (forest trees constructed randomness). forest basically implies creating many regression trees averaging predictions. assure individual trees , use bootstrap induce randomness. specifically, build \\(B\\) decision trees \\(T_1, \\ldots, T_B\\) using training sample. purpose, randomly select features included building tree. observation test set form prediction \\(\\hat{y} = \\frac{1}{B}\\sum\\limits_{=1}^B\\hat{y}_{T_i}\\).","code":""},{"path":"option-pricing-via-machine-learning.html","id":"neural-networks","chapter":"10 Option pricing via machine learning","heading":"10.2 Neural networks","text":"Roughly speaking, neural networks propagate information input layer, one multiple hidden layers, output layer. number units (neurons) input layer equal dimension predictors, output layer usually consists one neuron (regression) multiple neurons classification. output layer predicts future data, similar fitted value regression analysis. Neural networks theoretical underpinnings “universal approximators” smooth predictive association (Hornik 1991). complexity, however, ranks neural networks among least transparent, least interpretable, highly parameterized ML tools.neuron applies nonlinear “activation function” \\(f\\) aggregated signal \nsending output next layer\n\\[x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)\\]\neasiest case \\(f(x) = \\alpha + \\beta x\\) resembles linear regression, typical activation functions sigmoid (.e., \\(f(x) = (1+e^{-x})^{-1}\\)) ReLu (.e., \\(f(x) = max(x, 0)\\)).Neural networks gain flexibility chaining multiple layers together. Naturally, imposes many degrees freedom network architecture clear theoretical guidance exists. specification neural network requires, minimum, stance depth (number hidden layers), activation function, number neurons, connection structure units (dense sparse), application regularization techniques avoid overfitting. Finally, learning means choose optimal parameters relying numerical optimization, often requires specifying appropriate learning rate.Despite computational challenges, implementation R tedious can use API tensorflow.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"option-pricing","chapter":"10 Option pricing via machine learning","heading":"10.3 Option pricing","text":"apply ML methods relevant field finance, focus option pricing. application core taken Hull (2020). basic form, call options give owner right obligation buy specific stock (underlying) specific price (strike price \\(K\\)) specific date (exercise date \\(T\\)). Black–Scholes price (Black Scholes 1973) call option non-dividend-paying underlying stock given \n\\[\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left[\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right]\n\\end{aligned}\n\\]\n\\(C(S, T)\\) price option function today’s stock price underlying, \\(S\\), time maturity\\(T\\), \\(r_f\\) risk-free interest rate, \\(\\sigma\\) volatility underlying stock return. \\(\\Phi\\) cumulative distribution function standard normal random variable.Black-Scholes equation provides way compute arbitrage-free price call option parameters \\(S, K, r_f, T\\), \\(\\sigma\\) specified (arguably, parameters easy specify except \\(\\sigma\\) estimated). simple R function allows computing price .","code":"\nblack_scholes_price <- function(S = 50, K = 70, r = 0, T = 1, sigma = 0.2) {\n  d1 <- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  value <- S * pnorm(d1) - K * exp(-r * T) * pnorm(d1 - sigma * sqrt(T))\n  return(value)\n}"},{"path":"option-pricing-via-machine-learning.html","id":"learning-black-scholes","chapter":"10 Option pricing via machine learning","heading":"10.4 Learning Black-Scholes","text":"illustrate concept ML showing ML methods learn Black-Scholes equation observing different specifications corresponding prices without us revealing exact pricing equation.","code":""},{"path":"option-pricing-via-machine-learning.html","id":"data-simulation","chapter":"10 Option pricing via machine learning","heading":"10.4.1 Data simulation","text":"end, start simulated data. compute option prices call options grid different combinations times maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), current stock prices (S). code , add idiosyncratic error term observation prices considered exactly reflect values implied Black-Scholes equation.code generates 1.5 million random parameter constellations. values, two observed prices reflecting Black-Scholes prices given random innovation term pollutes observed prices.Next, split data training set (contains 1% observed option prices) test set used final evaluation. Note entire grid possible combinations contains 3148992 different specifications. Thus, sample learn Black-Scholes price contains 31489 observations therefore relatively small.\norder keep analysis reproducible, use set.seed(). random seed specifies start point computer generates random number sequence ensures simulated data across different machines.process training dataset fit different ML models. define recipe defines processing steps purpose. specific case, want explain observed price five variables enter Black-Scholes equation. true price (stored black_scholes) obviously used fit model. recipe also reflects standardize predictors ensure variable exhibits sample average zero sample standard deviation one.","code":"\noption_prices <- expand_grid(\n  S = 40:60, # Stock price\n  K = 20:90, # Strike price\n  r = seq(from = 0, to = 0.05, by = 0.01), # Risk-free rate\n  T = seq(from = 3 / 12, to = 2, by = 1 / 12), # Time to maturity\n  sigma = seq(from = 0.1, to = 0.8, by = 0.1) # Volatility\n) %>%\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map(black_scholes, function(x) x + rnorm(2, sd = 0.15))\n  ) %>%\n  unnest(observed_price)\nset.seed(420)\nsplit <- initial_split(option_prices, prop = 1 / 100)\ndata_folds <- vfold_cv(training(split), v = 10)\nrec <- recipe(observed_price ~ .,\n  data = option_prices\n) %>%\n  step_rm(black_scholes) %>%\n  step_normalize(all_predictors())"},{"path":"option-pricing-via-machine-learning.html","id":"single-layer-networks-and-random-forests","chapter":"10 Option pricing via machine learning","heading":"10.4.2 Single layer networks and random forests","text":"Next, show fit neural network data. Note requires keras installed local machine. function mlp package parsnip provides functionality initialize single layer, feed-forward neural network. specification defines single layer feed-forward neural network 15 hidden units. set number training iterations epochs = 500. option set_mode(\"regression\") specifies linear activation function output layer.verbose=0 argument prevents logging results. can follow straightforward tidymodel workflow chapter : Define workflow, equip recipe, specify associated model. Finally, fit model training data.familiar tidymodel workflow, piece cake fit models parsnip family. instance, model initializes random forest 50 trees contained ensemble require least 20 observations node.Fitting model follows exactly convention neural network .","code":"\nnnet_model <- mlp(\n  epochs = 500,\n  hidden_units = 10,\n) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"keras\", verbose = FALSE)\nnn_fit <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(nnet_model) %>%\n  fit(data = training(split))\nrf_model <- rand_forest(\n  trees = 50,\n  min_n = 20\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\nrf_fit <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(rf_model) %>%\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning.html","id":"deep-neural-networks","chapter":"10 Option pricing via machine learning","heading":"10.4.3 Deep neural networks","text":"Note tidymodels workflow extremely convenient, sophisticated deep neural networks supported yet (January 2022). reason, code snippet illustrates initialize sequential model three hidden layers 10 units per layer. keras package provides convenient interface flexible enough handle different activation functions. compile command defines loss function model predictions evaluated.train neural network, provide inputs (x) variable predict (y) fit parameters. Note slightly tedious use method extract_mold(nn_fit). Instead simply using raw data, fit neural network processed data used single-layer feed-forward network. difference simply calling x = training(data) %>% select(-observed_price, -black_scholes)? Recall recipe standardizes variables columns unit standard deviation zero mean. , adds consistency ensure models trained using recipe change recipe reflected performance model. final note potentially irritating observation: Note fit() alters keras model - one instances function R alters input function call object model anymore!","code":"\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 10, activation = \"sigmoid\", input_shape = 5) %>%\n  layer_dense(units = 10, activation = \"sigmoid\") %>%\n  layer_dense(units = 10, activation = \"sigmoid\") %>%\n  layer_dense(units = 1, activation = \"linear\") %>%\n  compile(\n    loss = \"mean_absolute_error\"\n  )\nmodel## Model: \"sequential_1\"\n## _______________________________________________________\n##  Layer (type)           Output Shape          Param #  \n## =======================================================\n##  dense_5 (Dense)        (None, 10)            60       \n##  dense_4 (Dense)        (None, 10)            110      \n##  dense_3 (Dense)        (None, 10)            110      \n##  dense_2 (Dense)        (None, 1)             11       \n## =======================================================\n## Total params: 291\n## Trainable params: 291\n## Non-trainable params: 0\n## _______________________________________________________\nmodel %>%\n  fit(\n    x = extract_mold(nn_fit)$predictors %>% as.matrix(),\n    y = extract_mold(nn_fit)$outcomes %>% pull(observed_price),\n    epochs = 500, verbose = FALSE\n  )"},{"path":"option-pricing-via-machine-learning.html","id":"universal-approximation","chapter":"10 Option pricing via machine learning","heading":"10.4.4 Universal approximation","text":"evaluate results, implement one model: principle, non-linear function can also approximated linear model containing input variables’ polynomial expansions. illustrate , first define new recipe, rec_linear, processes training data even . include polynomials tenth degree predictor add possible pairwise interaction terms. final recipe step, step_lincomb, removes potentially redundant variables (instance, interaction \\(r^4\\) \\(r^5\\) term \\(r^9\\)). fit Lasso regression model pre-specified penalty term (consult chapter factor selection tune model hyperparameters).","code":"\nrec_linear <- rec %>%\n  step_poly(all_predictors(), degree = 10, options = list(raw = T)) %>%\n  step_interact(terms = ~ all_predictors():all_predictors()) %>%\n  step_lincomb(all_predictors())\n\nlm_model <- linear_reg(penalty = 0.01) %>%\n  set_engine(\"glmnet\")\n\nlm_fit <- workflow() %>%\n  add_recipe(rec_linear) %>%\n  add_model(lm_model) %>%\n  fit(data = training(split))"},{"path":"option-pricing-via-machine-learning.html","id":"prediction-evaluation","chapter":"10 Option pricing via machine learning","heading":"10.5 Prediction evaluation","text":"Finally, collect predictions compare --sample prediction error evaluated ten thousand new data points. Note evaluation, use call extract_mold ensure use pre-processing steps testing data across model. also use somewhat advanced functionality hardhat::forge, provides easy, consistent, robust pre-processor prediction time.lines , use fitted models generate predictions entire test data set option prices. evaluate absolute pricing error one possible measure pricing accuracy, defined absolute value difference predicted option price theoretical correct option price Black-Scholes model.results can summarized follow: ) ML methods seem able price call options observing training test set. ii) average prediction errors increase far -money options, especially Single Layer neural network Random Forests. ii) Random forest Lasso seem perform consistently worse prediction option prices Neural networks. iii) complexity deep neural network relative single layer neural network result better --sample predictions.","code":"\nout_of_sample_data <- testing(split) %>% slice_sample(n = 10000)\n\npredictive_performance <- model %>%\n  predict(forge(out_of_sample_data, extract_mold(nn_fit)$blueprint)$predictors %>% as.matrix()) %>%\n  as.vector() %>%\n  tibble(\"Deep NN\" = .) %>%\n  bind_cols(nn_fit %>%\n    predict(out_of_sample_data)) %>%\n  rename(\"Single layer\" = .pred) %>%\n  bind_cols(lm_fit %>% predict(out_of_sample_data)) %>%\n  rename(\"Lasso\" = .pred) %>%\n  bind_cols(rf_fit %>% predict(out_of_sample_data)) %>%\n  rename(\"Random forest\" = .pred) %>%\n  bind_cols(out_of_sample_data) %>%\n  pivot_longer(\"Deep NN\":\"Random forest\", names_to = \"Model\") %>%\n  mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\npredictive_performance %>%\n  ggplot(aes(x = moneyness, y = pricing_error, color = Model)) +\n  geom_jitter(alpha = 0.05) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Absolut prediction error (USD)\",\n    title = \"Prediction errors: Call option prices\"\n  )"},{"path":"option-pricing-via-machine-learning.html","id":"exercises-8","chapter":"10 Option pricing via machine learning","heading":"10.6 Exercises","text":"Write function takes y matrix predictors X inputs returns characterization relevant parameters regression tree 1 branch.Create function creates predictions new matrix predictors `newX´ based estimated regression tree.Use package rpart grow tree based training data use illustration tools rpart understand characteristics tree deems relevant option pricing.Make use training test set choose optimal depth (number sample splits) tree.Use ‘keras’ initialize sequential neural network can take predictors training dataset input, contains least one hidden layer, generates continuous predictions. sounds harder : see simple regression example . many parameters neural network aim fit ?Next, compile object. important specify loss function. Illustrate difference predictive accuracy different architecture choices.","code":""},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies","chapter":"11 Parametric portfolio policies","heading":"11 Parametric portfolio policies","text":"chapter, introduce different portfolio performance measures evaluate compare portfolio allocation strategies.\npurpose, introduce direct way estimate optimal portfolio weights large-scale cross-sectional applications. precisely, approach Brandt, Santa-Clara, Valkanov (2009) proposes parametrize optimal portfolio weights function stock characteristics directly, instead estimating stock’s expected return, variance, covariances stocks prior step.\nchose weights function characteristics maximize expected utility investor. approach feasible large portfolio dimensions (entire CRSP universe) proposed Brandt, Santa-Clara, Valkanov (2009). review paper Brandt (2010) provides excellent treatment related portfolio choice methods.","code":""},{"path":"parametric-portfolio-policies.html","id":"data-preparation-6","chapter":"11 Parametric portfolio policies","heading":"11.1 Data preparation","text":"get started, load required packages alongside monthly CRSP file, forms investment universe. load data SQLite-database introduced chapter “Accessing & managing financial data”.evaluate performance portfolios, use monthly market returns benchmark compute CAPM alphas.Next, retrieve stock characteristics shown effect expected returns expected variances (even higher moments) return distribution. particular, record lagged one-year return momentum (momentum_lag), defined compounded return months \\(t − 13\\) \\(t − 2\\) firm. second characteristic firm’s market equity (size_lag), defined log price per share times number shares outstanding. construct correct lagged values, use approach introduced chapter “Accessing & managing financial data”.","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\ntidy_finance <- dbConnect(SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") %>%\n  collect()\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") %>%\n  collect()\ncrsp_monthly_lags <- crsp_monthly %>%\n  transmute(permno,\n    month_12 = month %m+% months(12),\n    mktcap\n  )\n\ncrsp_monthly <- crsp_monthly %>%\n  inner_join(crsp_monthly_lags,\n    by = c(\"permno\", \"month\" = \"month_12\"),\n    suffix = c(\"\", \"_12\")\n  )\n\ndata_portfolios <- crsp_monthly %>%\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_12,\n    size_lag = log(mktcap_lag)\n  ) %>%\n  drop_na(contains(\"lag\"))"},{"path":"parametric-portfolio-policies.html","id":"parametric-portfolio-policies-1","chapter":"11 Parametric portfolio policies","heading":"11.2 Parametric portfolio policies","text":"basic idea parametric portfolio weights follows. Suppose date \\(t\\) \\(N_t\\) stocks investment universe, stock \\(\\) return \\(r_{, t+1}\\) associated vector firm characteristics \\(x_{, t}\\) time-series momentum market capitalization. investor’s problem choose portfolio weights \\(w_{,t}\\) maximize expected utility portfolio return:\n\\[\\begin{aligned}\n\\max_{w} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{=1}^{N_t}w_{,t}r_{,t+1}\\right)\\right]\n\\end{aligned}\\]\n\\(u(\\cdot)\\) denotes utility function.stock characteristics show ? parameterize optimal portfolio weights function stock characteristic \\(x_{,t}\\) following linear specification portfolio weights:\n\\[w_{,t} = \\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t},\\]\n\\(\\bar{w}_{,t}\\) stock’s weight benchmark portfolio (use value-weighted naive portfolio application ), \\(\\theta\\) vector coefficients going estimate, \\(\\hat{x}_{,t}\\) characteristics stock \\(\\), cross-sectionally standardized zero mean unit standard deviation.Intuitively, portfolio strategy form active portfolio management relative performance benchmark. Deviations benchmark portfolio derived individual stock characteristics. Note construction weights sum one \\(\\sum_{=1}^{N_t}\\hat{x}_{,t} = 0\\) due standardization. Moreover, coefficients constant across assets time. implicit assumption characteristics fully capture aspects joint distribution returns relevant forming optimal portfolios.first implement cross-sectional standardization entire CRSP universe. also keep track (lagged) relative market capitalization relative_mktcap, represent value-weighted benchmark portfolio, n denotes number traded assets \\(N_t\\), use construct naive portfolio benchmark.","code":"\ndata_portfolios <- data_portfolios %>%\n  group_by(month) %>%\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) %>%\n  ungroup() %>%\n  select(-mktcap_lag, -altprc)"},{"path":"parametric-portfolio-policies.html","id":"computing-portfolio-weights","chapter":"11 Parametric portfolio policies","heading":"11.3 Computing portfolio weights","text":"Next, move identify optimal choices \\(\\theta\\). rewrite optimization problem together weight parametrization can estimate \\(\\theta\\) maximize objective function based sample\n\\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{=1}^{N_t}\\left(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\right)r_{,t+1}\\right).\n\\end{aligned}\\]\nallocation strategy straightforward number parameters estimate small. Instead tedious specification \\(N_t\\) dimensional vector expected returns \\(N_t(N_t+1)/2\\) free elements covariance matrix, need focus application vector \\(\\theta\\). \\(\\theta\\) contains two elements application - relative deviation benchmark due size momentum.get feeling performance allocation strategy, start arbitrary initial vector \\(\\theta_0\\). next step choose \\(\\theta\\) optimally maximize objective function. automatically detect number parameters counting number columns lagged values.function compute_portfolio_weights computes portfolio weights \\(\\bar{w}_{,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{,t}\\) according parametrization given value \\(\\theta_0\\) \\(\\theta\\). Everything happens within single pipeline, hence provide short walkthrough.first compute characteristic_tilt, tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{, t}\\) resemble deviation benchmark portfolio. Next, compute benchmark portfolio weight_benchmark, can reasonable set portfolio weights. case, choose either value equal-weighted allocation.\nweight_tilt completes picture contains final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt deviate benchmark portfolio depending stock characteristics.final lines go bit implement simple version -short sale constraint. generally straightforward ensure portfolio weight constraints via parameterization, simply normalize portfolio weights enforced positive. Finally, make sure normalized weights sum one . \\[w_{,t}^+ = \\frac{\\max(0, w_{,t})}{\\sum\\limits_{j=1}^{N_t}\\max(0, w_{,t})}.\\]following function computes optimal portfolio weights way just described.next step compute portfolio weights arbitrary vector \\(\\theta_0\\). example , use value-weighted portfolio benchmark allow negative portfolio weights.","code":"\nn_parameters <- sum(grepl(\"lag\", colnames(data_portfolios)))\ntheta <- rep(1.5, n_parameters)\nnames(theta) <- colnames(data_portfolios)[grepl(\n  \"lag\",\n  colnames(data_portfolios)\n)]\ncompute_portfolio_weights <- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data %>%\n    group_by(month) %>%\n    bind_cols(\n      characteristic_tilt = data %>%\n        transmute(across(contains(\"lag\"), ~ . / n)) %>%\n        as.matrix() %*% theta %>% as.numeric()\n    ) %>%\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) %>%\n    ungroup()\n}\nweights_crsp <- compute_portfolio_weights(theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)"},{"path":"parametric-portfolio-policies.html","id":"portfolio-performance","chapter":"11 Parametric portfolio policies","heading":"11.4 Portfolio performance","text":"computed weights optimal way? likely , picked \\(\\theta_0\\) arbitrarily. evaluate performance allocation strategy, one can think many different approaches. original paper, Brandt, Santa-Clara, Valkanov (2009) focus simple evaluation hypothetical utility agent equipped power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^\\gamma}{1-\\gamma}\\), \\(\\gamma\\) risk aversion factor.noted, Gehrig, Sögner, Westerkamp (2020) warn leading case constant relative risk aversion (CRRA) strong assumptions properties returns, variables used implement parametric portfolio policy \nparameter space necessary obtain well defined optimization problem.doubt, many ways evaluate portfolio. function provides summary kinds interesting measures can considered relevant. need evaluation measures? depends: original paper Brandt, Santa-Clara, Valkanov (2009) cares expected utility choose \\(\\theta\\). However, want choose optimal values achieve highest performance putting constraints portfolio weights, helpful everything one function.Let us take look different portfolio strategies evaluation measures.value-weighted portfolio delivers annualized return 6 percent clearly outperforms tilted portfolio, irrespective whether evaluate expected utility, Sharpe ratio CAPM alpha. can conclude market beta close one strategies (naturally almost identically 1 value-weighted benchmark portfolio). comes distribution portfolio weights, see benchmark portfolio weight takes less extreme positions (lower average absolute weights lower maximum weight). definition, value-weighted benchmark take negative positions, tilted portfolio also takes short positions.","code":"\npower_utility <- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\nevaluate_portfolio <- function(weights_crsp,\n                               full_evaluation = TRUE) {\n  evaluation <- weights_crsp %>%\n    group_by(month) %>%\n    summarize(\n      return_tilt = weighted.mean(ret_excess, weight_tilt),\n      return_benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) %>%\n    pivot_longer(-month,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) %>%\n    group_by(model) %>%\n    left_join(factors_ff_monthly, by = \"month\") %>%\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(12 * portfolio_return),\n      \"SD return\" = 100 * sqrt(12) * sd(portfolio_return),\n      \"Sharpe ratio\" = mean(portfolio_return) / sd(portfolio_return),\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n    )) %>%\n    mutate(model = gsub(\"return_\", \"\", model)) %>%\n    pivot_longer(-model, names_to = \"measure\") %>%\n    pivot_wider(names_from = model, values_from = value)\n\n  if (full_evaluation) {\n    weight_evaluation <- weights_crsp %>%\n      select(month, contains(\"weight\")) %>%\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") %>%\n      group_by(model, month) %>%\n      transmute(tibble(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight < 0]),\n        \"Avg. fraction of negative weights\" = sum(weight < 0) / n()\n      )) %>%\n      group_by(model) %>%\n      summarize(across(-month, ~ 100 * mean(.))) %>%\n      mutate(model = gsub(\"weight_\", \"\", model)) %>%\n      pivot_longer(-model, names_to = \"measure\") %>%\n      pivot_wider(names_from = model, values_from = value)\n    evaluation <- bind_rows(evaluation, weight_evaluation)\n  }\n  return(evaluation)\n}\nevaluate_portfolio(weights_crsp) %>% print(n = Inf)## # A tibble: 11 × 3\n##    measure                           benchmark     tilt\n##    <chr>                                 <dbl>    <dbl>\n##  1 Expected utility                   -2.49e-1 -0.262  \n##  2 Average return                      6.86e+0 -0.604  \n##  3 SD return                           1.53e+1 21.0    \n##  4 Sharpe ratio                        1.29e-1 -0.00831\n##  5 CAPM alpha                          1.08e-4 -0.00574\n##  6 Market beta                         9.92e-1  0.927  \n##  7 Absolute weight                     2.46e-2  0.0631 \n##  8 Max. weight                         3.52e+0  3.65   \n##  9 Min. weight                         2.78e-5 -0.145  \n## 10 Avg. sum of negative weights        0       78.0    \n## 11 Avg. fraction of negative weights   0       49.4"},{"path":"parametric-portfolio-policies.html","id":"optimal-parameter-choice","chapter":"11 Parametric portfolio policies","heading":"11.5 Optimal parameter choice","text":"Next, move choice \\(\\theta\\) actually aims improve () performance measures. first define helper function compute_objective_function, pass optimizer.may wonder return negative value objective function. simply due common convention optimization procedures search minima default. minimizing negative value objective function, get maximum value result.\nbasic form, R optimization relies function optim. main inputs, function requires initial guess parameters objective function minimize. Now, fully equipped compute optimal values \\(\\hat\\theta\\), maximize hypothetical expected utility investor.resulting values \\(\\hat\\theta\\) easy interpret intuitively. Expected utility increases tilting weights value-weighted portfolio towards smaller stocks (negative coefficient size) towards past winners (positive value momentum).","code":"\ncompute_objective_function <- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data <- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function <- evaluate_portfolio(processed_data,\n                                           full_evaluation = FALSE\n  ) %>%\n    filter(measure == objective_measure) %>%\n    pull(tilt)\n\n  return(-objective_function)\n}\noptimal_theta <- optim(\n  par = theta,\n  compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)\n\noptimal_theta$par## momentum_lag     size_lag \n##        0.189       -2.007"},{"path":"parametric-portfolio-policies.html","id":"more-model-specifications","chapter":"11 Parametric portfolio policies","heading":"11.6 More model specifications","text":"portfolio perform different model specifications? purpose, compute performance number different modeling choices based entire CRSP sample. next code chunk performs heavy lifting.Finally, can compare results. table shows summary statistics possible combinations: equal- value-weighted benchmark portfolio, without short-selling constraints, tilted towards maximizing expected utility.results indicate average annualized Sharpe ratio equal-weighted portfolio exceeds Sharpe ratio value-weighted benchmark portfolio. Nevertheless, starting weighted value portfolio benchmark tilting optimally respect momentum small stocks yields highest Sharpe ratio across specifications. Imposing short-sale constraints improve performance portfolios application.","code":"\nfull_model_grid <- expand_grid(\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE),\n  data = list(data_portfolios)\n) %>%\n  mutate(optimal_theta = pmap(\n    .l = list(\n      data,\n      value_weighting,\n      allow_short_selling\n    ),\n    .f = ~ optim(\n      par = theta,\n      compute_objective_function,\n      data = ..1,\n      objective_measure = \"Expected utility\",\n      value_weighting = ..2,\n      allow_short_selling = ..3\n    )$par\n  ))\nperformance_table <- full_model_grid %>%\n  mutate(\n    processed_data = pmap(\n      .l = list(optimal_theta, data, value_weighting, allow_short_selling),\n      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)\n    ),\n    portfolio_evaluation = map(processed_data,\n      evaluate_portfolio,\n      full_evaluation = TRUE\n    )\n  ) %>%\n  select(value_weighting, allow_short_selling, portfolio_evaluation) %>%\n  unnest(portfolio_evaluation)\n\nperformance_table %>%\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) %>%\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) %>%\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) %>%\n  select(measure, `EW    `, `VW    `, sort(contains(\"Optimal\"))) %>%\n  print(n = 11, max_extra_cols = 7)## # A tibble: 11 × 7\n##    measure             `EW    ` `VW    ` `VW  Optimal `\n##    <chr>                  <dbl>    <dbl>          <dbl>\n##  1 Expected utility    -0.250   -2.49e-1       -0.247  \n##  2 Average return      10.5      6.86e+0       14.7    \n##  3 SD return           20.3      1.53e+1       20.6    \n##  4 Sharpe ratio         0.149    1.29e-1        0.206  \n##  5 CAPM alpha           0.00231  1.08e-4        0.00649\n##  6 Market beta          1.13     9.92e-1        1.01   \n##  7 Absolute weight      0.0246   2.46e-2        0.0379 \n##  8 Max. weight          0.0246   3.52e+0        3.34   \n##  9 Min. weight          0.0246   2.78e-5       -0.0327 \n## 10 Avg. sum of negati…  0        0             27.9    \n## 11 Avg. fraction of n…  0        0             38.8    \n## # … with 3 more variables:\n## #   `VW (no s.) Optimal ` <dbl>, `EW  Optimal ` <dbl>,\n## #   `EW (no s.) Optimal ` <dbl>"},{"path":"parametric-portfolio-policies.html","id":"exercises-9","chapter":"11 Parametric portfolio policies","heading":"11.7 Exercises","text":"estimated parameters \\(\\hat\\theta\\) portfolio performance change objective maximize Sharpe ratio instead hypothetical expected utility?code flexible sense can easily add new firm characteristics. Construct new characteristic evaluate corresponding coefficient \\(\\hat\\theta_i\\).Tweak function optimal_thetasuch can impose additional performance constraints order determine \\(\\hat\\theta\\) maximizes expected utility constraint market beta 1.portfolio performance resemble realistic --sample backtesting procedure? Verify robustness results first estimating \\(\\hat\\theta\\) based past data . , use recent periods evaluate actual portfolio performance.formulating portfolio problem statistical estimation problem, can easily obtain standard errors coefficients weight function. Brandt, Santa-Clara, Valkanov (2009) provide relevant derivations paper Equation (10). Implement small function computes standard errors \\(\\hat\\theta\\).","code":""},{"path":"constrained-optimization-and-backtesting.html","id":"constrained-optimization-and-backtesting","chapter":"12 Constrained optimization and backtesting","heading":"12 Constrained optimization and backtesting","text":"chapter, conduct portfolio backtesting realistic setting including transaction costs investment constraints -short-selling rules. start standard mean-variance efficient portfolios. , introduce constraints step--step. Numerical constrained optimization performed packages quadprog (quadratic objective functions typical mean-variance framework) alabama (general, non-linear objectives constraints).","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(quadprog) \nlibrary(alabama)"},{"path":"constrained-optimization-and-backtesting.html","id":"data-preparation-7","chapter":"12 Constrained optimization and backtesting","heading":"12.1 Data preparation","text":"start loading required data SQLite-database introduced chapter “Accessing & managing financial data”. simplicity, restrict investment universe monthly Fama-French industry portfolio returns following application.","code":"\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", \n                          extended_types = TRUE)\n\nindustry_returns <- tbl(tidy_finance, \"industries_ff_monthly\") %>% \n  collect() \n\nindustry_returns <- industry_returns %>% \n  select(-month)"},{"path":"constrained-optimization-and-backtesting.html","id":"recap-of-portfolio-choice","chapter":"12 Constrained optimization and backtesting","heading":"12.2 Recap of portfolio choice","text":"common objective portfolio optimization find mean-variance efficient portfolio weights, .e., allocation delivers lowest possible return variance given minimum level expected returns. extreme case, investor concerned portfolio variance, may choose implement minimum variance portfolio (MVP) weights given solution \n\\[w_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1\\]\n\\(\\Sigma\\) \\((N \\times N)\\) covariance matrix returns. optimal weights \\(\\omega_\\text{mvp}\\) can found analytically \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). terms code, math equivalent following.Next, consider investor aims achieve minimum variance given required expected portfolio return \\(\\bar{\\mu}\\) chooses\n\\[w_\\text{eff}({\\bar{\\mu}}) =\\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ } \\omega'\\mu \\geq \\bar{\\mu}.\\]\ncan shown (see Exercises) portfolio choice problem can equivalently formulated investor mean-variance preferences risk aversion factor \\(\\gamma\\). investor aims choose portfolio weights \n\\[ w^*_\\gamma = \\arg\\max w' \\mu - \\frac{\\gamma}{2}w'\\Sigma w\\quad s.t. w'\\iota = 1.\\]\nsolution optimal portfolio choice problem :\n\\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\]\nEmpirically, classical solution imposes many problems. particular, estimates \\(\\mu_t\\) noisy short horizons, (\\(N \\times N\\)) matrix \\(\\Sigma_t\\) contains \\(N(N-1)/2\\) distinct elements thus, estimation error huge. Even worse, asset universe contains assets available time periods \\((N > T)\\), sample covariance matrix longer positive definite inverse \\(\\Sigma^{-1}\\) exist anymore. top estimation uncertainty, transaction costs major concern. Rebalancing portfolios costly, , therefore, optimal choice depend investor’s current holdings.","code":"\nSigma <- cov(industry_returns)\nw_mvp <- solve(Sigma) %*% rep(1, ncol(Sigma))\nw_mvp <- as.vector(w_mvp / sum(w_mvp))"},{"path":"constrained-optimization-and-backtesting.html","id":"estimation-uncertainty-and-transaction-costs","chapter":"12 Constrained optimization and backtesting","heading":"12.3 Estimation uncertainty and transaction costs","text":"empirical evidence regarding performance mean-variance optimization procedure simply plug sample estimates \\(\\hat \\mu_t\\) \\(\\hat \\Sigma_t\\) can summarized rather briefly: mean-variance optimization performs poorly! literature discusses many proposals overcome empirical issues. instance, one may impose form regularization \\(\\Sigma\\), rely Bayesian priors inspired theoretical asset pricing models, use high-frequency data improve forecasting. One unifying framework works easily, effectively (even large dimensions), purely inspired economic arguments ex-ante adjustment transaction costs (Hautsch Voigt 2019).Assume returns multivariate normal distribution \\(p_t({r}_{t+1}|\\mathcal{M})=N(\\mu,\\Sigma)\\). Additionally, assume quadratic transaction costs penalize rebalancing \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) :=\\nu_t\\left(\\omega_{t+1}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\]\ncost parameter \\(\\beta>0\\) \\(\\omega_{t^+} := {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). Note \\(\\omega_{t^+}\\) differs mechanically \\(\\omega_t\\) due returns past period.\n, optimal portfolio choice \n\\[\\begin{aligned}\\omega_{t+1} ^* &:=  \\arg\\max_{\\omega \\\\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\\\\n&=\\arg\\max_{\\omega\\\\mathbb{R}^N,\\text{ }  \\iota'\\omega=1}\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega ,\\end{aligned}\\]\n\n\\[\\mu^*:=\\mu+\\beta \\omega_{t^+} \\quad  \\text{} \\quad \\Sigma^*:=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\n\\]\nresult, adjusting transaction costs implies standard mean-variance optimal portfolio choice adjusted return parameters \\(\\Sigma^*\\) \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]alternative formulation optimal portfolio can derived follows:\n\\[\\omega_{t+1} ^*=\\arg\\max_{\\omega\\\\mathbb{R}^N,\\text{ }  \\iota'\\omega=1}\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega.\\]\noptimal weights correspond mean-variance portfolio vector expected returns assets currently exhibit higher weight considered delivering higher expected return.","code":""},{"path":"constrained-optimization-and-backtesting.html","id":"optimal-portfolio-choice","chapter":"12 Constrained optimization and backtesting","heading":"12.4 Optimal portfolio choice","text":"function implements efficient portfolio weight general form, allowing transaction costs (conditional holdings reallocation). \\(\\beta=0\\), computation resembles standard mean-variance efficient framework. gamma denotes coefficient risk aversion,beta transaction cost parameter w_prev weights rebalancing.effect transaction costs different levels risk aversion optimal portfolio choice? following lines code analyze distance MVP portfolio implemented investor different values transaction cost parameter \\(\\beta\\) risk aversion \\(\\gamma\\).figure shows initial portfolio always (sample) MVP higher transaction costs parameter \\(\\beta\\), smaller rebalancing initial portfolio (always set MVP weights example). addition, risk aversion \\(\\gamma\\) increases, efficient portfolio closer MVP weights investor desires less rebalancing initial holdings.","code":"\ncompute_efficient_weight <- function(Sigma,\n                                     mu,\n                                     gamma = 2, \n                                     beta = 0, # transaction costs\n                                     w_prev = 1/ncol(Sigma) * rep(1, ncol(Sigma))){ \n\n  iota <- rep(1, ncol(Sigma))\n  Sigma_processed <- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed <- mu + beta * w_prev\n  \n  Sigma_inverse <- solve(Sigma_processed)\n  \n  w_mvp <- Sigma_inverse %*% iota\n  w_mvp <- as.vector(w_mvp / sum(w_mvp))\n  w_opt <- w_mvp  + 1/gamma * \n    (Sigma_inverse - 1 / sum(Sigma_inverse) * Sigma_inverse %*% iota %*% t(iota) %*% Sigma_inverse) %*% \n    mu_processed\n  return(as.vector(w_opt))\n}\n\nmu <- colMeans(industry_returns)\ncompute_efficient_weight(Sigma, mu)##  [1] -0.1914  0.2291 -0.3242  0.8333  1.0955 -0.6081\n##  [7]  0.8525 -0.1642 -1.7668  0.6720 -0.6077  0.8922\n## [13]  0.1179 -0.0314  1.0545  0.0556  0.3095 -0.2607\n## [19] -1.5556 -0.3682  0.1000  0.6472  0.3684  0.5106\n## [25]  0.0425  0.6461  0.0226  0.7373 -0.1546  0.1093\n## [31] -0.0435 -0.4141 -1.2728  1.1405 -0.7336 -0.0200\n## [37]  0.6600  0.6606 -0.1942 -0.4036  0.3793 -0.5185\n## [43] -0.2023  0.1821 -0.3338  0.6036 -1.2973  1.1322\n## [49] -1.5879\ntransaction_costs <- expand_grid(gamma = c(2, 4, 8, 20),\n                                 beta = 20 * qexp((1:99)/100)) %>% \n  mutate(weights = map2(.x = gamma, \n                        .y = beta,\n                        ~compute_efficient_weight(Sigma,\n                                                  mu,\n                                                  gamma = .x,\n                                                  beta = .y / 10000,\n                                                  w_prev = w_mvp)),\n         concentration = map_dbl(weights, ~sum(abs(. - w_mvp))))\n\ntransaction_costs %>% \n  mutate(`Risk aversion` = as_factor(gamma)) %>% \n  ggplot(aes(x = beta, y = concentration, color = `Risk aversion`)) + \n  geom_line() +\n  scale_x_sqrt() +\n  labs(x = \"Transaction cost parameter\", \n       y = \"Distance from MVP\",\n       title = \"Optimal portfolio weights for different risk aversion and transaction cost\")"},{"path":"constrained-optimization-and-backtesting.html","id":"constrained-optimization","chapter":"12 Constrained optimization and backtesting","heading":"12.5 Constrained optimization","text":"Next, introduce constraints optimization procedure. often, typical constraints short-selling restrictions prevent analytical solutions optimal portfolio weights. However, numerical optimization allows computing solutions constrained problems. purpose mean-variance optimization, rely solve.QP() function package quadprog.function solve.QP() delivers numerical solutions quadratic programming problems form\n\\[\\min(-\\mu \\omega + 1/2 \\omega' \\Sigma \\omega) \\text{ s.t. } ' \\omega >= b_0.\\]\nfunction takes one argument (meq) number equality constraints. Therefore, matrix \\(\\) simply vector ones ensure weights sum one. case short-selling constraints, matrix \\(\\) form\n\\[= \\begin{pmatrix}1 & 1& \\ldots&1 \\\\1 & 0 &\\ldots&0\\\\0 & 1 &\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}'\\qquad b_0 = \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}.\\]\ndive unconstrained optimization, revisit unconstrained problem replicate analytical solutions minimum variance efficient portfolio weights . verify output equal solution. Note round first six digits avoid differences higher digits might arise due inherent imprecision numerical estimation procedures. just discussed, set Amat matrix column ones bvec 1 enforce constraint weights must sum one. meq=1 means one (one) constraints must satisfied equality.complex optimization routines, optimization task view provides overview wast optimization landscape R.Next, approach problems analytical solutions exist. First, additionally impose short-sale constraints, implies \\(N\\) inequality constraints form \\(w_i >=0\\).solve.QP fast benefits clear structure quadratic objective linear constraints. However, optimization typically requires flexibility. example, show compute optimal weights, subject -called regulation T-constraint, requires sum absolute portfolio weights smaller 1.5. constraint implies initial margin requirement 50% , therefore, also non-linear objective function. Thus, can longer rely solve.QP(). Instead, rely package alabama, requires separate definition objective constraint functions.figure shows optimal allocation weights across 49 industries four different strategies considered far: minimum variance, efficient portfolio \\(\\gamma\\) = 2, efficient portfolio short-sale constraints, Regulation-T constrained portfolio.move , want propose final allocation strategy, reflects somewhat realistic structure transaction costs instead quadratic specification used . function computes efficient portfolio weights adjusting \\(L_1\\) transaction costs \\(\\beta\\sum\\limits_{=1}^N |(w_{, t+1} - w_{, t^+})|\\). closed-form solution exists, rely non-linear optimization procedures.","code":"\nn_industries <- ncol(industry_returns) \n\nw_mvp_numerical <- solve.QP(Dmat = Sigma,\n                            dvec = rep(0, n_industries), \n                            Amat = cbind(rep(1, n_industries)), \n                            bvec = 1, \n                            meq = 1) \n\nall(round(w_mvp, 6) == round(w_mvp_numerical$solution, 6))## [1] TRUE\nw_efficient_numerical <- solve.QP(Dmat = 2 * Sigma,\n                                  dvec = mu, \n                                  Amat = cbind(rep(1, n_industries)),\n                                  bvec = 1, \n                                  meq = 1)\n\nall(round(compute_efficient_weight(Sigma, mu), 6) == round(w_efficient_numerical$solution, 6))## [1] TRUE\nw_no_short_sale <- solve.QP(Dmat = 2 * Sigma,\n                            dvec = mu, \n                            Amat = cbind(1, diag(n_industries)), \n                            bvec = c(1, rep(0, n_industries)), \n                            meq = 1)\nw_no_short_sale$solution##  [1] -1.22e-17  6.56e-17 -4.30e-16  5.10e-03  4.89e-01\n##  [6]  2.43e-16  2.41e-01  3.48e-16  3.20e-16  8.36e-17\n## [11]  1.77e-16  2.65e-01 -7.15e-17  1.72e-15 -1.55e-16\n## [16] -6.11e-17  1.11e-16  1.39e-16  1.13e-16 -9.96e-16\n## [21]  7.58e-16 -6.00e-17 -2.62e-17  3.34e-18 -1.25e-16\n## [26] -2.74e-16  5.42e-18  0.00e+00 -3.38e-17 -2.68e-17\n## [31] -3.03e-17  5.79e-17 -2.80e-16 -3.44e-15  3.11e-17\n## [36]  2.35e-17 -5.81e-17  8.81e-17  1.40e-16 -8.30e-17\n## [41] -4.08e-16  7.88e-16 -2.48e-17 -1.44e-16  1.63e-15\n## [46]  2.69e-16 -1.39e-16 -2.12e-17  0.00e+00\ninitial_weights <- 1 / n_industries * rep(1, n_industries)\nobjective <- function(w, gamma = 2) -t(w) %*% (1+mu) + gamma / 2 * t(w)%*%Sigma%*%w\ninequality_constraints <- function(w, reg_t = 1.5) return(reg_t - sum(abs(w)))\nequality_constraints <- function(w) return(sum(w) - 1)\n\nw_reg_t <- constrOptim.nl(\n  par = initial_weights,\n  hin = inequality_constraints,\n  fn = objective, \n  heq = equality_constraints,\n  control.outer = list(trace = FALSE))\nw_reg_t$par##  [1] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\n##  [8] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\n## [15] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\n## [22] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\n## [29] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\n## [36] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\n## [43] 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204 0.0204\ntibble(`No short-sale` = w_no_short_sale$solution, \n       `Minimum Variance` = w_mvp, \n       `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n       `Regulation-T` = w_reg_t$par,\n       Industry = colnames(industry_returns)) %>%\n  pivot_longer(-Industry, \n               names_to = \"Strategy\") %>% \n  ggplot(aes(fill = Strategy, \n             y = value, \n             x = Industry)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() + \n  labs(y = \"Allocation weight\",\n       title =\" Optimal allocations for different investment rules\") +\n  scale_y_continuous(labels = scales::percent)\ncompute_efficient_weight_L1_TC <- function(mu,\n                                          Sigma, \n                                          gamma = 2, \n                                          beta = 0, \n                                          initial_weights = 1 / ncol(sigma) * rep(1, ncol(sigma))) {\n  \n  objective <- function(w) -t(w) %*% mu + gamma / 2* t(w) %*% Sigma %*% w + (beta / 10000) / 2 * sum(abs(w - initial_weights))\n\n  w_optimal <- constrOptim.nl(\n    par = initial_weights,\n    fn = objective, \n    heq = function(w){sum(w) - 1},\n    control.outer = list(trace = FALSE))\n  \n  w_optimal$par\n}"},{"path":"constrained-optimization-and-backtesting.html","id":"out-of-sample-backtesting","chapter":"12 Constrained optimization and backtesting","heading":"12.6 Out-of-sample backtesting","text":"sake simplicity, committed one fundamental error computing portfolio weights . used full sample data determine optimal allocation. implement strategy beginning 2000s, need know returns evolve 2020. interesting methodological point view, evaluate performance portfolios reasonable --sample fashion. next backtesting application three strategies. backtest, recompute optimal weights just based past available data.also define two helper functions: one adjust weights due returns one performance evaluation, compute realized returns net transaction costs.lines define general setup. consider 120 periods past update parameter estimates recomputing portfolio weights. , update portfolio weights costly affects performance. portfolio weights determine portfolio return. period later, current portfolio weights changed form foundation transaction costs incurred next period. consider three different competing strategies: mean-variance efficient portfolio, mean-variance efficient portfolio ex-ante adjustment transaction costs, naive portfolio, allocates wealth equally across different assets.following code chunk performs rolling-window estimation. period, estimation window contains returns available current period. Note use sample moments, might use advanced estimators practice.Finally, get evaluation portfolio strategies net--transaction costs. Note compute annualized returns standard deviations.results clearly speak mean-variance optimization. Turnover huge investor considers portfolio’s expected return variance. Effectively, mean-variance portfolio generates negative annualized return adjusting transaction costs. time, naive portfolio turns perform well. fact, performance gains transaction-cost adjusted mean-variance portfolio small. --sample Sharpe ratio slightly higher naive portfolio. Note extreme effect turnover penalization turnover: MV (TC) effectively resembles buy--hold strategy updates portfolio estimated parameters \\(\\hat\\mu_t\\) \\(\\hat\\Sigma_t\\)indicate current allocation far away optimal theoretical portfolio.","code":"\nwindow_length <- 120 \nperiods <- nrow(industry_returns) - window_length \n\nbeta <- 50\ngamma <- 2\n\nperformance_values <- matrix(NA, \n                             nrow = periods, \n                             ncol = 3) # A matrix to collect all returns\ncolnames(performance_values) <- c(\"raw_return\", \"turnover\", \"net_return\") \n\nperformance_values <- list(\"MV (TC)\" = performance_values, \n                           \"Naive\" = performance_values, \n                           \"MV\" = performance_values)\n\nw_prev_1 <- w_prev_2 <- w_prev_3 <- rep(1 / n_industries, \n                                        n_industries)\nadjust_weights <- function(w, next_return){\n  w_prev <- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance <- function(w, w_previous, next_return, beta = 50){\n  raw_return <- as.matrix(next_return) %*% w\n  turnover <- sum(abs(w - w_previous))\n  net_return <- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\nfor(p in 1:periods){\n  \n  returns_window <- industry_returns[p : (p + window_length - 1), ]\n  next_return <- industry_returns[p + window_length, ] %>% as.matrix()\n  \n  Sigma <- cov(returns_window) \n  mu <- 0 * colMeans(returns_window) \n  \n  # Transaction-cost adjusted portfolio\n  w_1 <- compute_efficient_weight_L1_TC(mu = mu, \n                                        Sigma = Sigma, \n                                        beta = beta, \n                                        gamma = gamma,\n                                        initial_weights = w_prev_1)\n  \n  performance_values[[1]][p, ] <- evaluate_performance(w_1, \n                                                       w_prev_1, \n                                                       next_return, \n                                                       beta = beta)\n  \n  w_prev_1 <- adjust_weights(w_1, next_return)\n  \n  # Naive portfolio\n  w_2 <- rep(1 / n_industries, n_industries)\n  \n  performance_values[[2]][p, ] <- evaluate_performance(w_2, \n                                                       w_prev_2, \n                                                       next_return)\n  \n  w_prev_2 <- adjust_weights(w_2, next_return)\n  \n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 <- compute_efficient_weight(Sigma = Sigma,\n                                  mu = mu, \n                                  gamma = gamma)\n  \n  performance_values[[3]][p, ] <- evaluate_performance(w_3, \n                                                       w_prev_3, \n                                                       next_return)\n  \n  w_prev_3 <- adjust_weights(w_3, next_return)\n}\nperformance <- lapply(performance_values, as_tibble) %>% \n  bind_rows(.id = \"strategy\")\n\nperformance %>%\n  group_by(strategy) %>%\n  summarize(Mean = 12 * mean(100 * net_return),\n            SD = sqrt(12) * sd(100 * net_return), \n            `Sharpe ratio` = if_else(Mean > 0, Mean / SD, NA_real_),\n            Turnover = 100 * mean(turnover))## # A tibble: 3 × 5\n##   strategy  Mean    SD `Sharpe ratio` Turnover\n##   <chr>    <dbl> <dbl>          <dbl>    <dbl>\n## 1 MV       -24.4  14.3         NA     569.    \n## 2 MV (TC)   11.6  16.4          0.708  11.1   \n## 3 Naive     12.2  17.3          0.704   0.0658"},{"path":"constrained-optimization-and-backtesting.html","id":"exercises-10","chapter":"12 Constrained optimization and backtesting","heading":"12.7 Exercises","text":"argue investor quadratic utility function certainty equivalent \\[\\max_w CE(w) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\]\nfaces equivalent optimization problem framework portfolio weights chosen aim minimize volatility given pre-specified level expected returns\n\\[\\min_w \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ } \\iota'\\omega = 1. \\] Proof equivalence optimal portfolio weights cases.Consider portfolio choice problem transaction-cost adjusted certainty equivalent maximization risk aversion parameter \\(\\gamma\\)\n\\[\\omega_{t+1} ^* :=  \\arg\\max_{\\omega \\\\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\]\n\\(\\Sigma\\) \\(\\mu\\) (estimators ) variance-covariance matrix returns vector expected returns. Assume now transaction costs quadratic rebalancing proportional stock illiquidity \n\\[\\nu_t\\left(\\omega, \\mathbf{\\beta}\\right) := \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) diagonal matrix \\(ill_1, \\ldots, ill_N\\). Derive closed-form solution mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based transaction cost specification . Discuss effect illiquidity \\(ill_i\\) individual portfolio weights relative investor myopically ignores transaction costs decision.Use solution previous exercise update function compute_efficient_weight can compute optimal weights conditional matrix \\(B\\) illiquidity measures.Illustrate evolution optimal weights naive portfolio efficient portfolio mean-standard deviation diagram.always optimal choose \\(\\beta\\) optimization problem value used evaluating portfolio performance? words: Can optimal choose theoretically sub-optimal portfolios based transaction cost considerations reflect actual incurred costs? Evaluate --sample Sharpe ratio transaction costs range different values imposed \\(\\beta\\) values.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"cover-design.html","id":"cover-design","chapter":"A Cover design","heading":"A Cover design","text":"","code":"\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(SQLite(), \"data/tidy_finance.sqlite\", extended_types = TRUE)\nmfac <- tbl(tidy_finance, \"factors_ff_daily\") %>%\n  collect()\n\ncp <- coord_polar(direction = -1, clip = \"on\")\ncp$is_free <- function() TRUE\n\nplot_data <- mfac %>%\n  select(date, mkt_excess) %>%\n  group_by(year = lubridate::floor_date(date, \"year\")) %>%\n  mutate(group_id = cur_group_id())\n\nplot_data <- plot_data %>%\n  mutate(\n    group_id = if_else(group_id >= 28, group_id + 4, group_id + 0),\n    group_id = if_else(group_id >= 36, group_id + 4, group_id + 0),\n    group_id = if_else(group_id >= 44, group_id + 4, group_id + 0)\n  ) %>%\n  bind_rows(plot_data %>%\n    filter(group_id %in% c(28:31, 36:39, 44:47)) %>%\n    mutate(mkt_excess = NA)) %>%\n  group_by(group_id) %>%\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) %>%\n  filter(year >= \"1961-01-01\")\n\ncolors <- wes_palette(\"Zissou1\", n_groups(plot_data), type = \"continuous\")\nlevels <- plot_data %>%\n  distinct(group_id, vola) %>%\n  arrange(vola) %>%\n  pull(vola)\n\nplot <- plot_data %>%\n  mutate(vola = factor(vola, levels = levels)) %>%\n  ggplot() +\n  aes(x = day, y = mkt_excess, group = group_id, fill = vola) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = as_factor(vola)\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id, ncol = 8, scales = \"free\") +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\n# ggsave(\n#   plot = plot, width = 8, height = 9,\n#   filename = \"cover.jpg\", bg = \"white\"\n# )"}]
