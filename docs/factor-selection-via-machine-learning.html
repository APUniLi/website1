<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Factor selection via machine learning | Tidy Finance</title>
<meta name="author" content="Christoph Scheuch, Patrick Weiss and Stefan Voigt">
<meta name="description" content="The aim of this section is twofold: From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning using tidyverse principles. tidymodels...">
<meta name="generator" content="bookdown 0.24.4 with bs4_book()">
<meta property="og:title" content="Chapter 9 Factor selection via machine learning | Tidy Finance">
<meta property="og:type" content="book">
<meta property="og:description" content="The aim of this section is twofold: From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning using tidyverse principles. tidymodels...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Factor selection via machine learning | Tidy Finance">
<meta name="twitter:description" content="The aim of this section is twofold: From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning using tidyverse principles. tidymodels...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Tidy Finance</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Prerequisites</a></li>
<li><a class="" href="introduction-to-tidy-finance.html"><span class="header-section-number">2</span> Introduction to Tidy Finance</a></li>
<li><a class="" href="accessing-managing-financial-data.html"><span class="header-section-number">3</span> Accessing &amp; Managing Financial Data</a></li>
<li><a class="" href="estimating-beta.html"><span class="header-section-number">4</span> Estimating Beta</a></li>
<li><a class="" href="univariate-sorts.html"><span class="header-section-number">5</span> Univariate Sorts</a></li>
<li><a class="" href="univariate-sorts-firm-size.html"><span class="header-section-number">6</span> Univariate Sorts: Firm Size</a></li>
<li><a class="" href="bivariate-sorts-value.html"><span class="header-section-number">7</span> Bivariate Sorts: Value</a></li>
<li><a class="" href="bivariate-sorts-replication-of-fama-french-factors.html"><span class="header-section-number">8</span> Bivariate Sorts: Replication of Fama &amp; French Factors</a></li>
<li><a class="active" href="factor-selection-via-machine-learning.html"><span class="header-section-number">9</span> Factor selection via machine learning</a></li>
<li><a class="" href="option-pricing-via-machine-learning-methods.html"><span class="header-section-number">10</span> Option Pricing via Machine learning methods</a></li>
<li><a class="" href="parametric-portfolio-policies.html"><span class="header-section-number">11</span> Parametric Portfolio Policies</a></li>
<li><a class="" href="constraint-optimization-and-portfolio-backtesting.html"><span class="header-section-number">12</span> Constraint Optimization and Portfolio Backtesting</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="factor-selection-via-machine-learning" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Factor selection via machine learning<a class="anchor" aria-label="anchor" href="#factor-selection-via-machine-learning"><i class="fas fa-link"></i></a>
</h1>
<p>The aim of this section is twofold: From a data science perspective, we introduce <code>tidymodels</code>, a collection of packages for modeling and machine learning using <code>tidyverse</code> principles. <code>tidymodels</code> comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the <em>factor zoo</em> (<span class="citation">(<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Cochrane2011" role="doc-biblioref">Cochrane 2011</a>)</span>). In previous chapters, we illustrate that stock-characteristics such as size provide valuable pricing information in addition to the stock beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists “discovered” a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role for pricing applications). Therefore, the challenge these days rather is: <em>Do we really believe in the relevance of 300+ risk factors?</em>.</p>
<p>We introduce Lasso and Ridge Regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model <em>tuning</em> with elastic net regularization as a popular example. We implement and showcase the entire cycle from model specification, training and forecast evaluation within the <code>tidymodels</code> universe. While the tools can in general be applied to an abundance of interesting asset pricing problems, we apply penalized regressions to identify macro-economic variables and asset pricing factors that help to explain a cross-section of industry portfolios.</p>
<div id="brief-theoretical-background" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Brief theoretical background<a class="anchor" aria-label="anchor" href="#brief-theoretical-background"><i class="fas fa-link"></i></a>
</h2>
<p>This is a book about <em>doing</em> empirical work in a tidy manner and we refer to any of the many excellent textbook treatments of machine learning methods and especially penalized regressions for some deeper discussion. Instead, we just briefly summarize the idea of Lasso and Ridge regressions as well as the more general elastic net before we turn to the fascinating question on <em>how</em> to implement, tune and use such models with the <code>tidymodels</code> workflows.</p>
<p>To set the stage, we start with the definition of a linear model: suppose we have data <span class="math inline">\((y_t, x_t), t = 1,\ldots, T\)</span> where <span class="math inline">\(x_t\)</span> is a <span class="math inline">\((K \times 1)\)</span> vector of regressors and <span class="math inline">\(y_t\)</span> is the response for observation <span class="math inline">\(t\)</span>.
The linear model takes the form <span class="math inline">\(y_t = \beta' x_t + \varepsilon_t\)</span> with some error term <span class="math inline">\(\varepsilon_t\)</span> and has been studied in abundance. The well-known ordinary-least square (OLS) estimator for the <span class="math inline">\((K \times 1)\)</span> vector <span class="math inline">\(\beta\)</span> minimizes the sum of squared residuals and is then <span class="math inline">\(\hat{\beta} = \left(\sum\limits_{t=1}^T x_t'x_t\right)^{-1} \sum\limits_{t=1}^T x_t'y_t\)</span>.
While often we are interested in the estimated coefficient vector <span class="math inline">\(\hat\beta\)</span>, machine learning most of the time is about the predictive performance. For a new observation <span class="math inline">\(\tilde{x}_t\)</span>, the linear model generates predictions such that <span class="math inline">\(\hat y_t = E\left(y|x_t = \tilde x_t\right) = \hat\beta' \tilde x_t\)</span>.
Is this the best we can do?
Not really: Instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by reducing the variance of the estimator <span class="math inline">\(\hat\beta\)</span>. At the same time, it seems appealing to restrict the set of regressors to few meaningful ones if possible. In other words, if <span class="math inline">\(K\)</span> is large such as for the number of proposed factors in the asset pricing literature, it may be a desirable feature to <em>select</em> reasonable factors and to set <span class="math inline">\(\hat\beta_k = 0\)</span> for some redundant factors.</p>
<p>It should be clear that the promised benefits of penalized regressions come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that <span class="math inline">\(E\left(\hat\beta\right) \neq \beta\)</span>. What is the effect of such a bias-variance trade-off? To understand the necessary considerations, assume the following data-generating process for <span class="math inline">\(y\)</span>: <span class="math display">\[y = f(x) + \varepsilon, \quad \varepsilon \sim (0, \sigma_\varepsilon^2)\]</span> While the properties of <span class="math inline">\(\hat\beta\)</span> as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. For instance, the mean-squared error (MSE) depends on our model choice as follow: <span class="math display">\[\begin{aligned}
&amp;=E((y-\hat{f}(\textbf{x}))^2)=E((f(\textbf{x})+\epsilon-\hat{f}(\textbf{x}))^2)\\
&amp;= \underbrace{E((f(\textbf{x})-\hat{f}(\textbf{x}))^2)}_{\text{total quadratic error}}+\underbrace{E(\epsilon^2)}_{\text{irreducible error}} \\
&amp;= E\left(\hat{f}(\textbf{x})^2\right)+E\left(f(\textbf{x})^2\right)-2E\left(f(\textbf{x})\hat{f}(\textbf{x})\right)+\sigma_\varepsilon^2\\
&amp;=E\left(\hat{f}(\textbf{x})^2\right)+f(\textbf{x})^2-2f(\textbf{x})E\left(\hat{f}(\textbf{x})\right)+\sigma_\varepsilon^2\\
&amp;=\underbrace{\text{Var}\left(\hat{f}(\textbf{x})\right)}_{\text{variance of model}}+ \underbrace{E\left((f(\textbf{x})-\hat{f}(\textbf{x}))\right)^2}_{\text{squared bias}} +\sigma_\varepsilon^2. 
\end{aligned}\]</span> While no model can reduce <span class="math inline">\(\sigma_\varepsilon^2\)</span>, a biased estimator with small variance may have a lower mean squared error than an unbiased estimator.</p>
<div id="ridge-regression" class="section level3" number="9.1.1">
<h3>
<span class="header-section-number">9.1.1</span> Ridge Regression<a class="anchor" aria-label="anchor" href="#ridge-regression"><i class="fas fa-link"></i></a>
</h3>
<p>One such biased estimator is known as Ridge regression. <span class="citation">(<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Hoerl1970" role="doc-biblioref">Hoerl and Kennard 1970</a>)</span> propose to minimize the sum of squared errors <em>while simultaneously imposing a penalty on the <span class="math inline">\(L_2\)</span> norm of the parameters</em> <span class="math inline">\(\hat\beta\)</span>. Formally, this means that for a penalty factor <span class="math inline">\(\lambda\geq 0\)</span> the minimization problem takes the form <span class="math inline">\(\min_\beta \left(y - X\beta\right)'\left(y - X\beta\right)\text{ s.t. } \beta'\beta \leq \lambda\)</span>. Here, <span class="math inline">\(X = \left(x_1 \ldots x_T\right)'\)</span> and <span class="math inline">\(y = \left(y_1, \ldots, y_T\right)'\)</span>. A closed-form solution for the resulting regression coefficient vector <span class="math inline">\(\beta^\text{ridge}\)</span> exists: <span class="math display">\[\hat{\beta}^\text{ridge} = \left(X'X + \lambda I\right)^{-1}X'y.\]</span> A couple of observations are worth noting: <span class="math inline">\(\hat\beta^\text{ridge} = \hat\beta\)</span> for <span class="math inline">\(\lambda = 0\)</span> and <span class="math inline">\(\hat\beta^\text{ridge} \rightarrow 0\)</span> for <span class="math inline">\(\lambda\rightarrow \infty\)</span>. Also for <span class="math inline">\(\lambda &gt; 0\)</span>, <span class="math inline">\(\left(X'X + \lambda I\right)\)</span> is non-singular even if <span class="math inline">\(X'X\)</span> is which means that <span class="math inline">\(\hat\beta^\text{ridge}\)</span> exists even if <span class="math inline">\(\hat\beta\)</span> is not defined. But note also that the Ridge estimator requires careful choice of the hyperparameter <span class="math inline">\(\lambda\)</span> which controls the <em>amount of regularization</em>.
Usually, <span class="math inline">\(X\)</span> contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that <span class="math inline">\(y\)</span> is simply demeaned before computing <span class="math inline">\(\hat\beta^\text{ridge}\)</span>.</p>
<p>What about the statistical properties of the Ridge estimator? First, the bad news is that <span class="math inline">\(\hat\beta^\text{ridge}\)</span> is a biased estimator of <span class="math inline">\(\beta\)</span>. However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is <em>smaller</em> than the variance of the ordinary least square estimator. We encourage you to verify these two statements in the exercises. As a result, we face a trade-off: Ridge regression sacrifice some bias to achieve a smaller variance than the OLS estimator.</p>
</div>
<div id="lasso" class="section level3" number="9.1.2">
<h3>
<span class="header-section-number">9.1.2</span> Lasso<a class="anchor" aria-label="anchor" href="#lasso"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative to Ridge regression is the Lasso (<em>l</em>east <em>a</em>bsolute <em>s</em>hrinkage and <em>s</em>election <em>o</em>perator). Similar to Ridge regression, the Lasso (Tibshirani, 1996) is a penalized and hence biased estimator.
The main difference to Ridge regression is that the Lasso does not only <em>shrink</em> coefficients but effectively selects variables by setting coefficients for <em>irrelevant</em> variables to zero. Lasso implements a <span class="math inline">\(L_1\)</span> penalization on the parameters such that: <span class="math display">\[\hat\beta^\text{Lasso} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda\sum\limits_{k=1}^K|\beta_k|.\]</span> There is no closed form solution for <span class="math inline">\(\hat\beta^\text{Lasso}\)</span> in the above maximization problem but efficient algorithms exist (e.g., the R package <code>glmnet</code>). Like for Ridge regression, the hyperparameter <span class="math inline">\(\lambda\)</span> has to be specified beforehand.</p>
</div>
<div id="elastic-net" class="section level3" number="9.1.3">
<h3>
<span class="header-section-number">9.1.3</span> Elastic Net<a class="anchor" aria-label="anchor" href="#elastic-net"><i class="fas fa-link"></i></a>
</h3>
<p>The elastic net <span class="citation">(<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Zou2005" role="doc-biblioref">Zou and Hastie 2005</a>)</span> combines <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> penalization and encourages a grouping effect where strongly correlated predictors tend to be in or out of the model together. This more general framework considers the following optimization problem: <span class="math display">\[\hat\beta^\text{EN} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda(1-\rho)\sum\limits_{k=1}^K|\beta_k| +\frac{1}{2}\lambda\rho\sum\limits_{k=1}^K\beta_k^2\]</span> Now, we have to chose 2 hyperparameters: the <em>shrinkage</em> factor <span class="math inline">\(\lambda\)</span> and the <em>weighting parameter</em> <span class="math inline">\(\rho\)</span>. The elastic net resembles Lasso for <span class="math inline">\(\rho = 1\)</span> and Ridge regression for <span class="math inline">\(\rho = 0\)</span>.
While the R package <code>glmnet</code> provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the <code>glmnet</code> package or the <code>tidymodels</code> back-end.</p>
</div>
</div>
<div id="data-preparation-2" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Data preparation<a class="anchor" aria-label="anchor" href="#data-preparation-2"><i class="fas fa-link"></i></a>
</h2>
<p>To get started, we load the required packages and data. The main focus is on the workflows behind the amazing <code>tidymodels</code> package collection.</p>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://rsqlite.r-dbi.org">RSQLite</a></span><span class="op">)</span> <span class="co"># To gather the data</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span> <span class="co"># For ML applications</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/DavisVaughan/furrr">furrr</a></span><span class="op">)</span> <span class="co"># For parallelization</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://glmnet.stanford.edu">glmnet</a></span><span class="op">)</span> <span class="co"># For penalized regressions</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/business-science/timetk">timetk</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haozhu233.github.io/kableExtra/">kableExtra</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://scales.r-lib.org">scales</a></span><span class="op">)</span></code></pre></div>
<p>In this analysis we use 4 different data sources. We start with two different set of factor portfolio returns which have been suggested as representing useful risk factor exposure and thus should be relevant when it comes to asset pricing applications.</p>
<ul>
<li>The standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big and high-minus-low book-to-market valuation sorts) as of <span class="citation"><a href="constraint-optimization-and-portfolio-backtesting.html#ref-Fama1993" role="doc-biblioref">Fama and French</a> (<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Fama1993" role="doc-biblioref">1993</a>)</span>
</li>
<li>Monthly q-factor returns from <span class="citation">(<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Hou2015" role="doc-biblioref">Hou, Xue, and Zhang 2014</a>)</span>. The factors contain the size factor, the investment factor, the return-on-equity factor and the expected growth factor</li>
</ul>
<p>Next, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to <span class="citation">(<a href="#ref-Cochrane2005" role="doc-biblioref"><strong>Cochrane2005?</strong></a>)</span> on the role of conditioning information.</p>
<ul>
<li>Our set of macroeconomic predictors comes from the paper “A Comprehensive Look at The Empirical Performance of Equity Premium Prediction” <span class="citation">(<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Goyal2008" role="doc-biblioref">Welch and Goyal 2008</a>)</span>. The data has been updated by the authors until 2020 and comprises of monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the Dividend Price Ratio, Earnings Price Ratio, Stock Variance, Net Equity Expansion, Treasury Bill rate and inflation</li>
</ul>
<p>Finally, we need a set of <em>test assets</em>. The aim is to understand which of the plenty factors and macroeconomic variable combinations proof useful to explain the cross-section of returns of our test assets.</p>
<ul>
<li>In line with many existing papers we use monthly portfolio returns from 10 different industries according to the <a href="https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/det_10_ind_port.html">definition from Kenneth French’s homepage</a> as test assets.</li>
</ul>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tidy_finance</span> <span class="op">&lt;-</span> <span class="fu">dbConnect</span><span class="op">(</span><span class="fu"><a href="https://rsqlite.r-dbi.org/reference/SQLite.html">SQLite</a></span><span class="op">(</span><span class="op">)</span>, <span class="st">"data/tidy_finance.sqlite"</span>, extended_types <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># Load factor returns</span>
<span class="va">factors_ff_monthly</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/tbl.html">tbl</a></span><span class="op">(</span><span class="va">tidy_finance</span>, <span class="st">"factors_ff_monthly"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/compute.html">collect</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename_with</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"factor_ff_"</span>, <span class="va">.</span><span class="op">)</span>, <span class="op">-</span><span class="va">month</span><span class="op">)</span>

<span class="va">factors_q_monthly</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/tbl.html">tbl</a></span><span class="op">(</span><span class="va">tidy_finance</span>, <span class="st">"factors_q_monthly"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/compute.html">collect</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename_with</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"factor_q_"</span>, <span class="va">.</span><span class="op">)</span>, <span class="op">-</span><span class="va">month</span><span class="op">)</span>

<span class="co"># Load macroeconomic variables</span>
<span class="va">macro_predictors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/tbl.html">tbl</a></span><span class="op">(</span><span class="va">tidy_finance</span>, <span class="st">"macro_predictors"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/compute.html">collect</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename_with</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"macro_"</span>, <span class="va">.</span><span class="op">)</span>, <span class="op">-</span><span class="va">month</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">macro_rp_div</span><span class="op">)</span>

<span class="co"># Load test assets</span>
<span class="va">industries_ff_monthly</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/tbl.html">tbl</a></span><span class="op">(</span><span class="va">tidy_finance</span>, <span class="st">"industries_ff_monthly"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/compute.html">collect</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">-</span><span class="va">month</span>, 
               names_to <span class="op">=</span> <span class="st">"industry"</span>, values_to <span class="op">=</span> <span class="st">"ret"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>industry <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">industry</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>We combine all observations into one data frame.</p>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">industries_ff_monthly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html">left_join</a></span><span class="op">(</span><span class="va">factors_ff_monthly</span>, by <span class="op">=</span> <span class="st">"month"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html">left_join</a></span><span class="op">(</span><span class="va">factors_q_monthly</span>, by <span class="op">=</span> <span class="st">"month"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html">left_join</a></span><span class="op">(</span><span class="va">macro_predictors</span>, by <span class="op">=</span> <span class="st">"month"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    ret <span class="op">=</span> <span class="va">ret</span> <span class="op">-</span> <span class="va">factor_ff_rf</span>
  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="co"># Compute excess returns</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">month</span>, <span class="va">industry</span>, <span class="va">ret</span>, <span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/drop_na.html">drop_na</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>Our data contains 22 columns of regressors with the 13 macro variables and 8 factor returns for each month. The panel ranges from januar 1967 until november 2020. Table <span class="citation">(<a href="#ref-ref" role="doc-biblioref"><strong>ref?</strong></a>(tab:industryreturns))</span> provides summary statistics for the 10 industries such as the sample standard deviation and the minimum and maximum monthly excess returns.</p>
<div class="sourceCode" id="cb146"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">industry</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>ret <span class="op">=</span> <span class="fl">100</span> <span class="op">*</span> <span class="va">ret</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span>,
    sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span>,
    min <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span>,
    median <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span>,
    max <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span>
  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/knitr/man/kable.html">kable</a></span><span class="op">(</span>caption <span class="op">=</span> <span class="st">"Summary statistics: Industry excess returns in percent."</span>,
        digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:industryreturns">Table 9.1: </span>Summary statistics: Industry excess returns in percent.
</caption>
<thead><tr>
<th style="text-align:left;">
industry
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
median
</th>
<th style="text-align:right;">
max
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
NoDur
</td>
<td style="text-align:right;">
0.700
</td>
<td style="text-align:right;">
18.27
</td>
<td style="text-align:right;">
-21.62
</td>
<td style="text-align:right;">
0.77
</td>
<td style="text-align:right;">
18.27
</td>
</tr>
<tr>
<td style="text-align:left;">
Durbl
</td>
<td style="text-align:right;">
0.635
</td>
<td style="text-align:right;">
45.26
</td>
<td style="text-align:right;">
-33.01
</td>
<td style="text-align:right;">
0.54
</td>
<td style="text-align:right;">
45.26
</td>
</tr>
<tr>
<td style="text-align:left;">
Manuf
</td>
<td style="text-align:right;">
0.613
</td>
<td style="text-align:right;">
17.32
</td>
<td style="text-align:right;">
-27.95
</td>
<td style="text-align:right;">
0.91
</td>
<td style="text-align:right;">
17.32
</td>
</tr>
<tr>
<td style="text-align:left;">
Enrgy
</td>
<td style="text-align:right;">
0.550
</td>
<td style="text-align:right;">
32.38
</td>
<td style="text-align:right;">
-34.61
</td>
<td style="text-align:right;">
0.60
</td>
<td style="text-align:right;">
32.38
</td>
</tr>
<tr>
<td style="text-align:left;">
HiTec
</td>
<td style="text-align:right;">
0.682
</td>
<td style="text-align:right;">
20.32
</td>
<td style="text-align:right;">
-26.52
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
20.32
</td>
</tr>
<tr>
<td style="text-align:left;">
Telcm
</td>
<td style="text-align:right;">
0.543
</td>
<td style="text-align:right;">
21.20
</td>
<td style="text-align:right;">
-16.30
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
21.20
</td>
</tr>
<tr>
<td style="text-align:left;">
Shops
</td>
<td style="text-align:right;">
0.746
</td>
<td style="text-align:right;">
25.41
</td>
<td style="text-align:right;">
-28.66
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
25.41
</td>
</tr>
<tr>
<td style="text-align:left;">
Hlth
</td>
<td style="text-align:right;">
0.702
</td>
<td style="text-align:right;">
29.01
</td>
<td style="text-align:right;">
-21.05
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
29.01
</td>
</tr>
<tr>
<td style="text-align:left;">
Utils
</td>
<td style="text-align:right;">
0.481
</td>
<td style="text-align:right;">
18.26
</td>
<td style="text-align:right;">
-13.13
</td>
<td style="text-align:right;">
0.65
</td>
<td style="text-align:right;">
18.26
</td>
</tr>
<tr>
<td style="text-align:left;">
Other
</td>
<td style="text-align:right;">
0.565
</td>
<td style="text-align:right;">
19.73
</td>
<td style="text-align:right;">
-24.20
</td>
<td style="text-align:right;">
1.02
</td>
<td style="text-align:right;">
19.73
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="the-tidymodels-workflow" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> The tidymodels workflow<a class="anchor" aria-label="anchor" href="#the-tidymodels-workflow"><i class="fas fa-link"></i></a>
</h2>
<p>To illustrate penalized linear regressions, we employ the <code>tidymodels</code> collection of packages for modeling and machine learning using <code>tidyverse</code> principles. You can simply use <code>install.packages("tidymodels")</code> to get access to all the related packages. We recommend to check out the work of Max Kuhn and Julia Silge: They continuously write on a great book ‘<a href="https://www.tmwr.org/">Tidy Modeling with R</a>’ using tidy principles.</p>
<p>The <code>tidymodels</code> workflow encompasses the main stages of the modeling process: pre-processing of data, model fitting, and post-processing of results. As we demonstrate below, <code>tidymodels</code> provides efficient workflows that can be updated with low effort.</p>
<p>Using the ideas of Ridge and Lasso regression, the following example guides you through (i) preprocessing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.</p>
<p>To start, we restrict our analysis to just one industry: Manufacturing. We first split the sample into a <em>training</em> and a <em>test</em> set. For that purpose, <code>tidymodels</code> provides the function <code>initial_time_split</code> from the <code>rsample</code> package. The split takes the last 20% of the data as test set which is not used for any model tuning. We use this test set to evaluate the predictive accuracy in an out-of-sample scenario.</p>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">split</span> <span class="op">&lt;-</span> <span class="fu">initial_time_split</span><span class="op">(</span>
  <span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">industry</span> <span class="op">==</span> <span class="st">"Manuf"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">industry</span><span class="op">)</span>,
  prop <span class="op">=</span> <span class="fl">4</span> <span class="op">/</span> <span class="fl">5</span>
<span class="op">)</span>
<span class="va">split</span></code></pre></div>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;517/130/647&gt;</code></pre>
<p>The object <code>split</code> simply takes track of the observation index which belongs to the training and the test set. We can call the training set with <code>training(split)</code>, while we can extract the test set with <code>testing(split)</code>.</p>
<div id="preprocess-data" class="section level3" number="9.3.1">
<h3>
<span class="header-section-number">9.3.1</span> Preprocess Data<a class="anchor" aria-label="anchor" href="#preprocess-data"><i class="fas fa-link"></i></a>
</h3>
<p>Recipes help you preprocess your data before training your model. Recipes are a series of preprocessing steps such as variable selection, transformation or converting qualitative predictors to indicator variables. Each recipe starts with a <code>formula</code> which defines the general structure of the dataset and the role of each variable (regressor or dependent variable). For our dataset, our recipe contains the following steps before we fit any model:</p>
<ul>
<li>Our formula defines that we want to explain excess returns with all available predictors</li>
<li>We exclude the column <em>month</em> from the analysis</li>
<li>We include all interaction terms between factors and macro economic predictors</li>
<li>We demean and scale each regressor such that the standard deviation is one</li>
</ul>
<div class="sourceCode" id="cb149"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rec</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="va">ret</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">step_rm</span><span class="op">(</span><span class="va">month</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="co"># remove date variable</span>
  <span class="fu">step_interact</span><span class="op">(</span>terms <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">contains</a></span><span class="op">(</span><span class="st">"factor"</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">contains</a></span><span class="op">(</span><span class="st">"macro"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">step_center</span><span class="op">(</span><span class="va">ret</span>, skip <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p>A table of all available recipe steps can be found <a href="https://www.tidymodels.org/find/recipes/">here</a>. As of <code>format(Sys.Date(), "%Y")</code>, more than 100 different processing steps are available! One important point: the definition of a recipe does not imply any calculations yet but rather provides a <em>description</em> of the tasks which are applied later. As a result, it is very easy to <em>reuse</em> recipes for different models and thus make sure that the outcomes are comparable as they are based on the same input.
In the example above, it does not make a difference whether the input <code>data = training(split)</code> or <code>data = testing(split)</code> is used.
All that matters at this early stage are the column names and types.</p>
<p>We can apply the recipe to any data with suitable structure. The code below combines two different functions: <code>prep</code> estimates the required parameters from a training set that can be later applied to other data sets. <code>bake</code> applies the processed computations to new data.</p>
<div class="sourceCode" id="cb150"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmp_data</span> <span class="op">&lt;-</span> <span class="fu">bake</span><span class="op">(</span><span class="fu">prep</span><span class="op">(</span><span class="va">rec</span>, <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span>, new_data <span class="op">=</span> <span class="fu"><a href="https://dbplyr.tidyverse.org/reference/testing.html">testing</a></span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span>
<span class="va">tmp_data</span></code></pre></div>
<pre><code>## # A tibble: 130 x 126
##    factor_ff_rf factor_ff_mkt_excess factor_ff_smb factor_ff_hml factor_q_me
##           &lt;dbl&gt;                &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;
##  1        -1.92               0.644        0.298           0.947      0.371 
##  2        -1.88               1.27         0.387           0.607      0.527 
##  3        -1.88               0.341        1.43            0.836      1.12  
##  4        -1.88              -1.80        -0.0411         -0.963     -0.0921
##  5        -1.88              -1.29        -0.627          -1.73      -0.850 
##  6        -1.88               1.41         0.00517        -0.240      0.0440
##  7        -1.88              -1.12        -0.978          -0.801     -1.25  
##  8        -1.88               1.97         1.15           -1.21       1.13  
##  9        -1.88               0.747        0.304          -0.979      0.109 
## 10        -1.88               0.0387       1.09           -0.436      1.04  
## # ... with 120 more rows, and 121 more variables: factor_q_ia &lt;dbl&gt;,
## #   factor_q_roe &lt;dbl&gt;, factor_q_eg &lt;dbl&gt;, macro_dp &lt;dbl&gt;, macro_dy &lt;dbl&gt;,
## #   macro_ep &lt;dbl&gt;, macro_de &lt;dbl&gt;, macro_svar &lt;dbl&gt;, macro_bm &lt;dbl&gt;,
## #   macro_ntis &lt;dbl&gt;, macro_tbl &lt;dbl&gt;, macro_lty &lt;dbl&gt;, macro_ltr &lt;dbl&gt;,
## #   macro_tms &lt;dbl&gt;, macro_dfy &lt;dbl&gt;, macro_infl &lt;dbl&gt;, ret &lt;dbl&gt;,
## #   factor_ff_rf_x_macro_dp &lt;dbl&gt;, factor_ff_rf_x_macro_dy &lt;dbl&gt;,
## #   factor_ff_rf_x_macro_ep &lt;dbl&gt;, factor_ff_rf_x_macro_de &lt;dbl&gt;, ...</code></pre>
<p>Note that the resulting data contains the 130 observations from the test set and 126 columns. Why so many? Recall that the recipe states to compute every possible interaction term between the factors and predictors which increases the dimension of the data matrix substantially.</p>
<p>You may ask at this stage: Why should I use a recipe instead of simply using the data wrangling commands such as <code>mutate</code> or <code>select</code>? <code>tidymodels</code> beauty is that there is a lot happening under the hood. Recall, that for the simple scaling step you actually have to compute the standard deviation of each column, then <em>store</em> this value and apply the identical transformation to the new dataset, e.g. <code>testing(split)</code>. A prepped <code>recipe</code> stores these values and hands them on once you <code>bake</code> a novel dataset. Easy as pie with <code>tidymodels</code>, isn’t it?</p>
</div>
<div id="build-a-model" class="section level3" number="9.3.2">
<h3>
<span class="header-section-number">9.3.2</span> Build a Model<a class="anchor" aria-label="anchor" href="#build-a-model"><i class="fas fa-link"></i></a>
</h3>
<p>Next we can build an actual model based on our pre-processed data. In line with the definition from above, we estimate regression coefficients of a Lasso regression such that we get<br><span class="math display">\[\hat\beta_\lambda^\text{Lasso} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda\sum\limits_{k=1}^K|\beta_k|.\]</span> We want to emphasize that the <code>tidymodels</code> workflow for <em>any</em> model is very similar, irrespective of the specific model. As you will see further below, it is a piece of cake to fit Ridge regression coefficients and - later - Neural networks or Random forests with basically the same code. The structure is always as follows: create a so-called <code>workflow</code> and use the <code>fit</code> function. A table with all available model APIs is available <a href="https://www.tidymodels.org/find/parsnip/">here</a>.
For now, we start with the linear regression model with a given value for the penalty factor <span class="math inline">\(\lambda\)</span>. In the setup below, <code>mixture</code> denotes the value of <span class="math inline">\(\rho\)</span>, hence setting <code>mixture = 1</code> implies the Lasso.</p>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lm_model</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>
  penalty <span class="op">=</span> <span class="fl">0.0001</span>,
  mixture <span class="op">=</span> <span class="fl">1</span>
<span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span>, intercept <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></code></pre></div>
<p>That’s it - we are done! The object <code>lm_model</code> contains the definition of our model with all required information. Note that <code>set_engine("glmnet")</code> indicates the API character of the <code>tidymodels</code> workflow: Under the hood, the package <code>glmnet</code> is doing the heavy lifting, while <code>linear_reg</code> provides a unified framework to collect the inputs. The <code>workflow</code> ends with combining everything necessary for the (serious) data science workflow: a recipe and a model. Now we are ready to use <code>fit</code>.</p>
<div class="sourceCode" id="cb153"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lm_fit</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">rec</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">lm_model</span><span class="op">)</span>
<span class="va">lm_fit</span></code></pre></div>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: linear_reg()
## 
## -- Preprocessor ----------------------------------------------------------------
## 4 Recipe Steps
## 
## * step_rm()
## * step_interact()
## * step_normalize()
## * step_center()
## 
## -- Model -----------------------------------------------------------------------
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 1e-04
##   mixture = 1
## 
## Engine-Specific Arguments:
##   intercept = FALSE
## 
## Computational engine: glmnet</code></pre>
</div>
<div id="fit-a-model" class="section level3" number="9.3.3">
<h3>
<span class="header-section-number">9.3.3</span> Fit a Model<a class="anchor" aria-label="anchor" href="#fit-a-model"><i class="fas fa-link"></i></a>
</h3>
<p>We use the training data to fit the model. The training data is pre-processed according to our recipe steps and the Lasso regression coefficients are computed. First, we focus on the predicted values <span class="math inline">\(\hat{y}_t = x_t\hat\beta^\text{Lasso}.\)</span> Figure <span class="citation">(<a href="#ref-ref" role="doc-biblioref"><strong>ref?</strong></a>(fig:industrypremia))</span> illustrates the projections for the <em>entire</em> time series of the Manufacturing industry portfolio returns. The grey area indicates the out-of-sample period which has not been used to fit the model.</p>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">predicted_values</span> <span class="op">&lt;-</span> <span class="va">lm_fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">industry</span> <span class="op">==</span> <span class="st">"Manuf"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_cols</a></span><span class="op">(</span><span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">industry</span> <span class="op">==</span> <span class="st">"Manuf"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">month</span>, <span class="va">.pred</span>, <span class="va">ret</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">-</span><span class="va">month</span>, names_to <span class="op">=</span> <span class="st">"Variable"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Variable <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span>
    <span class="va">Variable</span> <span class="op">==</span> <span class="st">".pred"</span> <span class="op">~</span> <span class="st">"Fitted Value"</span>,
    <span class="va">Variable</span> <span class="op">==</span> <span class="st">"ret"</span> <span class="op">~</span> <span class="st">"Realization"</span>
  <span class="op">)</span><span class="op">)</span> 

<span class="va">predicted_values</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">month</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">Variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="cn">NULL</span>,
    y <span class="op">=</span> <span class="cn">NULL</span>,
    color <span class="op">=</span> <span class="cn">NULL</span>,
    title <span class="op">=</span> <span class="st">"Monthly Realized and Fitted Manufacturing Industry Risk Premia"</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_date.html">scale_x_date</a></span><span class="op">(</span>
    breaks <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.Date.html">seq.Date</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, to <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, by <span class="op">=</span> <span class="st">"5 years"</span><span class="op">)</span>,
    minor_breaks <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.Date.html">seq.Date</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, to <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, by <span class="op">=</span> <span class="st">"1 years"</span><span class="op">)</span>,
    expand <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>,
    labels <span class="op">=</span> <span class="fu"><a href="https://scales.r-lib.org/reference/label_date.html">date_format</a></span><span class="op">(</span><span class="st">"%Y"</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_continuous</a></span><span class="op">(</span>
    labels <span class="op">=</span> <span class="va">percent</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_rect</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>
    xmin <span class="op">=</span> <span class="fu"><a href="https://dbplyr.tidyverse.org/reference/testing.html">testing</a></span><span class="op">(</span><span class="va">split</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">month</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="op">)</span>,
    xmax <span class="op">=</span> <span class="fu"><a href="https://dbplyr.tidyverse.org/reference/testing.html">testing</a></span><span class="op">(</span><span class="va">split</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">month</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="op">)</span>,
    ymin <span class="op">=</span> <span class="op">-</span><span class="cn">Inf</span>, ymax <span class="op">=</span> <span class="cn">Inf</span>
  <span class="op">)</span>,
  alpha <span class="op">=</span> <span class="fl">0.005</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"bottom"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="40_factor_selection_with_machine_learning_files/figure-html/industrypremia-1.png" width="768" style="display: block; margin: auto;"></div>
<p>How do the estimated coefficients look like? To analyse these values and to illustrate the difference between the <code>tidymodels</code> workflow and the underlying <code>glmnet</code> package, it is worth to compute the coefficients <span class="math inline">\(\hat\beta^\text{Lasso}\)</span> directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample. Note that <code>glmnet</code> actually takes a vector <code>y</code> and the matrix of regressors <span class="math inline">\(X\)</span> as input. Moreover, <code>glmnet</code> requires choosing the penalty parameter <span class="math inline">\(\alpha\)</span> which corresponds to <span class="math inline">\(\rho\)</span> in the notation above. Such details do not need consideration when using the <code>tidymodels</code> model API.</p>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tmp_data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">ret</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="va">tmp_data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span>

<span class="va">fit_lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span>
  x <span class="op">=</span> <span class="va">x</span>,
  y <span class="op">=</span> <span class="va">y</span>,
  alpha <span class="op">=</span> <span class="fl">1</span>, intercept <span class="op">=</span> <span class="cn">FALSE</span>, standardize <span class="op">=</span> <span class="cn">FALSE</span>,
  lambda.min.ratio <span class="op">=</span> <span class="fl">0</span>
<span class="op">)</span> <span class="co"># Lasso</span>

<span class="va">fit_ridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span>
  x <span class="op">=</span> <span class="va">x</span>,
  y <span class="op">=</span> <span class="va">y</span>,
  alpha <span class="op">=</span> <span class="fl">0</span>, intercept <span class="op">=</span> <span class="cn">FALSE</span>, standardize <span class="op">=</span> <span class="cn">FALSE</span>,
  lambda.min.ratio <span class="op">=</span> <span class="fl">0</span>
<span class="op">)</span> <span class="co"># Ridge</span></code></pre></div>
<p>The objects <code>fit_lasso</code> and <code>fit_ridge</code> contain an entire sequence of estimated coefficients for multiple values of the penalty factor <span class="math inline">\(\lambda\)</span>. The figure below illustrates how the trajectory of the regression coefficients as a function of the penalty factor <span class="math inline">\(\lambda\)</span>. As the penalty factor increases, both Lasso and Ridge coefficients converge to zero.</p>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_rows</a></span><span class="op">(</span>
  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="va">fit_lasso</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Model <span class="op">=</span> <span class="st">"Lasso"</span><span class="op">)</span>,
  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="va">fit_ridge</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Model <span class="op">=</span> <span class="st">"Ridge"</span><span class="op">)</span>
<span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span><span class="st">"Variable"</span> <span class="op">=</span> <span class="va">term</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lambda</span>, y <span class="op">=</span> <span class="va">estimate</span>, color <span class="op">=</span> <span class="va">Variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">Model</span>, scales <span class="op">=</span> <span class="st">"free_x"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="st">"Lambda"</span>, y <span class="op">=</span> <span class="st">""</span>,
    title <span class="op">=</span> <span class="st">"Estimated Coefficients paths as a function of the penalty factor Lambda"</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="40_factor_selection_with_machine_learning_files/figure-html/unnamed-chunk-11-1.png" width="768" style="display: block; margin: auto;"></div>
<div class="rmdnote">
<p>One word of caution: The package <code>glmnet</code> computes estimates of the coefficients <span class="math inline">\(\hat\beta\)</span> based on numerical optimization procedures. As a result the estimated coefficients for the special case with no regularization (<span class="math inline">\(\lambda = 0\)</span>) can deviate from the standard OLS estimates.</p>
</div>
</div>
<div id="tune-a-model" class="section level3" number="9.3.4">
<h3>
<span class="header-section-number">9.3.4</span> Tune a Model<a class="anchor" aria-label="anchor" href="#tune-a-model"><i class="fas fa-link"></i></a>
</h3>
<p>To compute <span class="math inline">\(\hat\beta_\lambda^\text{Lasso}\)</span> , we simply imposed a value for the penalty hyperparameter <span class="math inline">\(\lambda\)</span>. Model tuning is the process of optimally selecting such hyperparameters. <code>tidymodels</code> provides extensive tuning options based on so-called <em>cross validation</em>. Again, we refer to any treatment of cross-validation to get a more detailed discussion of the statistical underpinnings, here we focus on the general idea and the implementation with <code>tidymodels</code>.</p>
<p>The goal for choosing <span class="math inline">\(\lambda\)</span> (or any other hyperparameter, for instance <span class="math inline">\(\rho\)</span>) is to find a way to produce predictors <span class="math inline">\(\hat{Y}\)</span> for an outcome <span class="math inline">\(Y\)</span> that minimizes the mean squared prediction error <span class="math inline">\(\text{MSPE} = E\left( \frac{1}{T}\sum_{t=1}^T (\hat{y}_t - y_t)^2 \right)\)</span>. Unfortunately, <em>MSPE</em> is not directly observable and we can only compute an estimate because our data is random and because we do not observe the entire population.</p>
<p>Obviously, if we train an algorithm on the same data that we use to compute the error, our estimate <span class="math inline">\(\hat{\text{MSPE}}\)</span> would indicate way better predictive accuracy than what we can expect in a real out-of-sample data. The result is called overfitting.</p>
<p>Cross validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many mean squared prediction errors obtained by creating predictions for <span class="math inline">\(K\)</span> new random samples of the data, none of them used to train the algorithm <span class="math inline">\(\frac{1}{K} \sum_{k=1}^K \frac{1}{T}\sum_{t=1}^T \left(\hat{y}_t^k - y_t^k\right)^2\)</span>. In practice, this is done by carving out a piece of our data and pretend it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the <em>optimal</em> hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g. <span class="math inline">\(\lambda\)</span>) and evaluate the predictive accuracy on an <em>independent</em> sample. This works as follows:</p>
<ol style="list-style-type: decimal">
<li>Specify a grid of hyperparameters</li>
<li>Obtain predictors <span class="math inline">\(\hat{y}_i(\lambda)\)</span> to denote the predictors for the used parameters <span class="math inline">\(\lambda\)</span>
</li>
<li>Compute <span class="math display">\[
\text{MSPE}(\lambda) = \frac{1}{K} \sum_{k=1}^K \frac{1}{T}\sum_{t=1}^T \left(\hat{y}_t^k(\lambda) - y_t^k\right)^2 
\]</span> With K-fold cross validation, we do this computation <span class="math inline">\(K\)</span> times. Simply pick a validation set with <span class="math inline">\(M=T/K\)</span> observations at random and think of these as random samples <span class="math inline">\(y_1^k, \dots, y_\tilde{T}^k\)</span>, with <span class="math inline">\(k=1\)</span>.</li>
</ol>
<p>How should you pick <span class="math inline">\(K\)</span>? Large values of <span class="math inline">\(K\)</span> are preferable because the training data better imitates the original data. However, larger values of <span class="math inline">\(K\)</span> will have much higher computation time.
<code>tidymodels</code> provides all required tools to conduct <span class="math inline">\(K\)</span>-fold cross validation. We just have to update our model specification and let <code>tidymodels</code> know which parameters to tune. In our case we specify both the penalty factor <span class="math inline">\(\lambda\)</span> as well as the mixing factor <span class="math inline">\(\rho\)</span> as <em>free</em> parameters.</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lm_model</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>
  penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>,
  mixture <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>
<span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span>

<span class="co"># Update the existing model</span>
<span class="va">lm_fit</span> <span class="op">&lt;-</span> <span class="va">lm_fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">update_model</span><span class="op">(</span><span class="va">lm_model</span><span class="op">)</span></code></pre></div>
<p>For our sample, we consider time-series cross validation sample. That is, we tune our models with 20 random samples of length 5 years with a validation period of 4 years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate <span class="math inline">\(\hat{\text{MSPE}}\)</span> in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.</p>
<div class="sourceCode" id="cb159"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">data_folds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/timetk/man/time_series_cv.html">time_series_cv</a></span><span class="op">(</span>
  data        <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span>,
  date_var    <span class="op">=</span> <span class="va">month</span>,
  initial     <span class="op">=</span> <span class="st">"5 years"</span>,
  assess      <span class="op">=</span> <span class="st">"48 months"</span>,
  cumulative  <span class="op">=</span> <span class="cn">FALSE</span>,
  slice_limit <span class="op">=</span> <span class="fl">20</span>
<span class="op">)</span></code></pre></div>
<p>Then, we evaluate the performance for a grid of different penalty values. <code>tidymodels</code> provides functionalities to construct a suitable grid of hyperparameters with <code>grid_regular</code>. The code chunk below creates a grid of <span class="math inline">\(10 \times 3\)</span> hyperparameters and the function <code>tune_grid</code> does the tasks of evaluating all the models for each fold.</p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lm_tune</span> <span class="op">&lt;-</span> <span class="va">lm_fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">tune_grid</span><span class="op">(</span>
    resample <span class="op">=</span> <span class="va">data_folds</span>,
    grid <span class="op">=</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">penalty</span><span class="op">(</span><span class="op">)</span>, <span class="fu">mixture</span><span class="op">(</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,
    metrics <span class="op">=</span> <span class="fu">metric_set</span><span class="op">(</span><span class="va">rmse</span><span class="op">)</span>
  <span class="op">)</span></code></pre></div>
<p>After the tuning process, we collect the evaluation metrics (RMSE in our example) to identify the <em>optimal</em> model. Figure <a href="#fig:cvplot"><strong>??</strong></a> illustrates the average validation set the root mean-squared error for each value of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\rho\)</span>.</p>
<div class="sourceCode" id="cb161"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">lm_tune</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"bottom"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>y <span class="op">=</span> <span class="st">"Root mean-squared prediction error"</span>,
       title <span class="op">=</span> <span class="st">"MSPE for manufacturing excess returns"</span>,
       subtitle <span class="op">=</span> <span class="st">"Lasso (1.0), Ridge (0.0) and Elastic net (0.5) with different levels of regularization."</span><span class="op">)</span></code></pre></div>
<div class="inline-figure">
<img src="40_factor_selection_with_machine_learning_files/figure-html/cvplot-1.png" width="768" style="display: block; margin: auto;">
The figure shows that the cross-validated mean squared prediction error drops for both, Lasso and the Elastic Net and spike afterwards. For Ridge regression, the MSPE simply increases above a certain threshold. Recall, that the larger the regularization, the more restricted the model becomes. Thus, we would chooose the model with the lowest MSPE which happens to exhibit some intermediate level of regularization.</div>
</div>
<div id="parallelized-workflow" class="section level3" number="9.3.5">
<h3>
<span class="header-section-number">9.3.5</span> Parallelized Workflow<a class="anchor" aria-label="anchor" href="#parallelized-workflow"><i class="fas fa-link"></i></a>
</h3>
<p>Our starting point was the question: which factors determine industry returns? To illustrate the entire workflow, we now run the penalized regressions for all 10 industries. We want to identify relevant variables by fitting Lasso models for each of the time-series of industry returns. More specifically, for each industry, we perform cross-validation to identify the optimal penalty factor <span class="math inline">\(\lambda\)</span>. Then, we use the set of <code>finalize_*</code> functions which take a list or tibble of tuning parameter values and update objects with those values. After determining the best model, we compute the final fit on the entire training set and analyse the estimated coefficients.</p>
<p>First we define the Lasso model with one tuning parameter:</p>
<div class="sourceCode" id="cb162"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lasso_model</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>
  penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>,
  mixture <span class="op">=</span> <span class="fl">1</span>
<span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span>

<span class="va">lm_fit</span> <span class="op">&lt;-</span> <span class="va">lm_fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">update_model</span><span class="op">(</span><span class="va">lasso_model</span><span class="op">)</span></code></pre></div>
<p>The following task can be easily parallelized to substantially reduce computing time. We use the parallelization capabilities of <code>furrr</code>. Note that we can also just recycle all the steps from above and collect them in a function.</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">select_variables</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">input</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Split into training and testing dataset</span>
  <span class="va">split</span> <span class="op">&lt;-</span> <span class="fu">initial_time_split</span><span class="op">(</span><span class="va">input</span>, prop <span class="op">=</span> <span class="fl">4</span> <span class="op">/</span> <span class="fl">5</span><span class="op">)</span>

  <span class="co"># Data folds for cross validation</span>
  <span class="va">data_folds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/timetk/man/time_series_cv.html">time_series_cv</a></span><span class="op">(</span>
    data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span>,
    date_var <span class="op">=</span> <span class="va">month</span>,
    initial <span class="op">=</span> <span class="st">"5 years"</span>,
    assess <span class="op">=</span> <span class="st">"48 months"</span>,
    cumulative <span class="op">=</span> <span class="cn">FALSE</span>,
    slice_limit <span class="op">=</span> <span class="fl">20</span>
  <span class="op">)</span>

  <span class="co"># Model tuning with the Lasso model</span>
  <span class="va">lm_tune</span> <span class="op">&lt;-</span> <span class="va">lm_fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu">tune_grid</span><span class="op">(</span>
      resample <span class="op">=</span> <span class="va">data_folds</span>,
      grid <span class="op">=</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">penalty</span><span class="op">(</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span>,
      metrics <span class="op">=</span> <span class="fu">metric_set</span><span class="op">(</span><span class="va">rmse</span><span class="op">)</span>
    <span class="op">)</span>

  <span class="co"># Finalizing: Identify the best model and fit with the training data</span>
  <span class="va">lasso_lowest_rmse</span> <span class="op">&lt;-</span> <span class="va">lm_tune</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select_by_one_std_err</span><span class="op">(</span><span class="st">"rmse"</span><span class="op">)</span>
  <span class="va">lasso_final</span> <span class="op">&lt;-</span> <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">lm_fit</span>, <span class="va">lasso_lowest_rmse</span><span class="op">)</span>
  <span class="va">lasso_final_fit</span> <span class="op">&lt;-</span> <span class="fu">last_fit</span><span class="op">(</span><span class="va">lasso_final</span>, <span class="va">split</span>, metrics <span class="op">=</span> <span class="fu">metric_set</span><span class="op">(</span><span class="va">rmse</span><span class="op">)</span><span class="op">)</span>

  <span class="co"># Extract the estimated coefficients</span>
  <span class="va">lasso_final_fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu">extract_fit_parsnip</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
      term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"factor_|macro_|industry_"</span>, <span class="st">""</span>, <span class="va">term</span><span class="op">)</span>
    <span class="op">)</span>
<span class="op">}</span>

<span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="va">multisession</span>, workers <span class="op">=</span> <span class="fu"><a href="https://future.futureverse.org/reference/re-exports.html">availableCores</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="co"># Parallelization</span>

<span class="va">selected_factors</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/nest.html">nest</a></span><span class="op">(</span>data <span class="op">=</span> <span class="op">-</span><span class="va">industry</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="co"># Computation by industry</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>selected_variables <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/furrr/man/future_map.html">future_map</a></span><span class="op">(</span><span class="va">data</span>, <span class="va">select_variables</span>,
    .options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/furrr/man/furrr_options.html">furrr_options</a></span><span class="op">(</span>seed <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  <span class="op">)</span><span class="op">)</span> <span class="co"># Seed is required to make the cross-validation process reproducible</span></code></pre></div>
<p>What has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry we did it for 10 in parallel. Now we just have to do some housekeeping and keep only variables which Lasso does <em>not</em> set to zero. We illustrate the results in a heat map.</p>
<div class="sourceCode" id="cb164"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">selected_factors</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/nest.html">unnest</a></span><span class="op">(</span><span class="va">selected_variables</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span>
    <span class="va">term</span> <span class="op">!=</span> <span class="st">"(Intercept)"</span>,
    <span class="va">estimate</span> <span class="op">!=</span> <span class="fl">0</span>
  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html">add_count</a></span><span class="op">(</span><span class="va">term</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"NA|ff_|q_"</span>, <span class="st">""</span>, <span class="va">term</span><span class="op">)</span>,
    term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"_x_"</span>, <span class="st">" "</span>, <span class="va">term</span><span class="op">)</span>,
    term <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_reorder.html">fct_reorder</a></span><span class="op">(</span><span class="fu"><a href="https://forcats.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">term</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span>,
    term <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_lump.html">fct_lump_min</a></span><span class="op">(</span><span class="va">term</span>, min <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>,
    selected <span class="op">=</span> <span class="fl">1</span>
  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">term</span> <span class="op">!=</span> <span class="st">"Other"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>term <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_drop.html">fct_drop</a></span><span class="op">(</span><span class="va">term</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">industry</span>, <span class="va">term</span>, fill <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>selected <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">industry</span>,
    <span class="va">term</span>,
    fill <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">selected</span><span class="op">)</span>
  <span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_tile</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_fill_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"white"</span>, <span class="st">"cornflowerblue"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"None"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="cn">NULL</span>, y <span class="op">=</span> <span class="cn">NULL</span>,
    title <span class="op">=</span> <span class="st">"Selected variables for different industries"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="40_factor_selection_with_machine_learning_files/figure-html/unnamed-chunk-17-1.png" width="768" style="display: block; margin: auto;"></div>
<p>The heat map conveys two main insights: First, we see a lot of white which simply means that a lot of the factors, macroeconomic variables and also the interaction terms are not relevant when it comes to explain the cross-section of returns across the industry portfolios. In fact, only the market factor and the return-on-equity factor play a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While not even the market factor is selected by Lasso for Utilities (which means the proposed model essentially just contains an intercept), quite a number of factors are selected for, e.g., High-Tech and Energy but they do not coincide at all. In other words, there seems to be a clear picture that we do not need a lot of factors, but Lasso does not provide conses across industries when it comes to pricing abilities.</p>
</div>
</div>
<div id="exercises-1" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-1"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Write a function that requires three inputs, <code>y</code> (a <span class="math inline">\(T\)</span> vector), <code>X</code> (a <span class="math inline">\((T \times K)\)</span> matrix), and <code>lambda</code> and which returns the <strong>Ridge</strong> estimator (a <span class="math inline">\(K\)</span> vector) for given penalization parameter <span class="math inline">\(\lambda\)</span>. Recall that the intercept should not be penalized. Therefore, your function should allow to indicate whether <span class="math inline">\(X\)</span> contains a vector of ones as first column which should be exempt from the <span class="math inline">\(L_2\)</span> penalty.</li>
<li>Compute the <span class="math inline">\(L_2\)</span> norm (<span class="math inline">\(\beta'\beta\)</span>) for the regression coefficients based on the predictive regression from the previous exercise for a range of <span class="math inline">\(\lambda\)</span>’s and illustrate the effect the penalization in a suitable figure.</li>
<li>Now, write a function that requires three inputs, <code>y</code> (a <span class="math inline">\(T\)</span> vector), <code>X</code> (a <span class="math inline">\((T \times K)\)</span> matrix), and ’lambda` and which returns the <strong>Lasso</strong> estimator (a <span class="math inline">\(K\)</span> vector) for given penalization parameter <span class="math inline">\(\lambda\)</span>. Recall that the intercept should not be penalized. Therefore, your function should allow to indicate whether <span class="math inline">\(X\)</span> contains a vector of ones as first column which should be exempt from the <span class="math inline">\(L_1\)</span> penalty.</li>
<li>After you are really sure you understand what Ridge and Lasso regression are doing, familiarize yourself with the documentation of the package <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code>. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients not only for Ridge and Lasso but also for combinations therefore, commonly called <em>elastic nets</em>.</li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="bivariate-sorts-replication-of-fama-french-factors.html"><span class="header-section-number">8</span> Bivariate Sorts: Replication of Fama &amp; French Factors</a></div>
<div class="next"><a href="option-pricing-via-machine-learning-methods.html"><span class="header-section-number">10</span> Option Pricing via Machine learning methods</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#factor-selection-via-machine-learning"><span class="header-section-number">9</span> Factor selection via machine learning</a></li>
<li>
<a class="nav-link" href="#brief-theoretical-background"><span class="header-section-number">9.1</span> Brief theoretical background</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#ridge-regression"><span class="header-section-number">9.1.1</span> Ridge Regression</a></li>
<li><a class="nav-link" href="#lasso"><span class="header-section-number">9.1.2</span> Lasso</a></li>
<li><a class="nav-link" href="#elastic-net"><span class="header-section-number">9.1.3</span> Elastic Net</a></li>
</ul>
</li>
<li><a class="nav-link" href="#data-preparation-2"><span class="header-section-number">9.2</span> Data preparation</a></li>
<li>
<a class="nav-link" href="#the-tidymodels-workflow"><span class="header-section-number">9.3</span> The tidymodels workflow</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#preprocess-data"><span class="header-section-number">9.3.1</span> Preprocess Data</a></li>
<li><a class="nav-link" href="#build-a-model"><span class="header-section-number">9.3.2</span> Build a Model</a></li>
<li><a class="nav-link" href="#fit-a-model"><span class="header-section-number">9.3.3</span> Fit a Model</a></li>
<li><a class="nav-link" href="#tune-a-model"><span class="header-section-number">9.3.4</span> Tune a Model</a></li>
<li><a class="nav-link" href="#parallelized-workflow"><span class="header-section-number">9.3.5</span> Parallelized Workflow</a></li>
</ul>
</li>
<li><a class="nav-link" href="#exercises-1"><span class="header-section-number">9.4</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Tidy Finance</strong>" was written by Christoph Scheuch, Patrick Weiss and Stefan Voigt. It was last built on 2022-01-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
