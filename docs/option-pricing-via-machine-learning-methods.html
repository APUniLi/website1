<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Option Pricing via Machine learning methods | Tidy Finance</title>
<meta name="author" content="Christoph Scheuch, Patrick Weiss and Stefan Voigt">
<meta name="description" content="Machine learning is seen as a part of artificial intelligence. Machine learning algorithms build a model based on training data in order to make predictions or decisions without being explicitly...">
<meta name="generator" content="bookdown 0.24.4 with bs4_book()">
<meta property="og:title" content="Chapter 10 Option Pricing via Machine learning methods | Tidy Finance">
<meta property="og:type" content="book">
<meta property="og:description" content="Machine learning is seen as a part of artificial intelligence. Machine learning algorithms build a model based on training data in order to make predictions or decisions without being explicitly...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Option Pricing via Machine learning methods | Tidy Finance">
<meta name="twitter:description" content="Machine learning is seen as a part of artificial intelligence. Machine learning algorithms build a model based on training data in order to make predictions or decisions without being explicitly...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Tidy Finance</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Welcome</a></li>
<li><a class="" href="introduction-to-tidy-finance.html"><span class="header-section-number">2</span> Introduction to Tidy Finance</a></li>
<li><a class="" href="accessing-managing-financial-data.html"><span class="header-section-number">3</span> Accessing &amp; Managing Financial Data</a></li>
<li><a class="" href="beta-estimation.html"><span class="header-section-number">4</span> Beta Estimation</a></li>
<li><a class="" href="univariate-sorts.html"><span class="header-section-number">5</span> Univariate Sorts</a></li>
<li><a class="" href="univariate-sorts-firm-size.html"><span class="header-section-number">6</span> Univariate Sorts: Firm Size</a></li>
<li><a class="" href="bivariate-sorts-value.html"><span class="header-section-number">7</span> Bivariate Sorts: Value</a></li>
<li><a class="" href="bivariate-sorts-replication-of-fama-french-factors.html"><span class="header-section-number">8</span> Bivariate Sorts: Replication of Fama &amp; French Factors</a></li>
<li><a class="" href="factor-selection-via-machine-learning.html"><span class="header-section-number">9</span> Factor selection via machine learning</a></li>
<li><a class="active" href="option-pricing-via-machine-learning-methods.html"><span class="header-section-number">10</span> Option Pricing via Machine learning methods</a></li>
<li><a class="" href="parametric-portfolio-policies.html"><span class="header-section-number">11</span> Parametric Portfolio Policies</a></li>
<li><a class="" href="constraint-optimization-and-portfolio-backtesting.html"><span class="header-section-number">12</span> Constraint Optimization and Portfolio Backtesting</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="option-pricing-via-machine-learning-methods" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Option Pricing via Machine learning methods<a class="anchor" aria-label="anchor" href="#option-pricing-via-machine-learning-methods"><i class="fas fa-link"></i></a>
</h1>
<p>Machine learning is seen as a part of artificial intelligence.
Machine learning algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so.
While Machine learning can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to a simple application in Option pricing. More specifically, we are going to create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for Call options. Then, we train different models to <em>learn</em> how to price Call options without prior knowledge of the theoretical underpinnings of the famous Option pricing equation.</p>
<p>The roadmap is as follows: We first provide a very brief introduction into regression trees, random forests and neural networks. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues.
We show how to implement random forests and deep neural networks with tidy principles using <code>tidymodels</code> or <code>tensorflow</code> for more complicated network structures.</p>
<div class="rmdnote">
<p>In order to replicate the analysis regarding neural networks in this chapter, you have to install <code>TensorFlow</code> on your system which requires administrator rights on your machine. Parts of this can be done from within R, just follow <a href="https://tensorflow.rstudio.com/installation/">these quick start instructions</a>.</p>
</div>
<p>Throughout this chapter we need the following packages.</p>
<div class="sourceCode" id="cb203"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://keras.rstudio.com">keras</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidymodels/hardhat">hardhat</a></span><span class="op">)</span></code></pre></div>
<div id="regression-trees-and-random-forests" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Regression trees and random forests<a class="anchor" aria-label="anchor" href="#regression-trees-and-random-forests"><i class="fas fa-link"></i></a>
</h2>
<p>Regression trees have become a popular machine learning approach for incorporating multiway predictor interactions. Trees are fully nonparametric and possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each. A tree “grows” in a sequence of steps. At each step, a new “branch” sorts the data which is left over from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into rectangular partitions, and approximates the unknown function <span class="math inline">\(f(x)\)</span> with the average value of the outcome variable within each partition</p>
<p>We partition the predictor space into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>. For any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span> we estimate <span class="math inline">\(f(x)\)</span> with the average of the training observations, <span class="math inline">\(\hat y_i\)</span>, for which the associated predictor <span class="math inline">\(x_i\)</span> is also in <span class="math inline">\(R_j\)</span>. Once we select a partition <span class="math inline">\(\mathbf{x}\)</span> to split in order to create the new partitions, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions, which we will call <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(R_2(j,s)\)</span>, that split our observations in the current partition by asking if <span class="math inline">\(x_j\)</span> is bigger than <span class="math inline">\(s\)</span>:
<span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span>
To pick <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> we find the pair that minimizes the residual sum of square (RSS):
<span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span>
Note: Unlike for the sample variance, we don’t scale by the number of elements <span class="math inline">\(R_k(j, s)\)</span>! As in the chapter on penalized regressions, the first relevant question to ask is: What are the hyperparameters decisions? Instead of a regularization parameter, trees are fully determined by the number branches used to generate the partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).</p>
<p>Single tree models suffer from high variance. Random forests address the shortcomings of decision trees. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). A forest basically implies to create many regression trees and average their predictions. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. More specifically, we build <span class="math inline">\(B\)</span> decision trees <span class="math inline">\(T_1, \ldots, T_B\)</span> using the training sample. For that purpose we randomly select features to be included in the building of each tree. For each observation in the test set we then form a prediction <span class="math inline">\(\hat{y} = \frac{1}{B}\sum\limits_{i=1}^B\hat{y}_{T_i}\)</span>.</p>
</div>
<div id="neural-networks" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Neural Networks<a class="anchor" aria-label="anchor" href="#neural-networks"><i class="fas fa-link"></i></a>
</h2>
<p>Roughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as “universal approximators” for any smooth predictive association <span class="citation">(<a href="constraint-optimization-and-portfolio-backtesting.html#ref-Hornik1991" role="doc-biblioref">Hornik 1991</a>)</span>. Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized machine learning tools</p>
<p>Each neuron applies a nonlinear “activation function” <span class="math inline">\(f\)</span> to its aggregated signal before
sending its output to the next layer
<span class="math display">\[x_k^l = f\left(\theta^k_{0} + \sum\limits_{j = 1}^{N ^l}z_j\theta_{l,j}^k\right)\]</span>
While the easiest case where <span class="math inline">\(f(x) = \alpha + \beta x\)</span> resembles linear regression, typical activation functions are sigmoid (<span class="math inline">\(f(x) = (1+e^{-x})^{-1}\)</span>) or ReLu (<span class="math inline">\(f(x) = max(x, 0)\)</span>)</p>
<p>Neural networks gain there flexibility form chaining multiple layers together. Naturally, this imposes a large number of degrees of freedom on the network architecture for which no clear theoretical guidance exist. The specification of a neural network requires, at a minimum, a stancen on depth (number of hidden layers), the activation function, the number of neurons, the
connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, <em>learning</em> means to choose optimal parameters relys on numerical optimization which often requires to specify an appropriate learning
rate.</p>
<p>Despite the computational challenges, implementation in R is not tedious at all because we
can use the API to <code>tensorflow</code>.</p>
</div>
<div id="option-pricing" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Option Pricing<a class="anchor" aria-label="anchor" href="#option-pricing"><i class="fas fa-link"></i></a>
</h2>
<p>To apply Machine Learning methods in a relevant field of finance we focus on option pricing. In its most basic form, Call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price <span class="math inline">\(K\)</span>) at a specific date (the exercise date <span class="math inline">\(T\)</span>). The Black–Scholes price ([Black1971]) of a call option for a non-dividend-paying underlying stock is given by
<span class="math display">\[
\begin{aligned}
  C(S, T) &amp;= \Phi(d_1)S - \Phi(d_1 - \sigma\sqrt{T})Ke^{-r T} \\
     d_1 &amp;= \frac{1}{\sigma\sqrt{T}}\left[\ln\left(\frac{S}{K}\right) + \left(r_f + \frac{\sigma^2}{2}\right)T\right]
\end{aligned}
\]</span>
where <span class="math inline">\(C(S, T)\)</span> is the price of the option as a function of today’s stock price of the underlying, <span class="math inline">\(S\)</span>, with time to maturity<span class="math inline">\(T\)</span>, <span class="math inline">\(r_f\)</span> is the risk-free interest rate, and <span class="math inline">\(\sigma\)</span> is the volatility of the underlying stock return. <span class="math inline">\(\Phi\)</span> is the cumulative distribution function of a standard normal random variable.</p>
<p>The Black-Scholes equation provides an easy way to compute the arbitrage-free price of a Call option once the parameters <span class="math inline">\(S, K, r_f, T\)</span> and <span class="math inline">\(\sigma\)</span> are specified (arguably, all parameters are easily to specify except for <span class="math inline">\(\sigma\)</span> which has to be estimated). A simple <code>R</code> function allows to compute the price as we do below.</p>
<div class="sourceCode" id="cb204"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">black_scholes_price</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">S</span> <span class="op">=</span> <span class="fl">50</span>, <span class="va">K</span> <span class="op">=</span> <span class="fl">70</span>, <span class="va">r</span> <span class="op">=</span> <span class="fl">0</span>, <span class="va">T</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">sigma</span> <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Arbitrage-free price of a Call option</span>
  <span class="va">d1</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">S</span> <span class="op">/</span> <span class="va">K</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="va">r</span> <span class="op">+</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="cn">T</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">sigma</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="cn">T</span><span class="op">)</span><span class="op">)</span>
  <span class="va">value</span> <span class="op">&lt;-</span> <span class="va">S</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">d1</span><span class="op">)</span> <span class="op">-</span> <span class="va">K</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="va">r</span> <span class="op">*</span> <span class="cn">T</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">d1</span> <span class="op">-</span> <span class="va">sigma</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="cn">T</span><span class="op">)</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
</div>
<div id="learning-black-scholes" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Learning Black-Scholes<a class="anchor" aria-label="anchor" href="#learning-black-scholes"><i class="fas fa-link"></i></a>
</h2>
<p>We illustrate the concept of machine learning by showing how machine learning methods <em>learn</em> the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation.</p>
<div id="data-simulation" class="section level3" number="10.4.1">
<h3>
<span class="header-section-number">10.4.1</span> Data simulation<a class="anchor" aria-label="anchor" href="#data-simulation"><i class="fas fa-link"></i></a>
</h3>
<p>To that end we start with simulated data. We compute option prices for Call options for a grid of different combinations of times to maturity (<code>T</code>), risk-free rate (<code>r</code>), volatility (<code>sigma</code>), strike prices (<code>K</code>) and current stock prices (<code>S</code>). In the code below we add an idiosyncratic error term to each observation such that the prices which are considered observed do not exactly reflect the values implied by the Black-Scholes equation.</p>
<div class="sourceCode" id="cb205"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">option_prices</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/expand_grid.html">expand_grid</a></span><span class="op">(</span>
  S <span class="op">=</span> <span class="fl">40</span><span class="op">:</span><span class="fl">60</span>, <span class="co"># stock price</span>
  K <span class="op">=</span> <span class="fl">20</span><span class="op">:</span><span class="fl">90</span>, <span class="co"># strike price</span>
  r <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">0.05</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span>, <span class="co"># risk-free rate</span>
  T <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">3</span> <span class="op">/</span> <span class="fl">12</span>, to <span class="op">=</span> <span class="fl">2</span>, by <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="fl">12</span><span class="op">)</span>, <span class="co"># Time to maturity</span>
  sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.1</span>, to <span class="op">=</span> <span class="fl">0.8</span>, by <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>
<span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>   <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    black_scholes <span class="op">=</span> <span class="fu">black_scholes_price</span><span class="op">(</span><span class="va">S</span>, <span class="va">K</span>, <span class="va">r</span>, <span class="cn">T</span>, <span class="va">sigma</span><span class="op">)</span>, <span class="co"># Option price in theory</span>
    observed_price <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="va">black_scholes</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">2</span>, sd <span class="op">=</span> <span class="fl">0.15</span><span class="op">)</span><span class="op">)</span>
  <span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="co"># Add some random deviations to each option price</span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/nest.html">unnest</a></span><span class="op">(</span><span class="va">observed_price</span><span class="op">)</span></code></pre></div>
<p>The code above generates 1.574496^{6} random parameter constellations and for each of these values two <em>observed</em> prices which reflect the Black-Scholes prices and a random innovation term which <em>pollutes</em> the observed prices.</p>
<p>Next, we split the data into a training set (which contains 1% of all the observed option prices) and a test set which is only going to be used for the final evaluation. Note that the entire grid of possible combinations contains 3148992 different specifications, thus the sample to learn the Black-Scholes price contains only 3.1489^{4} and therefore is relatively small.
In order to keep the analysis reproducible, we use <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code>. A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines.</p>
<div class="sourceCode" id="cb206"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42809</span><span class="op">)</span> <span class="co"># Ensure the analysis can be reproduced</span>
<span class="va">split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">option_prices</span>, prop <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="fl">100</span><span class="op">)</span></code></pre></div>
<p>We process the training dataset further before we fit the different Machine learning models. For that purpose we define a <code>recipe</code> which defines all processing steps. For our specific case we want to explain the observed price by the 5 variables that enter the Black-Scholes equation. The <em>true</em> price should obviously not be used to fit the model. The recipe also reflect that we standardize all predictors to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.</p>
<div class="sourceCode" id="cb207"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rec</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="va">observed_price</span> <span class="op">~</span> <span class="va">.</span>,
  data <span class="op">=</span> <span class="va">option_prices</span>
<span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">step_rm</span><span class="op">(</span><span class="va">black_scholes</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="co"># Exclude the true price</span>
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Next, we propose two ways to fit a neural network to the data. Note that both require that <code>keras</code> is installed on your local machine. The function <code>mlp</code> from the package <code>parsnip</code> provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with 20 hidden units. We set the number of training iterations to <code>epochs = 75</code>. The option <code>set_mode("regression")</code> specifies a linear activation function for the output layer.</p>
</div>
<div id="random-forests-and-single-layer-networks" class="section level3" number="10.4.2">
<h3>
<span class="header-section-number">10.4.2</span> Random forests and single layer networks<a class="anchor" aria-label="anchor" href="#random-forests-and-single-layer-networks"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb208"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Single layer neural network</span>
<span class="va">nnet_model</span> <span class="op">&lt;-</span> <span class="fu">mlp</span><span class="op">(</span>
  epochs <span class="op">=</span> <span class="fl">75</span>,
  hidden_units <span class="op">=</span> <span class="fl">20</span>
<span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"keras"</span>, verbose <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="co"># `verbose=0` argument prevents logging the results</span></code></pre></div>
<p>We can follow the straightforward <code>tidymodel</code> workflow as in the chapter before: Define a workflow, equip it with the recipe and the associated model. Finally, fit the model with the training data.</p>
<div class="sourceCode" id="cb209"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nn_fit</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">rec</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">nnet_model</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Once you are familiar the <code>tidymodel</code> workflow, it is a piece of cake to fit other models from the <code>parsnip</code> family. For instance, the model below initializes a random forest with 50 trees contained in the ensemble where we require at least 20 observations in a node.</p>
<div class="sourceCode" id="cb210"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rf_model</span> <span class="op">&lt;-</span> <span class="fu">rand_forest</span><span class="op">(</span>
  trees <span class="op">=</span> <span class="fl">50</span>,
  min_n <span class="op">=</span> <span class="fl">20</span>
<span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"ranger"</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span></code></pre></div>
<p>Fitting the model follows exactly the same convention as for the neural network before.</p>
<div class="sourceCode" id="cb211"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rf_fit</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">rec</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">rf_model</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span></code></pre></div>
</div>
<div id="deep-neural-networks" class="section level3" number="10.4.3">
<h3>
<span class="header-section-number">10.4.3</span> Deep neural networks<a class="anchor" aria-label="anchor" href="#deep-neural-networks"><i class="fas fa-link"></i></a>
</h3>
<p>Note that while the <code>tidymodels</code> workflow is extremely convenient, more sophisticated <em>deep</em> neural networks are not supported yet (as of January 2022). For that reason, the code snippet below illustrates how to initialize a sequential model with 3 hidden layers with 20 units per layer. The <code>keras</code> package provides a convenient interface and is flexible enough to handle different activation functions. The <code>compile</code> command defines the loss function with which the model predictions are evaluated.</p>
<div class="sourceCode" id="cb212"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://keras.rstudio.com/reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://keras.rstudio.com/reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">20</span>, activation <span class="op">=</span> <span class="st">"sigmoid"</span>, input_shape <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://keras.rstudio.com/reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">20</span>, activation <span class="op">=</span> <span class="st">"sigmoid"</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://keras.rstudio.com/reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">20</span>, activation <span class="op">=</span> <span class="st">"sigmoid"</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://keras.rstudio.com/reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">1</span>, activation <span class="op">=</span> <span class="st">"linear"</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html">compile</a></span><span class="op">(</span>
    loss <span class="op">=</span> <span class="st">"mean_absolute_error"</span>
  <span class="op">)</span>
<span class="va">model</span></code></pre></div>
<pre><code>## Model
## Model: "sequential_1"
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## dense_5 (Dense)                     (None, 20)                      120         
## ________________________________________________________________________________
## dense_4 (Dense)                     (None, 20)                      420         
## ________________________________________________________________________________
## dense_3 (Dense)                     (None, 20)                      420         
## ________________________________________________________________________________
## dense_2 (Dense)                     (None, 1)                       21          
## ================================================================================
## Total params: 981
## Trainable params: 981
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<p>To train the neural network, we simply provide the inputs (<code>x</code>) and the variable to predict (<code>y</code>) and then fit the parameters. Note the slightly tedious use of the method <code>extract_mold(nn_fit)</code>: instead of simply using the <strong>raw</strong> data, we fit the neural network with the same processed data that is used for the single-layer feed-forward network. What is the difference to simply calling <code>x = training(data) %&gt;% select(-observed_price, -black_scholes)</code>? Recall, that the recipe standardizes the variables such that all columns have unit standard deviation and zero mean. Further, it adds consistency if we ensure that all models are trained using the same recipe such that a change in the recipe is reflected in the performance of any model. A final note on a potentially irritating observation: Note that <code><a href="https://generics.r-lib.org/reference/fit.html">fit()</a></code> alters the <code>keras</code> model: this is one of the few instances where a function in <code>R</code> alters the <em>input</em> such that after calling the function the object <code>model</code> is not going to be same anymore!</p>
<div class="sourceCode" id="cb214"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/hardhat/man/hardhat-extract.html">extract_mold</a></span><span class="op">(</span><span class="va">nn_fit</span><span class="op">)</span><span class="op">$</span><span class="va">predictors</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span>,
    y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/hardhat/man/hardhat-extract.html">extract_mold</a></span><span class="op">(</span><span class="va">nn_fit</span><span class="op">)</span><span class="op">$</span><span class="va">outcomes</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">observed_price</span><span class="op">)</span>,
    epochs <span class="op">=</span> <span class="fl">75</span>, verbose <span class="op">=</span> <span class="fl">0</span>
  <span class="op">)</span></code></pre></div>
</div>
<div id="universal-approximation" class="section level3" number="10.4.4">
<h3>
<span class="header-section-number">10.4.4</span> Universal approximation<a class="anchor" aria-label="anchor" href="#universal-approximation"><i class="fas fa-link"></i></a>
</h3>
<p>Before it comes to evaluation we implement one more final model: In principle, any non-linear function can also be approximated by a linear model that contains polynomial expansions of the input variables. To illustrate this we first define a new recipe, <code>rec_linear</code>, which processes the training data even further: We include polynomials up to the tenth degree of each predictor and then add all possible pairwise interaction terms. The final recipe step <code>step_lincomb</code> removes potentially redundant variables (for instance, the interaction between <span class="math inline">\(r^4\)</span> and <span class="math inline">\(r^5\)</span> is the same as the term <span class="math inline">\(r^9\)</span>). We fit a Lasso regression model with a pre-specified penalty term (consult the chapter on factor selection on how to tune the model hyperparameters).</p>
<div class="sourceCode" id="cb215"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rec_linear</span> <span class="op">&lt;-</span> <span class="va">rec</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">step_poly</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span>, degree <span class="op">=</span> <span class="fl">10</span>, options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>raw <span class="op">=</span> <span class="cn">T</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">step_interact</span><span class="op">(</span>terms <span class="op">=</span> <span class="op">~</span> <span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">:</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">step_lincomb</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>

<span class="va">lm_model</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>penalty <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span>

<span class="va">lm_fit</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">rec_linear</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">lm_model</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split</span><span class="op">)</span><span class="op">)</span></code></pre></div>
</div>
</div>
<div id="evaluating-predictions" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Evaluating predictions<a class="anchor" aria-label="anchor" href="#evaluating-predictions"><i class="fas fa-link"></i></a>
</h2>
<p>Finally, we collect all predictions to compare the <em>out-of-sample</em> prediction error.
Note, that for the evaluation we use, again, the call to <code>extract_mold</code> to ensure that we use the same pre-processing steps for the testing data across each model. We make also use of the somewhat advanced functionality in <code><a href="https://rdrr.io/pkg/hardhat/man/forge.html">hardhat::forge</a></code> which provides an easy, consistent, and robust pre-processor at prediction time.</p>
<div class="sourceCode" id="cb216"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">out_of_sample_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dbplyr.tidyverse.org/reference/testing.html">testing</a></span><span class="op">(</span><span class="va">split</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html">slice_sample</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10000</span><span class="op">)</span> <span class="co"># We evaluate the predictions based on 100k new data points</span>

<span class="va">predictive_performance</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/hardhat/man/forge.html">forge</a></span><span class="op">(</span><span class="va">out_of_sample_data</span>, <span class="fu"><a href="https://rdrr.io/pkg/hardhat/man/hardhat-extract.html">extract_mold</a></span><span class="op">(</span><span class="va">nn_fit</span><span class="op">)</span><span class="op">$</span><span class="va">blueprint</span><span class="op">)</span><span class="op">$</span><span class="va">predictors</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span><span class="st">"Deep NN"</span> <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_cols</a></span><span class="op">(</span><span class="va">nn_fit</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">out_of_sample_data</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span><span class="st">"Single Layer"</span> <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_cols</a></span><span class="op">(</span><span class="va">lm_fit</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">out_of_sample_data</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span><span class="st">"Lasso"</span> <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_cols</a></span><span class="op">(</span><span class="va">rf_fit</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">out_of_sample_data</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span><span class="st">"Random Forest"</span> <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_cols</a></span><span class="op">(</span><span class="va">out_of_sample_data</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="st">"Deep NN"</span><span class="op">:</span><span class="st">"Random Forest"</span>, names_to <span class="op">=</span> <span class="st">"Model"</span><span class="op">)</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    moneyness <span class="op">=</span> <span class="op">(</span><span class="va">S</span> <span class="op">-</span> <span class="va">K</span><span class="op">)</span>,
    pricing_error <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="op">(</span><span class="va">value</span> <span class="op">-</span> <span class="va">black_scholes</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="co"># mean squared prediction error</span>
  <span class="op">)</span> </code></pre></div>
<p>In the lines above we use each of the fitted models to generate predictions for the entire test data set of option prices. As one possible measure of pricing accuracy we evaluate the absolute pricing error, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model.</p>
<div class="sourceCode" id="cb217"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">predictive_performance</span> <span class="op"><a href="https://keras.rstudio.com/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">moneyness</span>, y <span class="op">=</span> <span class="va">pricing_error</span>, color <span class="op">=</span> <span class="va">Model</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_jitter.html">geom_jitter</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"bottom"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Moneyness (S - K)"</span>, color <span class="op">=</span> <span class="cn">NULL</span>, 
       y <span class="op">=</span> <span class="st">"Mean Squared Prediction Error (USD)"</span>, 
       title <span class="op">=</span> <span class="st">"Prediction Errors: Call option prices"</span><span class="op">)</span> </code></pre></div>
<div class="inline-figure"><img src="41_option_pricing_via_machine_learning_files/figure-html/unnamed-chunk-15-1.png" width="768" style="display: block; margin: auto;"></div>
<p>The results can be summarized as follow: i) All machine learning methods seem to be able to <em>price</em> Call options after observing the training test set. ii) The average prediction errors increase for far out-of-the money options, especially for the Single Layer neural network. ii) Random forest seems to perform consistently better in prediction option prices than the Single Layer network. iii) The deep neural network yields the best out-of-sample predictions.</p>
</div>
<div id="exercises-2" class="section level2" number="10.6">
<h2>
<span class="header-section-number">10.6</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-2"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Write a function that takes <code>y</code>, a matrix of predictors <code>X</code> as inputs and returns a characterization of the relevant parameters of a regression tree with <strong>1</strong> branch.</li>
<li>Create a function that allows to create predictions for a new matrix of predictors `newX´ based on the estimated regression tree.</li>
<li>Use the package <code>rpart</code> to <em>grow</em> a tree based on the training data and use the illustration tools in <code>rpart</code> to understand which characteristics the tree deems relevant for option pricing.</li>
<li>Make use of a training and a test set to choose the optimal depth (number of sample splits) of the tree</li>
<li>Use ‘keras’ to initialize a sequential neural network which can take the predictors from the training dataset as input, contains at least one hidden layer and generates continuous predictions. <em>This sounds harder than it is: </em>see a simply <a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/">regression example here</a>. How many parameters does the neural network you aim to fit have?</li>
</ol>
<ul>
<li>Next, compile the object. It is important that you specify a loss function. Illustrate the difference in predictive accuracy for different architecture choices.</li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="factor-selection-via-machine-learning.html"><span class="header-section-number">9</span> Factor selection via machine learning</a></div>
<div class="next"><a href="parametric-portfolio-policies.html"><span class="header-section-number">11</span> Parametric Portfolio Policies</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#option-pricing-via-machine-learning-methods"><span class="header-section-number">10</span> Option Pricing via Machine learning methods</a></li>
<li><a class="nav-link" href="#regression-trees-and-random-forests"><span class="header-section-number">10.1</span> Regression trees and random forests</a></li>
<li><a class="nav-link" href="#neural-networks"><span class="header-section-number">10.2</span> Neural Networks</a></li>
<li><a class="nav-link" href="#option-pricing"><span class="header-section-number">10.3</span> Option Pricing</a></li>
<li>
<a class="nav-link" href="#learning-black-scholes"><span class="header-section-number">10.4</span> Learning Black-Scholes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#data-simulation"><span class="header-section-number">10.4.1</span> Data simulation</a></li>
<li><a class="nav-link" href="#random-forests-and-single-layer-networks"><span class="header-section-number">10.4.2</span> Random forests and single layer networks</a></li>
<li><a class="nav-link" href="#deep-neural-networks"><span class="header-section-number">10.4.3</span> Deep neural networks</a></li>
<li><a class="nav-link" href="#universal-approximation"><span class="header-section-number">10.4.4</span> Universal approximation</a></li>
</ul>
</li>
<li><a class="nav-link" href="#evaluating-predictions"><span class="header-section-number">10.5</span> Evaluating predictions</a></li>
<li><a class="nav-link" href="#exercises-2"><span class="header-section-number">10.6</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Tidy Finance</strong>" was written by Christoph Scheuch, Patrick Weiss and Stefan Voigt. It was last built on 2022-01-25.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
