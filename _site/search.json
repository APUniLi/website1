[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a static page where we can introduce ourselves and the mission of Tidy Finance."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "This is a dynamic page that shows all blog posts in the /posts directory.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBlog Template\n\n\n0 min\n\n\n\nTemplate\n\n\n\nThis is a blog template\n\n\n\nChristoph Scheuch\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n\n4 min\n\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cover-and-logo-design.html",
    "href": "cover-and-logo-design.html",
    "title": "Cover and Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_daily <- tbl(\n  tidy_finance,\n  \"factors_ff_daily\"\n) |>\n  collect()\n\ndata_plot <- factors_ff_daily |>\n  select(date, mkt_excess) |>\n  group_by(year = floor_date(date, \"year\")) |>\n  mutate(group_id = cur_group_id())\n\ndata_plot <- data_plot |>\n  group_by(group_id) |>\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |>\n  filter(year >= \"1962-01-01\" & year <= \"2021-12-31\")\n\nlevels <- data_plot |>\n  distinct(group_id, vola) |>\n  arrange(vola) |>\n  pull(vola)\n\ncp <- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free <- function() TRUE\ncolors <- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover <- data_plot |>\n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo <- data_plot |>\n  ungroup() |> \n  filter(year == \"2021-01-01\") |> \n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website.png\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson."
  },
  {
    "objectID": "hex-sticker.html",
    "href": "hex-sticker.html",
    "title": "Hex Sticker",
    "section": "",
    "text": "library(hexSticker)\n\nsticker(\"logo-website.png\", \n        package = \"Tidy Finance\", \n        p_size = 20, p_color = \"black\",\n        s_x = 1, s_y = 0.75, s_width = 0.7, s_height = 0.7, asp = 0.9,\n        h_color = \"#3b9ab2\",\n        h_fill = \"white\",\n        url = \"tidy-finance.org\",\n        filename = \"hex-sticker.png\")\n\n\n\n\nTidy Finance HEX Sticker"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "This website is the online version of Tidy Finance with R, a book currently under development and intended for print release via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, and Patrick Weiss.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\nFinancial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from undergrad to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common data sets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the needy-greedy choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham and Grolemund (2016) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nWe believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). In its essence, tidy data follows these three principles:\n\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), and forcats (Wickham 2021).\n\nThroughout the book we use the native pipe |>, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %>% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to the second edition of Wickham and Grolemund (2016).\n\n\n\n\n\nBefore we continue, make sure you have all the software you need for this book:\n\nInstall R and RStudio. To get a walk-through of the installation for every major operating system, follow the steps outlined in this summary. The whole process should be done in a few clicks. If you wonder about the difference: R is an open-source language and environment for statistical computing and graphics, free to download and use. While R runs the computations, RStudio is an integrated development environment that provides an interface by adding many convenient features and tools. We suggest doing all the coding in RStudio.\nOpen RStudio and install the tidyverse. Not sure how it works? You will find helpful information on how to install packages in this brief summary.\n\nIf you are new to R, we recommend starting with the following sources:\n\nA very gentle and good introduction to the workings of R can be found in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham and Grolemund (2016), is available online and for free: R for Data Science explains the majority of the tools we use in our book.\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages.\n\n\n\n\nWe met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute teaching award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is affiliated with Reykjavik University and Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in a top journal in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows: > Scheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}\n\n\n\nThis book was written in RStudio using bookdown (Xie 2016). The website is hosted with GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.2.2 (2022-10-31, Innocent and Trusting) and the following packages: \n\n\n\n\n \n  \n    Package \n    Version \n  \n \n\n  \n    dbplyr \n    2.3.0 \n  \n  \n    dplyr \n    1.0.10 \n  \n  \n    forcats \n    0.5.2 \n  \n  \n    frenchdata \n    0.2.0 \n  \n  \n    ggplot2 \n    3.4.0 \n  \n  \n    googledrive \n    2.0.0 \n  \n  \n    jsonlite \n    1.8.0 \n  \n  \n    kableExtra \n    1.3.4 \n  \n  \n    lubridate \n    1.9.0 \n  \n  \n    purrr \n    1.0.1 \n  \n  \n    readr \n    2.1.3 \n  \n  \n    readxl \n    1.4.1 \n  \n  \n    renv \n    0.16.0 \n  \n  \n    rmarkdown \n    2.18 \n  \n  \n    RSQLite \n    2.2.20 \n  \n  \n    scales \n    1.2.1 \n  \n  \n    stringr \n    1.4.0 \n  \n  \n    tibble \n    3.1.8 \n  \n  \n    tidyquant \n    1.0.6 \n  \n  \n    tidyr \n    1.2.1 \n  \n  \n    tidyverse \n    1.3.2"
  },
  {
    "objectID": "introduction-to-tidy-finance.html",
    "href": "introduction-to-tidy-finance.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "The main aim of this chapter is to familiarize yourself with the tidyverse. We start by downloading and visualizing stock data from Yahoo!Finance. Then we move to a simple portfolio choice problem and construct the efficient frontier. These examples introduce you to our approach of Tidy Finance."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#working-with-stock-market-data",
    "href": "introduction-to-tidy-finance.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with Stock Market Data",
    "text": "Working with Stock Market Data\nAt the start of each session, we load the required packages. Throughout the entire book, we always use the tidyverse (Wickham et al. 2019). In this chapter, we also load the convenient tidyquant package (Dancho and Vaughan 2022) to download price data. This package provides a convenient wrapper for various quantitative functions compatible with the tidyverse.\nYou typically have to install a package once before you can load it. In case you have not done this yet, call install.packages(\"tidyquant\"). If you have trouble using tidyquant, check out the corresponding documentation.\n\nlibrary(tidyverse)\nlibrary(tidyquant)\n\nWe first download daily prices for one stock market ticker, e.g., the Apple stock, AAPL, directly from the data provider Yahoo!Finance. To download the data, you can use the command tq_get. If you do not know how to use it, make sure you read the help file by calling ?tq_get. We especially recommend taking a look at the examples section of the documentation. We request daily data for a period of more than 20 years.\n\nprices <- tq_get(\"AAPL\",\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\nprices\n\n# A tibble: 5,535 × 8\n  symbol date        open  high   low close    volume adjusted\n  <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800    0.852\n2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600    0.780\n3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600    0.792\n4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800    0.723\n5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400    0.757\n# … with 5,530 more rows\n\n\n tq_get downloads stock market data from Yahoo!Finance if you do not specify another data source. The function returns a tibble with eight quite self-explanatory columns: symbol, date, the market prices at the open, high, low, and close, the daily volume (in the number of traded shares), and the adjusted price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the ggplot2 package (Wickham 2016) to visualize the time series of adjusted prices in Figure 1 . This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012).\n\nprices |>\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Apple stock prices between beginning of 2000 and end of 2021\"\n  )\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n Instead of analyzing prices, we compute daily net returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted day \\(t\\) price. In that context, the function lag() is helpful, which returns the previous value in a vector.\n\nreturns <- prices |>\n  arrange(date) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret)\nreturns\n\n# A tibble: 5,535 × 3\n  symbol date           ret\n  <chr>  <date>       <dbl>\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# … with 5,530 more rows\n\n\nThe resulting tibble contains three columns, where the last contains the daily returns (ret). Note that the first entry naturally contains a missing value (NA) because there is no previous price. Obviously, the use of lag() would be meaningless if the time series is not ordered by ascending dates. The command arrange() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use arrange(desc(date)).\nFor the upcoming examples, we remove missing values as these would require separate treatment when computing, e.g., sample averages. In general, however, make sure you understand why NA values occur and carefully examine if you can simply get rid of these observations.\n\nreturns <- returns |>\n  drop_na(ret)\n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. For convenience, we multiply the returns by 100 to get returns in percent for the visualizations. Additionally, we add a dashed line that indicates the 5 percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most 5 percent. The 5 percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to stylized facts of returns.\n\nquantile_05 <- quantile(returns |> pull(ret) * 100, probs = 0.05)\nreturns |>\n  ggplot(aes(x = ret * 100)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns in percent\"\n  )\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical 5 percent quantile.\n\n\n\n\nHere, bins = 100 determines the number of bins used in the illustration and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns. A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.\n\nreturns |>\n  mutate(ret = ret * 100) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))\n\n# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           <dbl>        <dbl>         <dbl>         <dbl>\n1          0.130         2.52         -51.9          13.9\n\n\nWe see that the maximum daily return was 13.905 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing group_by(year = year(date)), where the call year(date) returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the return distribution.\n\nreturns |>\n  mutate(ret = ret * 100) |>\n  group_by(year = year(date)) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)\n\n# A tibble: 22 × 5\n    year daily_mean daily_sd daily_min daily_max\n   <dbl>      <dbl>    <dbl>     <dbl>     <dbl>\n 1  2000   -0.346       5.49    -51.9      13.7 \n 2  2001    0.233       3.93    -17.2      12.9 \n 3  2002   -0.121       3.05    -15.0       8.46\n 4  2003    0.186       2.34     -8.14     11.3 \n 5  2004    0.470       2.55     -5.58     13.2 \n 6  2005    0.349       2.45     -9.21      9.12\n 7  2006    0.0949      2.43     -6.33     11.8 \n 8  2007    0.366       2.38     -7.02     10.5 \n 9  2008   -0.265       3.67    -17.9      13.9 \n10  2009    0.382       2.14     -5.02      6.76\n11  2010    0.183       1.69     -4.96      7.69\n12  2011    0.104       1.65     -5.59      5.89\n13  2012    0.130       1.86     -6.44      8.87\n14  2013    0.0472      1.80    -12.4       5.14\n15  2014    0.145       1.36     -7.99      8.20\n16  2015    0.00199     1.68     -6.12      5.74\n17  2016    0.0575      1.47     -6.57      6.50\n18  2017    0.164       1.11     -3.88      6.10\n19  2018   -0.00573     1.81     -6.63      7.04\n20  2019    0.266       1.65     -9.96      6.83\n21  2020    0.281       2.94    -12.9      12.0 \n22  2021    0.133       1.58     -4.17      5.39\n\n\n\nIn case you wonder: the additional argument .names = \"{.fn}\" in across() determines how to name the output columns. The specification is rather flexible and allows almost arbitrary column names, which can be useful for reporting. The print() function simply controls the output options for the R console."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "href": "introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of tickers (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the tidyverse magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets as you like. The following code takes any vector of tickers, e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\"), and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for an arbitrary number of assets. We perform the analysis with data from all current constituents of the Dow Jones Industrial Average index. \n\nticker <- tq_index(\"DOW\")\n\nGetting holdings for DOW\n\nticker\n\n# A tibble: 30 × 8\n  symbol company          ident…¹ sedol weight sector share…² local…³\n  <chr>  <chr>            <chr>   <chr>  <dbl> <chr>    <dbl> <chr>  \n1 UNH    UnitedHealth Gr… 91324P… 2917… 0.0952 Healt… 5757810 USD    \n2 GS     Goldman Sachs G… 38141G… 2407… 0.0684 Finan… 5757810 USD    \n3 HD     Home Depot Inc.  437076… 2434… 0.0618 Consu… 5757810 USD    \n4 MCD    McDonald's Corp… 580135… 2550… 0.0528 Consu… 5757810 USD    \n5 AMGN   Amgen Inc.       031162… 2023… 0.0511 Healt… 5757810 USD    \n# … with 25 more rows, and abbreviated variable names ¹​identifier,\n#   ²​shares_held, ³​local_currency\n\n\nConveniently, tidyquant provides a function to get all stocks in a stock index with a single call (similarly, tq_exchange(\"NASDAQ\") delivers all stocks currently listed on the NASDAQ exchange). \n\nindex_prices <- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\n\nThe resulting tibble contains 158033 daily observations for 30 different corporations. Figure 3 illustrates the time series of downloaded adjusted prices for each of the constituents of the Dow Jones index. Make sure you understand every single line of code! (What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nindex_prices |>\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of DOW index constituents\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\nDo you notice the small differences relative to the code we used before? tq_get(ticker) returns a tibble for several symbols as well. All we need to do to illustrate all tickers simultaneously is to include color = symbol in the ggplot2 aesthetics. In this way, we generate a separate line for each ticker. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates the point well.\nThe same holds for stock returns. Before computing the returns, we use group_by(symbol) such that the mutate() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: group_by(symbol) is the key to aggregating the time series into ticker-specific variables of interest.\n\nall_returns <- index_prices |>\n  group_by(symbol) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret) |>\n  drop_na(ret)\nall_returns |>\n  mutate(ret = ret * 100) |>\n  group_by(symbol) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)\n\n# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   <chr>       <dbl>    <dbl>     <dbl>     <dbl>\n 1 AAPL       0.130      2.52     -51.9      13.9\n 2 AMGN       0.0475     1.99     -13.4      15.1\n 3 AXP        0.0547     2.30     -17.6      21.9\n 4 BA         0.0614     2.20     -23.8      24.3\n 5 CAT        0.0699     2.04     -14.5      14.7\n 6 CRM        0.128      2.68     -27.1      26.0\n 7 CSCO       0.0370     2.39     -16.2      24.4\n 8 CVX        0.0486     1.75     -22.1      22.7\n 9 DIS        0.0531     1.93     -18.4      16.0\n10 DOW        0.0799     2.81     -21.7      20.9\n11 GS         0.0584     2.33     -19.0      26.5\n12 HD         0.0601     1.94     -28.7      14.1\n13 HON        0.0523     1.95     -17.4      28.2\n14 IBM        0.0262     1.66     -15.5      12.0\n15 INTC       0.0400     2.36     -22.0      20.1\n16 JNJ        0.0415     1.23     -15.8      12.2\n17 JPM        0.0625     2.44     -20.7      25.1\n18 KO         0.0329     1.33     -10.1      13.9\n19 MCD        0.0553     1.48     -15.9      18.1\n20 MMM        0.0452     1.49     -12.9      12.6\n21 MRK        0.0325     1.70     -26.8      13.0\n22 MSFT       0.0587     1.93     -15.6      19.6\n23 NKE        0.0824     1.90     -19.8      15.5\n24 PG         0.0398     1.34     -30.2      12.0\n25 TRV        0.0554     1.85     -20.8      25.6\n26 UNH        0.101      2.00     -18.6      34.8\n27 V          0.0995     1.89     -13.6      15.0\n28 VZ         0.0287     1.51     -11.8      14.6\n29 WBA        0.0340     1.81     -15.0      16.6\n30 WMT        0.0321     1.49     -10.2      11.7\n\n\n\nNote that you are now also equipped with all tools to download price data for each ticker listed in the S&P 500 index with the same number of lines of code. Just use ticker <- tq_index(\"SP500\"), which provides you with a tibble that contains each symbol that is (currently) part of the S&P 500. However, don’t try this if you are not prepared to wait for a couple of minutes because this is quite some data to download!"
  },
  {
    "objectID": "introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "href": "introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "title": "Introduction to Tidy Finance",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol can also make sense. For instance, suppose you are interested in answering the question: are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e9 (R can handle scientific notation) denotes daily trading volume in billion USD.\n\nvolume <- index_prices |>\n  group_by(date) |>\n  summarize(volume = sum(volume * close / 1e9))\nvolume |>\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of DOW index constitutens\"\n  )\n\n\n\n\nFigure 4: Total daily trading volume in billion USD.\n\n\n\n\nFigure 4 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 5, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\nvolume |>\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of DOW index constituents\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 5: Total daily trading volume in billion USD.\n\n\n\n\nDo you understand where the warning ## Warning: Removed 1 rows containing missing values (geom_point). comes from and what it means? Purely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#portfolio-choice-problems",
    "href": "introduction-to-tidy-finance.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio Choice Problems",
    "text": "Portfolio Choice Problems\nIn the previous part, we show how to download stock market data and inspect it with graphs and summary statistics. Now, we move to a typical question in Finance: how to allocate wealth across different assets optimally. The standard framework for optimal portfolio selection considers investors that prefer higher future returns but dislike future return volatility (defined as the square root of the return variance): the mean-variance investor (Markowitz 1952).\n An essential tool to evaluate portfolios in the mean-variance context is the efficient frontier, the set of portfolios which satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility (the square root of the variance, i.e., the risk), see, e.g., Merton (1972). We compute and visualize the efficient frontier for several stocks. First, we extract each asset’s monthly returns. In order to keep things simple, we work with a balanced panel and exclude DOW constituents for which we do not observe a price on every single trading day since the year 2000.\n\nindex_prices <- index_prices |>\n  group_by(symbol) |>\n  mutate(n = n()) |>\n  ungroup() |>\n  filter(n == max(n)) |>\n  select(-n)\nreturns <- index_prices |>\n  mutate(month = floor_date(date, \"month\")) |>\n  group_by(symbol, month) |>\n  summarize(price = last(adjusted), .groups = \"drop_last\") |>\n  mutate(ret = price / lag(price) - 1) |>\n  drop_na(ret) |>\n  select(-price)\n\nHere, floor_date() is a function from the lubridate package (Grolemund and Wickham 2011), which provides useful functions to work with dates and times.\nNext, we transform the returns from a tidy tibble into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) tickers and one row for each of the \\(T\\) trading days to compute the sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] where \\(r_t\\) is the \\(N\\) vector of returns on date \\(t\\) and the sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'.\\] We achieve this by using pivot_wider() with the new column names from the column symbol and setting the values to ret. We compute the vector of sample average returns and the sample variance-covariance matrix, which we consider as proxies for the parameters of the distribution of future stock returns. Thus, for simplicity, we refer to \\(\\Sigma\\) and \\(\\mu\\) instead of explicitly highlighting that the sample moments are estimates. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account.\n\nreturns_matrix <- returns |>\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |>\n  select(-month)\nSigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\n\nThen, we compute the minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) as well as the expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) and volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to \\[\\omega_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } \\sum\\limits_{i=1}^Nw_i = 1.\\] The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\), where \\(\\iota\\) is a vector of ones and \\(\\Sigma^{-1}\\) is the inverse of \\(\\Sigma\\).\n\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(Sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\ntibble(\n  average_ret = as.numeric(t(mvp_weights) %*% mu),\n  volatility = as.numeric(sqrt(t(mvp_weights) %*% Sigma %*% mvp_weights))\n)\n\n# A tibble: 1 × 2\n  average_ret volatility\n        <dbl>      <dbl>\n1     0.00857     0.0314\n\n\nThe command solve(A, b) returns the solution of a system of equations \\(Ax = b\\). If b is not provided, as in the example above, it defaults to the identity matrix such that solve(Sigma) delivers \\(\\Sigma^{-1}\\) (if a unique solution exists).\nNote that the monthly volatility of the minimum variance portfolio is of the same order of magnitude as the daily standard deviation of the individual components. Thus, the diversification benefits in terms of risk reduction are tremendous!\nNext, we set out to find the weights for a portfolio that achieves, as an example, three times the expected return of the minimum variance portfolio. However, mean-variance investors are not interested in any portfolio that achieves the required return but rather in the efficient portfolio, i.e., the portfolio with the lowest standard deviation. If you wonder where the solution \\(\\omega_\\text{eff}\\) comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance given a minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, their objective function is to choose \\(\\omega_\\text{eff}\\) as the solution to \\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\]\nThe code below implements the analytic solution to this optimization problem for a benchmark return \\(\\bar\\mu\\), which we set to 3 times the expected return of the minimum variance portfolio. We encourage you to verify that it is correct.\n\nmu_bar <- 3 * t(mvp_weights) %*% mu\nC <- as.numeric(t(iota) %*% solve(Sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(Sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(Sigma) %*% mu)\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights +\n  lambda_tilde / 2 * (solve(Sigma) %*% mu - D * mvp_weights)"
  },
  {
    "objectID": "introduction-to-tidy-finance.html#the-efficient-frontier",
    "href": "introduction-to-tidy-finance.html#the-efficient-frontier",
    "title": "Introduction to Tidy Finance",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio \\(w_{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. The code below implements the construction of the efficient frontier, which characterizes the highest expected return achievable at each level of risk. To understand the code better, make sure to familiarize yourself with the inner workings of the for loop.\n\nc <- seq(from = -0.4, to = 1.9, by = 0.01)\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\nfor (i in seq_along(c)) {\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights\n  res$mu[i] <- 12 * 100 * t(w) %*% mu\n  res$sd[i] <- 12 * sqrt(100) * sqrt(t(w) %*% Sigma %*% w)\n}\n\nThe code above proceeds in two steps: First, we compute a vector of combination weights \\(c\\) and then we evaluate the resulting linear combination with \\(c\\in\\mathbb{R}\\):\n\\[w^* = cw_\\text{eff}(\\bar\\mu) + (1-c)w_{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] with \\(\\lambda^* = 2\\frac{c\\bar\\mu + (1-c)\\tilde\\mu - D/C}{E-D^2/C}\\) where \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). Finally, it is simple to visualize the efficient frontier alongside the two efficient portfolios within one powerful figure using ggplot2 (see Figure 6). We also add the individual stocks in the same call. We compute annualized returns based on the simple assumption that monthly returns are independent and identically distributed. Thus, the average annualized return is just 12 times the expected monthly return.\n\nres |>\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point(\n    data = res |> filter(c %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point(\n    data = tibble(\n      mu = 12 * 100 * mu,\n      sd = 12 * 10 * sqrt(diag(Sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Efficient frontier for DOW index constituents\"\n  )\n\n\n\n\nFigure 6: The big dots indicate the location of the minimum variance and efficient tangency portfolios, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\nThe line in Figure 6 indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters \\(\\Sigma\\) and \\(\\mu\\) as given)."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#exercises",
    "href": "introduction-to-tidy-finance.html#exercises",
    "title": "Introduction to Tidy Finance",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market ticker of your choice from Yahoo!Finance with tq_get() from the tidyquant package. Plot two time series of the ticker’s un-adjusted and adjusted closing prices. Explain the differences.\nCompute daily net returns for the asset and visualize the distribution of daily returns in a histogram. Also, use geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns within the histogram. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns\nTake your code from before and generalize it such that you can perform all the computations for an arbitrary vector of tickers (e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\")). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nConsider the research question: Are days with high aggregate trading volume often also days with large absolute price changes? Find an appropriate visualization to analyze the question.\nCompute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.\nIn the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?\nThe efficient frontier characterizes the portfolios with the highest expected return for different levels of risk, i.e., standard deviation. Identify the portfolio with the highest expected return per standard deviation. Hint: the ratio of expected return to standard deviation is an important concept in Finance."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "posts/blog-template/index.html",
    "href": "posts/blog-template/index.html",
    "title": "Blog Template",
    "section": "",
    "text": "This is the first post in the Tidy Finance blog!\nNote that all blog posts are frozen, i.e., when we render the entire site the computations are not re-run, but rather read from the previously frozen results\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/blog-template/index.html#subsection-header",
    "href": "posts/blog-template/index.html#subsection-header",
    "title": "Blog Template",
    "section": "Subsection Header",
    "text": "Subsection Header\nH2 headers will appear as sub-level navigation items on the right.\n\nSubsubsection Header\nH3 headers will be ignored for the navigation."
  },
  {
    "objectID": "posts/op-ed-tidy-finance/index.html",
    "href": "posts/op-ed-tidy-finance/index.html",
    "title": "What is Tidy Finance?",
    "section": "",
    "text": "Empirical finance can be tedious. Many standard tasks, such as cleaning data or forming factor portfolios, require a lot of effort. The code to produce even seminal results is typically opaque. Why should researchers have to reinvent the wheel over and over again?\n\nTidy Finance with R is our take on how to conduct empirical research in financial economics from scratch. Whether you are an industry professional looking to expand your quant skills, a graduate student diving into the finance world, or an academic researcher, this book shows you how to use R to master applications in asset pricing, portfolio optimization, risk management, and option pricing.\nWe wrote this book to provide a comprehensive guide to using R for financial analysis. Our book collects all the tools we wish we would have had at hand at the beginning of our graduate studies in finance. Without transparent code for standard procedures, numerous replication efforts (and their failures) feel like a waste of resources. We have been there, as probably everybody working with data has. Since we kicked off our careers, we have constantly updated our methods, coding styles, and workflows. Our book reflects our lessons learned. By sharing them, we aim to help others avoid dead ends.\nWorking on problems that countless others have already solved in secrecy is not just tedious, it even may have detrimental effects. In a recent study1 together with hundreds of research teams from across the globe, Albert J. Menkveld, the best-publishing Dutch economist according to Economentop 40, shows that without a standard path to do empirical analysis, results may vary substantially. Even if teams set out to analyze the same research question based on the same data, implementation details are important and deserve more than treatment as subtleties.\nThere will always be multiple acceptable ways to test relevant research questions. So why should it matter that our book lifts our curtain on reproducible finance by providing a fully transparent code base for many typical financial applications? First and foremost, we hope to inspire others to make their research truly reproducible. This is not a purely theoretical exercise: our examples start with data preparation and conclude with communicating results to get readers to do empirical analysis on real data as fast as possible. We believe that the need for precise academic writing does not stop where the code begins. Understanding and agreeing on standard procedures hopefully frees up resources to focus on what matters: a novel research project, a seminar paper, or a thorough analysis for your employer. If our book helps to provide a foundation for discussions on which determinants render code useful, we have achieved much more than we were hoping for at the beginning of this project.\nUnlike typical stylized examples, our book starts with the problems of any serious research project. The often overlooked challenge behind empirical research may seem trivial at first glance: we need data to conduct our analyses. Finance is not an exception: raw data, often hidden behind proprietary financial data sources, requires cleaning before there is any hope of extracting valuable insights from it. While you can despise data cleaning, you cannot ignore it.\nWe describe and provide the code to prepare typical open-source and proprietary financial data sources (e.g., CRSP, Compustat, Mergent FISD, TRACE). We reuse these data in all the subsequent chapters, which we keep as self-contained as possible. The empirical applications range from key concepts of empirical asset pricing (beta estimation, portfolio sorts, performance analysis, Fama-French factors) to modeling and machine learning applications (fixed effects estimation, clustering standard errors, difference-in-difference estimators, ridge regression, Lasso, Elastic net, random forests, neural networks) and portfolio optimization techniques.\nNecessarily, our book reflects our opinionated perspective on how to perform empirical analyses. From our experience as researchers and instructors, we believe in the value of the workflows we teach and apply daily. The entire book rests on two core concepts: coding principles using the tidyverse family of R packages and tidy data.\nWe base our book entirely on the open-source programming language R. R and the tidyverse community provide established tools to perform typical data science tasks, ranging from cleaning and manipulation to plotting and modeling. R is hence the ideal environment to develop an accessible code base for future finance students. The concept of tidy data refers to organizing financial data in a structured and consistent way, allowing for easy analysis and understanding.2 Taken together, tidy data and code help achieve the ultimate goal: to provide a fundamentally human-centered experience that makes it easier to teach, learn, and replicate the code of others – or even your own!\nWe are convinced that empirical research in finance is in desperate need of reproducible code to form standards for otherwise repetitive tasks. Instructors and researchers have already reached out to us with grateful words about our book. Tidy Finance finds its way into lecture halls across the globe already today. Various recent developments support our call for increased transparency. For instance, Cam Harvey, the former editor of the Journal of Finance, and a former president of the American Finance Association, openly argues that the profession needs to tackle the replication crisis.3 Top journals in financial economics increasingly adopt code and data-sharing policies to increase transparency. The industry and academia are aware and concerned (if not alarmed) about these issues, which is why we believe that the timing for publishing Tidy Finance with R could not be better.\n\n\n\n\nFootnotes\n\n\nMenkveld, A. J. et al. (2022). “Non-standard Errors”. http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nWigglesworth, R. (2021). The hidden ‘replication crisis’ of finance. Financial Times. https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0↩︎"
  },
  {
    "objectID": "accessing-managing-financial-data.html",
    "href": "accessing-managing-financial-data.html",
    "title": "Accessing & Managing Financial Data",
    "section": "",
    "text": "In this chapter, we suggest a way to organize your financial data. Everybody, who has experience with data, is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nThe package lubridate provides convenient tools to work with dates and times (Grolemund and Wickham 2011). The package scales (Wickham and Seidel 2022) provides useful scale functions for visualizations.\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on."
  },
  {
    "objectID": "accessing-managing-financial-data.html#fama-french-data",
    "href": "accessing-managing-financial-data.html#fama-french-data",
    "title": "Accessing & Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, there is a neat package by Nelson Areal that allows us to access the data easily: the frenchdata package provides functions to download and read data sets from Prof. Kenneth French finance data library (Areal 2021). \n\nlibrary(frenchdata)\n\nWe can use the main function of the package to download monthly Fama-French factors. The set 3 Factors includes the return time series of the market, size, and value factors alongside the risk-free rates. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the site, check the raw data files to appreciate the time you can save thanks to frenchdata.\n\nfactors_ff_monthly_raw <- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff_monthly <- factors_ff_monthly_raw$subsets$data[[1]] |>\n  transmute(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(month >= start_date & month <= end_date)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff_daily_raw <- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff_daily <- factors_ff_daily_raw$subsets$data[[1]] |>\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(date >= start_date & date <= end_date)\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw <- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly <- industries_ff_monthly_raw$subsets$data[[1]] |>\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |>\n  mutate(across(where(is.numeric), ~ . / 100)) |>\n  select(month, everything(), -date) |>\n  filter(month >= start_date & month <= end_date)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling get_french_data_list(). For an alternative to download Fama-French data, check out the FFdownload package by Sebastian Stöckl."
  },
  {
    "objectID": "accessing-managing-financial-data.html#q-factors",
    "href": "accessing-managing-financial-data.html#q-factors",
    "title": "Accessing & Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link <-\n  \"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2021.csv\"\n\nfactors_q_monthly <- read_csv(factors_q_monthly_link) |>\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |>\n  select(-R_F, -R_MKT, -year) |>\n  rename_with(~ str_remove(., \"R_\")) |>\n  rename_with(~ str_to_lower(.)) |>\n  mutate(across(-month, ~ . / 100)) |>\n  filter(month >= start_date & month <= end_date)"
  },
  {
    "objectID": "accessing-managing-financial-data.html#macroeconomic-predictors",
    "href": "accessing-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing & Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2021 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google drive location, we need additional packages to access the data directly from our R session. Therefore, we load readxl to read the XLSX-file (Wickham and Bryan 2022) and googledrive for the Google drive connection (D’Agostino McGowan and Bryan 2021).\n\nlibrary(readxl)\nlibrary(googledrive)\n\nUsually, you need to authenticate if you interact with Google drive directly in R. Since the data is stored via a public link, we can proceed without any authentication.\n\ndrive_deauth()\n\nThe drive_download() function from the googledrive package allows us to download the data and store it locally.\n\nmacro_predictors_link <-\n  \"https://docs.google.com/spreadsheets/d/1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2\"\n\ndrive_download(\n  macro_predictors_link,\n  path = \"data/macro_predictors.xlsx\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997)\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors <- read_xlsx(\n  \"data/macro_predictors.xlsx\",\n  sheet = \"Monthly\"\n) |>\n  mutate(month = ym(yyyymm)) |>\n  mutate(across(where(is.character), as.numeric)) |>\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |>\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |>\n  filter(month >= start_date & month <= end_date) |>\n  drop_na()\n\nFinally, after reading in the macro predictors to our memory, we remove the raw data file from our temporary storage.\n\nfile.remove(\"data/macro_predictors.xlsx\")\n\n[1] TRUE"
  },
  {
    "objectID": "accessing-managing-financial-data.html#other-macroeconomic-data",
    "href": "accessing-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing & Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar tidyquant package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date,\n  to = end_date\n) |>\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)]\n  )\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. The tidyquant package provides access to around 10,000 time series of the FRED database. If your desired time series is not included, we recommend working with the fredr package (Boysel and Vaughan 2021). Note that you need to get an API key to use its functionality. We refer to the package documentation for details."
  },
  {
    "objectID": "accessing-managing-financial-data.html#setting-up-a-database",
    "href": "accessing-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing & Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the dplyr functions. We refer to this tutorial for more information on SQL.\nThere are two packages that make working with SQLite in R very simple: RSQLite (Müller et al. 2022) embeds the SQLite database engine in R, and dbplyr (Wickham, Girlich, and Ruiz 2022) is the database back-end for dplyr. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting dplyr into SQL. Check out the RSQLite and dbplyr vignettes for more information.\n\nlibrary(RSQLite)\nlibrary(dbplyr)\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Note that we use the extended_types=TRUE option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance.sqlite in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function dbWriteTable(), which copies the data to our SQLite-database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_monthly\",\n    value = factors_ff_monthly,\n    overwrite = TRUE\n  )\n\nWe can use the remote table as an in-memory data frame by building a connection via tbl().\n\n\n\n\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\n\nAll dplyr calls are evaluated lazily, i.e., the data is not in our R session’s memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.\n\nfactors_ff_monthly_db |>\n  select(month, rf)\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.40.0 [data/tidy_finance.sqlite]\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with more rows\n\n\nIf we want to have the whole table in memory, we need to collect() it. You will see that we regularly load the data into the memory in the next chapters.\n\nfactors_ff_monthly_db |>\n  select(month, rf) |>\n  collect()\n\n# A tibble: 744 × 2\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with 739 more rows\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_daily\",\n    value = factors_ff_daily,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"industries_ff_monthly\",\n    value = industries_ff_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"factors_q_monthly\",\n    value = factors_q_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"macro_predictors\",\n    value = macro_predictors,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"cpi_monthly\",\n    value = cpi_monthly,\n    overwrite = TRUE\n  )\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly <- factors_q_monthly |> collect()"
  },
  {
    "objectID": "accessing-managing-financial-data.html#managing-sqlite-databases",
    "href": "accessing-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing & Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the dbSendQuery() function.\n\ndbSendQuery(tidy_finance, \"VACUUM\")\n\n<SQLiteResult>\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\n\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read up in this tutorial. \nApart from cleaning up, you might be interested in listing all the tables that are currently in your database. You can do this via the dbListTables() function.\n\ndbListTables(tidy_finance)\n\nWarning: Closing open result set, pending rows\n\n\n[1] \"cpi_monthly\"           \"factors_ff_daily\"     \n[3] \"factors_ff_monthly\"    \"factors_q_monthly\"    \n[5] \"industries_ff_monthly\" \"macro_predictors\"     \n\n\nThis function comes in handy if you are unsure about the correct naming of the tables in your database."
  },
  {
    "objectID": "accessing-managing-financial-data.html#exercises",
    "href": "accessing-managing-financial-data.html#exercises",
    "title": "Accessing & Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via read_csv(). Validate that you get the same data as via the frenchdata package.\nDownload the Fama-French 5 factors using the frenchdata package. Use get_french_data_list() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the resulting rf, mkt_excess, smb, and hml columns to factors_ff_monthly. Explain any differences you might find."
  },
  {
    "objectID": "cover-logo-design.html",
    "href": "cover-logo-design.html",
    "title": "Cover & Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_daily <- tbl(\n  tidy_finance,\n  \"factors_ff_daily\"\n) |>\n  collect()\n\ndata_plot <- factors_ff_daily |>\n  select(date, mkt_excess) |>\n  group_by(year = floor_date(date, \"year\")) |>\n  mutate(group_id = cur_group_id())\n\ndata_plot <- data_plot |>\n  group_by(group_id) |>\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |>\n  filter(year >= \"1962-01-01\" & year <= \"2021-12-31\")\n\nlevels <- data_plot |>\n  distinct(group_id, vola) |>\n  arrange(vola) |>\n  pull(vola)\n\ncp <- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free <- function() TRUE\ncolors <- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover <- data_plot |>\n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo <- data_plot |>\n  ungroup() |> \n  filter(year == \"2021-01-01\") |> \n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website.png\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson."
  }
]