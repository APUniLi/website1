[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a static page where we can introduce ourselves and the mission of Tidy Finance."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tidy Finance Blog",
    "section": "",
    "text": "This is a dynamic page that shows all blog posts in the /posts directory.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBlog Template\n\n\n0 min\n\n\n\nTemplate\n\n\n\nThis is a blog template\n\n\n\nChristoph Scheuch\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tidy Finance?\n\n\n4 min\n\n\n\nOp-Ed\n\n\n\nAn op-ed about the motives behind Tidy Finance with R\n\n\n\nChristoph Scheuch, Stefan Voigt, Patrick Weiss\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cover-and-logo-design.html",
    "href": "cover-and-logo-design.html",
    "title": "Cover and Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_daily <- tbl(\n  tidy_finance,\n  \"factors_ff_daily\"\n) |>\n  collect()\n\ndata_plot <- factors_ff_daily |>\n  select(date, mkt_excess) |>\n  group_by(year = floor_date(date, \"year\")) |>\n  mutate(group_id = cur_group_id())\n\ndata_plot <- data_plot |>\n  group_by(group_id) |>\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |>\n  filter(year >= \"1962-01-01\" & year <= \"2021-12-31\")\n\nlevels <- data_plot |>\n  distinct(group_id, vola) |>\n  arrange(vola) |>\n  pull(vola)\n\ncp <- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free <- function() TRUE\ncolors <- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover <- data_plot |>\n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo <- data_plot |>\n  ungroup() |> \n  filter(year == \"2021-01-01\") |> \n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website.png\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson."
  },
  {
    "objectID": "hex-sticker.html",
    "href": "hex-sticker.html",
    "title": "Hex Sticker",
    "section": "",
    "text": "library(hexSticker)\n\nsticker(\"logo-website.png\", \n        package = \"Tidy Finance\", \n        p_size = 20, p_color = \"black\",\n        s_x = 1, s_y = 0.75, s_width = 0.7, s_height = 0.7, asp = 0.9,\n        h_color = \"#3b9ab2\",\n        h_fill = \"white\",\n        url = \"tidy-finance.org\",\n        filename = \"hex-sticker.png\")\n\n\n\n\nTidy Finance HEX Sticker"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "This website is the online version of Tidy Finance with R, a book currently under development and intended for print release via Chapman & Hall/CRC. The book is the result of a joint effort of Christoph Scheuch, Stefan Voigt, and Patrick Weiss.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections. Additionally, let us know if you found the text helpful. We look forward to hearing from you!\n\n\n\nFinancial economics is a vibrant area of research, a central part of all business activities, and at least implicitly relevant to our everyday life. Despite its relevance for our society and a vast number of empirical studies of financial phenomena, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque. As graduate students, we were particularly surprised by the lack of public code for seminal papers or even textbooks on key concepts of financial economics. The lack of transparent code not only leads to numerous replication efforts (and their failures) but also constitutes a waste of resources on problems that countless others have already solved in secrecy.\nThis book aims to lift the curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others to share their code publicly and take part in our journey toward more reproducible research in the future.\n\n\n\nWe write this book for three audiences:\n\nStudents who want to acquire the basic tools required to conduct financial research ranging from undergrad to graduate level. The book’s structure is simple enough such that the material is sufficient for self-study purposes.\nInstructors who look for materials to teach courses in empirical finance or financial economics. We provide plenty of examples and focus on intuitive explanations that can easily be adjusted or expanded. At the end of each chapter, we provide exercises that we hope inspire students to dig deeper.\nData analysts or statisticians who work on issues dealing with financial data and who need practical tools to succeed.\n\n\n\n\nThe book is currently divided into five parts:\n\nThe first part introduces you to important concepts around which our approach to Tidy Finance revolves.\nThe second part provides tools to organize your data and prepare the most common data sets used in financial research. Although many important data are behind paywalls, we start by describing different open-source data and how to download them. We then move on to prepare two of the most popular datasets in financial research: CRSP and Compustat. Then, we cover corporate bond data from TRACE. We reuse the data from these chapters in all subsequent chapters. The last chapter of this part contains an overview of common alternative data providers for which direct access vie R packages exist.\nThe third part deals with key concepts of empirical asset pricing, such as beta estimation, portfolio sorts, performance analysis, and asset pricing regressions.\nIn the fourth part, we apply linear models to panel data and machine learning methods to problems in factor selection and option pricing.\nThe last part provides approaches for parametric, constrained portfolio optimization, and backtesting procedures.\n\nEach chapter is self-contained and can be read individually. Yet the data chapters provide an important background necessary for data management in all other chapters.\n\n\n\nThis book is about empirical work. While we assume only basic knowledge of statistics and econometrics, we do not provide detailed treatments of the underlying theoretical models or methods applied in this book. Instead, you find references to the seminal academic work in journal articles or textbooks for more detailed treatments. We believe that our comparative advantage is to provide a thorough implementation of typical approaches such as portfolio sorts, backtesting procedures, regressions, machine learning methods, or other related topics in empirical finance. We enrich our implementations by discussing the needy-greedy choices you face while conducting empirical analyses. We hence refrain from deriving theoretical models or extensively discussing the statistical properties of well-established tools.\nOur book is close in spirit to other books that provide fully reproducible code for financial applications. We view them as complementary to our work and want to highlight the differences:\n\nRegenstein Jr (2018) provides an excellent introduction and discussion of different tools for standard applications in finance (e.g., how to compute returns and sample standard deviations of a time series of stock returns). In contrast, our book clearly focuses on applications of the state-of-the-art for academic research in finance. We thus fill a niche that allows aspiring researchers or instructors to rely on a well-designed code base.\nCoqueret and Guida (2020) constitute a great compendium to our book with respect to applications related to return prediction and portfolio formation. The book primarily targets practitioners and has a hands-on focus. Our book, in contrast, relies on the typical databases used in financial research and focuses on the preparation of such datasets for academic applications. In addition, our chapter on machine learning focuses on factor selection instead of return prediction.\n\nAlthough we emphasize the importance of reproducible workflow principles, we do not provide introductions to some of the core tools that we relied on to create and maintain this book:\n\nVersion control systems such as Git are vital in managing any programming project. Originally designed to organize the collaboration of software developers, even solo data analysts will benefit from adopting version control. Git also makes it simple to publicly share code and allow others to reproduce your findings. We refer to Bryan (2022) for a gentle introduction to the (sometimes painful) life with Git. \nGood communication of results is a key ingredient to reproducible and transparent research. To compile this book, we heavily draw on a suite of fantastic open-source tools. First, Wickham (2016) provide a highly customizable yet easy-to-use system for creating data visualizations. Wickham and Grolemund (2016) provides an intuitive introduction to creating graphics using this approach. Second, in our daily work and to compile this book, we used the markdown-based authoring framework described in Xie, Allaire, and Grolemund (2018) and Xie, Dervieux, and Riederer (2020). Markdown documents are fully reproducible and support dozens of static and dynamic output formats. Lastly, Xie (2016) tremendously facilitates authoring markdown-based books. We do not provide introductions to these tools, as the resources above already provide easily accessible tutorials.\n\nGood writing is also important for the presentation of findings. We neither claim to be experts in this domain nor do we try to sound particularly academic. On the contrary, we deliberately use a more colloquial language to describe all the methods and results presented in this book in order to allow our readers to relate more easily to the rather technical content. For those who desire more guidance with respect to formal academic writing for financial economics, we recommend Kiesling (2003), Cochrane (2005), and Jacobsen (2014), who all provide essential tips (condensed to a few pages).\n\n\n\n\nWe believe that R (R Core Team 2022) is among the best choices for a programming language in the area of finance. Some of our favorite features include:\n\nR is free and open-source, so that you can use it in academic and professional contexts.\nA diverse and active online community works on a broad range of tools.\nA massive set of actively maintained packages for all kinds of applications exists, e.g., data manipulation, visualization, machine learning, etc.\nPowerful tools for communication, e.g., Rmarkdown and shiny, are readily available.\nRStudio is one of the best development environments for interactive data analysis.\nStrong foundations of functional programming are provided.\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran, etc.\n\nFor more information on why R is great, we refer to Wickham et al. (2019).\n\n\n\nAs you start working with data, you quickly realize that you spend a lot of time reading, cleaning, and transforming your data. In fact, it is often said that more than 80% of data analysis is spent on preparing data. By tidying data, we want to structure data sets to facilitate further analyses. As Wickham (2014) puts it:\n\n[T]idy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). In its essence, tidy data follows these three principles:\n\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThroughout this book, we try to follow these principles as best as we can. If you want to learn more about tidy data principles in an informal manner, we refer you to this vignette as part of Wickham and Girlich (2022).\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we try to follow:\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans.\n\nIn particular, we heavily draw on a set of packages called the tidyverse (Wickham et al. 2019). The tidyverse is a consistent set of packages for all data analysis tasks, ranging from importing and wrangling to visualizing and modeling data with the same grammar. In addition to explicit tidy principles, the tidyverse has further benefits: (i) if you master one package, it is easier to master others, and (ii) the core packages are developed and maintained by the Public Benefit Company Posit. These core packages contained in the tidyverse are: ggplot2 (Wickham 2016), dplyr (Wickham et al. 2022), tidyr (Wickham and Girlich 2022), readr (Wickham, Hester, and Bryan 2022), purrr (Henry and Wickham 2020), tibble (Müller and Wickham 2022), stringr (Wickham 2019), and forcats (Wickham 2021).\n\nThroughout the book we use the native pipe |>, a powerful tool to clearly express a sequence of operations. Readers familiar with the tidyverse may be used to the predecessor %>% that is part of the magrittr package. For all our applications, the native and magrittr pipe behave identically, so we opt for the one that is simpler and part of base R. For a more thorough discussion on the subtle differences between the two pipes, we refer to the second edition of Wickham and Grolemund (2016).\n\n\n\n\n\nBefore we continue, make sure you have all the software you need for this book:\n\nInstall R and RStudio. To get a walk-through of the installation for every major operating system, follow the steps outlined in this summary. The whole process should be done in a few clicks. If you wonder about the difference: R is an open-source language and environment for statistical computing and graphics, free to download and use. While R runs the computations, RStudio is an integrated development environment that provides an interface by adding many convenient features and tools. We suggest doing all the coding in RStudio.\nOpen RStudio and install the tidyverse. Not sure how it works? You will find helpful information on how to install packages in this brief summary.\n\nIf you are new to R, we recommend starting with the following sources:\n\nA very gentle and good introduction to the workings of R can be found in the form of the weighted dice project. Once you are done setting up R on your machine, try to follow the instructions in this project.\nThe main book on the tidyverse, Wickham and Grolemund (2016), is available online and for free: R for Data Science explains the majority of the tools we use in our book.\nIf you are an instructor searching to effectively teach R and data science methods, we recommend taking a look at the excellent data science toolbox by Mine Cetinkaya-Rundel.\nRStudio provides a range of excellent cheat sheets with extensive information on how to use the tidyverse packages.\n\n\n\n\nWe met at the Vienna Graduate School of Finance from which each of us graduated with a different focus but a shared passion: coding with R. We continue to sharpen our R skills as part of our current occupations:\n\nChristoph Scheuch is the Director of Product at the social trading platform wikifolio.com. He is responsible for product planning, execution, and monitoring and manages a team of data scientists to analyze user behavior and develop data-driven products. Christoph is also an external lecturer at the Vienna University of Economics and Business, where he teaches finance students how to manage empirical projects.\nStefan Voigt is Assistant Professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute. His research focuses on blockchain technology, high-frequency trading, and financial econometrics. Stefan’s research has been published in the leading finance and econometrics journals. He received the Danish Finance Institute teaching award 2022 for his courses for students and practitioners on empirical finance based on this book.\nPatrick Weiss is affiliated with Reykjavik University and Vienna University of Economics and Business. His research activity centers around the intersection of empirical asset pricing and corporate finance. Patrick is especially passionate about empirical asset pricing and has published research in a top journal in financial economics.\n\n\n\n\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e., public domain. You can cite this project as follows: > Scheuch, C., Voigt, S., & Weiss, P. (2023). Tidy Finance with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b23237\n@book{scheuch2023,\n  title = {Tidy Finance with R},\n  author = {Scheuch, Christoph and Voigt, Stefan and Weiss, Patrick},\n  year = {2023},\n  publisher = {Chapman and Hall/CRC},\n  edition  = {1st},\n  url = {https://tidy-finance.org},\n  doi = {https://doi.org/10.1201/b23237}\n}\n\n\n\nThis book was written in RStudio using bookdown (Xie 2016). The website is hosted with GitHub Pages. The complete source is available from GitHub. We generated all plots in this book using ggplot2 and its classic dark-on-light theme (theme_bw()).\nThis version of the book was built with R (R Core Team 2022) version 4.2.2 (2022-10-31, Innocent and Trusting) and the following packages: \n\n\n\n\n \n  \n    Package \n    Version \n  \n \n\n  \n    dbplyr \n    2.3.0 \n  \n  \n    dplyr \n    1.0.10 \n  \n  \n    forcats \n    0.5.2 \n  \n  \n    frenchdata \n    0.2.0 \n  \n  \n    ggplot2 \n    3.4.0 \n  \n  \n    googledrive \n    2.0.0 \n  \n  \n    jsonlite \n    1.8.0 \n  \n  \n    kableExtra \n    1.3.4 \n  \n  \n    lubridate \n    1.9.0 \n  \n  \n    purrr \n    1.0.1 \n  \n  \n    readr \n    2.1.3 \n  \n  \n    readxl \n    1.4.1 \n  \n  \n    renv \n    0.16.0 \n  \n  \n    rmarkdown \n    2.18 \n  \n  \n    RSQLite \n    2.2.20 \n  \n  \n    scales \n    1.2.1 \n  \n  \n    stringr \n    1.4.0 \n  \n  \n    tibble \n    3.1.8 \n  \n  \n    tidyquant \n    1.0.6 \n  \n  \n    tidyr \n    1.2.1 \n  \n  \n    tidyverse \n    1.3.2"
  },
  {
    "objectID": "introduction-to-tidy-finance.html",
    "href": "introduction-to-tidy-finance.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "The main aim of this chapter is to familiarize yourself with the tidyverse. We start by downloading and visualizing stock data from Yahoo!Finance. Then we move to a simple portfolio choice problem and construct the efficient frontier. These examples introduce you to our approach of Tidy Finance."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#working-with-stock-market-data",
    "href": "introduction-to-tidy-finance.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with Stock Market Data",
    "text": "Working with Stock Market Data\nAt the start of each session, we load the required packages. Throughout the entire book, we always use the tidyverse (Wickham et al. 2019). In this chapter, we also load the convenient tidyquant package (Dancho and Vaughan 2022) to download price data. This package provides a convenient wrapper for various quantitative functions compatible with the tidyverse.\nYou typically have to install a package once before you can load it. In case you have not done this yet, call install.packages(\"tidyquant\"). If you have trouble using tidyquant, check out the corresponding documentation.\n\nlibrary(tidyverse)\nlibrary(tidyquant)\n\nWe first download daily prices for one stock market ticker, e.g., the Apple stock, AAPL, directly from the data provider Yahoo!Finance. To download the data, you can use the command tq_get. If you do not know how to use it, make sure you read the help file by calling ?tq_get. We especially recommend taking a look at the examples section of the documentation. We request daily data for a period of more than 20 years.\n\nprices <- tq_get(\"AAPL\",\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\nprices\n\n# A tibble: 5,535 × 8\n  symbol date        open  high   low close    volume adjusted\n  <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 AAPL   2000-01-03 0.936 1.00  0.908 0.999 535796800    0.852\n2 AAPL   2000-01-04 0.967 0.988 0.903 0.915 512377600    0.780\n3 AAPL   2000-01-05 0.926 0.987 0.920 0.929 778321600    0.792\n4 AAPL   2000-01-06 0.948 0.955 0.848 0.848 767972800    0.723\n5 AAPL   2000-01-07 0.862 0.902 0.853 0.888 460734400    0.757\n# … with 5,530 more rows\n\n\n tq_get downloads stock market data from Yahoo!Finance if you do not specify another data source. The function returns a tibble with eight quite self-explanatory columns: symbol, date, the market prices at the open, high, low, and close, the daily volume (in the number of traded shares), and the adjusted price in USD. The adjusted prices are corrected for anything that might affect the stock price after the market closes, e.g., stock splits and dividends. These actions affect the quoted prices, but they have no direct impact on the investors who hold the stock. Therefore, we often rely on adjusted prices when it comes to analyzing the returns an investor would have earned by holding the stock continuously.\nNext, we use the ggplot2 package (Wickham 2016) to visualize the time series of adjusted prices in Figure 1 . This package takes care of visualization tasks based on the principles of the grammar of graphics (Wilkinson 2012).\n\nprices |>\n  ggplot(aes(x = date, y = adjusted)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Apple stock prices between beginning of 2000 and end of 2021\"\n  )\n\n\n\n\nFigure 1: Prices are in USD, adjusted for dividend payments and stock splits.\n\n\n\n\n Instead of analyzing prices, we compute daily net returns defined as \\(r_t = p_t / p_{t-1} - 1\\), where \\(p_t\\) is the adjusted day \\(t\\) price. In that context, the function lag() is helpful, which returns the previous value in a vector.\n\nreturns <- prices |>\n  arrange(date) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret)\nreturns\n\n# A tibble: 5,535 × 3\n  symbol date           ret\n  <chr>  <date>       <dbl>\n1 AAPL   2000-01-03 NA     \n2 AAPL   2000-01-04 -0.0843\n3 AAPL   2000-01-05  0.0146\n4 AAPL   2000-01-06 -0.0865\n5 AAPL   2000-01-07  0.0474\n# … with 5,530 more rows\n\n\nThe resulting tibble contains three columns, where the last contains the daily returns (ret). Note that the first entry naturally contains a missing value (NA) because there is no previous price. Obviously, the use of lag() would be meaningless if the time series is not ordered by ascending dates. The command arrange() provides a convenient way to order observations in the correct way for our application. In case you want to order observations by descending dates, you can use arrange(desc(date)).\nFor the upcoming examples, we remove missing values as these would require separate treatment when computing, e.g., sample averages. In general, however, make sure you understand why NA values occur and carefully examine if you can simply get rid of these observations.\n\nreturns <- returns |>\n  drop_na(ret)\n\nNext, we visualize the distribution of daily returns in a histogram in Figure 2. For convenience, we multiply the returns by 100 to get returns in percent for the visualizations. Additionally, we add a dashed line that indicates the 5 percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most 5 percent. The 5 percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators. We refer to Tsay (2010) for a more thorough introduction to stylized facts of returns.\n\nquantile_05 <- quantile(returns |> pull(ret) * 100, probs = 0.05)\nreturns |>\n  ggplot(aes(x = ret * 100)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns in percent\"\n  )\n\n\n\n\nFigure 2: The dotted vertical line indicates the historical 5 percent quantile.\n\n\n\n\nHere, bins = 100 determines the number of bins used in the illustration and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns. A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.\n\nreturns |>\n  mutate(ret = ret * 100) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    )\n  ))\n\n# A tibble: 1 × 4\n  ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max\n           <dbl>        <dbl>         <dbl>         <dbl>\n1          0.130         2.52         -51.9          13.9\n\n\nWe see that the maximum daily return was 13.905 percent. Perhaps not surprisingly, the average daily return is close to but slightly above 0. In line with the illustration above, the large losses on the day with the minimum returns indicate a strong asymmetry in the distribution of returns.\nYou can also compute these summary statistics for each year individually by imposing group_by(year = year(date)), where the call year(date) returns the year. More specifically, the few lines of code below compute the summary statistics from above for individual groups of data defined by year. The summary statistics, therefore, allow an eyeball analysis of the time-series dynamics of the return distribution.\n\nreturns |>\n  mutate(ret = ret * 100) |>\n  group_by(year = year(date)) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)\n\n# A tibble: 22 × 5\n    year daily_mean daily_sd daily_min daily_max\n   <dbl>      <dbl>    <dbl>     <dbl>     <dbl>\n 1  2000   -0.346       5.49    -51.9      13.7 \n 2  2001    0.233       3.93    -17.2      12.9 \n 3  2002   -0.121       3.05    -15.0       8.46\n 4  2003    0.186       2.34     -8.14     11.3 \n 5  2004    0.470       2.55     -5.58     13.2 \n 6  2005    0.349       2.45     -9.21      9.12\n 7  2006    0.0949      2.43     -6.33     11.8 \n 8  2007    0.366       2.38     -7.02     10.5 \n 9  2008   -0.265       3.67    -17.9      13.9 \n10  2009    0.382       2.14     -5.02      6.76\n11  2010    0.183       1.69     -4.96      7.69\n12  2011    0.104       1.65     -5.59      5.89\n13  2012    0.130       1.86     -6.44      8.87\n14  2013    0.0472      1.80    -12.4       5.14\n15  2014    0.145       1.36     -7.99      8.20\n16  2015    0.00199     1.68     -6.12      5.74\n17  2016    0.0575      1.47     -6.57      6.50\n18  2017    0.164       1.11     -3.88      6.10\n19  2018   -0.00573     1.81     -6.63      7.04\n20  2019    0.266       1.65     -9.96      6.83\n21  2020    0.281       2.94    -12.9      12.0 \n22  2021    0.133       1.58     -4.17      5.39\n\n\n\nIn case you wonder: the additional argument .names = \"{.fn}\" in across() determines how to name the output columns. The specification is rather flexible and allows almost arbitrary column names, which can be useful for reporting. The print() function simply controls the output options for the R console."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "href": "introduction-to-tidy-finance.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling Up the Analysis",
    "text": "Scaling Up the Analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of tickers (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nThis is where the tidyverse magic starts: tidy data makes it extremely easy to generalize the computations from before to as many assets as you like. The following code takes any vector of tickers, e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\"), and automates the download as well as the plot of the price time series. In the end, we create the table of summary statistics for an arbitrary number of assets. We perform the analysis with data from all current constituents of the Dow Jones Industrial Average index. \n\nticker <- tq_index(\"DOW\")\n\nGetting holdings for DOW\n\nticker\n\n# A tibble: 30 × 8\n  symbol company          ident…¹ sedol weight sector share…² local…³\n  <chr>  <chr>            <chr>   <chr>  <dbl> <chr>    <dbl> <chr>  \n1 UNH    UnitedHealth Gr… 91324P… 2917… 0.0952 Healt… 5757810 USD    \n2 GS     Goldman Sachs G… 38141G… 2407… 0.0684 Finan… 5757810 USD    \n3 HD     Home Depot Inc.  437076… 2434… 0.0618 Consu… 5757810 USD    \n4 MCD    McDonald's Corp… 580135… 2550… 0.0528 Consu… 5757810 USD    \n5 AMGN   Amgen Inc.       031162… 2023… 0.0511 Healt… 5757810 USD    \n# … with 25 more rows, and abbreviated variable names ¹​identifier,\n#   ²​shares_held, ³​local_currency\n\n\nConveniently, tidyquant provides a function to get all stocks in a stock index with a single call (similarly, tq_exchange(\"NASDAQ\") delivers all stocks currently listed on the NASDAQ exchange). \n\nindex_prices <- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = \"2021-12-31\"\n)\n\nThe resulting tibble contains 158033 daily observations for 30 different corporations. Figure 3 illustrates the time series of downloaded adjusted prices for each of the constituents of the Dow Jones index. Make sure you understand every single line of code! (What are the arguments of aes()? Which alternative geoms could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes.\n\nindex_prices |>\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of DOW index constituents\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 3: Prices in USD, adjusted for dividend payments and stock splits.\n\n\n\n\nDo you notice the small differences relative to the code we used before? tq_get(ticker) returns a tibble for several symbols as well. All we need to do to illustrate all tickers simultaneously is to include color = symbol in the ggplot2 aesthetics. In this way, we generate a separate line for each ticker. Of course, there are simply too many lines on this graph to identify the individual stocks properly, but it illustrates the point well.\nThe same holds for stock returns. Before computing the returns, we use group_by(symbol) such that the mutate() command is performed for each symbol individually. The same logic also applies to the computation of summary statistics: group_by(symbol) is the key to aggregating the time series into ticker-specific variables of interest.\n\nall_returns <- index_prices |>\n  group_by(symbol) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret) |>\n  drop_na(ret)\nall_returns |>\n  mutate(ret = ret * 100) |>\n  group_by(symbol) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)\n\n# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   <chr>       <dbl>    <dbl>     <dbl>     <dbl>\n 1 AAPL       0.130      2.52     -51.9      13.9\n 2 AMGN       0.0475     1.99     -13.4      15.1\n 3 AXP        0.0547     2.30     -17.6      21.9\n 4 BA         0.0614     2.20     -23.8      24.3\n 5 CAT        0.0699     2.04     -14.5      14.7\n 6 CRM        0.128      2.68     -27.1      26.0\n 7 CSCO       0.0370     2.39     -16.2      24.4\n 8 CVX        0.0486     1.75     -22.1      22.7\n 9 DIS        0.0531     1.93     -18.4      16.0\n10 DOW        0.0799     2.81     -21.7      20.9\n11 GS         0.0584     2.33     -19.0      26.5\n12 HD         0.0601     1.94     -28.7      14.1\n13 HON        0.0523     1.95     -17.4      28.2\n14 IBM        0.0262     1.66     -15.5      12.0\n15 INTC       0.0400     2.36     -22.0      20.1\n16 JNJ        0.0415     1.23     -15.8      12.2\n17 JPM        0.0625     2.44     -20.7      25.1\n18 KO         0.0329     1.33     -10.1      13.9\n19 MCD        0.0553     1.48     -15.9      18.1\n20 MMM        0.0452     1.49     -12.9      12.6\n21 MRK        0.0325     1.70     -26.8      13.0\n22 MSFT       0.0587     1.93     -15.6      19.6\n23 NKE        0.0824     1.90     -19.8      15.5\n24 PG         0.0398     1.34     -30.2      12.0\n25 TRV        0.0554     1.85     -20.8      25.6\n26 UNH        0.101      2.00     -18.6      34.8\n27 V          0.0995     1.89     -13.6      15.0\n28 VZ         0.0287     1.51     -11.8      14.6\n29 WBA        0.0340     1.81     -15.0      16.6\n30 WMT        0.0321     1.49     -10.2      11.7\n\n\n\nNote that you are now also equipped with all tools to download price data for each ticker listed in the S&P 500 index with the same number of lines of code. Just use ticker <- tq_index(\"SP500\"), which provides you with a tibble that contains each symbol that is (currently) part of the S&P 500. However, don’t try this if you are not prepared to wait for a couple of minutes because this is quite some data to download!"
  },
  {
    "objectID": "introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "href": "introduction-to-tidy-finance.html#other-forms-of-data-aggregation",
    "title": "Introduction to Tidy Finance",
    "section": "Other Forms of Data Aggregation",
    "text": "Other Forms of Data Aggregation\nOf course, aggregation across variables other than symbol can also make sense. For instance, suppose you are interested in answering the question: are days with high aggregate trading volume likely followed by days with high aggregate trading volume? To provide some initial analysis on this question, we take the downloaded data and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column volume is denoted in the number of traded shares. Thus, we multiply the trading volume with the daily closing price to get a proxy for the aggregate trading volume in USD. Scaling by 1e9 (R can handle scientific notation) denotes daily trading volume in billion USD.\n\nvolume <- index_prices |>\n  group_by(date) |>\n  summarize(volume = sum(volume * close / 1e9))\nvolume |>\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of DOW index constitutens\"\n  )\n\n\n\n\nFigure 4: Total daily trading volume in billion USD.\n\n\n\n\nFigure 4 indicates a clear upward trend in aggregated daily trading volume. In particular, since the outbreak of the COVID-19 pandemic, markets have processed substantial trading volumes, as analyzed, for instance, by Goldstein, Koijen, and Mueller (2021). One way to illustrate the persistence of trading volume would be to plot volume on day \\(t\\) against volume on day \\(t-1\\) as in the example below. In Figure 5, we add a dotted 45°-line to indicate a hypothetical one-to-one relation by geom_abline(), addressing potential differences in the axes’ scales.\n\nvolume |>\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n    linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of DOW index constituents\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 5: Total daily trading volume in billion USD.\n\n\n\n\nDo you understand where the warning ## Warning: Removed 1 rows containing missing values (geom_point). comes from and what it means? Purely eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#portfolio-choice-problems",
    "href": "introduction-to-tidy-finance.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio Choice Problems",
    "text": "Portfolio Choice Problems\nIn the previous part, we show how to download stock market data and inspect it with graphs and summary statistics. Now, we move to a typical question in Finance: how to allocate wealth across different assets optimally. The standard framework for optimal portfolio selection considers investors that prefer higher future returns but dislike future return volatility (defined as the square root of the return variance): the mean-variance investor (Markowitz 1952).\n An essential tool to evaluate portfolios in the mean-variance context is the efficient frontier, the set of portfolios which satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility (the square root of the variance, i.e., the risk), see, e.g., Merton (1972). We compute and visualize the efficient frontier for several stocks. First, we extract each asset’s monthly returns. In order to keep things simple, we work with a balanced panel and exclude DOW constituents for which we do not observe a price on every single trading day since the year 2000.\n\nindex_prices <- index_prices |>\n  group_by(symbol) |>\n  mutate(n = n()) |>\n  ungroup() |>\n  filter(n == max(n)) |>\n  select(-n)\nreturns <- index_prices |>\n  mutate(month = floor_date(date, \"month\")) |>\n  group_by(symbol, month) |>\n  summarize(price = last(adjusted), .groups = \"drop_last\") |>\n  mutate(ret = price / lag(price) - 1) |>\n  drop_na(ret) |>\n  select(-price)\n\nHere, floor_date() is a function from the lubridate package (Grolemund and Wickham 2011), which provides useful functions to work with dates and times.\nNext, we transform the returns from a tidy tibble into a \\((T \\times N)\\) matrix with one column for each of the \\(N\\) tickers and one row for each of the \\(T\\) trading days to compute the sample average return vector \\[\\hat\\mu = \\frac{1}{T}\\sum\\limits_{t=1}^T r_t\\] where \\(r_t\\) is the \\(N\\) vector of returns on date \\(t\\) and the sample covariance matrix \\[\\hat\\Sigma = \\frac{1}{T-1}\\sum\\limits_{t=1}^T (r_t - \\hat\\mu)(r_t - \\hat\\mu)'.\\] We achieve this by using pivot_wider() with the new column names from the column symbol and setting the values to ret. We compute the vector of sample average returns and the sample variance-covariance matrix, which we consider as proxies for the parameters of the distribution of future stock returns. Thus, for simplicity, we refer to \\(\\Sigma\\) and \\(\\mu\\) instead of explicitly highlighting that the sample moments are estimates. In later chapters, we discuss the issues that arise once we take estimation uncertainty into account.\n\nreturns_matrix <- returns |>\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |>\n  select(-month)\nSigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\n\nThen, we compute the minimum variance portfolio weights \\(\\omega_\\text{mvp}\\) as well as the expected portfolio return \\(\\omega_\\text{mvp}'\\mu\\) and volatility \\(\\sqrt{\\omega_\\text{mvp}'\\Sigma\\omega_\\text{mvp}}\\) of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to \\[\\omega_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } \\sum\\limits_{i=1}^Nw_i = 1.\\] The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\), where \\(\\iota\\) is a vector of ones and \\(\\Sigma^{-1}\\) is the inverse of \\(\\Sigma\\).\n\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(Sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\ntibble(\n  average_ret = as.numeric(t(mvp_weights) %*% mu),\n  volatility = as.numeric(sqrt(t(mvp_weights) %*% Sigma %*% mvp_weights))\n)\n\n# A tibble: 1 × 2\n  average_ret volatility\n        <dbl>      <dbl>\n1     0.00857     0.0314\n\n\nThe command solve(A, b) returns the solution of a system of equations \\(Ax = b\\). If b is not provided, as in the example above, it defaults to the identity matrix such that solve(Sigma) delivers \\(\\Sigma^{-1}\\) (if a unique solution exists).\nNote that the monthly volatility of the minimum variance portfolio is of the same order of magnitude as the daily standard deviation of the individual components. Thus, the diversification benefits in terms of risk reduction are tremendous!\nNext, we set out to find the weights for a portfolio that achieves, as an example, three times the expected return of the minimum variance portfolio. However, mean-variance investors are not interested in any portfolio that achieves the required return but rather in the efficient portfolio, i.e., the portfolio with the lowest standard deviation. If you wonder where the solution \\(\\omega_\\text{eff}\\) comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance given a minimum acceptable expected return \\(\\bar{\\mu}\\). Hence, their objective function is to choose \\(\\omega_\\text{eff}\\) as the solution to \\[\\omega_\\text{eff}(\\bar{\\mu}) = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\]\nThe code below implements the analytic solution to this optimization problem for a benchmark return \\(\\bar\\mu\\), which we set to 3 times the expected return of the minimum variance portfolio. We encourage you to verify that it is correct.\n\nmu_bar <- 3 * t(mvp_weights) %*% mu\nC <- as.numeric(t(iota) %*% solve(Sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(Sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(Sigma) %*% mu)\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights +\n  lambda_tilde / 2 * (solve(Sigma) %*% mu - D * mvp_weights)"
  },
  {
    "objectID": "introduction-to-tidy-finance.html#the-efficient-frontier",
    "href": "introduction-to-tidy-finance.html#the-efficient-frontier",
    "title": "Introduction to Tidy Finance",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio \\(w_{mvp}\\) and the efficient portfolio for a higher required level of expected returns \\(\\omega_\\text{eff}(\\bar{\\mu})\\), we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio. The code below implements the construction of the efficient frontier, which characterizes the highest expected return achievable at each level of risk. To understand the code better, make sure to familiarize yourself with the inner workings of the for loop.\n\nc <- seq(from = -0.4, to = 1.9, by = 0.01)\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\nfor (i in seq_along(c)) {\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights\n  res$mu[i] <- 12 * 100 * t(w) %*% mu\n  res$sd[i] <- 12 * sqrt(100) * sqrt(t(w) %*% Sigma %*% w)\n}\n\nThe code above proceeds in two steps: First, we compute a vector of combination weights \\(c\\) and then we evaluate the resulting linear combination with \\(c\\in\\mathbb{R}\\):\n\\[w^* = cw_\\text{eff}(\\bar\\mu) + (1-c)w_{mvp} = \\omega_\\text{mvp} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu -\\frac{D}{C}\\Sigma^{-1}\\iota \\right)\\] with \\(\\lambda^* = 2\\frac{c\\bar\\mu + (1-c)\\tilde\\mu - D/C}{E-D^2/C}\\) where \\(C = \\iota'\\Sigma^{-1}\\iota\\), \\(D=\\iota'\\Sigma^{-1}\\mu\\), and \\(E=\\mu'\\Sigma^{-1}\\mu\\). Finally, it is simple to visualize the efficient frontier alongside the two efficient portfolios within one powerful figure using ggplot2 (see Figure 6). We also add the individual stocks in the same call. We compute annualized returns based on the simple assumption that monthly returns are independent and identically distributed. Thus, the average annualized return is just 12 times the expected monthly return.\n\nres |>\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point(\n    data = res |> filter(c %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point(\n    data = tibble(\n      mu = 12 * 100 * mu,\n      sd = 12 * 10 * sqrt(diag(Sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Efficient frontier for DOW index constituents\"\n  )\n\n\n\n\nFigure 6: The big dots indicate the location of the minimum variance and efficient tangency portfolios, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\nThe line in Figure 6 indicates the efficient frontier: the set of portfolios a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters \\(\\Sigma\\) and \\(\\mu\\) as given)."
  },
  {
    "objectID": "introduction-to-tidy-finance.html#exercises",
    "href": "introduction-to-tidy-finance.html#exercises",
    "title": "Introduction to Tidy Finance",
    "section": "Exercises",
    "text": "Exercises\n\nDownload daily prices for another stock market ticker of your choice from Yahoo!Finance with tq_get() from the tidyquant package. Plot two time series of the ticker’s un-adjusted and adjusted closing prices. Explain the differences.\nCompute daily net returns for the asset and visualize the distribution of daily returns in a histogram. Also, use geom_vline() to add a dashed line that indicates the 5 percent quantile of the daily returns within the histogram. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns\nTake your code from before and generalize it such that you can perform all the computations for an arbitrary vector of tickers (e.g., ticker <- c(\"AAPL\", \"MMM\", \"BA\")). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nConsider the research question: Are days with high aggregate trading volume often also days with large absolute price changes? Find an appropriate visualization to analyze the question.\nCompute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.\nIn the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?\nThe efficient frontier characterizes the portfolios with the highest expected return for different levels of risk, i.e., standard deviation. Identify the portfolio with the highest expected return per standard deviation. Hint: the ratio of expected return to standard deviation is an important concept in Finance."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Tidy Finance",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-NC-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution, NonCommercial, and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-NC-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "posts/blog-template/index.html",
    "href": "posts/blog-template/index.html",
    "title": "Blog Template",
    "section": "",
    "text": "This is the first post in the Tidy Finance blog!\nNote that all blog posts are frozen, i.e., when we render the entire site the computations are not re-run, but rather read from the previously frozen results\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/blog-template/index.html#subsection-header",
    "href": "posts/blog-template/index.html#subsection-header",
    "title": "Blog Template",
    "section": "Subsection Header",
    "text": "Subsection Header\nH2 headers will appear as sub-level navigation items on the right.\n\nSubsubsection Header\nH3 headers will be ignored for the navigation."
  },
  {
    "objectID": "posts/op-ed-tidy-finance/index.html",
    "href": "posts/op-ed-tidy-finance/index.html",
    "title": "What is Tidy Finance?",
    "section": "",
    "text": "Empirical finance can be tedious. Many standard tasks, such as cleaning data or forming factor portfolios, require a lot of effort. The code to produce even seminal results is typically opaque. Why should researchers have to reinvent the wheel over and over again?\n\nTidy Finance with R is our take on how to conduct empirical research in financial economics from scratch. Whether you are an industry professional looking to expand your quant skills, a graduate student diving into the finance world, or an academic researcher, this book shows you how to use R to master applications in asset pricing, portfolio optimization, risk management, and option pricing.\nWe wrote this book to provide a comprehensive guide to using R for financial analysis. Our book collects all the tools we wish we would have had at hand at the beginning of our graduate studies in finance. Without transparent code for standard procedures, numerous replication efforts (and their failures) feel like a waste of resources. We have been there, as probably everybody working with data has. Since we kicked off our careers, we have constantly updated our methods, coding styles, and workflows. Our book reflects our lessons learned. By sharing them, we aim to help others avoid dead ends.\nWorking on problems that countless others have already solved in secrecy is not just tedious, it even may have detrimental effects. In a recent study1 together with hundreds of research teams from across the globe, Albert J. Menkveld, the best-publishing Dutch economist according to Economentop 40, shows that without a standard path to do empirical analysis, results may vary substantially. Even if teams set out to analyze the same research question based on the same data, implementation details are important and deserve more than treatment as subtleties.\nThere will always be multiple acceptable ways to test relevant research questions. So why should it matter that our book lifts our curtain on reproducible finance by providing a fully transparent code base for many typical financial applications? First and foremost, we hope to inspire others to make their research truly reproducible. This is not a purely theoretical exercise: our examples start with data preparation and conclude with communicating results to get readers to do empirical analysis on real data as fast as possible. We believe that the need for precise academic writing does not stop where the code begins. Understanding and agreeing on standard procedures hopefully frees up resources to focus on what matters: a novel research project, a seminar paper, or a thorough analysis for your employer. If our book helps to provide a foundation for discussions on which determinants render code useful, we have achieved much more than we were hoping for at the beginning of this project.\nUnlike typical stylized examples, our book starts with the problems of any serious research project. The often overlooked challenge behind empirical research may seem trivial at first glance: we need data to conduct our analyses. Finance is not an exception: raw data, often hidden behind proprietary financial data sources, requires cleaning before there is any hope of extracting valuable insights from it. While you can despise data cleaning, you cannot ignore it.\nWe describe and provide the code to prepare typical open-source and proprietary financial data sources (e.g., CRSP, Compustat, Mergent FISD, TRACE). We reuse these data in all the subsequent chapters, which we keep as self-contained as possible. The empirical applications range from key concepts of empirical asset pricing (beta estimation, portfolio sorts, performance analysis, Fama-French factors) to modeling and machine learning applications (fixed effects estimation, clustering standard errors, difference-in-difference estimators, ridge regression, Lasso, Elastic net, random forests, neural networks) and portfolio optimization techniques.\nNecessarily, our book reflects our opinionated perspective on how to perform empirical analyses. From our experience as researchers and instructors, we believe in the value of the workflows we teach and apply daily. The entire book rests on two core concepts: coding principles using the tidyverse family of R packages and tidy data.\nWe base our book entirely on the open-source programming language R. R and the tidyverse community provide established tools to perform typical data science tasks, ranging from cleaning and manipulation to plotting and modeling. R is hence the ideal environment to develop an accessible code base for future finance students. The concept of tidy data refers to organizing financial data in a structured and consistent way, allowing for easy analysis and understanding.2 Taken together, tidy data and code help achieve the ultimate goal: to provide a fundamentally human-centered experience that makes it easier to teach, learn, and replicate the code of others – or even your own!\nWe are convinced that empirical research in finance is in desperate need of reproducible code to form standards for otherwise repetitive tasks. Instructors and researchers have already reached out to us with grateful words about our book. Tidy Finance finds its way into lecture halls across the globe already today. Various recent developments support our call for increased transparency. For instance, Cam Harvey, the former editor of the Journal of Finance, and a former president of the American Finance Association, openly argues that the profession needs to tackle the replication crisis.3 Top journals in financial economics increasingly adopt code and data-sharing policies to increase transparency. The industry and academia are aware and concerned (if not alarmed) about these issues, which is why we believe that the timing for publishing Tidy Finance with R could not be better.\n\n\n\n\nFootnotes\n\n\nMenkveld, A. J. et al. (2022). “Non-standard Errors”. http://dx.doi.org/10.2139/ssrn.3961574↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nWigglesworth, R. (2021). The hidden ‘replication crisis’ of finance. Financial Times. https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0↩︎"
  },
  {
    "objectID": "accessing-managing-financial-data.html",
    "href": "accessing-managing-financial-data.html",
    "title": "Accessing & Managing Financial Data",
    "section": "",
    "text": "In this chapter, we suggest a way to organize your financial data. Everybody, who has experience with data, is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nThe package lubridate provides convenient tools to work with dates and times (Grolemund and Wickham 2011). The package scales (Wickham and Seidel 2022) provides useful scale functions for visualizations.\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on."
  },
  {
    "objectID": "accessing-managing-financial-data.html#fama-french-data",
    "href": "accessing-managing-financial-data.html#fama-french-data",
    "title": "Accessing & Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, there is a neat package by Nelson Areal that allows us to access the data easily: the frenchdata package provides functions to download and read data sets from Prof. Kenneth French finance data library (Areal 2021). \n\nlibrary(frenchdata)\n\nWe can use the main function of the package to download monthly Fama-French factors. The set 3 Factors includes the return time series of the market, size, and value factors alongside the risk-free rates. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the site, check the raw data files to appreciate the time you can save thanks to frenchdata.\n\nfactors_ff_monthly_raw <- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff_monthly <- factors_ff_monthly_raw$subsets$data[[1]] |>\n  transmute(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(month >= start_date & month <= end_date)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff_daily_raw <- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff_daily <- factors_ff_daily_raw$subsets$data[[1]] |>\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(date >= start_date & date <= end_date)\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw <- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly <- industries_ff_monthly_raw$subsets$data[[1]] |>\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |>\n  mutate(across(where(is.numeric), ~ . / 100)) |>\n  select(month, everything(), -date) |>\n  filter(month >= start_date & month <= end_date)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling get_french_data_list(). For an alternative to download Fama-French data, check out the FFdownload package by Sebastian Stöckl."
  },
  {
    "objectID": "accessing-managing-financial-data.html#q-factors",
    "href": "accessing-managing-financial-data.html#q-factors",
    "title": "Accessing & Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link <-\n  \"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2021.csv\"\n\nfactors_q_monthly <- read_csv(factors_q_monthly_link) |>\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |>\n  select(-R_F, -R_MKT, -year) |>\n  rename_with(~ str_remove(., \"R_\")) |>\n  rename_with(~ str_to_lower(.)) |>\n  mutate(across(-month, ~ . / 100)) |>\n  filter(month >= start_date & month <= end_date)"
  },
  {
    "objectID": "accessing-managing-financial-data.html#macroeconomic-predictors",
    "href": "accessing-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing & Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2021 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google drive location, we need additional packages to access the data directly from our R session. Therefore, we load readxl to read the XLSX-file (Wickham and Bryan 2022) and googledrive for the Google drive connection (D’Agostino McGowan and Bryan 2021).\n\nlibrary(readxl)\nlibrary(googledrive)\n\nUsually, you need to authenticate if you interact with Google drive directly in R. Since the data is stored via a public link, we can proceed without any authentication.\n\ndrive_deauth()\n\nThe drive_download() function from the googledrive package allows us to download the data and store it locally.\n\nmacro_predictors_link <-\n  \"https://docs.google.com/spreadsheets/d/1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2\"\n\ndrive_download(\n  macro_predictors_link,\n  path = \"data/macro_predictors.xlsx\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997)\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors <- read_xlsx(\n  \"data/macro_predictors.xlsx\",\n  sheet = \"Monthly\"\n) |>\n  mutate(month = ym(yyyymm)) |>\n  mutate(across(where(is.character), as.numeric)) |>\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |>\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |>\n  filter(month >= start_date & month <= end_date) |>\n  drop_na()\n\nFinally, after reading in the macro predictors to our memory, we remove the raw data file from our temporary storage.\n\nfile.remove(\"data/macro_predictors.xlsx\")\n\n[1] TRUE"
  },
  {
    "objectID": "accessing-managing-financial-data.html#other-macroeconomic-data",
    "href": "accessing-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing & Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar tidyquant package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date,\n  to = end_date\n) |>\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)]\n  )\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. The tidyquant package provides access to around 10,000 time series of the FRED database. If your desired time series is not included, we recommend working with the fredr package (Boysel and Vaughan 2021). Note that you need to get an API key to use its functionality. We refer to the package documentation for details."
  },
  {
    "objectID": "accessing-managing-financial-data.html#setting-up-a-database",
    "href": "accessing-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing & Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the dplyr functions. We refer to this tutorial for more information on SQL.\nThere are two packages that make working with SQLite in R very simple: RSQLite (Müller et al. 2022) embeds the SQLite database engine in R, and dbplyr (Wickham, Girlich, and Ruiz 2022) is the database back-end for dplyr. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting dplyr into SQL. Check out the RSQLite and dbplyr vignettes for more information.\n\nlibrary(RSQLite)\nlibrary(dbplyr)\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Note that we use the extended_types=TRUE option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance.sqlite in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function dbWriteTable(), which copies the data to our SQLite-database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_monthly\",\n    value = factors_ff_monthly,\n    overwrite = TRUE\n  )\n\nWe can use the remote table as an in-memory data frame by building a connection via tbl().\n\n\n\n\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\n\nAll dplyr calls are evaluated lazily, i.e., the data is not in our R session’s memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.\n\nfactors_ff_monthly_db |>\n  select(month, rf)\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.40.0 [data/tidy_finance.sqlite]\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with more rows\n\n\nIf we want to have the whole table in memory, we need to collect() it. You will see that we regularly load the data into the memory in the next chapters.\n\nfactors_ff_monthly_db |>\n  select(month, rf) |>\n  collect()\n\n# A tibble: 744 × 2\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with 739 more rows\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_daily\",\n    value = factors_ff_daily,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"industries_ff_monthly\",\n    value = industries_ff_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"factors_q_monthly\",\n    value = factors_q_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"macro_predictors\",\n    value = macro_predictors,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"cpi_monthly\",\n    value = cpi_monthly,\n    overwrite = TRUE\n  )\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly <- factors_q_monthly |> collect()"
  },
  {
    "objectID": "accessing-managing-financial-data.html#managing-sqlite-databases",
    "href": "accessing-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing & Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the dbSendQuery() function.\n\ndbSendQuery(tidy_finance, \"VACUUM\")\n\n<SQLiteResult>\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\n\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read up in this tutorial. \nApart from cleaning up, you might be interested in listing all the tables that are currently in your database. You can do this via the dbListTables() function.\n\ndbListTables(tidy_finance)\n\nWarning: Closing open result set, pending rows\n\n\n[1] \"cpi_monthly\"           \"factors_ff_daily\"     \n[3] \"factors_ff_monthly\"    \"factors_q_monthly\"    \n[5] \"industries_ff_monthly\" \"macro_predictors\"     \n\n\nThis function comes in handy if you are unsure about the correct naming of the tables in your database."
  },
  {
    "objectID": "accessing-managing-financial-data.html#exercises",
    "href": "accessing-managing-financial-data.html#exercises",
    "title": "Accessing & Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via read_csv(). Validate that you get the same data as via the frenchdata package.\nDownload the Fama-French 5 factors using the frenchdata package. Use get_french_data_list() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the resulting rf, mkt_excess, smb, and hml columns to factors_ff_monthly. Explain any differences you might find."
  },
  {
    "objectID": "cover-logo-design.html",
    "href": "cover-logo-design.html",
    "title": "Cover & Logo Design",
    "section": "",
    "text": "The cover of the book is inspired by the fast growing generative art community in R. Generative art refers to art that in whole or in part has been created with the use of an autonomous system. Instead of creating random dynamics we rely on what is core to the book: The evolution of financial markets. Each circle in the cover figure corresponds to daily market return within one year of our sample. Deviations from the circle line indicate positive or negative returns. The colors are determined by the standard deviation of market returns during the particular year. The few lines of code below replicate the entire figure. We use the Wes Andersen color palette (also throughout the entire book), provided by the package wesanderson (Ram and Wickham 2018)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RSQLite)\nlibrary(wesanderson)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_daily <- tbl(\n  tidy_finance,\n  \"factors_ff_daily\"\n) |>\n  collect()\n\ndata_plot <- factors_ff_daily |>\n  select(date, mkt_excess) |>\n  group_by(year = floor_date(date, \"year\")) |>\n  mutate(group_id = cur_group_id())\n\ndata_plot <- data_plot |>\n  group_by(group_id) |>\n  mutate(\n    day = 2 * pi * (1:n()) / 252,\n    ymin = pmin(1 + mkt_excess, 1),\n    ymax = pmax(1 + mkt_excess, 1),\n    vola = sd(mkt_excess)\n  ) |>\n  filter(year >= \"1962-01-01\" & year <= \"2021-12-31\")\n\nlevels <- data_plot |>\n  distinct(group_id, vola) |>\n  arrange(vola) |>\n  pull(vola)\n\ncp <- coord_polar(\n  direction = -1,\n  clip = \"on\"\n)\n\ncp$is_free <- function() TRUE\ncolors <- wes_palette(\"Zissou1\",\n  n_groups(data_plot),\n  type = \"continuous\"\n)\n\ncover <- data_plot |>\n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    group = group_id,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  facet_wrap(~group_id,\n    ncol = 10,\n    scales = \"free\"\n  ) +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    panel.spacing = unit(-5, \"lines\")\n  ) +\n  scale_fill_manual(values = colors)\n\nggsave(\n plot = cover,\n width = 10,\n height = 6,\n filename = \"cover.png\",\n bg = \"white\"\n)\n\nTo generate our logo, we focus on year 2021 - the end of the sample period at the time we published tidy-finance.org for the first time.\n\nlogo <- data_plot |>\n  ungroup() |> \n  filter(year == \"2021-01-01\") |> \n  mutate(vola = factor(vola, levels = levels)) |>\n  ggplot(aes(\n    x = day,\n    y = mkt_excess,\n    fill = vola\n  )) +\n  cp +\n  geom_ribbon(aes(\n    ymin = ymin,\n    ymax = ymax,\n    fill = vola\n  ), alpha = 0.90) +\n  theme_void() +\n  theme(\n    strip.text.x = element_blank(),\n    legend.position = \"None\",\n    plot.margin = unit(c(-0.15,-0.15,-0.15,-0.15), \"null\")\n  ) +\n  scale_fill_manual(values =  \"white\") \n\nggsave(\n plot = logo,\n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website-white.png\",\n)\n\nggsave(\n plot = logo +\n    scale_fill_manual(values =  wes_palette(\"Zissou1\")[1]), \n width = 840,\n height = 840,\n units = \"px\",\n filename = \"logo-website.png\",\n)\n\n\n\n\n\nReferences\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson palette generator. https://CRAN.R-project.org/package=wesanderson."
  },
  {
    "objectID": "accessing-and-managing-financial-data.html",
    "href": "accessing-and-managing-financial-data.html",
    "title": "Accessing and Managing Financial Data",
    "section": "",
    "text": "In this chapter, we suggest a way to organize your financial data. Everybody, who has experience with data, is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages.\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series. We store all the data in a single database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\nFirst, we load the global packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nThe package lubridate provides convenient tools to work with dates and times (Grolemund and Wickham 2011). The package scales (Wickham and Seidel 2022) provides useful scale functions for visualizations.\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on."
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#fama-french-data",
    "href": "accessing-and-managing-financial-data.html#fama-french-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Fama-French Data",
    "text": "Fama-French Data\nWe start by downloading some famous Fama-French factors (e.g., Fama and French 1993) and portfolio returns commonly used in empirical asset pricing. Fortunately, there is a neat package by Nelson Areal that allows us to access the data easily: the frenchdata package provides functions to download and read data sets from Prof. Kenneth French finance data library (Areal 2021). \n\nlibrary(frenchdata)\n\nWe can use the main function of the package to download monthly Fama-French factors. The set 3 Factors includes the return time series of the market, size, and value factors alongside the risk-free rates. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French’s finance data library directly. If you are on the site, check the raw data files to appreciate the time you can save thanks to frenchdata.\n\nfactors_ff_monthly_raw <- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff_monthly <- factors_ff_monthly_raw$subsets$data[[1]] |>\n  transmute(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(month >= start_date & month <= end_date)\n\nIt is straightforward to download the corresponding daily Fama-French factors with the same function.\n\nfactors_ff_daily_raw <- download_french_data(\"Fama/French 3 Factors [Daily]\")\nfactors_ff_daily <- factors_ff_daily_raw$subsets$data[[1]] |>\n  transmute(\n    date = ymd(date),\n    rf = as.numeric(RF) / 100,\n    mkt_excess = as.numeric(`Mkt-RF`) / 100,\n    smb = as.numeric(SMB) / 100,\n    hml = as.numeric(HML) / 100\n  ) |>\n  filter(date >= start_date & date <= end_date)\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\n\nindustries_ff_monthly_raw <- download_french_data(\"10 Industry Portfolios\")\nindustries_ff_monthly <- industries_ff_monthly_raw$subsets$data[[1]] |>\n  mutate(month = floor_date(ymd(str_c(date, \"01\")), \"month\")) |>\n  mutate(across(where(is.numeric), ~ . / 100)) |>\n  select(month, everything(), -date) |>\n  filter(month >= start_date & month <= end_date)\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French’s homepage. You should check out the other sets by calling get_french_data_list(). For an alternative to download Fama-French data, check out the FFdownload package by Sebastian Stöckl."
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#q-factors",
    "href": "accessing-and-managing-financial-data.html#q-factors",
    "title": "Accessing and Managing Financial Data",
    "section": "q-Factors",
    "text": "q-Factors\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the Hou, Xue, and Zhang (2014) q-factor model. We refer to the extended background information provided by the original authors for further information. The q factors can be downloaded directly from the authors’ homepage from within read_csv().\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the “R_”-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on try. You can check out style guides available online, e.g., Hadley Wickham’s tidyverse style guide.\n\nfactors_q_monthly_link <-\n  \"http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2021.csv\"\n\nfactors_q_monthly <- read_csv(factors_q_monthly_link) |>\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |>\n  select(-R_F, -R_MKT, -year) |>\n  rename_with(~ str_remove(., \"R_\")) |>\n  rename_with(~ str_to_lower(.)) |>\n  mutate(across(-month, ~ . / 100)) |>\n  filter(month >= start_date & month <= end_date)"
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "href": "accessing-and-managing-financial-data.html#macroeconomic-predictors",
    "title": "Accessing and Managing Financial Data",
    "section": "Macroeconomic Predictors",
    "text": "Macroeconomic Predictors\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. Welch and Goyal (2008) comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2021 on Amit Goyal’s website. Since the data is an XLSX-file stored on a public Google drive location, we need additional packages to access the data directly from our R session. Therefore, we load readxl to read the XLSX-file (Wickham and Bryan 2022) and googledrive for the Google drive connection (D’Agostino McGowan and Bryan 2021).\n\nlibrary(readxl)\nlibrary(googledrive)\n\nUsually, you need to authenticate if you interact with Google drive directly in R. Since the data is stored via a public link, we can proceed without any authentication.\n\ndrive_deauth()\n\nThe drive_download() function from the googledrive package allows us to download the data and store it locally.\n\nmacro_predictors_link <-\n  \"https://docs.google.com/spreadsheets/d/1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2\"\n\ndrive_download(\n  macro_predictors_link,\n  path = \"data/macro_predictors.xlsx\"\n)\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\nThe dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (Campbell and Shiller 1988; Campbell and Yogo 2006).\nDividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).\nEarnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index (Campbell and Shiller 1988).\nDividend payout ratio (de), the difference between the log of dividends and the log of earnings (Lamont 1998).\nStock variance (svar), the sum of squared daily returns on the S&P 500 index (Guo 2006).\nBook-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average (Kothari and Shanken 1997)\nNet equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks (Campbell, Hilscher, and Szilagyi 2008).\nTreasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis (Campbell 1987).\nLong-term yield (lty), the long-term government bond yield from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nLong-term rate of returns (ltr), the long-term government bond returns from Ibbotson’s Stocks, Bonds, Bills, and Inflation Yearbook (Welch and Goyal 2008).\nTerm spread (tms), the difference between the long-term yield on government bonds and the Treasury bill (Campbell 1987).\nDefault yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields (Fama and French 1989).\nInflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics (Campbell and Vuolteenaho 2004).\n\nFor variable definitions and the required data transformations, you can consult the material on Amit Goyal’s website.\n\nmacro_predictors <- read_xlsx(\n  \"data/macro_predictors.xlsx\",\n  sheet = \"Monthly\"\n) |>\n  mutate(month = ym(yyyymm)) |>\n  mutate(across(where(is.character), as.numeric)) |>\n  mutate(\n    IndexDiv = Index + D12,\n    logret = log(IndexDiv) - log(lag(IndexDiv)),\n    Rfree = log(Rfree + 1),\n    rp_div = lead(logret - Rfree, 1), # Future excess market return\n    dp = log(D12) - log(Index), # Dividend Price ratio\n    dy = log(D12) - log(lag(Index)), # Dividend yield\n    ep = log(E12) - log(Index), # Earnings price ratio\n    de = log(D12) - log(E12), # Dividend payout ratio\n    tms = lty - tbl, # Term spread\n    dfy = BAA - AAA # Default yield spread\n  ) |>\n  select(month, rp_div, dp, dy, ep, de, svar,\n    bm = `b/m`, ntis, tbl, lty, ltr,\n    tms, dfy, infl\n  ) |>\n  filter(month >= start_date & month <= end_date) |>\n  drop_na()\n\nFinally, after reading in the macro predictors to our memory, we remove the raw data file from our temporary storage.\n\nfile.remove(\"data/macro_predictors.xlsx\")\n\n[1] TRUE"
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "href": "accessing-and-managing-financial-data.html#other-macroeconomic-data",
    "title": "Accessing and Managing Financial Data",
    "section": "Other Macroeconomic Data",
    "text": "Other Macroeconomic Data\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar tidyquant package to fetch consumer price index (CPI) data that can be found under the CPIAUCNS key.\n\nlibrary(tidyquant)\n\ncpi_monthly <- tq_get(\"CPIAUCNS\",\n  get = \"economic.data\",\n  from = start_date,\n  to = end_date\n) |>\n  transmute(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)]\n  )\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the PCU2122212122210 key. The tidyquant package provides access to around 10,000 time series of the FRED database. If your desired time series is not included, we recommend working with the fredr package (Boysel and Vaughan 2021). Note that you need to get an API key to use its functionality. We refer to the package documentation for details."
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#setting-up-a-database",
    "href": "accessing-and-managing-financial-data.html#setting-up-a-database",
    "title": "Accessing and Managing Financial Data",
    "section": "Setting Up a Database",
    "text": "Setting Up a Database\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code.\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an SQLite database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that SQL (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the dplyr functions. We refer to this tutorial for more information on SQL.\nThere are two packages that make working with SQLite in R very simple: RSQLite (Müller et al. 2022) embeds the SQLite database engine in R, and dbplyr (Wickham, Girlich, and Ruiz 2022) is the database back-end for dplyr. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting dplyr into SQL. Check out the RSQLite and dbplyr vignettes for more information.\n\nlibrary(RSQLite)\nlibrary(dbplyr)\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Note that we use the extended_types=TRUE option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers. We will use the resulting file tidy_finance.sqlite in the subfolder data for all subsequent chapters to retrieve our data.\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function dbWriteTable(), which copies the data to our SQLite-database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_monthly\",\n    value = factors_ff_monthly,\n    overwrite = TRUE\n  )\n\nWe can use the remote table as an in-memory data frame by building a connection via tbl().\n\n\n\n\nfactors_ff_monthly_db <- tbl(tidy_finance, \"factors_ff_monthly\")\n\nAll dplyr calls are evaluated lazily, i.e., the data is not in our R session’s memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.\n\nfactors_ff_monthly_db |>\n  select(month, rf)\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.40.0 [data/tidy_finance.sqlite]\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with more rows\n\n\nIf we want to have the whole table in memory, we need to collect() it. You will see that we regularly load the data into the memory in the next chapters.\n\nfactors_ff_monthly_db |>\n  select(month, rf) |>\n  collect()\n\n# A tibble: 744 × 2\n  month          rf\n  <date>      <dbl>\n1 1960-01-01 0.0033\n2 1960-02-01 0.0029\n3 1960-03-01 0.0035\n4 1960-04-01 0.0019\n5 1960-05-01 0.0027\n# … with 739 more rows\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages.\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database.\n\n  dbWriteTable(tidy_finance,\n    \"factors_ff_daily\",\n    value = factors_ff_daily,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"industries_ff_monthly\",\n    value = industries_ff_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"factors_q_monthly\",\n    value = factors_q_monthly,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"macro_predictors\",\n    value = macro_predictors,\n    overwrite = TRUE\n  )\n\n  dbWriteTable(tidy_finance,\n    \"cpi_monthly\",\n    value = cpi_monthly,\n    overwrite = TRUE\n  )\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\")\nfactors_q_monthly <- factors_q_monthly |> collect()"
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "href": "accessing-and-managing-financial-data.html#managing-sqlite-databases",
    "title": "Accessing and Managing Financial Data",
    "section": "Managing SQLite Databases",
    "text": "Managing SQLite Databases\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\nTo optimize the database file, you can run the VACUUM command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the dbSendQuery() function.\n\ndbSendQuery(tidy_finance, \"VACUUM\")\n\n<SQLiteResult>\n  SQL  VACUUM\n  ROWS Fetched: 0 [complete]\n       Changed: 0\n\n\nThe VACUUM command actually performs a couple of additional cleaning steps, which you can read up in this tutorial. \nApart from cleaning up, you might be interested in listing all the tables that are currently in your database. You can do this via the dbListTables() function.\n\ndbListTables(tidy_finance)\n\nWarning: Closing open result set, pending rows\n\n\n [1] \"beta\"                  \"compustat\"            \n [3] \"cpi_monthly\"           \"crsp_daily\"           \n [5] \"crsp_monthly\"          \"factors_ff_daily\"     \n [7] \"factors_ff_monthly\"    \"factors_q_monthly\"    \n [9] \"industries_ff_monthly\" \"macro_predictors\"     \n[11] \"mergent\"               \"trace_enhanced\"       \n\n\nThis function comes in handy if you are unsure about the correct naming of the tables in your database."
  },
  {
    "objectID": "accessing-and-managing-financial-data.html#exercises",
    "href": "accessing-and-managing-financial-data.html#exercises",
    "title": "Accessing and Managing Financial Data",
    "section": "Exercises",
    "text": "Exercises\n\nDownload the monthly Fama-French factors manually from Ken French’s data library and read them in via read_csv(). Validate that you get the same data as via the frenchdata package.\nDownload the Fama-French 5 factors using the frenchdata package. Use get_french_data_list() to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the resulting rf, mkt_excess, smb, and hml columns to factors_ff_monthly. Explain any differences you might find."
  },
  {
    "objectID": "wrds-crsp-and-compustat.html",
    "href": "wrds-crsp-and-compustat.html",
    "title": "WRDS, CRSP, and Compustat",
    "section": "",
    "text": "This chapter shows how to connect to Wharton Research Data Services (WRDS), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the SQLite-database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.\nFirst, we load the packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them.\nWe use the same date range as in the previous chapter to ensure consistency."
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#accessing-wrds",
    "href": "wrds-crsp-and-compustat.html#accessing-wrds",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Accessing WRDS",
    "text": "Accessing WRDS\nWRDS is the most widely used source for asset and firm-specific financial data used in academic settings. WRDS is a data platform that provides data validation, flexible delivery options, and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the PostgreSQL engine. This database engine is just as easy to handle with R as SQLite. We use the RPostgres package to establish a connection to the WRDS database (Wickham, Ooms, and Müller 2022). Note that you could also use the odbc package to connect to a PostgreSQL database, but then you need to install the appropriate drivers yourself. RPostgres already contains a suitable driver.\n\nlibrary(RPostgres)\n\nTo establish a connection, you use the function dbConnect() with the following arguments. Note that you need to replace the user and password fields with your own credentials. We defined system variables for the purpose of this book because we obviously do not want (and are not allowed) to share our credentials with the rest of the world.\n\nwrds <- dbConnect(\n  Postgres(),\n  host = \"wrds-pgdata.wharton.upenn.edu\",\n  dbname = \"wrds\",\n  port = 9737,\n  sslmode = \"require\",\n  user = Sys.getenv(\"user\"),\n  password = Sys.getenv(\"password\")\n)\n\nThe remote connection to WRDS is very useful. Yet, the database itself contains many different tables. You can check the WRDS homepage to identify the table’s name you are looking for (if you go beyond our exposition). Alternatively, you can also query the data structure with the function dbSendQuery(). If you are interested, there is an exercise below that is based on WRDS’ tutorial on “Querying WRDS Data using R”. Furthermore, the penultimate section of this chapter shows how to investigate the structure of databases."
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "href": "wrds-crsp-and-compustat.html#downloading-and-preparing-crsp",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Downloading and Preparing CRSP",
    "text": "Downloading and Preparing CRSP\nThe Center for Research in Security Prices (CRSP) provides the most widely used data for US stocks. We use the wrds connection object that we just created to first access monthly CRSP return data. Actually, we need three tables to get the desired data: (i) the CRSP monthly security file,\n\nmsf_db <- tbl(wrds, in_schema(\"crsp\", \"msf\"))\n\n\nthe identifying information,\n\n\nmsenames_db <- tbl(wrds, in_schema(\"crsp\", \"msenames\"))\n\nand (iii) the delisting information.\n\nmsedelist_db <- tbl(wrds, in_schema(\"crsp\", \"msedelist\"))\n\nWe use the three remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we keep only data in the time windows of interest, (ii) we keep only US-listed stocks as identified via share codes shrcd 10 and 11, and (iii) we keep only months within permno-specific start dates namedt and end dates nameendt. In addition, we add delisting codes and returns. You can read up in the great textbook of Bali, Engle, and Murray (2016) for an extensive discussion on the filters we apply in the code below.\n\ncrsp_monthly <- msf_db |>\n  filter(date >= start_date & date <= end_date) |>\n  inner_join(\n    msenames_db |>\n      filter(shrcd %in% c(10, 11)) |>\n      select(permno, exchcd, siccd, namedt, nameendt),\n    by = c(\"permno\")\n  ) |>\n  filter(date >= namedt & date <= nameendt) |>\n  mutate(month = floor_date(date, \"month\")) |>\n  left_join(\n    msedelist_db |>\n      select(permno, dlstdt, dlret, dlstcd) |>\n      mutate(month = floor_date(dlstdt, \"month\")),\n    by = c(\"permno\", \"month\")\n  ) |>\n  select(\n    permno, # Security identifier\n    date, # Date of the observation\n    month, # Month of the observation\n    ret, # Return\n    shrout, # Shares outstanding (in thousands)\n    altprc, # Last traded price in a month\n    exchcd, # Exchange code\n    siccd, # Industry code\n    dlret, # Delisting return\n    dlstcd # Delisting code\n  ) |>\n  collect() |>\n  mutate(\n    month = ymd(month),\n    shrout = shrout * 1000\n  )\n\nNow, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares shrout and the last traded price in a month altprc. Note that in contrast to returns ret, these two variables are not adjusted ex-post for any corporate actions like stock splits. Moreover, the altprc is negative whenever the last traded price does not exist, and CRSP decides to report the mid-quote of the last available order book instead. Hence, we take the absolute value of the market cap. We also keep the market cap in millions of USD just for convenience as we do not want to print huge numbers in our figures and tables. In addition, we set zero market cap to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\ncrsp_monthly <- crsp_monthly |>\n  mutate(\n    mktcap = abs(shrout * altprc) / 1000000,\n    mktcap = na_if(mktcap, 0)\n  )\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.\n\nmktcap_lag <- crsp_monthly |>\n  mutate(month = month %m+% months(1)) |>\n  select(permno, month, mktcap_lag = mktcap)\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(mktcap_lag, by = c(\"permno\", \"month\"))\n\nIf you wonder why we do not use the lag() function, e.g., via crsp_monthly |> group_by(permno) |> mutate(mktcap_lag = lag(mktcap)), take a look at the exercises.\nNext, we follow Bali, Engle, and Murray (2016) in transforming listing exchange codes to explicit exchange names.\n\ncrsp_monthly <- crsp_monthly |>\n  mutate(exchange = case_when(\n    exchcd %in% c(1, 31) ~ \"NYSE\",\n    exchcd %in% c(2, 32) ~ \"AMEX\",\n    exchcd %in% c(3, 33) ~ \"NASDAQ\",\n    TRUE ~ \"Other\"\n  ))\n\nSimilarly, we transform industry codes to industry descriptions following Bali, Engle, and Murray (2016). Notice that there are also other categorizations of industries (e.g., Fama and French 1997) that are commonly used.\n\ncrsp_monthly <- crsp_monthly |>\n  mutate(industry = case_when(\n    siccd >= 1 & siccd <= 999 ~ \"Agriculture\",\n    siccd >= 1000 & siccd <= 1499 ~ \"Mining\",\n    siccd >= 1500 & siccd <= 1799 ~ \"Construction\",\n    siccd >= 2000 & siccd <= 3999 ~ \"Manufacturing\",\n    siccd >= 4000 & siccd <= 4899 ~ \"Transportation\",\n    siccd >= 4900 & siccd <= 4999 ~ \"Utilities\",\n    siccd >= 5000 & siccd <= 5199 ~ \"Wholesale\",\n    siccd >= 5200 & siccd <= 5999 ~ \"Retail\",\n    siccd >= 6000 & siccd <= 6799 ~ \"Finance\",\n    siccd >= 7000 & siccd <= 8999 ~ \"Services\",\n    siccd >= 9000 & siccd <= 9999 ~ \"Public\",\n    TRUE ~ \"Missing\"\n  ))\n\nWe also construct returns adjusted for delistings as described by Bali, Engle, and Murray (2016). The delisting of a security usually results when a company ceases operations, declares bankruptcy, merges, does not meet listing requirements, or seeks to become private. The adjustment tries to reflect the returns of investors who bought the stock in the month before the delisting and held it until the delisting date. After this transformation, we can drop the delisting returns and codes.\n\ncrsp_monthly <- crsp_monthly |>\n  mutate(ret_adj = case_when(\n    is.na(dlstcd) ~ ret,\n    !is.na(dlstcd) & !is.na(dlret) ~ dlret,\n    dlstcd %in% c(500, 520, 580, 584) |\n      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,\n    dlstcd == 100 ~ ret,\n    TRUE ~ -1\n  )) |>\n  select(-c(dlret, dlstcd))\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop adjusted returns and the risk-free rate from our tibble. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the tibble factors_ff_monthly.\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(factors_ff_monthly |> select(month, rf),\n    by = \"month\"\n  ) |>\n  mutate(\n    ret_excess = ret_adj - rf,\n    ret_excess = pmax(ret_excess, -1)\n  ) |>\n  select(-ret_adj, -rf)\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\ncrsp_monthly <- crsp_monthly |>\n  drop_na(ret_excess, mktcap, mktcap_lag)\n\nFinally, we store the monthly CRSP file in our database.\n\n  dbWriteTable(tidy_finance,\n    \"crsp_monthly\",\n    value = crsp_monthly,\n    overwrite = TRUE\n  )"
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "href": "wrds-crsp-and-compustat.html#first-glimpse-of-the-crsp-sample",
    "title": "WRDS, CRSP, and Compustat",
    "section": "First Glimpse of the CRSP Sample",
    "text": "First Glimpse of the CRSP Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns.\nFigure 1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2021, there were 2,779 stocks with a primary listing on NASDAQ, 1,395 on NYSE, 145 on AMEX, and only one belonged to the other category.\n\ncrsp_monthly |>\n  count(exchange, date) |>\n  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by listing exchange\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 1: Number of stocks in the CRSP sample listed at each of the US exchanges.\n\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 2. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange and plotting it just as if they were in memory. All values in Figure 2 are at the end of 2021 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\ntbl(tidy_finance, \"crsp_monthly\") |>\n  left_join(tbl(tidy_finance, \"cpi_monthly\"), by = \"month\") |>\n  group_by(month, exchange) |>\n  summarize(\n    mktcap = sum(mktcap, na.rm = TRUE) / cpi,\n    .groups = \"drop\"\n  ) |>\n  collect() |>\n  mutate(month = ymd(month)) |>\n  ggplot(aes(\n    x = month, y = mktcap / 1000,\n    color = exchange, linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 2: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2021.\n\n\n\n\nOf course, performing the computation in the database is not really meaningful because we can easily pull all the required data into our memory. The code chunk above is slower than performing the same steps on tables that are already in memory. However, we just want to illustrate that you can perform many things in the database before loading the data into your memory. Before we proceed, we load the monthly CPI data.\n\ncpi_monthly <- tbl(tidy_finance, \"cpi_monthly\") |>\n  collect()\n\nNext, we look at the same descriptive statistics by industry. Figure 3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\ncrsp_monthly_industry <- crsp_monthly |>\n  left_join(cpi_monthly, by = \"month\") |>\n  group_by(month, industry) |>\n  summarize(\n    securities = n_distinct(permno),\n    mktcap = sum(mktcap) / mean(cpi),\n    .groups = \"drop\"\n  )\n\ncrsp_monthly_industry |>\n  ggplot(aes(\n    x = month,\n    y = securities,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly number of securities by industry\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 3: Number of stocks in the CRSP sample associated with different industries.\n\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 4. All values are again in terms of billions of end of 2021 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\ncrsp_monthly_industry |>\n  ggplot(aes(\n    x = month,\n    y = mktcap / 1000,\n    color = industry,\n    linetype = industry\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly total market cap by industry in billions as of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\nFigure 4: Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2021."
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#daily-crsp-data",
    "href": "wrds-crsp-and-compustat.html#daily-crsp-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Daily CRSP Data",
    "text": "Daily CRSP Data\nBefore we turn to accounting data, we provide a proposal for downloading daily CRSP data. While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire data set to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session.\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easy to handle. That is, instead of downloading data about many stocks all at once, download the data in small batches for each stock consecutively. Such operations can be implemented in for()-loops, where we download, prepare, and store the data for a single stock in each iteration. This operation might nonetheless take a couple of hours, so you have to be patient either way (we often run such code overnight). To keep track of the progress, you can use txtProgressBar(). Eventually, we end up with more than 68 million rows of daily return data. Note that we only store the identifying information that we actually need, namely permno, date, and month alongside the excess returns. We thus ensure that our local database contains only the data we actually use and that we can load the full daily data into our memory later. Notice that we also use the function dbWriteTable() here with the option to append the new data to an existing table, when we process the second and all following batches.\n\ndsf_db <- tbl(wrds, in_schema(\"crsp\", \"dsf\"))\n\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\") |>\n  collect()\n\npermnos <- tbl(tidy_finance, \"crsp_monthly\") |>\n  distinct(permno) |>\n  pull()\n\nprogress <- txtProgressBar(\n  min = 0,\n  max = length(permnos),\n  initial = 0,\n  style = 3\n)\n\nfor (j in 1:length(permnos)) {\n  permno_sub <- permnos[j]\n  crsp_daily_sub <- dsf_db |>\n    filter(permno == permno_sub &\n      date >= start_date & date <= end_date) |>\n    select(permno, date, ret) |>\n    collect() |>\n    drop_na()\n\n  if (nrow(crsp_daily_sub) > 0) {\n    crsp_daily_sub <- crsp_daily_sub |>\n      mutate(month = floor_date(date, \"month\")) |>\n      left_join(factors_ff_daily |>\n        select(date, rf), by = \"date\") |>\n      mutate(\n        ret_excess = ret - rf,\n        ret_excess = pmax(ret_excess, -1)\n      ) |>\n      select(permno, date, month, ret_excess)\n\n    dbWriteTable(tidy_finance,\n        \"crsp_daily\",\n        value = crsp_daily_sub,\n        overwrite = ifelse(j == 1, TRUE, FALSE),\n        append = ifelse(j != 1, TRUE, FALSE)\n      )\n  }\n  setTxtProgressBar(progress, j)\n}\n\nclose(progress)\n\ncrsp_daily_db <- tbl(tidy_finance, \"crsp_daily\")"
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#preparing-compustat-data",
    "href": "wrds-crsp-and-compustat.html#preparing-compustat-data",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Preparing Compustat data",
    "text": "Preparing Compustat data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by S&P Global Market Intelligence, which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.\nTo access Compustat data, we can again tap WRDS, which hosts the funda table that contains annual firm-level information on North American companies.\n\nfunda_db <- tbl(wrds, in_schema(\"comp\", \"funda\"))\n\nWe follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, (ii) in the standard format (i.e., consolidated information in standard presentation), and (iii) only data in the desired time window.\n\ncompustat <- funda_db |>\n  filter(\n    indfmt == \"INDL\" &\n      datafmt == \"STD\" &\n      consol == \"C\" &\n      datadate >= start_date & datadate <= end_date\n  ) |>\n  select(\n    gvkey, # Firm identifier\n    datadate, # Date of the accounting data\n    seq, # Stockholders' equity\n    ceq, # Total common/ordinary equity\n    at, # Total assets\n    lt, # Total liabilities\n    txditc, # Deferred taxes and investment tax credit\n    txdb, # Deferred taxes\n    itcb, # Investment tax credit\n    pstkrv, # Preferred stock redemption value\n    pstkl, # Preferred stock liquidating value\n    pstk, # Preferred stock par value\n    capx, # Capital investment\n    oancf # Operating cash flow\n  ) |>\n  collect()\n\nNext, we calculate the book value of preferred stock and equity inspired by the variable definition in Ken French’s data library. Note that we set negative or zero equity to missing which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncompustat <- compustat |>\n  mutate(\n    be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    be = if_else(be <= 0, as.numeric(NA), be)\n  )\n\nWe keep only the last available information for each firm-year group. Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2021). Therefore, datadate is not the date when data was made available to the public. Check out the exercises for more insights into the peculiarities of datadate.\n\ncompustat <- compustat |>\n  mutate(year = year(datadate)) |>\n  group_by(gvkey, year) |>\n  filter(datadate == max(datadate)) |>\n  ungroup()\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n  dbWriteTable(tidy_finance,\n    \"compustat\",\n    value = compustat,\n    overwrite = TRUE\n  )"
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "href": "wrds-crsp-and-compustat.html#merging-crsp-with-compustat",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Merging CRSP with Compustat",
    "text": "Merging CRSP with Compustat\nUnfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses permno for stocks, while Compustat uses gvkey to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the CRSP-Compustat Merged table (provided by CRSP).\n\nccmxpf_linktable_db <- tbl(\n  wrds,\n  in_schema(\"crsp\", \"ccmxpf_linktable\")\n)\n\nThe linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in Bali, Engle, and Murray (2016). Note also that currently active links have no end date, so we just enter the current date via today().\n\nccmxpf_linktable <- ccmxpf_linktable_db |>\n  filter(linktype %in% c(\"LU\", \"LC\") &\n    linkprim %in% c(\"P\", \"C\") &\n    usedflag == 1) |>\n  select(permno = lpermno, gvkey, linkdt, linkenddt) |>\n  collect() |>\n  mutate(linkenddt = replace_na(linkenddt, today()))\n\nWe use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat gvkey to our monthly stock data.\n\nccm_links <- crsp_monthly |>\n  inner_join(ccmxpf_linktable, by = \"permno\") |>\n  filter(!is.na(gvkey) & (date >= linkdt & date <= linkenddt)) |>\n  select(permno, gvkey, date)\n\ncrsp_monthly <- crsp_monthly |>\n  left_join(ccm_links, by = c(\"permno\", \"date\"))\n\nAs the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.\n\n  dbWriteTable(tidy_finance,\n    \"crsp_monthly\",\n    value = crsp_monthly,\n    overwrite = TRUE\n  )\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 60s but hovers around 80% for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.\n\ncrsp_monthly |>\n  group_by(permno, year = year(month)) |>\n  filter(date == max(date)) |>\n  ungroup() |>\n  left_join(compustat, by = c(\"gvkey\", \"year\")) |>\n  group_by(exchange, year) |>\n  summarize(\n    share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),\n    .groups = \"drop\"\n  ) |>\n  ggplot(aes(\n    x = year, \n    y = share, \n    color = exchange,\n    linetype = exchange\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\nFigure 5: End-of-year share of securities with book equity values by listing exchange."
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "href": "wrds-crsp-and-compustat.html#some-tricks-for-postgresql-databases",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Some Tricks for PostgreSQL Databases",
    "text": "Some Tricks for PostgreSQL Databases\nAs we mentioned above, the WRDS database runs on PostgreSQL rather than SQLite. Finding the right tables for your data needs can be tricky in the WRDS PostgreSQL instance, as the tables are organized in schemas. If you wonder what the purpose of schemas is, check out this documetation. For instance, if you want to find all tables that live in the crsp schema, you run\n\ndbListObjects(wrds, Id(schema = \"crsp\"))\n\nThis operation returns a list of all tables that belong to the crsp family on WRDS, e.g., <Id> schema = crsp, table = msenames. Similarly, you can fetch a list of all tables that belong to the comp family via\n\ndbListObjects(wrds, Id(schema = \"comp\"))\n\nIf you want to get all schemas, then run\n\ndbListObjects(wrds)"
  },
  {
    "objectID": "wrds-crsp-and-compustat.html#exercises",
    "href": "wrds-crsp-and-compustat.html#exercises",
    "title": "WRDS, CRSP, and Compustat",
    "section": "Exercises",
    "text": "Exercises\n\nCheck out the structure of the WRDS database by sending queries in the spirit of “Querying WRDS Data using R” and verify the output with dbListObjects(). How many tables are associated with CRSP? Can you identify what is stored within msp500?\nCompute mkt_cap_lag using lag(mktcap) rather than joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are they different?\nIn the main part, we look at the distribution of market capitalization across exchanges and industries. Now, plot the average market capitalization of firms for each exchange and industry. What do you find?\ndatadate refers to the date to which the fiscal year of a corresponding firm refers to. Count the number of observations in Compustat by month of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?\nGo back to the original Compustat data in funda_db and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?\nRepeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?"
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html",
    "href": "constrained-optimization-and-backtesting.html",
    "title": "Constrained Optimization and Backtesting",
    "section": "",
    "text": "In this chapter, we conduct portfolio backtesting in a realistic setting by including transaction costs and investment constraints such as no-short-selling rules. We start with standard mean-variance efficient portfolios and introduce constraints in a step-by-step manner. To do so, we rely on numerical optimization procedures in R. We conclude the chapter by providing an out-of-sample backtesting procedure for the different strategies that we introduce in this chapter.\nThroughout this chapter, we use the following packages:\nCompared to previous chapters, we introduce the quadprog package (Turlach, Weingessel, and Moler 2019) to perform numerical constrained optimization for quadratic objective functions and alabama (Varadhan 2022) for more general non-linear objective functions and constraints."
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#data-preparation",
    "href": "constrained-optimization-and-backtesting.html#data-preparation",
    "title": "Constrained Optimization and Backtesting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe start by loading the required data from our SQLite-database introduced in Chapters 2-4. For simplicity, we restrict our investment universe to the monthly Fama-French industry portfolio returns in the following application. \n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nindustry_returns <- tbl(tidy_finance, \"industries_ff_monthly\") |>\n  collect()\n\nindustry_returns <- industry_returns |>\n  select(-month)"
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "href": "constrained-optimization-and-backtesting.html#recap-of-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Recap of Portfolio Choice",
    "text": "Recap of Portfolio Choice\nA common objective for portfolio optimization is to find mean-variance efficient portfolio weights, i.e., the allocation which delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case, where the investor is only concerned about portfolio variance, she may choose to implement the minimum variance portfolio (MVP) weights which are given by the solution to \\[w_\\text{mvp} = \\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1\\] where \\(\\Sigma\\) is the \\((N \\times N)\\) covariance matrix of the returns. The optimal weights \\(\\omega_\\text{mvp}\\) can be found analytically and are \\(\\omega_\\text{mvp} = \\frac{\\Sigma^{-1}\\iota}{\\iota'\\Sigma^{-1}\\iota}\\). In terms of code, the math is equivalent to the following chunk. \n\nSigma <- cov(industry_returns)\nw_mvp <- solve(Sigma) %*% rep(1, ncol(Sigma))\nw_mvp <- as.vector(w_mvp / sum(w_mvp))\n\nNext, consider an investor who aims to achieve minimum variance given a required expected portfolio return \\(\\bar{\\mu}\\) such that she chooses \\[w_\\text{eff}({\\bar{\\mu}}) =\\arg\\min w'\\Sigma w \\text{ s.t. } w'\\iota = 1 \\text{ and } \\omega'\\mu \\geq \\bar{\\mu}.\\] We leave it as an exercise below to show that the portfolio choice problem can equivalently be formulated for an investor with mean-variance preferences and risk aversion factor \\(\\gamma\\). That means the investor aims to choose portfolio weights as the solution to \\[ w^*_\\gamma = \\arg\\max w' \\mu - \\frac{\\gamma}{2}w'\\Sigma w\\quad \\text{ s.t. } w'\\iota = 1.\\] The solution to the optimal portfolio choice problem is: \\[\\omega^*_{\\gamma}  = \\frac{1}{\\gamma}\\left(\\Sigma^{-1} - \\frac{1}{\\iota' \\Sigma^{-1}\\iota }\\Sigma^{-1}\\iota\\iota' \\Sigma^{-1} \\right) \\mu  + \\frac{1}{\\iota' \\Sigma^{-1} \\iota }\\Sigma^{-1} \\iota.\\] Empirically, this classical solution imposes many problems. In particular, the estimates of \\(\\mu\\) are noisy over short horizons, the (\\(N \\times N\\)) matrix \\(\\Sigma\\) contains \\(N(N-1)/2\\) distinct elements and thus, estimation error is huge. Seminal papers on the effect of ignoring estimation uncertainty, among others, are Brown (1976), Jobson and Korkie (1980), Jorion (1986), and Chopra and Ziemba (1993).\nEven worse, if the asset universe contains more assets than available time periods \\((N > T)\\), the sample covariance matrix is no longer positive definite such that the inverse \\(\\Sigma^{-1}\\) does not exist anymore. To address estimation issues for vast-dimensional covariance matrices, regularization techniques are a popular tool (see, e.g., Ledoit and Wolf 2003, 2004, 2012; Fan, Fan, and Lv 2008).\nWhile the uncertainty associated with estimated parameters is challenging, the data-generating process is also unknown to the investor. In other words, model uncertainty reflects that it is ex-ante not even clear which parameters require estimation (for instance, if returns are driven by a factor model, selecting the universe of relevant factors imposes model uncertainty). Wang (2005) and Garlappi, Uppal, and Wang (2007) provide theoretical analysis on optimal portfolio choice under model and estimation uncertainty. In the most extreme case, Pflug, Pichler, and Wozabal (2012) shows that the naive portfolio which allocates equal wealth to all assets is the optimal choice for an investor averse to model uncertainty.\nOn top of the estimation uncertainty, transaction costs are a major concern. Rebalancing portfolios is costly, and, therefore, the optimal choice should depend on the investor’s current holdings. In the presence of transaction costs, the benefits of reallocating wealth may be smaller than the costs associated with turnover. This aspect has been investigated theoretically, among others, for one risky asset by Magill and Constantinides (1976) and Davis and Norman (1990). Subsequent extensions to the case with multiple assets have been proposed by Balduzzi and Lynch (1999) and Balduzzi and Lynch (2000). More recent papers on empirical approaches which explicitly account for transaction costs include Gârleanu and Pedersen (2013), and DeMiguel, Nogales, and Uppal (2014), and DeMiguel, Martín-Utrera, and Nogales (2015)."
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "href": "constrained-optimization-and-backtesting.html#estimation-uncertainty-and-transaction-costs",
    "title": "Constrained Optimization and Backtesting",
    "section": "Estimation Uncertainty and Transaction Costs",
    "text": "Estimation Uncertainty and Transaction Costs\nThe empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plug in some sample estimates \\(\\hat \\mu\\) and \\(\\hat \\Sigma\\) can be summarized rather briefly: mean-variance optimization performs poorly! The literature discusses many proposals to overcome these empirical issues. For instance, one may impose some form of regularization of \\(\\Sigma\\), rely on Bayesian priors inspired by theoretical asset pricing models (Kan and Zhou 2007) or use high-frequency data to improve forecasting (Hautsch, Kyj, and Malec 2015). One unifying framework that works easily, effectively (even for large dimensions), and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs (Hautsch and Voigt 2019).\nAssume that returns are from a multivariate normal distribution with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\), \\(N(\\mu,\\Sigma)\\). Additionally, we assume quadratic transaction costs which penalize rebalancing such that \\[\n\\begin{aligned}\n\\nu\\left(\\omega_{t+1},\\omega_{t^+}, \\beta\\right) = \\frac{\\beta}{2} \\left(\\omega_{t+1} - \\omega_{t^+}\\right)'\\left(\\omega_{t+1}- \\omega_{t^+}\\right),\\end{aligned}\\] with cost parameter \\(\\beta>0\\) and \\(\\omega_{t^+} = {\\omega_t \\circ (1 +r_{t})}/{\\iota' (\\omega_t \\circ (1 + r_{t}))}\\). \\(\\omega_{t^+}\\) denotes the portfolio weights just before rebalancing. Note that \\(\\omega_{t^+}\\) differs mechanically from \\(\\omega_t\\) due to the returns in the past period. Intuitively, transaction costs penalize portfolio performance when the portfolio is shifted from the current holdings \\(\\omega_{t^+}\\) to a new allocation \\(\\omega_{t+1}\\). In this setup, transaction costs do not increase linearly. Instead, larger rebalancing is penalized more heavily than small adjustments. Then, the optimal portfolio choice for an investor with mean variance preferences is \\[\\begin{aligned}\\omega_{t+1} ^* &=  \\arg\\max \\omega'\\mu - \\nu_t (\\omega,\\omega_{t^+}, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega \\text{ s.t. } \\iota'\\omega = 1\\\\\n&=\\arg\\max\n\\omega'\\mu^* - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t.} \\iota'\\omega=1,\\end{aligned}\\] where \\[\\mu^*=\\mu+\\beta \\omega_{t^+} \\quad  \\text{and} \\quad \\Sigma^*=\\Sigma + \\frac{\\beta}{\\gamma} I_N.\\] As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters \\(\\Sigma^*\\) and \\(\\mu^*\\): \\[\\omega^*_{t+1} = \\frac{1}{\\gamma}\\left(\\Sigma^{*-1} - \\frac{1}{\\iota' \\Sigma^{*-1}\\iota }\\Sigma^{*-1}\\iota\\iota' \\Sigma^{*-1} \\right) \\mu^*  + \\frac{1}{\\iota' \\Sigma^{*-1} \\iota }\\Sigma^{*-1} \\iota.\\]\nAn alternative formulation of the optimal portfolio can be derived as follows: \\[\\omega_{t+1} ^*=\\arg\\max\n\\omega'\\left(\\mu+\\beta\\left(\\omega_{t^+} - \\frac{1}{N}\\iota\\right)\\right) - \\frac{\\gamma}{2}\\omega'\\Sigma^* \\omega \\text{ s.t. } \\iota'\\omega=1.\\] The optimal weights correspond to a mean-variance portfolio, where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering a higher expected return."
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "href": "constrained-optimization-and-backtesting.html#optimal-portfolio-choice",
    "title": "Constrained Optimization and Backtesting",
    "section": "Optimal Portfolio Choice",
    "text": "Optimal Portfolio Choice\nThe function below implements the efficient portfolio weight in its general form, allowing for transaction costs (conditional on the holdings before reallocation). For \\(\\beta=0\\), the computation resembles the standard mean-variance efficient framework. gamma denotes the coefficient of risk aversion \\(\\gamma\\), beta is the transaction cost parameter \\(\\beta\\) and w_prev are the weights before rebalancing \\(\\omega_{t^+}\\).\n\ncompute_efficient_weight <- function(Sigma,\n                                     mu,\n                                     gamma = 2,\n                                     beta = 0, # transaction costs\n                                     w_prev = rep(\n                                       1 / ncol(Sigma),\n                                       ncol(Sigma)\n                                     )) {\n  iota <- rep(1, ncol(Sigma))\n  Sigma_processed <- Sigma + beta / gamma * diag(ncol(Sigma))\n  mu_processed <- mu + beta * w_prev\n\n  Sigma_inverse <- solve(Sigma_processed)\n\n  w_mvp <- Sigma_inverse %*% iota\n  w_mvp <- as.vector(w_mvp / sum(w_mvp))\n  w_opt <- w_mvp + 1 / gamma *\n    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%\n      mu_processed\n  return(as.vector(w_opt))\n}\n\nmu <- colMeans(industry_returns)\ncompute_efficient_weight(Sigma, mu)\n\n [1]  1.395  0.293 -1.390  0.477  0.363 -0.320  0.545  0.446 -0.132\n[10] -0.675\n\n\nThe portfolio weights above indicate the efficient portfolio for an investor with risk aversion coefficient \\(\\gamma=2\\) in absence of transaction costs. Some of the positions are negative which implies short-selling, most of the positions are rather extreme. For instance, a position of \\(-1\\) implies that the investor takes a short position worth her entire wealth to lever long positions in other assets. What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyze the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter \\(\\beta\\) and risk aversion \\(\\gamma\\).\n\ntransaction_costs <- expand_grid(\n  gamma = c(2, 4, 8, 20),\n  beta = 20 * qexp((1:99) / 100)\n) |>\n  mutate(\n    weights = map2(\n      .x = gamma,\n      .y = beta,\n      ~ compute_efficient_weight(Sigma,\n        mu,\n        gamma = .x,\n        beta = .y / 10000,\n        w_prev = w_mvp\n      )\n    ),\n    concentration = map_dbl(weights, ~ sum(abs(. - w_mvp)))\n  )\n\nThe code chunk above computes the optimal weight in presence of transaction cost for different values of \\(\\beta\\) and \\(\\gamma\\) but with the same initial allocation, the theoretical optimal minimum variance portfolio. Starting from the initial allocation, the investor chooses her optimal allocation along the efficient frontier to reflect her own risk preferences. If transaction costs would be absent, the investor would simply implement the mean-variance efficient allocation. If transaction costs make it costly to rebalance, her optimal portfolio choice reflects a shift toward the efficient portfolio, whereas her current portfolio anchors her investment.\n\ntransaction_costs |>\n  mutate(risk_aversion = as_factor(gamma)) |>\n  ggplot(aes(\n    x = beta,\n    y = concentration,\n    color = risk_aversion,\n    linetype = risk_aversion\n  )) +\n  geom_line() +\n  guides(linetype = \"none\") + \n  labs(\n    x = \"Transaction cost parameter\",\n    y = \"Distance from MVP\",\n    color = \"Risk aversion\",\n    title = \"Portfolio weights for different risk aversion and transaction cost\"\n  )\n\n\n\n\nFigure 1: The horizontal axis indicates the distance from the empirical minimum variance portfolio weight, measured by the sum of the absolute deviations of the chosen portfolio from the benchmark.\n\n\n\n\nFigure 1 shows rebalancing from the initial portfolio (which we always set to the minimum variance portfolio weights in this example). The higher the transaction costs parameter \\(\\beta\\), the smaller is the rebalancing from the initial portfolio. In addition, if risk aversion \\(\\gamma\\) increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings."
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#constrained-optimization",
    "href": "constrained-optimization-and-backtesting.html#constrained-optimization",
    "title": "Constrained Optimization and Backtesting",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\nNext, we introduce constraints to the above optimization procedure. Very often, typical constraints such as short-selling restrictions prevent analytical solutions for optimal portfolio weights (short-selling restrictions simply imply that negative weights are not allowed such that we require that \\(w_i \\geq 0 \\forall i\\)). However, numerical optimization allows computing the solutions to such constrained problems. For the purpose of mean-variance optimization, we rely on the solve.QP() function from the package quadprog.\nThe function solve.QP() delivers numerical solutions to quadratic programming problems of the form \\[\\min(-\\mu \\omega + 1/2 \\omega' \\Sigma \\omega) \\text{ s.t. } A' \\omega >= b_0.\\] The function takes one argument (meq) for the number of equality constraints. Therefore, the above matrix \\(A\\) is simply a vector of ones to ensure that the weights sum up to one. In the case of short-selling constraints, the matrix \\(A\\) is of the form \\[A' = \\begin{pmatrix}1 & 1& \\ldots&1 \\\\1 & 0 &\\ldots&0\\\\0 & 1 &\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}'\\qquad b_0 = \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}.\\]\nBefore we dive into constrained optimization, we revisit the unconstrained problem and replicate the analytical solutions for the minimum variance and efficient portfolio weights from above. We verify that the output is equal to the above solution. Note that near() is a safe way to compare two vectors for pairwise equality. The alternative == is sensitive to small differences that may occur due to the representation of floating points on a computer, while near() has a built-in tolerance. As just discussed, we set Amat to a matrix with a column of ones and bvec to 1 to enforce the constraint that weights must sum up to one. meq=1 means that one (out of one) constraints must be satisfied with equality.\n\nn_industries <- ncol(industry_returns)\n\nw_mvp_numerical <- solve.QP(\n  Dmat = Sigma,\n  dvec = rep(0, n_industries),\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(w_mvp, w_mvp_numerical$solution))\n\n[1] TRUE\n\nw_efficient_numerical <- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(rep(1, n_industries)),\n  bvec = 1,\n  meq = 1\n)\n\nall(near(compute_efficient_weight(Sigma, mu), w_efficient_numerical$solution))\n\n[1] TRUE\n\n\nThe result above shows that indeed the numerical procedure recovered the optimal weights for a scenario, where we already know the analytic solution. For more complex optimization routines, R’s optimization task view provides an overview of the vast optimization landscape. \nNext, we approach problems where no analytical solutions exist. First, we additionally impose short-sale constraints, which implies \\(N\\) inequality constraints of the form \\(w_i >=0\\).\n\nw_no_short_sale <- solve.QP(\n  Dmat = 2 * Sigma,\n  dvec = mu,\n  Amat = cbind(1, diag(n_industries)),\n  bvec = c(1, rep(0, n_industries)),\n  meq = 1\n)\nw_no_short_sale$solution\n\n [1] 5.17e-01 3.06e-18 4.27e-16 7.90e-02 3.47e-18 2.65e-17 1.48e-01\n [8] 2.56e-01 8.74e-18 0.00e+00\n\n\nAs expected, the resulting portfolio weights are all positive (up to numerical precision). Typically, the holdings in the presence of short-sale constraints are concentrated among way fewer assets than for the unrestricted case. You can verify that sum(w_no_short_sale$solution) returns 1. In other words: solve.QP() provides the numerical solution to a portfolio choice problem for a mean-variance investor with risk aversion gamma = 2, where negative holdings are forbidden.\nsolve.QP() is fast because it benefits from a very clear problem structure with a quadratic objective and linear constraints. However, optimization often requires more flexibility. As an example, we show how to compute optimal weights, subject to the so-called Regulation T-constraint, which requires that the sum of all absolute portfolio weights is smaller than 1.5, that is \\(\\sum_{i=1}^N |w_i| \\leq 1.5\\). The constraint enforces that a maximum of 50 percent of the allocated wealth can be allocated to short positions, thus implying an initial margin requirement of 50 percent. Imposing such a margin requirement reduces portfolio risks because extreme portfolio weights are not attainable anymore. The implementation of Regulation-T rules is numerically interesting because the margin constraints imply a non-linear constraint on the portfolio weights. Thus, we can no longer rely on solve.QP(), which is defined as solving quadratic programming problems with linear constraints. Instead, we rely on the package alabama, which requires a separate definition of objective and constraint functions.\n\ninitial_weights <- rep(\n  1 / n_industries,\n  n_industries\n)\n\nobjective <- function(w, gamma = 2) {\n  -t(w) %*% (1 + mu) +\n    gamma / 2 * t(w) %*% Sigma %*% w\n}\n\ninequality_constraints <- function(w, reg_t = 1.5) {\n  reg_t - sum(abs(w))\n}\n\nequality_constraints <- function(w) {\n  sum(w) - 1\n}\n\nw_reg_t <- constrOptim.nl(\n  par = initial_weights,\n  hin = inequality_constraints,\n  fn = objective,\n  heq = equality_constraints,\n  control.outer = list(trace = FALSE)\n)\nw_reg_t$par\n\n [1]  3.41e-01 -1.81e-06 -1.08e-01  1.40e-01  7.30e-02 -1.08e-02\n [7]  2.46e-01  3.14e-01  1.35e-01 -1.30e-01\n\n\nNote that the function constrOptim.nl() requires a starting vector of parameter values, i.e., an initial portfolio. Under the hood, alamaba performs numerical optimization by searching for a local minimum of the function objective() (subject to the equality constraints equality_constraints() and the inequality constraints inequality_constraints()). Note that the starting point should not matter if the algorithm identifies a global minimum.\nFigure 2 shows the optimal allocation weights across all 10 industries for the four different strategies considered so far: minimum variance, efficient portfolio with \\(\\gamma\\) = 2, efficient portfolio with short-sale constraints, and the Regulation-T constrained portfolio.\n\ntibble(\n  `No short-sale` = w_no_short_sale$solution,\n  `Minimum Variance` = w_mvp,\n  `Efficient portfolio` = compute_efficient_weight(Sigma, mu),\n  `Regulation-T` = w_reg_t$par,\n  Industry = colnames(industry_returns)\n) |>\n  pivot_longer(-Industry,\n    names_to = \"Strategy\",\n    values_to = \"weights\"\n  ) |>\n  ggplot(aes(\n    fill = Strategy,\n    y = weights,\n    x = Industry\n  )) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  coord_flip() +\n  labs(\n    y = \"Allocation weight\", fill = NULL,\n    title = \"Optimal allocations for different strategies\"\n  ) +\n  scale_y_continuous(labels = percent)\n\n\n\n\nFigure 2: Optimal allocation weights for the 10 industry portfolios and the 4 different allocation strategies.\n\n\n\n\nThe results clearly indicate the effect of imposing additional constraints: the extreme holdings the investor implements if she follows the (theoretically optimal) efficient portfolio vanish under, e.g., the Regulation-T constraint. You may wonder why an investor would deviate from what is theoretically the optimal portfolio by imposing potentially arbitrary constraints. The short answer is: the efficient portfolio is only efficient if the true parameters of the data generating process correspond to the estimated parameters \\(\\hat\\Sigma\\) and \\(\\hat\\mu\\). Estimation uncertainty may thus lead to inefficient allocations. By imposing restrictions, we implicitly shrink the set of possible weights and prevent extreme allocations, which could result from error-maximization due to estimation uncertainty (Jagannathan and Ma 2003).\nBefore we move on, we want to propose a final allocation strategy, which reflects a somewhat more realistic structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for transaction costs of the form \\(\\beta\\sum_{i=1}^N |(w_{i, t+1} - w_{i, t^+})|\\). No closed-form solution exists, and we rely on non-linear optimization procedures.\n\ncompute_efficient_weight_L1_TC <- function(mu,\n                                           Sigma,\n                                           gamma = 2,\n                                           beta = 0,\n                                           initial_weights = rep(\n                                             1 / ncol(Sigma),\n                                             ncol(Sigma)\n                                           )) {\n  objective <- function(w) {\n    -t(w) %*% mu +\n      gamma / 2 * t(w) %*% Sigma %*% w +\n      (beta / 10000) / 2 * sum(abs(w - initial_weights))\n  }\n\n  w_optimal <- constrOptim.nl(\n    par = initial_weights,\n    fn = objective,\n    heq = function(w) {\n      sum(w) - 1\n    },\n    control.outer = list(trace = FALSE)\n  )\n\n  return(w_optimal$par)\n}"
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "href": "constrained-optimization-and-backtesting.html#out-of-sample-backtesting",
    "title": "Constrained Optimization and Backtesting",
    "section": "Out-of-Sample Backtesting",
    "text": "Out-of-Sample Backtesting\nFor the sake of simplicity, we committed one fundamental error in computing portfolio weights above: we used the full sample of the data to determine the optimal allocation (Arnott, Harvey, and Markowitz 2019). To implement this strategy at the beginning of the 2000s, you will need to know how the returns will evolve until 2021. While interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We do so next in a backtesting application for three strategies. For the backtest, we recompute optimal weights just based on past available data.\nThe few lines below define the general setup. We consider 120 periods from the past to update the parameter estimates before recomputing portfolio weights. Then, we update portfolio weights which is costly and affects the performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, the mean-variance efficient portfolio with ex-ante adjustment for transaction costs, and the naive portfolio, which allocates wealth equally across the different assets.\n\nwindow_length <- 120\nperiods <- nrow(industry_returns) - window_length\n\nbeta <- 50\ngamma <- 2\n\nperformance_values <- matrix(NA,\n  nrow = periods,\n  ncol = 3\n)\ncolnames(performance_values) <- c(\"raw_return\", \"turnover\", \"net_return\")\n\nperformance_values <- list(\n  \"MV (TC)\" = performance_values,\n  \"Naive\" = performance_values,\n  \"MV\" = performance_values\n)\n\nw_prev_1 <- w_prev_2 <- w_prev_3 <- rep(\n  1 / n_industries,\n  n_industries\n)\n\nWe also define two helper functions: one to adjust the weights due to returns and one for performance evaluation, where we compute realized returns net of transaction costs.\n\nadjust_weights <- function(w, next_return) {\n  w_prev <- 1 + w * next_return\n  as.numeric(w_prev / sum(as.vector(w_prev)))\n}\n\nevaluate_performance <- function(w, w_previous, next_return, beta = 50) {\n  raw_return <- as.matrix(next_return) %*% w\n  turnover <- sum(abs(w - w_previous))\n  net_return <- raw_return - beta / 10000 * turnover\n  c(raw_return, turnover, net_return)\n}\n\nThe following code chunk performs a rolling-window estimation, which we implement in a loop. In each period, the estimation window contains the returns available up to the current period. Note that we use the sample variance-covariance matrix and ignore the estimation of \\(\\hat\\mu\\) entirely, but you might use more advanced estimators in practice.\n\nfor (p in 1:periods) {\n  returns_window <- industry_returns[p:(p + window_length - 1), ]\n  next_return <- industry_returns[p + window_length, ] |> as.matrix()\n\n  Sigma <- cov(returns_window)\n  mu <- 0 * colMeans(returns_window)\n\n  # Transaction-cost adjusted portfolio\n  w_1 <- compute_efficient_weight_L1_TC(\n    mu = mu,\n    Sigma = Sigma,\n    beta = beta,\n    gamma = gamma,\n    initial_weights = w_prev_1\n  )\n\n  performance_values[[1]][p, ] <- evaluate_performance(w_1,\n    w_prev_1,\n    next_return,\n    beta = beta\n  )\n\n  w_prev_1 <- adjust_weights(w_1, next_return)\n\n  # Naive portfolio\n  w_2 <- rep(1 / n_industries, n_industries)\n\n  performance_values[[2]][p, ] <- evaluate_performance(\n    w_2,\n    w_prev_2,\n    next_return\n  )\n\n  w_prev_2 <- adjust_weights(w_2, next_return)\n\n  # Mean-variance efficient portfolio (w/o transaction costs)\n  w_3 <- compute_efficient_weight(\n    Sigma = Sigma,\n    mu = mu,\n    gamma = gamma\n  )\n\n  performance_values[[3]][p, ] <- evaluate_performance(\n    w_3,\n    w_prev_3,\n    next_return\n  )\n\n  w_prev_3 <- adjust_weights(w_3, next_return)\n}\n\nFinally, we get to the evaluation of the portfolio strategies net-of-transaction costs. Note that we compute annualized returns and standard deviations. \n\nperformance <- lapply(\n  performance_values,\n  as_tibble\n) |>\n  bind_rows(.id = \"strategy\")\n\nperformance |>\n  group_by(strategy) |>\n  summarize(\n    Mean = 12 * mean(100 * net_return),\n    SD = sqrt(12) * sd(100 * net_return),\n    `Sharpe ratio` = if_else(Mean > 0,\n      Mean / SD,\n      NA_real_\n    ),\n    Turnover = 100 * mean(turnover)\n  )\n\n# A tibble: 3 × 5\n  strategy   Mean    SD `Sharpe ratio` Turnover\n  <chr>     <dbl> <dbl>          <dbl>    <dbl>\n1 MV       -0.635  12.5         NA     213.    \n2 MV (TC)  12.3    15.0          0.820   0.0298\n3 Naive    12.3    15.0          0.818   0.230 \n\n\nThe results clearly speak against mean-variance optimization. Turnover is huge when the investor only considers her portfolio’s expected return and variance. Effectively, the mean-variance portfolio generates a negative annualized return after adjusting for transaction costs. At the same time, the naive portfolio turns out to perform very well. In fact, the performance gains of the transaction-cost adjusted mean-variance portfolio are small. The out-of-sample Sharpe ratio is slightly higher than for the naive portfolio. Note the extreme effect of turnover penalization on turnover: MV (TC) effectively resembles a buy-and-hold strategy which only updates the portfolio once the estimated parameters \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\)indicate that the current allocation is too far away from the optimal theoretical portfolio."
  },
  {
    "objectID": "constrained-optimization-and-backtesting.html#exercises",
    "href": "constrained-optimization-and-backtesting.html#exercises",
    "title": "Constrained Optimization and Backtesting",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the portfolio choice problem for transaction-cost adjusted certainty equivalent maximization with risk aversion parameter \\(\\gamma\\) \\[\\omega_{t+1} ^* =  \\arg\\max_{\\omega \\in \\mathbb{R}^N,  \\iota'\\omega = 1} \\omega'\\mu - \\nu_t (\\omega, \\beta) - \\frac{\\gamma}{2}\\omega'\\Sigma\\omega\\] where \\(\\Sigma\\) and \\(\\mu\\) are (estimators of) the variance-covariance matrix of the returns and the vector of expected returns. Assume for now that transaction costs are quadratic in rebalancing and proportional to stock illiquidity such that \\[\\nu_t\\left(\\omega, B\\right) = \\frac{\\beta}{2} \\left(\\omega - \\omega_{t^+}\\right)'B\\left(\\omega - \\omega_{t^+}\\right)\\] where \\(B = \\text{diag}(ill_1, \\ldots, ill_N)\\) is a diagonal matrix, where \\(ill_1, \\ldots, ill_N\\). Derive a closed-form solution for the mean-variance efficient portfolio \\(\\omega_{t+1} ^*\\) based on the transaction cost specification above. Discuss the effect of illiquidity \\(ill_i\\) on the individual portfolio weights relative to an investor that myopically ignores transaction costs in her decision.\nUse the solution from the previous exercise to update the function compute_efficient_weight() such that you can compute optimal weights conditional on a matrix \\(B\\) with illiquidity measures.\nIllustrate the evolution of the optimal weights from the naive portfolio to the efficient portfolio in the mean-standard deviation diagram.\nIs it always optimal to choose the same \\(\\beta\\) in the optimization problem than the value used in evaluating the portfolio performance? In other words: can it be optimal to choose theoretically sub-optimal portfolios based on transaction cost considerations that do not reflect the actual incurred costs? Evaluate the out-of-sample Sharpe ratio after transaction costs for a range of different values of imposed \\(\\beta\\) values."
  },
  {
    "objectID": "parametric-portfolio-policies.html",
    "href": "parametric-portfolio-policies.html",
    "title": "Parametric Portfolio Policies",
    "section": "",
    "text": "In this chapter, we apply different portfolio performance measures to evaluate and compare portfolio allocation strategies. For this purpose, we introduce a direct way to estimate optimal portfolio weights for large-scale cross-sectional applications. More precisely, the approach of Brandt, Santa-Clara, and Valkanov (2009) proposes to parametrize the optimal portfolio weights as a function of stock characteristics instead of estimating the stock’s expected return, variance, and covariances with other stocks in a prior step. We choose weights as a function of the characteristics, which maximize the expected utility of the investor. This approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brandt, Santa-Clara, and Valkanov (2009). See the review paper Brandt (2010) for an excellent treatment of related portfolio choice methods.\nThe current chapter relies on the following set of packages:"
  },
  {
    "objectID": "parametric-portfolio-policies.html#data-preparation",
    "href": "parametric-portfolio-policies.html#data-preparation",
    "title": "Parametric Portfolio Policies",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the monthly CRSP file, which forms our investment universe. We load the data from our SQLite-database introduced in Chapters 2-4.\n\ntidy_finance <- dbConnect(\n  SQLite(), \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |>\n  collect()\n\nTo evaluate the performance of portfolios, we further use monthly market returns as a benchmark to compute CAPM alphas.\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect()\n\nNext, we retrieve some stock characteristics that have been shown to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution. In particular, we record the lagged one-year return momentum (momentum_lag), defined as the compounded return between months \\(t-13\\) and \\(t-2\\) for each firm. In finance, momentum is the empirically observed tendency for rising asset prices to rise further, and falling prices to keep falling (Jegadeesh and Titman 1993). The second characteristic is the firm’s market equity (size_lag), defined as the log of the price per share times the number of shares outstanding (Banz 1981). To construct the correct lagged values, we use the approach introduced in Chapter 3.\n\ncrsp_monthly_lags <- crsp_monthly |>\n  transmute(permno,\n    month_13 = month %m+% months(13),\n    mktcap\n  )\n\ncrsp_monthly <- crsp_monthly |>\n  inner_join(crsp_monthly_lags,\n    by = c(\"permno\", \"month\" = \"month_13\"),\n    suffix = c(\"\", \"_13\")\n  )\n\ndata_portfolios <- crsp_monthly |>\n  mutate(\n    momentum_lag = mktcap_lag / mktcap_13,\n    size_lag = log(mktcap_lag)\n  ) |>\n  drop_na(contains(\"lag\"))"
  },
  {
    "objectID": "parametric-portfolio-policies.html#parametric-portfolio-policies",
    "href": "parametric-portfolio-policies.html#parametric-portfolio-policies",
    "title": "Parametric Portfolio Policies",
    "section": "Parametric Portfolio Policies",
    "text": "Parametric Portfolio Policies\nThe basic idea of parametric portfolio weights is as follows. Suppose that at each date \\(t\\) we have \\(N_t\\) stocks in the investment universe, where each stock \\(i\\) has a return of \\(r_{i, t+1}\\) and is associated with a vector of firm characteristics \\(x_{i, t}\\) such as time-series momentum or the market capitalization. The investor’s problem is to choose portfolio weights \\(w_{i,t}\\) to maximize the expected utility of the portfolio return: \\[\\begin{aligned}\n\\max_{w} E_t\\left(u(r_{p, t+1})\\right) = E_t\\left[u\\left(\\sum\\limits_{i=1}^{N_t}w_{i,t}r_{i,t+1}\\right)\\right]\n\\end{aligned}\\] where \\(u(\\cdot)\\) denotes the utility function.\nWhere do the stock characteristics show up? We parameterize the optimal portfolio weights as a function of the stock characteristic \\(x_{i,t}\\) with the following linear specification for the portfolio weights: \\[w_{i,t} = \\bar{w}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t},\\] where \\(\\bar{w}_{i,t}\\) is a stock’s weight in a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), \\(\\theta\\) is a vector of coefficients which we are going to estimate, and \\(\\hat{x}_{i,t}\\) are the characteristics of stock \\(i\\), cross-sectionally standardized to have zero mean and unit standard deviation.\nIntuitively, the portfolio strategy is a form of active portfolio management relative to a performance benchmark. Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction the weights sum up to one as \\(\\sum_{i=1}^{N_t}\\hat{x}_{i,t} = 0\\) due to the standardization. Moreover, the coefficients are constant across assets and over time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.\nWe first implement cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization relative_mktcap, which will represent the value-weighted benchmark portfolio, while n denotes the number of traded assets \\(N_t\\), which we use to construct the naive portfolio benchmark.\n\ndata_portfolios <- data_portfolios |>\n  group_by(month) |>\n  mutate(\n    n = n(),\n    relative_mktcap = mktcap_lag / sum(mktcap_lag),\n    across(contains(\"lag\"), ~ (. - mean(.)) / sd(.)),\n  ) |>\n  ungroup() |>\n  select(-mktcap_lag, -altprc)"
  },
  {
    "objectID": "parametric-portfolio-policies.html#computing-portfolio-weights",
    "href": "parametric-portfolio-policies.html#computing-portfolio-weights",
    "title": "Parametric Portfolio Policies",
    "section": "Computing Portfolio Weights",
    "text": "Computing Portfolio Weights\nNext, we move on to identify optimal choices of \\(\\theta\\). We rewrite the optimization problem together with the weight parametrization and can then estimate \\(\\theta\\) to maximize the objective function based on our sample \\[\\begin{aligned}\nE_t\\left(u(r_{p, t+1})\\right) = \\frac{1}{T}\\sum\\limits_{t=0}^{T-1}u\\left(\\sum\\limits_{i=1}^{N_t}\\left(\\bar{w}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\right)r_{i,t+1}\\right).\n\\end{aligned}\\] The allocation strategy is straightforward because the number of parameters to estimate is small. Instead of a tedious specification of the \\(N_t\\) dimensional vector of expected returns and the \\(N_t(N_t+1)/2\\) free elements of the covariance matrix, all we need to focus on in our application is the vector \\(\\theta\\). \\(\\theta\\) contains only two elements in our application - the relative deviation from the benchmark due to size and momentum.\nTo get a feeling for the performance of such an allocation strategy, we start with an arbitrary initial vector \\(\\theta_0\\). The next step is to choose \\(\\theta\\) optimally to maximize the objective function. We automatically detect the number of parameters by counting the number of columns with lagged values.\n\nn_parameters <- sum(str_detect(\n  colnames(data_portfolios), \"lag\"\n))\n\ntheta <- rep(1.5, n_parameters)\n\nnames(theta) <- colnames(data_portfolios)[str_detect(\n  colnames(data_portfolios), \"lag\"\n)]\n\nThe function compute_portfolio_weights() below computes the portfolio weights \\(\\bar{w}_{i,t} + \\frac{1}{N_t}\\theta'\\hat{x}_{i,t}\\) according to our parametrization for a given value \\(\\theta_0\\). Everything happens within a single pipeline. Hence, we provide a short walk-through.\nWe first compute characteristic_tilt, the tilting values \\(\\frac{1}{N_t}\\theta'\\hat{x}_{i, t}\\) which resemble the deviation from the benchmark portfolio. Next, we compute the benchmark portfolio weight_benchmark, which can be any reasonable set of portfolio weights. In our case, we choose either the value or equal-weighted allocation. weight_tilt completes the picture and contains the final portfolio weights weight_tilt = weight_benchmark + characteristic_tilt which deviate from the benchmark portfolio depending on the stock characteristics.\nThe final few lines go a bit further and implement a simple version of a no-short sale constraint. While it is generally not straightforward to ensure portfolio weight constraints via parameterization, we simply normalize the portfolio weights such that they are enforced to be positive. Finally, we make sure that the normalized weights sum up to one again: \\[w_{i,t}^+ = \\frac{\\max(0, w_{i,t})}{\\sum_{j=1}^{N_t}\\max(0, w_{i,t})}.\\]\nThe following function computes the optimal portfolio weights in the way just described.\n\ncompute_portfolio_weights <- function(theta,\n                                      data,\n                                      value_weighting = TRUE,\n                                      allow_short_selling = TRUE) {\n  data |>\n    group_by(month) |>\n    bind_cols(\n      characteristic_tilt = data |>\n        transmute(across(contains(\"lag\"), ~ . / n)) |>\n        as.matrix() %*% theta |> as.numeric()\n    ) |>\n    mutate(\n      # Definition of benchmark weight\n      weight_benchmark = case_when(\n        value_weighting == TRUE ~ relative_mktcap,\n        value_weighting == FALSE ~ 1 / n\n      ),\n      # Parametric portfolio weights\n      weight_tilt = weight_benchmark + characteristic_tilt,\n      # Short-sell constraint\n      weight_tilt = case_when(\n        allow_short_selling == TRUE ~ weight_tilt,\n        allow_short_selling == FALSE ~ pmax(0, weight_tilt)\n      ),\n      # Weights sum up to 1\n      weight_tilt = weight_tilt / sum(weight_tilt)\n    ) |>\n    ungroup()\n}\n\nIn the next step, we compute the portfolio weights for the arbitrary vector \\(\\theta_0\\). In the example below, we use the value-weighted portfolio as a benchmark and allow negative portfolio weights.\n\nweights_crsp <- compute_portfolio_weights(theta,\n  data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)"
  },
  {
    "objectID": "parametric-portfolio-policies.html#portfolio-performance",
    "href": "parametric-portfolio-policies.html#portfolio-performance",
    "title": "Parametric Portfolio Policies",
    "section": "Portfolio Performance",
    "text": "Portfolio Performance\n Are the computed weights optimal in any way? Most likely not, as we picked \\(\\theta_0\\) arbitrarily. To evaluate the performance of an allocation strategy, one can think of many different approaches. In their original paper, Brandt, Santa-Clara, and Valkanov (2009) focus on a simple evaluation of the hypothetical utility of an agent equipped with a power utility function \\(u_\\gamma(r) = \\frac{(1 + r)^\\gamma}{1-\\gamma}\\), where \\(\\gamma\\) is the risk aversion factor.\n\npower_utility <- function(r, gamma = 5) {\n  (1 + r)^(1 - gamma) / (1 - gamma)\n}\n\nWe want to note that Gehrig, Sögner, and Westerkamp (2020) warn that, in the leading case of constant relative risk aversion (CRRA), strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy, and the parameter space are necessary to obtain a well-defined optimization problem.\nNo doubt, there are many other ways to evaluate a portfolio. The function below provides a summary of all kinds of interesting measures that can be considered relevant. Do we need all these evaluation measures? It depends: the original paper Brandt, Santa-Clara, and Valkanov (2009) only cares about the expected utility to choose \\(\\theta\\). However, if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights, it is helpful to have everything in one function.\n\nevaluate_portfolio <- function(weights_crsp,\n                               full_evaluation = TRUE) {\n  evaluation <- weights_crsp |>\n    group_by(month) |>\n    summarize(\n      return_tilt = weighted.mean(ret_excess, weight_tilt),\n      return_benchmark = weighted.mean(ret_excess, weight_benchmark)\n    ) |>\n    pivot_longer(-month,\n      values_to = \"portfolio_return\",\n      names_to = \"model\"\n    ) |>\n    group_by(model) |>\n    left_join(factors_ff_monthly, by = \"month\") |>\n    summarize(tibble(\n      \"Expected utility\" = mean(power_utility(portfolio_return)),\n      \"Average return\" = 100 * mean(12 * portfolio_return),\n      \"SD return\" = 100 * sqrt(12) * sd(portfolio_return),\n      \"Sharpe ratio\" = sqrt(12) * mean(portfolio_return) / sd(portfolio_return),\n      \"CAPM alpha\" = coefficients(lm(portfolio_return ~ mkt_excess))[1],\n      \"Market beta\" = coefficients(lm(portfolio_return ~ mkt_excess))[2]\n    )) |>\n    mutate(model = str_remove(model, \"return_\")) |>\n    pivot_longer(-model, names_to = \"measure\") |>\n    pivot_wider(names_from = model, values_from = value)\n\n  if (full_evaluation) {\n    weight_evaluation <- weights_crsp |>\n      select(month, contains(\"weight\")) |>\n      pivot_longer(-month, values_to = \"weight\", names_to = \"model\") |>\n      group_by(model, month) |>\n      transmute(tibble(\n        \"Absolute weight\" = abs(weight),\n        \"Max. weight\" = max(weight),\n        \"Min. weight\" = min(weight),\n        \"Avg. sum of negative weights\" = -sum(weight[weight < 0]),\n        \"Avg. fraction of negative weights\" = sum(weight < 0) / n()\n      )) |>\n      group_by(model) |>\n      summarize(across(-month, ~ 100 * mean(.))) |>\n      mutate(model = str_remove(model, \"weight_\")) |>\n      pivot_longer(-model, names_to = \"measure\") |>\n      pivot_wider(names_from = model, values_from = value)\n    evaluation <- bind_rows(evaluation, weight_evaluation)\n  }\n  return(evaluation)\n}\n\n Let us take a look at the different portfolio strategies and evaluation measures.\n\nevaluate_portfolio(weights_crsp) |>\n  print(n = Inf)\n\n# A tibble: 11 × 3\n   measure                            benchmark     tilt\n   <chr>                                  <dbl>    <dbl>\n 1 Expected utility                  -0.249     -0.261  \n 2 Average return                     7.08       0.166  \n 3 SD return                         15.3       20.9    \n 4 Sharpe ratio                       0.463      0.00793\n 5 CAPM alpha                         0.000129  -0.00538\n 6 Market beta                        0.992      0.950  \n 7 Absolute weight                    0.0249     0.0637 \n 8 Max. weight                        3.55       3.68   \n 9 Min. weight                        0.0000277 -0.145  \n10 Avg. sum of negative weights       0         77.9    \n11 Avg. fraction of negative weights  0         49.5    \n\n\nThe value-weighted portfolio delivers an annualized return of more than 6 percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio, or the CAPM alpha. We can conclude the market beta is close to one for both strategies (naturally almost identically 1 for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions."
  },
  {
    "objectID": "parametric-portfolio-policies.html#optimal-parameter-choice",
    "href": "parametric-portfolio-policies.html#optimal-parameter-choice",
    "title": "Parametric Portfolio Policies",
    "section": "Optimal Parameter Choice",
    "text": "Optimal Parameter Choice\nNext, we move to a choice of \\(\\theta\\) that actually aims to improve some (or all) of the performance measures. We first define a helper function compute_objective_function(), which we then pass to an optimizer.\n\ncompute_objective_function <- function(theta,\n                                       data,\n                                       objective_measure = \"Expected utility\",\n                                       value_weighting = TRUE,\n                                       allow_short_selling = TRUE) {\n  processed_data <- compute_portfolio_weights(\n    theta,\n    data,\n    value_weighting,\n    allow_short_selling\n  )\n\n  objective_function <- evaluate_portfolio(processed_data,\n    full_evaluation = FALSE\n  ) |>\n    filter(measure == objective_measure) |>\n    pull(tilt)\n\n  return(-objective_function)\n}\n\nYou may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search for minima as a default. By minimizing the negative value of the objective function, we get the maximum value as a result. In its most basic form, R optimization relies on the function optim(). As main inputs, the function requires an initial guess of the parameters and the objective function to minimize. Now, we are fully equipped to compute the optimal values of \\(\\hat\\theta\\), which maximize the hypothetical expected utility of the investor.\n\noptimal_theta <- optim(\n  par = theta,\n  compute_objective_function,\n  objective_measure = \"Expected utility\",\n  data = data_portfolios,\n  value_weighting = TRUE,\n  allow_short_selling = TRUE\n)\n\noptimal_theta$par\n\nmomentum_lag     size_lag \n        0.31        -1.99 \n\n\nThe resulting values of \\(\\hat\\theta\\) are easy to interpret: intuitively, expected utility increases by tilting weights from the value-weighted portfolio toward smaller stocks (negative coefficient for size) and toward past winners (positive value for momentum). Both findings are in line with the well-documented size effect (Banz 1981) and the momentum anomaly (Jegadeesh and Titman 1993)."
  },
  {
    "objectID": "parametric-portfolio-policies.html#more-model-specifications",
    "href": "parametric-portfolio-policies.html#more-model-specifications",
    "title": "Parametric Portfolio Policies",
    "section": "More Model Specifications",
    "text": "More Model Specifications\nHow does the portfolio perform for different model specifications? For this purpose, we compute the performance of a number of different modeling choices based on the entire CRSP sample. The next code chunk performs all the heavy lifting.\n\nfull_model_grid <- expand_grid(\n  value_weighting = c(TRUE, FALSE),\n  allow_short_selling = c(TRUE, FALSE),\n  data = list(data_portfolios)\n) |>\n  mutate(optimal_theta = pmap(\n    .l = list(\n      data,\n      value_weighting,\n      allow_short_selling\n    ),\n    .f = ~ optim(\n      par = theta,\n      compute_objective_function,\n      data = ..1,\n      objective_measure = \"Expected utility\",\n      value_weighting = ..2,\n      allow_short_selling = ..3\n    )$par\n  ))\n\nFinally, we can compare the results. The table below shows summary statistics for all possible combinations: equal- or value-weighted benchmark portfolio, with or without short-selling constraints, and tilted toward maximizing expected utility.\n\nperformance_table <- full_model_grid |>\n  mutate(\n    processed_data = pmap(\n      .l = list(\n        optimal_theta,\n        data,\n        value_weighting,\n        allow_short_selling\n      ),\n      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)\n    ),\n    portfolio_evaluation = map(processed_data,\n      evaluate_portfolio,\n      full_evaluation = TRUE\n    )\n  ) |>\n  select(\n    value_weighting,\n    allow_short_selling,\n    portfolio_evaluation\n  ) |>\n  unnest(portfolio_evaluation)\n\nperformance_table |>\n  rename(\n    \" \" = benchmark,\n    Optimal = tilt\n  ) |>\n  mutate(\n    value_weighting = case_when(\n      value_weighting == TRUE ~ \"VW\",\n      value_weighting == FALSE ~ \"EW\"\n    ),\n    allow_short_selling = case_when(\n      allow_short_selling == TRUE ~ \"\",\n      allow_short_selling == FALSE ~ \"(no s.)\"\n    )\n  ) |>\n  pivot_wider(\n    names_from = value_weighting:allow_short_selling,\n    values_from = \" \":Optimal,\n    names_glue = \"{value_weighting} {allow_short_selling} {.value} \"\n  ) |>\n  select(\n    measure,\n    `EW    `,\n    `VW    `,\n    sort(contains(\"Optimal\"))\n  ) |>\n  print(n = 11)\n\n# A tibble: 11 × 7\n   measure      `EW    ` `VW    ` VW  Op…¹ VW (no…² EW  Op…³ EW (no…⁴\n   <chr>           <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 Expected ut… -0.250   -2.49e-1 -0.246   -0.247   -0.250   -2.50e-1\n 2 Average ret… 10.7      7.08e+0 14.9     13.6     13.1      8.09e+0\n 3 SD return    20.3      1.53e+1 20.5     19.5     22.5      1.70e+1\n 4 Sharpe ratio  0.525    4.63e-1  0.727    0.698    0.584    4.76e-1\n 5 CAPM alpha    0.00231  1.29e-4  0.00659  0.00531  0.00437  4.76e-4\n 6 Market beta   1.13     9.92e-1  1.01     1.04     1.13     1.08e+0\n 7 Absolute we…  0.0249   2.49e-2  0.0381   0.0249   0.0260   2.49e-2\n 8 Max. weight   0.0249   3.55e+0  3.37     2.69     0.157    2.21e-1\n 9 Min. weight   0.0249   2.77e-5 -0.0352   0       -0.0331   0      \n10 Avg. sum of…  0        0       27.4      0        2.27     0      \n11 Avg. fracti…  0        0       38.6      0        7.40     0      \n# … with abbreviated variable names ¹​`VW  Optimal `,\n#   ²​`VW (no s.) Optimal `, ³​`EW  Optimal `, ⁴​`EW (no s.) Optimal `\n\n\nThe results indicate that the average annualized Sharpe ratio of the equal-weighted portfolio exceeds the Sharpe ratio of the value-weighted benchmark portfolio. Nevertheless, starting with the weighted value portfolio as a benchmark and tilting optimally with respect to momentum and small stocks yields the highest Sharpe ratio across all specifications. Finally, imposing no short-sale constraints does not improve the performance of the portfolios in our application."
  },
  {
    "objectID": "parametric-portfolio-policies.html#exercises",
    "href": "parametric-portfolio-policies.html#exercises",
    "title": "Parametric Portfolio Policies",
    "section": "Exercises",
    "text": "Exercises\n\nHow do the estimated parameters \\(\\hat\\theta\\) and the portfolio performance change if your objective is to maximize the Sharpe ratio instead of the hypothetical expected utility?\nThe code above is very flexible in the sense that you can easily add new firm characteristics. Construct a new characteristic of your choice and evaluate the corresponding coefficient \\(\\hat\\theta_i\\).\nTweak the function optimal_theta() such that you can impose additional performance constraints in order to determine \\(\\hat\\theta\\), which maximizes expected utility under the constraint that the market beta is below 1.\nDoes the portfolio performance resemble a realistic out-of-sample backtesting procedure? Verify the robustness of the results by first estimating \\(\\hat\\theta\\) based on past data only. Then, use more recent periods to evaluate the actual portfolio performance.\nBy formulating the portfolio problem as a statistical estimation problem, you can easily obtain standard errors for the coefficients of the weight function. Brandt, Santa-Clara, and Valkanov (2009) provide the relevant derivations in their paper in Equation (10). Implement a small function that computes standard errors for \\(\\hat\\theta\\)."
  },
  {
    "objectID": "factor-selection-via-machine-learning.html",
    "href": "factor-selection-via-machine-learning.html",
    "title": "Factor Selection via Machine Learning",
    "section": "",
    "text": "The aim of this chapter is twofold. From a data science perspective, we introduce tidymodels, a collection of packages for modeling and machine learning (ML) using tidyverse principles. tidymodels comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the notion of factor zoo (Cochrane 2011) using ML methods. We introduce Lasso and Ridge regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model tuning with Elastic Net regularization as a popular example. We implement and showcase the entire cycle from model specification, training, and forecast evaluation within the tidymodels universe. While the tools can generally be applied to an abundance of interesting asset pricing problems, we apply penalized regressions for identifying macroeconomic variables and asset pricing factors that help explain a cross-section of industry portfolios.\nIn previous chapters, we illustrate that stock characteristics such as size provide valuable pricing information in addition to the market beta. Such findings question the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists discovered a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role in pricing applications). The search for factors that explain the cross-section of expected stock returns has produced hundreds of potential candidates, as noted more recently by Harvey, Liu, and Zhu (2016), Mclean and Pontiff (2016), and Hou, Xue, and Zhang (2020). Therefore, given the multitude of proposed risk factors, the challenge these days rather is: do we believe in the relevance of 300+ risk factors? During recent years, promising methods from the field of ML got applied to common finance applications. We refer to Mullainathan and Spiess (2017) for a treatment of ML from the perspective of an econometrician, Nagel (2021) for an excellent review of ML practices in asset pricing, Easley et al. (2020) for ML applications in (high-frequency) market microstructure, and Dixon, Halperin, and Bilokon (2020) for a detailed treatment of all methodological aspects."
  },
  {
    "objectID": "factor-selection-via-machine-learning.html#brief-theoretical-background",
    "href": "factor-selection-via-machine-learning.html#brief-theoretical-background",
    "title": "Factor Selection via Machine Learning",
    "section": "Brief Theoretical Background",
    "text": "Brief Theoretical Background\nThis is a book about doing empirical work in a tidy manner, and we refer to any of the many excellent textbook treatments of ML methods and especially penalized regressions for some deeper discussion. Excellent material is provided, for instance, by Hastie, Tibshirani, and Friedman (2009), Gareth et al. (2013), and De Prado (2018). Instead, we briefly summarize the idea of Lasso and Ridge regressions as well as the more general Elastic Net. Then, we turn to the fascinating question on how to implement, tune, and use such models with the tidymodels workflow.\nTo set the stage, we start with the definition of a linear model: suppose we have data \\((y_t, x_t), t = 1,\\ldots, T\\), where \\(x_t\\) is a \\((K \\times 1)\\) vector of regressors and \\(y_t\\) is the response for observation \\(t\\). The linear model takes the form \\(y_t = \\beta' x_t + \\varepsilon_t\\) with some error term \\(\\varepsilon_t\\) and has been studied in abundance. The well-known ordinary-least square (OLS) estimator for the \\((K \\times 1)\\) vector \\(\\beta\\) minimizes the sum of squared residuals and is then \\[\\hat{\\beta}^\\text{ols} = \\left(\\sum\\limits_{t=1}^T x_t'x_t\\right)^{-1} \\sum\\limits_{t=1}^T x_t'y_t.\\] \nWhile we are often interested in the estimated coefficient vector \\(\\hat\\beta^\\text{ols}\\), ML is about the predictive performance most of the time. For a new observation \\(\\tilde{x}_t\\), the linear model generates predictions such that \\[\\hat y_t = E\\left(y|x_t = \\tilde x_t\\right) = \\hat\\beta^\\text{ols}{}' \\tilde x_t.\\] Is this the best we can do? Not really: instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by choosing other estimators \\(\\hat{\\beta}\\) with lower variance than the estimator \\(\\hat\\beta^\\text{ols}\\). At the same time, it seems appealing to restrict the set of regressors to a few meaningful ones if possible. In other words, if \\(K\\) is large (such as for the number of proposed factors in the asset pricing literature), it may be a desirable feature to select reasonable factors and set \\(\\hat\\beta^{\\text{ols}}_k = 0\\) for some redundant factors.\nIt should be clear that the promised benefits of penalized regressions, i.e., reducing the mean squared error (MSE), come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that \\(E\\left(\\hat\\beta\\right) \\neq \\beta\\). What is the effect of such a bias-variance trade-off? To understand the implications, assume the following data-generating process for \\(y\\): \\[y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim (0, \\sigma_\\varepsilon^2)\\] We want to recover \\(f(x)\\), which denotes some unknown functional which maps the relationship between \\(x\\) and \\(y\\). While the properties of \\(\\hat\\beta^\\text{ols}\\) as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Alternative predictors \\(\\hat{f}(x)\\) could be more desirable: For instance, the MSE depends on our model choice as follows: \\[\\begin{aligned}\nMSE &=E((y-\\hat{f}(x))^2)=E((f(x)+\\epsilon-\\hat{f}(x))^2)\\\\\n&= \\underbrace{E((f(x)-\\hat{f}(x))^2)}_{\\text{total quadratic error}}+\\underbrace{E(\\epsilon^2)}_{\\text{irreducible error}} \\\\\n&= E\\left(\\hat{f}(x)^2\\right)+E\\left(f(x)^2\\right)-2E\\left(f(x)\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=E\\left(\\hat{f}(x)^2\\right)+f(x)^2-2f(x)E\\left(\\hat{f}(x)\\right)+\\sigma_\\varepsilon^2\\\\\n&=\\underbrace{\\text{Var}\\left(\\hat{f}(x)\\right)}_{\\text{variance of model}}+ \\underbrace{E\\left((f(x)-\\hat{f}(x))\\right)^2}_{\\text{squared bias}} +\\sigma_\\varepsilon^2.\n\\end{aligned}\\] While no model can reduce \\(\\sigma_\\varepsilon^2\\), a biased estimator with small variance may have a lower MSE than an unbiased estimator.\n\nRidge regression\n\nOne biased estimator is known as Ridge regression. Hoerl and Kennard (1970) propose to minimize the sum of squared errors while simultaneously imposing a penalty on the \\(L_2\\) norm of the parameters \\(\\hat\\beta\\). Formally, this means that for a penalty factor \\(\\lambda\\geq 0\\) the minimization problem takes the form \\(\\min_\\beta \\left(y - X\\beta\\right)'\\left(y - X\\beta\\right)\\text{ s.t. } \\beta'\\beta \\leq c\\). Here \\(c\\geq 0\\) is a constant that depends on the choice of \\(\\lambda\\). The larger \\(\\lambda\\), the smaller \\(c\\) (technically speaking, there is a one-to-one relationship between \\(\\lambda\\), which corresponds to the Lagrangian of the minimization problem above and \\(c\\)). Here, \\(X = \\left(x_1 \\ldots x_T\\right)'\\) and \\(y = \\left(y_1, \\ldots, y_T\\right)'\\). A closed-form solution for the resulting regression coefficient vector \\(\\beta^\\text{ridge}\\) exists: \\[\\hat{\\beta}^\\text{ridge} = \\left(X'X + \\lambda I\\right)^{-1}X'y.\\] A couple of observations are worth noting: \\(\\hat\\beta^\\text{ridge} = \\hat\\beta^\\text{ols}\\) for \\(\\lambda = 0\\) and \\(\\hat\\beta^\\text{ridge} \\rightarrow 0\\) for \\(\\lambda\\rightarrow \\infty\\). Also for \\(\\lambda > 0\\), \\(\\left(X'X + \\lambda I\\right)\\) is non-singular even if \\(X'X\\) is which means that \\(\\hat\\beta^\\text{ridge}\\) exists even if \\(\\hat\\beta\\) is not defined. However, note also that the Ridge estimator requires careful choice of the hyperparameter \\(\\lambda\\) which controls the amount of regularization: a larger value of \\(\\lambda\\) implies shrinkage of the regression coefficient toward 0, a smaller value of \\(\\lambda\\) reduces the bias of the resulting estimator.\n\nNote, that \\(X\\) usually contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that \\(y\\) is simply demeaned before computing \\(\\hat\\beta^\\text{ridge}\\).\n\nWhat about the statistical properties of the Ridge estimator? First, the bad news is that \\(\\hat\\beta^\\text{ridge}\\) is a biased estimator of \\(\\beta\\). However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is guaranteed to be smaller than the variance of the ordinary least square estimator. We encourage you to verify these two statements in the exercises. As a result, we face a trade-off: The Ridge regression sacrifices some unbiasedness to achieve a smaller variance than the OLS estimator.\n\n\nLasso\n\nAn alternative to Ridge regression is the Lasso (least absolute shrinkage and selection operator). Similar to Ridge regression, the Lasso (Tibshirani 1996) is a penalized and biased estimator. The main difference to Ridge regression is that Lasso does not only shrink coefficients but effectively selects variables by setting coefficients for irrelevant variables to zero. Lasso implements a \\(L_1\\) penalization on the parameters such that: \\[\\hat\\beta^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right)\\text{ s.t. } \\sum\\limits_{k=1}^K|\\beta_k| < c(\\lambda).\\] There is no closed form solution for \\(\\hat\\beta^\\text{Lasso}\\) in the above maximization problem but efficient algorithms exist (e.g., the R package glmnet). Like for Ridge regression, the hyperparameter \\(\\lambda\\) has to be specified beforehand.\n\n\nElastic Net\nThe Elastic Net (Zou and Hastie 2005) combines \\(L_1\\) with \\(L_2\\) penalization and encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. This more general framework considers the following optimization problem: \\[\\hat\\beta^\\text{EN} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda(1-\\rho)\\sum\\limits_{k=1}^K|\\beta_k| +\\frac{1}{2}\\lambda\\rho\\sum\\limits_{k=1}^K\\beta_k^2\\] Now, we have to chose two hyperparameters: the shrinkage factor \\(\\lambda\\) and the weighting parameter \\(\\rho\\). The Elastic Net resembles Lasso for \\(\\rho = 1\\) and Ridge regression for \\(\\rho = 0\\). While the R package glmnet provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the glmnet package or the tidymodels back-end."
  },
  {
    "objectID": "factor-selection-via-machine-learning.html#data-preparation",
    "href": "factor-selection-via-machine-learning.html#data-preparation",
    "title": "Factor Selection via Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we load the required packages and data. The main focus is on the workflow behind the tidymodels package collection (Kuhn and Wickham 2020). Kuhn and Silge (2018) provide a thorough introduction into all tidymodels components. glmnet (Simon et al. 2011) was developed and released in sync with Tibshirani (1996) and provides an R implementation of Elastic Net estimation. The package timetk (Dancho and Vaughan 2022) provides useful tools for time series data wrangling.\n\nlibrary(RSQLite)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(furrr)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(timetk)\n\nIn this analysis, we use four different data sources that we load from our SQLite-database introduced in Chapters 2-4. We start with two different sets of factor portfolio returns which have been suggested as representing practical risk factor exposure and thus should be relevant when it comes to asset pricing applications.\n\nThe standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts) defined in Fama and French (1992) and Fama and French (1993)\nMonthly q-factor returns from Hou, Xue, and Zhang (2014). The factors contain the size factor, the investment factor, the return-on-equity factor, and the expected growth factor\n\nNext, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effectively serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2009) on the role of conditioning information.\n\nOur set of macroeconomic predictors comes from Welch and Goyal (2008). The data has been updated by the authors until 2021 and contains monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the dividend price ratio, earnings price ratio, stock variance, net equity expansion, treasury bill rate, and inflation\n\nFinally, we need a set of test assets. The aim is to understand which of the plenty factors and macroeconomic variable combinations prove helpful in explaining our test assets’ cross-section of returns. In line with many existing papers, we use monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets.\n\ntidy_finance <- dbConnect(\n  SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |>\n  collect() |>\n  rename_with(~ str_c(\"factor_ff_\", .), -month)\n\nfactors_q_monthly <- tbl(tidy_finance, \"factors_q_monthly\") |>\n  collect() |>\n  rename_with(~ str_c(\"factor_q_\", .), -month)\n\nmacro_predictors <- tbl(tidy_finance, \"macro_predictors\") |>\n  collect() |>\n  rename_with(~ str_c(\"macro_\", .), -month) |>\n  select(-macro_rp_div)\n\nindustries_ff_monthly <- tbl(tidy_finance, \"industries_ff_monthly\") |>\n  collect() |>\n  pivot_longer(-month,\n    names_to = \"industry\", values_to = \"ret\"\n  ) |>\n  mutate(industry = as_factor(industry))\n\nWe combine all the monthly observations into one data frame.\n\ndata <- industries_ff_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  left_join(factors_q_monthly, by = \"month\") |>\n  left_join(macro_predictors, by = \"month\") |>\n  mutate(\n    ret = ret - factor_ff_rf\n  ) |>\n  select(month, industry, ret, everything()) |>\n  drop_na()\n\nOur data contains 22 columns of regressors with the 13 macro variables and 8 factor returns for each month. Figure 1 provides summary statistics for the 10 monthly industry excess returns in percent.\n\ndata |>\n  group_by(industry) |>\n  mutate(ret = ret) |>\n  ggplot(aes(x = industry, y = ret)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Excess return distributions by industry in percent\"\n  ) +\n  scale_y_continuous(\n    labels = percent\n  )\n\n\n\n\nFigure 1: The box plots show the monthly dispersion of returns for 10 different industries."
  },
  {
    "objectID": "factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "href": "factor-selection-via-machine-learning.html#the-tidymodels-workflow",
    "title": "Factor Selection via Machine Learning",
    "section": "The tidymodels Workflow",
    "text": "The tidymodels Workflow\nTo illustrate penalized linear regressions, we employ the tidymodels collection of packages for modeling and ML using tidyverse principles. You can simply use install.packages(\"tidymodels\") to get access to all the related packages. We recommend checking out the work of Kuhn and Silge (2018): They continuously write on their great book ‘Tidy Modeling with R’ using tidy principles.\nThe tidymodels workflow encompasses the main stages of the modeling process: pre-processing of data, model fitting, and post-processing of results. As we demonstrate below, tidymodels provides efficient workflows that you can update with low effort.\nUsing the ideas of Ridge and Lasso regressions, the following example guides you through (i) pre-processing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the “best” possible predictions.\nTo start, we restrict our analysis to just one industry: Manufacturing. We first split the sample into a training and a test set. For that purpose, tidymodels provides the function initial_time_split() from the rsample package (Silge et al. 2022). The split takes the last 20% of the data as a test set, which is not used for any model tuning. We use this test set to evaluate the predictive accuracy in an out-of-sample scenario.\n\nsplit <- initial_time_split(\n  data |>\n    filter(industry == \"Manuf\") |>\n    select(-industry),\n  prop = 4 / 5\n)\nsplit\n\n<Training/Testing/Total>\n<527/132/659>\n\n\nThe object split simply keeps track of the observations of the training and the test set. We can call the training set with training(split), while we can extract the test set with testing(split).\n\nPre-process data\nRecipes help you pre-process your data before training your model. Recipes are a series of pre-processing steps such as variable selection, transformation, or conversion of qualitative predictors to indicator variables. Each recipe starts with a formula that defines the general structure of the dataset and the role of each variable (regressor or dependent variable). For our dataset, our recipe contains the following steps before we fit any model:\n\nOur formula defines that we want to explain excess returns with all available predictors. The regression equation thus takes the form \\[r_{t} = \\alpha_0 + \\left(\\tilde f_t \\otimes \\tilde z_t\\right)B + \\varepsilon_t \\] where \\(r_t\\) is the vector of industry excess returns at time \\(t\\) and \\(\\tilde f_t\\) and \\(\\tilde z_t\\) are the (standardized) vectors of factor portfolio returns and macroeconomic variables\nWe exclude the column month from the analysis\nWe include all interaction terms between factors and macroeconomic predictors\nWe demean and scale each regressor such that the standard deviation is one\n\n\nrec <- recipe(ret ~ ., data = training(split)) |>\n  step_rm(month) |>\n  step_interact(terms = ~ contains(\"factor\"):contains(\"macro\")) |>\n  step_normalize(all_predictors()) |>\n  step_center(ret, skip = TRUE)\n\nA table of all available recipe steps can be found in the tidymodels documentation. As of 2023, more than 150 different processing steps are available! One important point: The definition of a recipe does not trigger any calculations yet but rather provides a description of the tasks to be applied. As a result, it is very easy to reuse recipes for different models and thus make sure that the outcomes are comparable as they are based on the same input. In the example above, it does not make a difference whether you use the input data = training(split) or data = testing(split). All that matters at this early stage are the column names and types.\nWe can apply the recipe to any data with a suitable structure. The code below combines two different functions: prep() estimates the required parameters from a training set that can be applied to other data sets later. bake() applies the processed computations to new data.\n\ndata_prep <- prep(rec, training(split))\n\nThe object data_prep contains information related to the different preprocessing steps applied to the training data: E.g., it is necessary to compute sample means and standard deviations to center and scale the variables.\n\ndata_bake <- bake(data_prep,\n  new_data = testing(split)\n)\ndata_bake\n\n# A tibble: 132 × 126\n  factor_ff…¹ facto…² facto…³ facto…⁴ facto…⁵ facto…⁶ facto…⁷ facto…⁸\n        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1       -1.80 1.37      0.139   1.14   0.0289   1.33  -1.63    -1.66 \n2       -1.80 0.333    -0.833   0.121 -0.745    0.450 -0.0117  -1.19 \n3       -1.80 0.656     0.399   0.297  0.327    0.510 -0.860   -0.905\n4       -1.80 0.00426   0.717  -0.756  0.470   -0.296 -0.590   -0.324\n5       -1.84 0.529    -0.177  -0.972 -0.390   -0.733  0.383    0.102\n# … with 127 more rows, 118 more variables: macro_dp <dbl>,\n#   macro_dy <dbl>, macro_ep <dbl>, macro_de <dbl>,\n#   macro_svar <dbl>, macro_bm <dbl>, macro_ntis <dbl>,\n#   macro_tbl <dbl>, macro_lty <dbl>, macro_ltr <dbl>,\n#   macro_tms <dbl>, macro_dfy <dbl>, macro_infl <dbl>, ret <dbl>,\n#   factor_ff_rf_x_macro_dp <dbl>, factor_ff_rf_x_macro_dy <dbl>,\n#   factor_ff_rf_x_macro_ep <dbl>, factor_ff_rf_x_macro_de <dbl>, …\n\n\nNote that the resulting data contains the 132 observations from the test set and 126 columns. Why so many? Recall that the recipe states to compute every possible interaction term between the factors and predictors, which increases the dimension of the data matrix substantially.\nYou may ask at this stage: why should I use a recipe instead of simply using the data wrangling commands such as mutate() or select()? tidymodels beauty is that a lot is happening under the hood. Recall, that for the simple scaling step, you actually have to compute the standard deviation of each column, then store this value, and apply the identical transformation to a different dataset, e.g., testing(split). A prepped recipe stores these values and hands them on once you bake() a novel dataset. Easy as pie with tidymodels, isn’t it?\n\n\nBuild a model\n Next, we can build an actual model based on our pre-processed data. In line with the definition above, we estimate regression coefficients of a Lasso regression such that we get \\[\\begin{aligned}\\hat\\beta_\\lambda^\\text{Lasso} = \\arg\\min_\\beta \\left(Y - X\\beta\\right)'\\left(Y - X\\beta\\right) + \\lambda\\sum\\limits_{k=1}^K|\\beta_k|.\\end{aligned}\\] We want to emphasize that the tidymodels workflow for any model is very similar, irrespective of the specific model. As you will see further below, it is straightforward to fit Ridge regression coefficients and - later - Neural networks or Random forests with basically the same code. The structure is always as follows: create a so-called workflow() and use the fit() function. A table with all available model APIs is available here. For now, we start with the linear regression model with a given value for the penalty factor \\(\\lambda\\). In the setup below, mixture denotes the value of \\(\\rho\\), hence setting mixture = 1 implies the Lasso.\n\nlm_model <- linear_reg(\n  penalty = 0.0001,\n  mixture = 1\n) |>\n  set_engine(\"glmnet\", intercept = FALSE)\n\nThat’s it - we are done! The object lm_model contains the definition of our model with all required information. Note that set_engine(\"glmnet\") indicates the API character of the tidymodels workflow: Under the hood, the package glmnet is doing the heavy lifting, while linear_reg() provides a unified framework to collect the inputs. The workflow ends with combining everything necessary for the serious data science workflow, namely, a recipe and a model.\n\nlm_fit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(lm_model)\nlm_fit\n\n══ Workflow ═════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ─────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_interact()\n• step_normalize()\n• step_center()\n\n── Model ────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-04\n  mixture = 1\n\nEngine-Specific Arguments:\n  intercept = FALSE\n\nComputational engine: glmnet \n\n\n\n\nFit a model\nWith the workflow from above, we are ready to use fit(). Typically, we use training data to fit the model. The training data is pre-processed according to our recipe steps, and the Lasso regression coefficients are computed. First, we focus on the predicted values \\(\\hat{y}_t = x_t\\hat\\beta^\\text{Lasso}.\\) Figure 2 illustrates the projections for the entire time series of the manufacturing industry portfolio returns. The grey area indicates the out-of-sample period, which we did not use to fit the model.\n\npredicted_values <- lm_fit |>\n  fit(data = training(split)) |>\n  predict(data |> filter(industry == \"Manuf\")) |>\n  bind_cols(data |> filter(industry == \"Manuf\")) |>\n  select(month,\n    \"Fitted value\" = .pred,\n    \"Realization\" = ret\n  ) |>\n  pivot_longer(-month, names_to = \"Variable\")\n\n\npredicted_values |>\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = Variable,\n    linetype = Variable\n    )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    linetype = NULL,\n    title = \"Monthly realized and fitted manufacturing industry risk premia\"\n  ) +\n  scale_x_date(\n    breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"5 years\"\n      )\n    },\n    minor_breaks = function(x) {\n      seq.Date(\n        from = min(x),\n        to = max(x),\n        by = \"1 years\"\n      )\n    },\n    expand = c(0, 0),\n    labels = date_format(\"%Y\")\n  ) +\n  scale_y_continuous(\n    labels = percent\n  ) +\n  annotate(\"rect\",\n    xmin = testing(split) |> pull(month) |> min(),\n    xmax = testing(split) |> pull(month) |> max(),\n    ymin = -Inf, ymax = Inf,\n    alpha = 0.5, fill = \"grey70\"\n  )\n\n\n\n\nFigure 2: The grey area corresponds to the out of sample period.\n\n\n\n\nWhat do the estimated coefficients look like? To analyze these values and to illustrate the difference between the tidymodels workflow and the underlying glmnet package, it is worth computing the coefficients \\(\\hat\\beta^\\text{Lasso}\\) directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample. Note that glmnet actually takes a vector y and the matrix of regressors \\(X\\) as input. Moreover, glmnet requires choosing the penalty parameter \\(\\alpha\\), which corresponds to \\(\\rho\\) in the notation above. When using the tidymodels model API, such details do not need consideration.\n\nx <- data_bake |>\n  select(-ret) |>\n  as.matrix()\ny <- data_bake |> pull(ret)\n\nfit_lasso <- glmnet(\n  x = x,\n  y = y,\n  alpha = 1,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nfit_ridge <- glmnet(\n  x = x,\n  y = y,\n  alpha = 0,\n  intercept = FALSE,\n  standardize = FALSE,\n  lambda.min.ratio = 0\n)\n\nThe objects fit_lasso and fit_ridge contain an entire sequence of estimated coefficients for multiple values of the penalty factor \\(\\lambda\\). Figure 3 illustrates the trajectories of the regression coefficients as a function of the penalty factor. Both Lasso and Ridge coefficients converge to zero as the penalty factor increases.\n\nbind_rows(\n  tidy(fit_lasso) |> mutate(Model = \"Lasso\"),\n  tidy(fit_ridge) |> mutate(Model = \"Ridge\")\n) |>\n  rename(\"Variable\" = term) |>\n  ggplot(aes(x = lambda, y = estimate, color = Variable)) +\n  geom_line() +\n  scale_x_log10() +\n  facet_wrap(~Model, scales = \"free_x\") +\n  labs(\n    x = \"Penalty factor (lambda)\", y = NULL,\n    title = \"Estimated coefficient paths for different penalty factors\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 3: The penalty parameters are chosen iteratively to resemble the path from no penalization to a model that excludes all variables.\n\n\n\n\n\nOne word of caution: The package glmnet computes estimates of the coefficients \\(\\hat\\beta\\) based on numerical optimization procedures. As a result, the estimated coefficients for the special case with no regularization (\\(\\lambda = 0\\)) can deviate from the standard OLS estimates.\n\n\n\nTune a model\nTo compute \\(\\hat\\beta_\\lambda^\\text{Lasso}\\) , we simply imposed a value for the penalty hyperparameter \\(\\lambda\\). Model tuning is the process of optimally selecting such hyperparameters. tidymodels provides extensive tuning options based on so-called cross-validation. Again, we refer to any treatment of cross-validation to get a more detailed discussion of the statistical underpinnings. Here we focus on the general idea and the implementation with tidymodels.\nThe goal for choosing \\(\\lambda\\) (or any other hyperparameter, e.g., \\(\\rho\\) for the Elastic Net) is to find a way to produce predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the mean squared prediction error \\(\\text{MSPE} = E\\left( \\frac{1}{T}\\sum_{t=1}^T (\\hat{y}_t - y_t)^2 \\right)\\). Unfortunately, the MSPE is not directly observable. We can only compute an estimate because our data is random and because we do not observe the entire population.\nObviously, if we train an algorithm on the same data that we use to compute the error, our estimate \\(\\hat{\\text{MSPE}}\\) would indicate way better predictive accuracy than what we can expect in real out-of-sample data. The result is called overfitting.\nCross-validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many MSPE obtained by creating predictions for \\(K\\) new random samples of the data, none of them used to train the algorithm \\(\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k - y_t^k\\right)^2\\). In practice, this is done by carving out a piece of our data and pretending it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the optimal hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g., \\(\\lambda\\)), and evaluate the predictive accuracy on an independent sample. This works as follows:\n\nSpecify a grid of hyperparameters\nObtain predictors \\(\\hat{y}_i(\\lambda)\\) to denote the predictors for the used parameters \\(\\lambda\\)\nCompute \\[\n\\text{MSPE}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{T}\\sum_{t=1}^T \\left(\\hat{y}_t^k(\\lambda) - y_t^k\\right)^2\n\\] With K-fold cross-validation, we do this computation \\(K\\) times. Simply pick a validation set with \\(M=T/K\\) observations at random and think of these as random samples \\(y_1^k, \\dots, y_{\\tilde{T}}^k\\), with \\(k=1\\)\n\nHow should you pick \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original data. However, larger values of \\(K\\) will have much higher computation time. tidymodels provides all required tools to conduct \\(K\\)-fold cross-validation. We just have to update our model specification and let tidymodels know which parameters to tune. In our case, we specify the penalty factor \\(\\lambda\\) as well as the mixing factor \\(\\rho\\) as free parameters. Note that it is simple to change an existing workflow with update_model().\n\nlm_model <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit |>\n  update_model(lm_model)\n\nFor our sample, we consider a time-series cross-validation sample. This means that we tune our models with 20 random samples of length five years with a validation period of four years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate \\(\\hat{\\text{MSPE}}\\) in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.\n\ndata_folds <- time_series_cv(\n  data        = training(split),\n  date_var    = month,\n  initial     = \"5 years\",\n  assess      = \"48 months\",\n  cumulative  = FALSE,\n  slice_limit = 20\n)\n\ndata_folds\n\n# Time Series Cross Validation Plan \n# A tibble: 20 × 2\n  splits          id     \n  <list>          <chr>  \n1 <split [60/48]> Slice01\n2 <split [60/48]> Slice02\n3 <split [60/48]> Slice03\n4 <split [60/48]> Slice04\n5 <split [60/48]> Slice05\n# … with 15 more rows\n\n\nThen, we evaluate the performance for a grid of different penalty values. tidymodels provides functionalities to construct a suitable grid of hyperparameters with grid_regular. The code chunk below creates a \\(10 \\times 3\\) hyperparameters grid. Then, the function tune_grid() evaluates all the models for each fold.\n\nlm_tune <- lm_fit |>\n  tune_grid(\n    resample = data_folds,\n    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),\n    metrics = metric_set(rmse)\n  )\n\nAfter the tuning process, we collect the evaluation metrics (the root mean-squared error in our example) to identify the optimal model. Figure 4 illustrates the average validation set’s root mean-squared error for each value of \\(\\lambda\\) and \\(\\rho\\).\n\nautoplot(lm_tune) + \n  aes(linetype = `Proportion of Lasso Penalty`) + \n  guides(linetype = \"none\") +\n  labs(\n    x = \"Penalty factor (lambda)\",\n    y = \"Root MSPE\",\n    title = \"Root MSPE for different penalty factors\"\n  )\n\n\n\n\nFigure 4: Evaluation of manufacturing excess returns for different penalty factors (lambda) and proportions of Lasso penalty (rho). 1.0 indicates Lasso, 0.5 indicates Elastic Net, and 0.0 indicates Ridge.\n\n\n\n\nFigure 4 shows that the cross-validated MSPE drops for Lasso and Elastic Net and spikes afterward. For Ridge regression, the MSPE increases above a certain threshold. Recall that the larger the regularization, the more restricted the model becomes. Thus, we would choose the model with the lowest MSPE.\n\n\nParallelized workflow\nOur starting point was the question: Which factors determine industry returns? While Avramov et al. (2022) provide a Bayesian analysis related to the research question above, we choose a simplified approach: To illustrate the entire workflow, we now run the penalized regressions for all ten industries. We want to identify relevant variables by fitting Lasso models for each industry returns time series. More specifically, we perform cross-validation for each industry to identify the optimal penalty factor \\(\\lambda\\). Then, we use the set of finalize_*()-functions that take a list or tibble of tuning parameter values and update objects with those values. After determining the best model, we compute the final fit on the entire training set and analyze the estimated coefficients.\nFirst, we define the Lasso model with one tuning parameter.\n\nlasso_model <- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |>\n  set_engine(\"glmnet\")\n\nlm_fit <- lm_fit |>\n  update_model(lasso_model)\n\nThe following task can be easily parallelized to reduce computing time substantially. We use the parallelization capabilities of furrr. Note that we can also just recycle all the steps from above and collect them in a function.\n\nselect_variables <- function(input) {\n  # Split into training and testing data\n  split <- initial_time_split(input, prop = 4 / 5)\n\n  # Data folds for cross-validation\n  data_folds <- time_series_cv(\n    data = training(split),\n    date_var = month,\n    initial = \"5 years\",\n    assess = \"48 months\",\n    cumulative = FALSE,\n    slice_limit = 20\n  )\n\n  # Model tuning with the Lasso model\n  lm_tune <- lm_fit |>\n    tune_grid(\n      resample = data_folds,\n      grid = grid_regular(penalty(), levels = c(10)),\n      metrics = metric_set(rmse)\n    )\n\n  # Identify the best model and fit with the training data\n  lasso_lowest_rmse <- lm_tune |> select_by_one_std_err(\"rmse\")\n  lasso_final <- finalize_workflow(lm_fit, lasso_lowest_rmse)\n  lasso_final_fit <- last_fit(lasso_final, split, metrics = metric_set(rmse))\n\n  # Extract the estimated coefficients\n  estimated_coefficients <- lasso_final_fit |>\n    extract_fit_parsnip() |>\n    tidy() |>\n    mutate(\n      term = str_remove_all(term, \"factor_|macro_|industry_\")\n    )\n\n  return(estimated_coefficients)\n}\n\n# Parallelization\nplan(multisession, workers = availableCores())\n\n# Computation by industry\nselected_factors <- data |>\n  nest(data = -industry) |>\n  mutate(selected_variables = future_map(\n    data, select_variables,\n    .options = furrr_options(seed = TRUE)\n  ))\n\nWhat has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry, we did it for ten in parallel. The final option seed = TRUE is required to make the cross-validation process reproducible. Now, we just have to do some housekeeping and keep only variables that Lasso does not set to zero. We illustrate the results in a heat map in Figure 5.\n\nselected_factors |>\n  unnest(selected_variables) |>\n  filter(\n    term != \"(Intercept)\",\n    estimate != 0\n  ) |>\n  add_count(term) |>\n  mutate(\n    term = str_remove_all(term, \"NA|ff_|q_\"),\n    term = str_replace_all(term, \"_x_\", \" \"),\n    term = fct_reorder(as_factor(term), n),\n    term = fct_lump_min(term, min = 2),\n    selected = 1\n  ) |>\n  filter(term != \"Other\") |>\n  mutate(term = fct_drop(term)) |>\n  complete(industry, term, fill = list(selected = 0)) |>\n  ggplot(aes(industry,\n    term,\n    fill = as_factor(selected)\n  )) +\n  geom_tile() +\n  scale_x_discrete(guide = guide_axis(angle = 70)) +\n  scale_fill_manual(values = c(\"white\", \"grey30\")) +\n  theme(legend.position = \"None\") +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Selected variables for different industries\"\n  )\n\n\n\n\nFigure 5: Grey areas indicate that the estimated Lasso regression coefficient is not set to zero. White fields show which variables get assigned a value of exactly zero.\n\n\n\n\nThe heat map in Figure 5 conveys two main insights. First, we see a lot of white, which means that many factors, macroeconomic variables, and interaction terms are not relevant for explaining the cross-section of returns across the industry portfolios. In fact, only the market factor and the return-on-equity factor play a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While barely any variable is selected by Lasso for Utilities, many factors are selected for, e.g., High-Tech and Durable, but they do not coincide at all. In other words, there seems to be a clear picture that we do not need many factors, but Lasso does not provide a factor that consistently provides pricing abilities across industries."
  },
  {
    "objectID": "factor-selection-via-machine-learning.html#exercises",
    "href": "factor-selection-via-machine-learning.html#exercises",
    "title": "Factor Selection via Machine Learning",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that requires three inputs, namely, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and then returns the Ridge estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of penalization in a suitable figure.\nNow, write a function that requires three inputs, namely,y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and ’lambda` and then returns the Lasso estimator (a \\(K\\) vector) for a given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should indicate whether \\(X\\) contains a vector of ones as the first column, which should be exempt from the \\(L_1\\) penalty.\nAfter you understand what Ridge and Lasso regressions are doing, familiarize yourself with the glmnet() package’s documentation. It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients for Ridge and Lasso and for combinations, commonly called Elastic Nets."
  }
]