--- 
title: "Tidy Finance"
author: "Christoph Scheuch, Patrick Weiss and Stefan Voigt"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
github-repo: voigtstefan/tidy_finance
description: "Tidy Finance with R"
---

# Prerequisites

This set of exercises is written for the students of the lecture "Advanced Empirical Finance: Topics and Data Science".
The exercise sets partially subsume work I created with my colleagues Christoph Scheuch and Patrick Weiss for the book *Tidy Finance*. You are very welcome to give us feedback on *every* aspect of the book such that we can improve the codes, explanations and general structure. Please contact me if you spot typos, mistakes or other issues that deserve more attention. 

Needless to say, you should try to solve each question on your own before you refer to my proposed answers. Optimally, you discuss issues with your peers, try to find hints either in the lecture slides or online, There are many ways to get answer to your questions, most directly you can simply post your questions on StackExchange or Absalon. 

As an absolute minimum before trying to solve the following exercises, make sure you are familiar with Garrett Grolemund's and Hadley Wickham's excellent book [R for Data Science](https://r4ds.had.co.nz/). I will make further information or references available on Absalon. 

Welcome!

In the course "Advanced Empirical Finance" we repeatedly ask: (How) can state-of-the art methods improve financial decision-making?

While the lecture covers all relevant theoretical aspects and is based on very recent academic papers, you should spend most of your effort on this course on actually doing empirical work! Get your computer ready to work on real problems for financial applications and discuss your code with your peers to acquire the necessary skills to make a difference either in the Finance industry or academia. 

## Things to get done before the first lecture

To dive right into it, there are a couple of prerequisites you should get done before the first lecture:

- [Install R and RStudio](https://rstudio-education.github.io/hopr/starting.html#starting). To get a walk-through of the installation (for every major operating system), follow the steps outlined [in this summary](https://rstudio-education.github.io/hopr/starting.html#starting). Should be done in few clicks. If you wonder about the difference: R is an open-source  language and environment for statistical computing and graphics, free to download and use. While R runs computations, RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. I suggest you do all coding exclusively with RStudio during the course (and beyond).
- Open RStudio and  run  the  following  line  of  code: `install.packages("tidyverse")`. Not sure how it works? You find helpful information on how to install packages in this [brief summary](https://rstudio-education.github.io/hopr/packages2.html). The tidyverse is “a framework for managing data that aims at making the cleaning and preparing steps much easier”. We are going to work almost exclusively with tidyverse packages.
- Check if everything works: Open Rstudio and type `library(tidyverse)`. You should then get a message like in the picture below.  Works? Done with the software setup!  

## What is R?

In this course we work with R. Together with Python, R is nowadays the de facto standard tool in finance and is unparalleled when it comes to handle data science applications.

*But so far I learned to code with Python, Ox, Matlab, …*

Excellent if you have prior knowledge in another coding language. Your start with R will be way easier. Also, as a Python user you can actually run your Python code in R (and vice versa) - so if you have already written some analysis for a prior project, no need to translate everything! If you are interested to do this at some point in time, check out this resource (we will provide a brief TA session on cross-platform operability later in the course).

*I have no experience whatsoever with programming*

Great, you nevertheless make the bar for the course requirements. We will start entirely from scratch (but with a really steep learning curve). Besides the applications in Finance, we will focus on state-of-the-art data science concepts that cover the entire life cycle of a successful empirical analysis. No matter if we consider a research project, a thesis, seminar paper or a thorough portfolio back testing procedure at an investment bank: the challenges are always similar. The figure below illustrates the components we will talk about: How do we get R connected to our datasets? How can we clean the data such that our analysis makes sense? How to visualize or change variables in order to estimate parameters or make financial decisions? And, finally, how can we communicate our results such that we are sure that our analysis is reliable, correct and flexible enough to quickly find out what happens if we have to change some input parameters.

*This sounds like a lot of work on the data science front. How do I get started?*

There are a couple of ways for you to get started, below a list with relevant material and suggestions.

1. As a member of this course you have **free access to all online tutorials of DataCamp** which provides excellent browser-based tutorials on the basics of R, the tidyverse and a lot of applications in Finance. In order to enjoy the benefits of completely free access, sign up to our own course site using [this link](https://www.datacamp.com/users/sign_up?group_invite=true&group_name=Advanced+Empirical+Finance%3A+Topics+and+Data+Science&group_type=academic&policy_version=&redirect=https%3A%2F%2Fwww.datacamp.com%2Fgroups%2Fshared_links%2F5942f03f4aed010fb1498f59b2613587bc1eae4c8410835827a65b4049731a22%3Fdc_referrer%3Dmain-group%26tos%3Dtrue) (careful: registration is only possible with an econ.ku.dk mail domain)
1. Only way to learn how to code is by getting your hands dirty. Make sure your system is ready to go by completing all technical requirements.
1. A very gentle and good introduction into the workings of R can be found [here](https://rstudio-education.github.io/hopr/project-1-weighted-dice.html). Once you are done with setting up R on your machine, try to follow the "weighted dice project".
1. The main book of this course is available online and for free: [R for Data Science](https://r4ds.had.co.nz/introduction.html) by Hadley Wickham and Garrett Grolemund explains everything we need. Throughout the course we will cover almost the entire content of the book. I advise you to read the first 3 Chapters for a gentle dive into the material before the lectures starts.

*Help, I am stuck! Where do I get help?*

To struggle in the beginning is absolutely fine, and you are not alone. Indeed, there are around 40 other students in the course that have similar questions. Make use of this community!

1. You can always post questions in the Discussion forum. The TA's, your colleagues and I will try to help you out.
1. You do not have to do all the work on your own. Search for a coding buddy to discuss your problems in teams (for the assignments you can hand-in in groups of up to 3 students!).
1. If you still feel like you are stuck (which is fine and can happen) reach out directly in class or during the office hour!

<!--chapter:end:index.Rmd-->

# Introduction to Tidy Finance

The main aim of this chapter is to familiarize yourself with the `tidyverse`. We start out by downloading and visualizing stock data before we move to a simple portfolio choice problem. These examples introduce you to our approach of *tidy finance*.

## Download and work with stock market data {#stock_market_data}

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = TRUE,
  cache = FALSE,
  fig.width = 8,
  fig.height = 4,
  fig.align = "center"
)

options(tibble.print_max = 4, 
        tibble.print_min = 4)
```

To download price data you can use the convenient `tidyquant`package. 
If you have trouble using `tidyquant`, check out the [documentation](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ01-core-functions-in-tidyquant.html#yahoo-finance). Start the session by loading the `tidyverse` and the `tidyquant` package as shown below. 

```{r}
# install.packages("tidyverse")
# install.packages("tidyquant")
library(tidyverse)
library(tidyquant)
```

We start and download daily prices for one stock market ticker, e.g. *AAPL*, directly from data provider Yahoo!Finance. To download the data you can use the command `tq_get`. If you do not know how to use it, make sure you read the help file by calling `?tq_get`. We especially recommended to take a look in the examples section in the documentation. 

```{r}
prices <- tq_get("AAPL", get = "stock.prices")
prices %>% head() # Take a glimpse on the data
```

`tq_get` downloads stock market data from Yahoo!Finance if you do not specify another data source. The function returns a tibble with 8 quite self-explanatory columns: *symbol*, *date*, the market prices at the *open, high, low* and *close*, the daily *volume* (in number of shares) and the *adjusted* price in USD which factors in anything that might affect the stock price after the market closes, e.g. stock splits, repurchases and dividends.  

Next, we use `ggplot` to visualize the time series of adjusted prices. 
```{r}
prices %>% # Simple visualization of the downloaded price time series
  ggplot(aes(x = date, y = adjusted)) +
  geom_line() +
  labs(
    x = NULL,
    y = NULL,
    title = "AAPL stock prices",
    subtitle = "Prices in USD, adjusted for dividend payments and stock splits"
  ) +
  theme_bw()
```
Next, we compute daily returns defined as $(p_t - p_{t-1}) / p_{t-1}$ where $p_t$ is the adjusted day $t$ price. 
```{r}
returns <- prices %>%
  arrange(date) %>%
  mutate(ret = adjusted / lag(adjusted) - 1) %>%
  select(symbol, date, ret)
returns %>% head()
```

The resulting tibble contains three columns where the last contains the daily returns. Note that the first entry naturally contains `NA` because there is no leading price. Note also that the computations require that the time series is ordered by date - otherwise, `lag` would be meaningless.
For the upcoming examples we remove missing values as these would require careful treatment when computing, e.g., sample averages. In general, however, make sure you understand why `NA` values occur and if you can simply get rid of these observations. 

```{r}
returns <- returns %>%
  drop_na(ret)
```
Next, we visualize the distribution of daily returns in a histogram. Just for fun, we also add a dashed red line that indicates the 5\% quantile of the daily returns to the histogram - this value is a (crude) proxy for the worst return of the stock with a probability of at least 5\%. 
```{r}
quantile_05 <- quantile(returns %>% pull(ret), 0.05) # Compute the 5 % quantile of the returns

returns %>% # create a histogram for daily returns
  ggplot(aes(x = ret)) +
  geom_histogram(bins = 100) +
  geom_vline(aes(xintercept = quantile_05),
    color = "red",
    linetype = "dashed"
  ) +
  labs(
    x = NULL, y = NULL,
    title = "Distribution of daily AAPL returns (in percent)",
    subtitle = "The dotted vertical line indicates the historical 5% quantile"
  ) +
  theme_bw()
```

Here, `bins = 100` determines the number of bins and hence implicitly the width of the bins. Before proceeding, make sure you understand how to use the geom `geom_vline()` to add a dotted red line that indicates the 5\% quantile of the daily returns. 
A typical task before proceeding with *any* data is to compute summary statistics for the main variables of interest. 

```{r}
returns %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    )
  )) %>%
  kableExtra::kable(digits = 3)
```

```{r}
# Alternatively: compute summary statistics for each year
returns %>%
  group_by(year = year(date)) %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    )
  )) %>%
  kableExtra::kable(digits = 3)
```

## Scale the analysis up: tidyverse-magic
As a next step, we want to take the code from before and generalize it such that all the computations are performed for an arbitrary vector of tickers or even for all stocks that represent an index. Following tidy principles, it turns out to be actually quite easy to automate the download, generate the plot of the price time series and the table of summary statistics for an arbitrary number of assets.

This is where the `tidyverse` magic starts: tidy data and a tidy workflow make it extremely easy to generalize the computations from before to as many assets you like. The following code takes any vector of tickers, e.g., `ticker <- c("AAPL", "MMM", "BA")`, and automates the download as well as the plot of the price time series. At the end, we create the table of summary statistics for an arbitrary number of assets. 

Figure @ref(fig:prices) illustrates the time series of downloaded *adjusted* prices for each of the 30 constituents of the Dow Jones index. Make sure you understand every single line of code! (What is the purpose of `%>%`? What are the arguments of `aes()`? Which alternative *geoms* could you use to visualize the time series? Hint: if you do not know the answers try to change the code to see what difference your intervention causes). 

```{r fig:prices}
ticker <- tq_index("DOW") # tidyquant delivers all constituents of the Dow Jones index
index_prices <- tq_get(ticker,
  get = "stock.prices",
  from = "2000-01-01"
) %>% # Exactly the same code as in the first part
  filter(symbol != "DOW") # Exclude the index itself

index_prices <- index_prices %>% # Remove assets that did not trade since January 1st 2000
  group_by(symbol) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  filter(n == max(n)) %>%
  select(-n)

index_prices %>%
  ggplot(aes(
    x = date,
    y = adjusted,
    color = symbol
  )) +
  geom_line() +
  labs(
    x = NULL,
    y = NULL,
    color = NULL,
    title = "DOW index stock prices",
    subtitle = "Prices in USD, adjusted for dividend payments and stock splits"
  ) +
  theme_bw() +
  theme(legend.position = "none")
```

Do you notice the small differences relative to the code we used before? `tq_get(ticker)` is able to return a tibble for several symbols as well. All we need to do to illustrate all tickers instead of only one is to include `color = symbol` in the `ggplot` aesthetics. In this way, we can generate a separate line for each ticker.

The same holds for returns as well. Before computing returns as before, we use `group_by(symbol)` such that the `mutate` command is performed for each symbol individually. Exactly the same logic applies for the computation of summary statistics: `group_by(symbol)` is the key to aggregate the time series into ticker-specific variables of interest. 

```{r}
all_returns <- index_prices %>%
  group_by(symbol) %>% # we perform the computations per symbol
  mutate(ret = adjusted / lag(adjusted) - 1) %>%
  select(symbol, date, ret) %>%
  drop_na(ret)

all_returns %>%
  group_by(symbol) %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    )
  )) %>%
  kableExtra::kable(digits = 3)
```

Note that you are now also equipped with all tools to download price data for *each* ticker listed in the SP500 index with the same number of lines of code. Just use `ticker <- tq_index("SP500")` which provides you with a tibble that contains each symbol that is (currently) part of the SP500. However, don't try this if you are not prepared to wait for a couple of minutes.

Sometimes, aggregation across other variables than `symbol` makes sense as well. For instance, suppose you are interested in the question: are days with high aggregate trading volume followed by high aggregate trading volume days? To provide some initial analysis on this question we take the downloaded tibble with prices and compute aggregate daily trading volume for all Dow Jones constituents in USD. Recall that the column *volume* is denoted in the number of traded shares. We multiply the trading volume with the daily closing price to get a measure of the aggregate trading volume in USD. Scaling by `1e9` denotes daily trading volume in billion USD.  

```{r}
volume <- index_prices %>%
  mutate(volume_usd = volume * close / 1e9) %>%
  group_by(date) %>%
  summarise(volume = sum(volume_usd))

volume %>% # Plot the time series of aggregate trading volume
  ggplot(aes(x = date, y = volume)) +
  geom_line() +
  labs(
    x = NULL, y = NULL,
    title = "Aggregate trading volume (billion USD)"
  ) +
  theme_bw()
```

One way to illustrate the persistence of trading volume would be to plot volume on day $t$ against volume on day $t-1$ as in the example below:

```{r aggregate-volume}
volume %>%
  ggplot(aes(x = lag(volume), y = volume)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1), linetype = "dotted") +
  labs(
    x = "Previous day aggregate trading volume (billion USD)",
    y = "Aggregate trading volume (billion USD)",
    title = "Persistence of trading volume"
  ) +
  theme_bw() +
  theme(legend.position = "None")
```

Do you understand where the warning `## Warning: Removed 1 rows containing missing values (geom_point).
` comes from and what it means? Pure eye-balling reveals that days with high trading volume are often followed by similarly high trading volume days.  

## Portfolio choice problems

A very typically question in Finance is how to optimally allocate wealth across different assets. The standard framework for optimal portfolio selection is based on investors that dislike portfolio return volatility and like higher expected returns: the mean-variance investor. An essential tool to evaluate portfolios is the efficient frontier, the set of portfolios which satisfy the condition that no other portfolio exists with a higher expected return but with the same standard deviation of return (i.e., the risk). Let us compute and visualize the efficient frontier for a number of stocks. First, we use our dataset to compute the *monthly* returns for each asset. 

```{r}
returns <- index_prices %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(symbol, month) %>%
  summarise(price = last(adjusted), .groups = "drop_last") %>%
  mutate(ret = price / lag(price) - 1) %>%
  drop_na(ret) %>%
  select(-price)
```

Next, we transform the returns from a tidy tibble into a $(T \times N)$ matrix with one column for each ticker to compute the covariance matrix $\Sigma$ and also the expected return vector $\mu$.
We compute the vector of sample average returns and the sample variance covariance matrix. 

```{r}
returns_matrix <- returns %>%
  pivot_wider(
    names_from = symbol,
    values_from = ret
  ) %>%
  select(-month)

sigma <- cov(returns_matrix)
mu <- colMeans(returns_matrix)
```

Then, we compute the minimum variance portfolio weight $\omega_\text{mvp}$ as well as the expected return $\omega_\text{mvp}'\mu$ and volatility $\sqrt{\omega_\text{mvp}'\Sigma\omega_\text{mvp}}$ of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to 
$$\arg\min w'\Sigma w \text{ s.t. } \sum\limits_{i=1}^Nw_i = 1.$$
It is easy to show analytically, that $\omega_\text{mvp} = \frac{\Sigma^{-1}\iota}{\iota'\Sigma^{-1}\iota}$ where $\iota$ is a vector of ones.

```{r}
N <- ncol(returns_matrix)
iota <- rep(1, N)
wmvp <- solve(sigma) %*% iota
wmvp <- wmvp / sum(wmvp)

c(t(wmvp) %*% mu, sqrt(t(wmvp) %*% sigma %*% wmvp)) # Expected return and volatility
```

Note that the *monthly* volatility of the minimum variance portfolio is of the same order of magnitude than the *daily* standard deviation of the individual components. Thus, the diversification benefits are tremendous!
Next we compute the efficient portfolio weights which achieves 3 times the expected return of the minimum variance portfolio. If you wonder where the solution $\omega_\text{eff}$ comes from: The efficient portfolio is chosen by an investor who aims to achieve minimum variance *given a minimum desired expected return* $\bar{\mu}$ such that her objective function is to choose $\omega_\text{eff}$ as the solution to
$$\arg\min w'\Sigma w \text{ s.t. } w'\iota = 1 \text{ and } \omega'\mu \geq \bar{\mu}.$$
The code below implements the analytic solution to this optimization problem, we encourage you to verify that it is correct. 

```{r}
# Compute efficient portfolio weights for given level of expected return
mu_bar <- 3 * t(wmvp) %*% mu # some benchmark return: 3 times the minimum variance portfolio expected return

C <- as.numeric(t(iota) %*% solve(sigma) %*% iota)
D <- as.numeric(t(iota) %*% solve(sigma) %*% mu)
E <- as.numeric(t(mu) %*% solve(sigma) %*% mu)

lambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))
weff <- wmvp + lambda_tilde / 2 * (solve(sigma) %*% mu - D / C * solve(sigma) %*% iota)
```

The two mutual fund theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio and the efficient portfolio for another required level of expected returns like above), we can characterize the entire efficient frontier by combining these two portfolios. This is done in the code below. Make sure to familiarize yourself with the inner workings of the `for` loop!.

```{r}
# Use the two mutual fund theorem
c <- seq(from = -0.4, to = 1.9, by = 0.01) # Some values for a linear combination of two efficient portfolio weights
res <- tibble(
  c = c,
  mu = NA,
  sd = NA
)

for (i in seq_along(c)) { # A for loop
  w <- (1 - c[i]) * wmvp + (c[i]) * weff # A portfolio of minimum variance and efficient portfolio
  res$mu[i] <- 12 * 100 * t(w) %*% mu # Portfolio expected return (annualized, in percent)
  res$sd[i] <- 12 * 10 * sqrt(t(w) %*% sigma %*% w) # Portfolio volatility (annualized, in percent)
}
```

Finally, it is simple to visualize everything within one, powerful figure using `ggplot2`. 
```{r}
# Visualize the efficient frontier
res %>%
  ggplot(aes(x = sd, y = mu)) +
  geom_point() + # Plot all sd/mu portfolio combinations
  geom_point(
    data = res %>% filter(c %in% c(0, 1)),
    color = "red",
    size = 4
  ) + # locate the minimum variance and efficient portfolio
  geom_point(
    data = tibble(mu = 12 * 100 * mu, sd = 12 * 10 * sqrt(diag(sigma))),
    aes(y = mu, x = sd), color = "blue", size = 1
  ) + # locate the individual assets
  theme_bw() + # make the plot a bit nicer
  labs(
    x = "Annualized standard deviation (in percent)",
    y = "Annualized expected return (in percent)",
    title = "Dow Jones asset returns and efficient frontier",
    subtitle = "Red dots indicate the location of the minimum variance and efficient tangency portfolio"
  )
```
The black efficient frontier indicates the set of portfolio a mean-variance efficient investor would choose from. Compare the performance relative to the individual assets (the blue dots) - it should become clear that diversifying yields massive performance gains (at least as long as we take the parameters $\Sigma$ and $\mu$ as given).

<!--chapter:end:10_introduction.Rmd-->

# Accessing & Managing Financial Data

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = TRUE,
  fig.width = 8,
  fig.height = 4,
  fig.align = "center"
)
```

In this chapter, we propose a way to organize your financial data. Everybody, who has experience with data, is familiar with storing data in the form of various data formats like CSV, XLS, XLSX or other delimited value stores. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects, as well as across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages. 

This chapter shows how to import different data sets (our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard .csv files, an .xlsx file stored in a public Google drive repositories and an SQL database connection). We store all the data in **one** database which makes it easy to retrieve and share data later on. 

First, we load the global packages that we use throughout this chapter. We load more more packages in the section where they are needed later on. 
```{r}
library(tidyverse)
library(lubridate)
library(scales)
```
Moreover, we initially define the date range for which we will fetch and store the financial data. In case you need another time frame, you simply need to adjust these dates. Our data starts with 1960 since most asset pricing studies use data from 1962 on. 
```{r}
start_date <- as.Date("1960-01-01")
end_date <- as.Date("2020-12-31")
```

## Downloading Fama-French Data

We start by downloading some famous Fama-French factors and portfolios which are commonly used in empirical asset pricing. Fortunately, there is a neat package by [Nelson Areal](https://github.com/nareal/frenchdata/) that allows us to easily access the data: the `frenchdata` package provides functions to download and read data sets from [Prof. Kenneth French finance data library](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html).
```{r}
library(frenchdata)
```
We can use the main function of the package to download monthly Fama-French factors. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately as the raw Fama-French data comes in very unpractical data format. For precise descriptions of the variables we suggest you consult [Prof. Kenneth French finance data library](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) directly.  
```{r}
factors_ff_monthly <- download_french_data("Fama/French 3 Factors")$subsets$data[[1]] %>%
  transmute(
    month = floor_date(ymd(paste0(date, "01")), "month"),
    rf = as.numeric(RF) / 100,
    mkt_excess = as.numeric(`Mkt-RF`) / 100,
    smb = as.numeric(SMB) / 100,
    hml = as.numeric(HML) / 100
  ) %>%
  filter(month >= start_date & month <= end_date)
```
With the same function, it is straight-forward to also download the corresponding daily Fama-French factors. 
```{r}
factors_ff_daily <- download_french_data("Fama/French 3 Factors [Daily]")$subsets$data[[1]] %>%
  transmute(
    date = ymd(date),
    rf = as.numeric(RF) / 100,
    mkt_excess = as.numeric(`Mkt-RF`) / 100,
    smb = as.numeric(SMB) / 100,
    hml = as.numeric(HML) / 100
  ) %>%
  filter(date >= start_date & date <= end_date)
```
In a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too. 
```{r}
industries_ff_monthly <- download_french_data("10 Industry Portfolios")$subsets$data[[1]] %>%
  mutate(month = floor_date(ymd(paste0(date, "01")), "month")) %>%
  mutate(across(where(is.numeric), ~ . / 100)) %>%
  select(month, everything(), -date) %>%
  filter(month >= start_date & month <= end_date)
```
It is worth take a look at all available portfolio return time series from Kenneth French homepage. The package makes this easy by calling `frenchdata::get_french_data_list()`.

## The q-factors: download and read-in .csv files
In recent years, the academic discourse experienced the rise of alternative factor portfolios, e.g. in the form of the Hou, Xue, Zhang (2015) **q**-factor model. We refer to the [extended background](http://global-q.org/background.html) information provided by the original authors for further information. The **q** factor returns can be downloaded directly from the authors homepage from within `read_csv()`.
```{r}
factors_q_monthly <- read_csv("http://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2020.csv") %>%
  mutate(month = as.Date(paste(year, month, "01", sep = "-"))) %>%
  select(-R_F, -R_MKT, -year) %>%
  rename_with(~ gsub("R_", "", .)) %>%
  rename_with(~ str_to_lower(.)) %>%
  mutate(across(-month, ~ . / 100)) %>%
  filter(month >= start_date & month <= end_date)
```

## Macroeconomic predictors: Goyal-Welchs .xlsx files
Our next data source is a set of macro variables that are often used as predictors for the equity premium. [Goyal & Welch (2007)](https://academic.oup.com/rfs/article-abstract/21/4/1455/1565737) comprehensively reexamine the performance of variables that have been suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2020 on [Amit Goyal's website](https://sites.google.com/view/agoyal145). Since the data is an XLSX file stored in a public Google drive location, we need additional packages to access the data directly from our R session. 
```{r}
library(readxl)
library(googledrive)
```
Usually, you need to authenticate if you interact with Google drive directly in R. Since the data is stored via a public link, we can proceed without any authentication. 
```{r}
drive_deauth()
```
The `drive_download()` function from the `googledrive` package allows us to download the data and store it locally.
```{r}
drive_download("https://drive.google.com/file/d/1ACbhdnIy0VbCWgsnXkjcddiV8HF4feWv/view",
  path = "data/macro_predictors.xlsx",
  overwrite = TRUE
)
```
Next, we read in the data and transform the columns to the variables that we later use. 
```{r}
macro_predictors <- read_xlsx("data/macro_predictors.xlsx", sheet = "Monthly") %>%
  mutate(month = ym(yyyymm)) %>%
  filter(month >= start_date & month <= end_date) %>%
  mutate(across(where(is.character), as.numeric)) %>%
  mutate(
    IndexDiv = Index + D12,
    logret = log(IndexDiv) - log(lag(IndexDiv)),
    Rfree = log(Rfree + 1),
    rp_div = lead(logret - Rfree, 1), # Future excess market return
    dp = log(D12) - log(Index), # Dividend Price ratio
    dy = log(D12) - log(lag(Index)), # Dividend Yield
    ep = log(E12) - log(Index), # Earnings Price ratio
    de = log(D12) - log(E12), # Dividend Payout Ratio
    tms = lty - tbl, # Term Spread
    dfy = BAA - AAA
  ) %>% # Default yield spread
  select(month, rp_div, dp, dy, ep, de, svar,
    bm = `b/m`, ntis, tbl, lty, ltr,
    tms, dfy, infl
  ) %>%
  drop_na()
```
Finally, after reading in the macro predictors to our memory, we remove the raw data file from our temporary storage. 
```{r}
file.remove("data/macro_predictors.xlsx")
```

## Setting-Up a Database

Now that we have downloaded some data from the web into the memory of our R session, let us set up a database to store that information for future use. There are many ways to set-up and organize a database, depending on the use case. For our purpose, the most efficient ways is to use an [SQLite](https://www.sqlite.org/index.html) database which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that [SQL](https://en.wikipedia.org/wiki/SQL) (Structured Query Language) is a standard language for accessing and manipulating databases and it heavily inspired `dplyr` functions. We refer to [this tutorial](https://www.w3schools.com/sql/sql_intro.asp) for more information to SQL. 

There are two packages that make working with SQLite in R very simple: `RSQLite` embeds the SQLite database engine in R and `dbplyr` is the database back-end for `dplyr`. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting `dplyr` into SQL. Check out the [RSQLite](https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html) and [dbplyr vignettes](https://db.rstudio.com/databases/sqlite/) for more information.
```{r}
library(RSQLite)
library(dbplyr)
```
An SQLite database is super simple to create. The code below is really all there is. Note that we use the `extended_types` option to enable date types when storing and fetching data, otherwise date columns are stored as integer values. 
```{r}
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)
```
Next, we create a remote table with the monthly Fama-French factor data. 
```{r}
factors_ff_monthly %>%
  dbWriteTable(tidy_finance, "factors_ff_monthly", ., overwrite = TRUE)
```
We can use the remote table as if it were an in-memory data frame by building a connection via `tbl()`.
```{r}
factors_ff_monthly_db <- tbl(tidy_finance, "factors_ff_monthly")
```
All `dplyr` calls are evaluated lazily, that is the data is not in the memory of our R session and actually the database does most of the work. You see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.  
```{r}
factors_ff_monthly_db %>%
  select(month, rf)
```
If we want to have the whole table in memory, then we need to `collect()` it.
```{r}
factors_ff_monthly_db %>%
  select(month, rf) %>%
  collect()
```
The last couple of code chunks are really all there is to organize a simple database! You can also very easily share the SQLite database across devices and programming languages. 

Before we move on to the next data source, let us also store the other four tables in our new SQLite database. 
```{r}
factors_ff_daily %>%
  dbWriteTable(tidy_finance, "factors_ff_daily", ., overwrite = TRUE)

industries_ff_monthly %>%
  dbWriteTable(tidy_finance, "industries_ff_monthly", ., overwrite = TRUE)

factors_q_monthly %>%
  dbWriteTable(tidy_finance, "factors_q_monthly", ., overwrite = TRUE)

macro_predictors %>%
  dbWriteTable(tidy_finance, "macro_predictors", ., overwrite = TRUE)
```

From now on, all you need to do to access data which is stored in the database is to follow 3 steps: i) Establish the connection to the SQLite file, ii) call the file you want to extract and iii) collect it. For your convenience, the following steps show all you need in a compact fashion
```{r, results = FALSE}
# Minimal setup to load data into your R session memory from a fresh session
library(tidyverse)
library(RSQLite)
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE) # Connection sqlite file
dbListTables(tidy_finance) # Check in case you do not know the the tables names within the SQlite database
factors_q_monthly <- tbl(tidy_finance, "factors_q_monthly") # Call the desired file
factors_q_monthly <- factors_q_monthly %>% collect() # Collect the files
```

## Accessing WRDS
[Wharton Research Data Services (WRDS)](https://wrds-www.wharton.upenn.edu/) is the most widely used source for asset and firm-specific financial data used in an academic context. WRDS is a data platform that provides data validation, flexible delivery options and access to many different data sources. The data at WRDS is also organized in an SQL database, although they use the [PostgreSQL](https://www.postgresql.org/) engine. This database engine is just as easy to handle with R as SQLite. We use the `RPostgres` package to establish a connection to the WRDS database. Note that you could also use the `odbc` package to connect to a PostgreSQL database, but then you need to install appropriate drivers yourself. The `RPostgres` already contains a suitable driver. 
```{r}
library(RPostgres)
```
To establish a connection, you use the following function. Note that you need to replace the `user` and `password` fields with your own credentials. We defined system variables for the purpose of this book because we obviously do not want to share our credentials with the rest of the world. 
```{r}
wrds <- dbConnect(
  Postgres(),
  host = "wrds-pgdata.wharton.upenn.edu",
  dbname = "wrds",
  port = 9737,
  sslmode = "require",
  user = Sys.getenv("user"),
  password = Sys.getenv("password")
)
```

## Downloading and Preparing CRSP
[The Center for Research in Security Prices (CRSP)](https://crsp.org/) provides the most widely used data for US stocks. We use the `wrds` connection object that we just created to first access monthly CRSP return data. Actually, we need 3 tables to get the desired data: (i) the CRSP monthly security file,
```{r}
msf_db <- tbl(wrds, in_schema("crsp", "msf"))
msf_db
```
(ii) the identifying information,
```{r}
msenames_db <- tbl(wrds, in_schema("crsp", "msenames"))
msenames_db
```
and (iii) the delisting information.
```{r}
msedelist_db <- tbl(wrds, in_schema("crsp", "msedelist"))
msedelist_db
```
We use the three remote tables to fetch the data that we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest. You can read up in the great textbook of [Bali, Engel & Murray (2016)](https://www.wiley.com/en-us/Empirical+Asset+Pricing%3A+The+Cross+Section+of+Stock+Returns-p-9781118095041) (BEM) for an extensive discussion on the filters which we apply in the code below. 
```{r}
crsp_monthly <- msf_db %>%
  # Keep only data in time window of interest
  filter(date >= start_date & date <= end_date) %>%
  # Keep only relevant share codes
  inner_join(msenames_db %>%
    filter(shrcd %in% c(10, 11)) %>% # US listed stocks
    select(permno, exchcd, siccd, namedt, nameendt), by = c("permno")) %>%
  # Check that the information is valid
  filter(date >= namedt & date <= nameendt) %>%
  # Add delisting information (i.e. delisting reason and return) by month
  mutate(month = floor_date(date, "month")) %>%
  left_join(msedelist_db %>%
    select(permno, dlstdt, dlret, dlstcd) %>%
    mutate(month = floor_date(dlstdt, "month")), by = c("permno", "month")) %>%
  # Keep only variables of interest
  select(
    permno, # Security identifier
    date, # Date of the observation
    month, # Month of the observation
    ret, # Return
    shrout, # Shares outstanding (in thousands)
    altprc, # Last traded price in a month
    exchcd, # Exchange code
    siccd, # Industry code
    dlret, # Delisting return
    dlstcd # Delisting code
  ) %>%
  mutate(
    month = as.Date(month),
    shrout = shrout * 1000
  ) %>%
  collect()
```
Now, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters. 

The first additional variable we create is market capitalization (`mktcap`). Note that we keep market cap in millions of US dollars just for convenience (we do not want to print huge numbers in our figures and tables). 
```{r}
crsp_monthly <- crsp_monthly %>%
  mutate(
    mktcap = abs(shrout * altprc) / 1000000, # market cap in millions of dollars
    mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap)
  ) # 0 market cap makes conceptually no sense, so we set it to missing
```
The next variable that we frequently use is the one-month *lagged* market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolios, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data. 
```{r}
mktcap_lag <- crsp_monthly %>%
  mutate(month = month %m+% months(1)) %>% # Add one month (%m+% takes care of date subtleties)
  select(permno, month, mktcap_lag = mktcap)

crsp_monthly <- crsp_monthly %>%
  left_join(mktcap_lag, by = c("permno", "month"))
```
If you wonder why we not simple use the `lag()` function, e.g. via `crsp_monthly %>% group_by(permno) %>% mutate(mktcap_lag = lag(mktcap))` take a look at the exercises.
Next, we follow BEM in transforming listing exchange codes to explicit exchange names. 
```{r}
crsp_monthly <- crsp_monthly %>%
  mutate(exchange = case_when(
    exchcd %in% c(1, 31) ~ "NYSE",
    exchcd %in% c(2, 32) ~ "AMEX",
    exchcd %in% c(3, 33) ~ "NASDAQ",
    TRUE ~ "Other"
  ))
```
Similarly, we transform industry codes to industry descriptions following BEM.
```{r}
crsp_monthly <- crsp_monthly %>%
  mutate(industry = case_when(
    siccd >= 1 & siccd <= 999 ~ "Agriculture",
    siccd >= 1000 & siccd <= 1499 ~ "Mining",
    siccd >= 1500 & siccd <= 1799 ~ "Construction",
    siccd >= 2000 & siccd <= 3999 ~ "Manufacturing",
    siccd >= 4000 & siccd <= 4899 ~ "Transportation",
    siccd >= 4900 & siccd <= 4999 ~ "Utilities",
    siccd >= 5000 & siccd <= 5199 ~ "Wholesale",
    siccd >= 5200 & siccd <= 5999 ~ "Retail",
    siccd >= 6000 & siccd <= 6799 ~ "Finance",
    siccd >= 7000 & siccd <= 8999 ~ "Services",
    siccd >= 9000 & siccd <= 9999 ~ "Public",
    TRUE ~ "Missing"
  ))
```
We also construct returns adjusted for delistings as described by BEM. After this transformation, we can drop the delisting returns and codes. 
```{r}
crsp_monthly <- crsp_monthly %>%
  mutate(ret_adj = case_when(
    is.na(dlstcd) ~ ret,
    !is.na(dlstcd) & !is.na(dlret) ~ dlret,
    dlstcd %in% c(500, 520, 580, 584) |
      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,
    dlstcd == 100 ~ ret,
    TRUE ~ -1
  )) %>%
  select(-c(dlret, dlstcd))
```
Next, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop adjusted returns and the risk-free rate from our tibble. 
```{r}
crsp_monthly <- crsp_monthly %>%
  left_join(factors_ff_monthly %>% select(month, rf), by = "month") %>%
  mutate(
    ret_excess = ret_adj - rf,
    # Ensure that excess returns are bounded by -1 from below
    ret_excess = pmax(ret_excess, -1)
  ) %>%
  select(-ret_adj, -rf)
```
Since excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization. 
```{r}
crsp_monthly <- crsp_monthly %>%
  drop_na(ret_excess, mktcap, mktcap_lag)
```
Finally, we store the monthly CRSP file in our database. 
```{r}
crsp_monthly %>%
  dbWriteTable(tidy_finance, "crsp_monthly", ., overwrite = TRUE)
```

## The CRSP sample
Before we move on to other data sources, let us look at some descriptive statistics of the CRSP sample which is our main source for stock returns. The next figure shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ exhibits a considerable large number of stocks. The number of stocks on AMEX is decreasing steadily over the last couple of decades. By the end of `r year(end_date)`, there are `r crsp_monthly %>% filter(date == max(date) & exchange == "NASDAQ") %>% count() %>% pull()` stocks on NASDAQ, `r crsp_monthly %>% filter(date == max(date) & exchange == "NYSE") %>% count() %>% pull()` on NYSE, `r crsp_monthly %>% filter(date == max(date) & exchange == "AMEX") %>% count() %>% pull()` on AMEX and only `r crsp_monthly %>% filter(date == max(date) & exchange == "Other") %>% count() %>% pull()` belong to the other category.
```{r}
crsp_monthly %>%
  count(exchange, date) %>%
  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly number of securities by exchange"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma) +
  theme_bw()
```
Next, we look at the aggregate market capitalization of the respective listing exchanges. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. We use the already familiar `tidyquant` package to fetch consumer price index (CPI) data from the [Federal Reserve Economic Data (FRED)](https://fred.stlouisfed.org/series/CPIAUCNS).
```{r}
library(tidyquant)

cpi_monthly <- tq_get("CPIAUCNS",
  get = "economic.data",
  from = start_date, to = end_date
) %>%
  transmute(
    month = floor_date(date, "month"),
    cpi = price / price[month == max(crsp_monthly$month)]
  )
```
As the CPI data might come in handy at some point, we also put it into our local database. 
```{r}
cpi_monthly %>%
  dbWriteTable(tidy_finance, "cpi_monthly", ., overwrite = TRUE)
```
In fact, we can use the tables in our database to calculate aggregate market caps by listing exchange and plot it just as if it were in memory. All values are in end of `year(end_date)` dollars to ensure inter-temporal comparability. NYSE listed stocks have by far the largest market capitalization, followed by NASDAQ listed stocks. 
```{r}
tbl(tidy_finance, "crsp_monthly") %>%
  left_join(tbl(tidy_finance, "cpi_monthly"), by = "month") %>%
  group_by(month, exchange) %>%
  summarize(
    securities = n_distinct(permno),
    mktcap = sum(mktcap, na.rm = TRUE) / cpi
  ) %>%
  collect() %>%
  mutate(month = as.Date(month)) %>%
  ggplot(aes(x = month, y = mktcap / 1000, color = exchange, linetype = exchange)) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly total market value (billions of Dec 2020 Dollars) by listing exchange"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma) +
  theme_bw()
```
Of course, performing the computation in the database is not really meaningful because we already have all the required data in memory. The code chunk above is slower than performing the same steps on tables that are already in memory. However, we just want to illustrate that you can perform many things in the database before loading the data into your memory. 

Next, we look at the same descriptive statistics by industry. The figure below plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks are apparently in Manufacturing albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to the the only category on the rise in recent years, even surpassing Manufacturing at the end of our sample period.
```{r}
crsp_monthly_industry <- crsp_monthly %>%
  left_join(cpi_monthly, by = "month") %>%
  group_by(month, industry) %>%
  summarize(
    securities = n_distinct(permno),
    mktcap = sum(mktcap) / mean(cpi),
    .groups = "drop"
  ) %>%
  ungroup()

crsp_monthly_industry %>%
  ggplot(aes(x = month, y = securities, color = industry, linetype = industry)) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly number of securities by industry"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma) +
  theme_bw()
```
We also compute the market value of all stocks belonging to the respective industries. All values are again in terms of billions of end of 2020 dollars. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Towards the end of the sample, however, financial firms and services begin to make up a substantial portion of the market value.
```{r}
crsp_monthly_industry %>%
  ggplot(aes(x = month, y = mktcap / 1000, color = industry, linetype = industry)) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly total market value (billions of Dec 2020 Dollars) by industry"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma) +
  theme_bw()
```

## Accessing daily CRSP data
Before we turn to Compustat data, we also want to provide a proposal for downloading daily CRSP data. While the monthly data from above typically fits easily into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can easily exceed 20GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database) and in our experience the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session. 

There is a simple solution to this challenge. As with many 'big data' problems, you can simply split up the big task into many small tasks that are easy to handle. That is, instead of downloading data about many stocks all at once, download the data in small batches for each stock consecutively. Such operations can be implemented in `for` loops where we download, prepare and store the data for a single stock in each iteration. This operation might nonetheless take a couple of hours, so you have to be patient either way. Eventually, we end up with more than 68 million rows of daily return data. Note that we only store the identifying information that we actually need, namely `permno`, `date`, `month` and excess returns. We thus ensure that our local database does not explode and later on we can load the full daily data into memory. 
```{r, eval = FALSE}
dsf_db <- tbl(wrds, in_schema("crsp", "dsf"))
permnos <- tbl(tidy_finance, "crsp_monthly") %>%
  distinct(permno) %>%
  pull()

progress <- txtProgressBar(min = 0, max = length(permnos), initial = 0, style = 3)
for (j in 1:length(permnos)) {
  permno_sub <- permnos[j]
  crsp_daily_sub <- dsf_db %>%
    filter(permno == permno_sub &
      date >= start_date & date <= end_date) %>%
    select(permno, date, ret) %>%
    collect() %>%
    drop_na()

  if (nrow(crsp_daily_sub)) {
    crsp_daily_sub <- crsp_daily_sub %>%
      mutate(month = floor_date(date, "month")) %>%
      left_join(factors_ff_daily %>%
        select(date, rf), by = "date") %>%
      mutate(
        ret_excess = ret - rf,
        ret_excess = pmax(ret_excess, -1)
      ) %>%
      select(permno, date, month, ret_excess)

    if (j == 1) {
      overwrite <- TRUE
      append <- FALSE
    } else {
      overwrite <- FALSE
      append <- TRUE
    }

    crsp_daily_sub %>%
      dbWriteTable(tidy_finance, "crsp_daily", ., overwrite = overwrite, append = append)
  }
  setTxtProgressBar(progress, j)
}
close(progress)

crsp_daily_db <- tbl(tidy_finance, "crsp_daily")
crsp_daily_db %>% count() # contains 68,895,667 rows
```

## Merging CRSP with Compustat
As you have seen above, the CRSP data only contains stock-specific information, so we need to tap another source for firm financials. These financial information are an important source of information that we use in portfolio analyses later on. The commonly used source for firm financial information is Compustat, which is a global data provider that provides financial, statistical and market information on active and inactive companies throughout the world. 

Unfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses `permno` for stocks, while Compustat uses `gvkey` to identify firms. Fortunately, there is a curated matching table on WRDS that allows us to merger CRSP and Compustat, so we create a connection to this remote table. 
```{r}
ccmxpf_linktable_db <- tbl(wrds, "ccmxpf_linktable")
```
However, we need to make sure that we keep only relevant links, again following the description outlined in BEM.
```{r}
ccmxpf_linktable <- ccmxpf_linktable_db %>%
  filter(linktype %in% c("LU", "LC") &
    linkprim %in% c("P", "C") &
    usedflag == 1) %>%
  select(permno = lpermno, gvkey, linkdt, linkenddt) %>%
  collect() %>%
  # Currently active links have no end date
  mutate(linkenddt = replace_na(linkenddt, Sys.Date()))
ccmxpf_linktable
```
We use this link table to create a new table where we have a mapping between stock identifier, firm identifier and month. We then add these links to the Compustat `gvkey` to our monthly stock data. 
```{r}
ccm_links <- crsp_monthly %>%
  inner_join(ccmxpf_linktable, by = "permno") %>%
  filter(!is.na(gvkey) & (date >= linkdt & date <= linkenddt)) %>%
  select(permno, gvkey, date)

crsp_monthly <- crsp_monthly %>%
  left_join(ccm_links, by = c("permno", "date"))
```
As a last step, we update the prepared monthly CRSP file with the linking information in our local database.
```{r}
crsp_monthly %>%
  dbWriteTable(tidy_finance, "crsp_monthly", ., overwrite = TRUE)
```

## Preparing Compustat Data

As we mentioned above, we use firm fundamentals provided by Compustat through WRDS. Compustat North America is a database of U.S. and Canadian fundamental and market information on active and inactive publicly held companies. For most companies, annual history is available back to 1950 and quarterly as well as monthly history goes back to 1962.

For most of our applications, we use annual information provided via the `funda` table. 
```{r}
funda_db <- tbl(wrds, "funda")
```
We follow the typical filter conventions and pull only data that we actually need. 
```{r}
compustat <- funda_db %>%
  filter(
    # Get only industrial fundamental data (i.e. ignore financial services)
    indfmt == "INDL" &
      # Get data in standard format (i.e. consolidated information in standard presentation)
      datafmt == "STD" &
      consol == "C" &
      # Get only data in the desired time window
      datadate >= start_date & datadate <= end_date
  ) %>%
  # Select only relevant columns
  select(
    gvkey, # Firm identifier
    datadate, # Date of the accounting data
    seq, # Stockholders' equity
    ceq, # Total common/ordinary equity
    at, # Total assets
    lt, # Total liabilities
    txditc, # Deferred taxes and investment tax credit
    txdb, # Deferred taxes
    itcb, # Investment tax credit
    pstkrv, # Preferred stock redemption value
    pstkl, # Preferred stock liquidating value
    pstk # Preferred stock par value
  ) %>%
  # Fetch data from server into memory
  collect()
```
Next, we calculate the book value of preferred stock and equity inspired by the [variable definition in Ken French's data library](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/variable_definitions.html).
```{r}
compustat <- compustat %>%
  mutate(
    be = coalesce(seq, ceq + pstk, at - lt) +
      coalesce(txditc, txdb + itcb, 0) -
      coalesce(pstkrv, pstkl, pstk, 0),
    # Negative or zero equity makes no sense because the firm would be dead
    be = if_else(be <= 0, as.numeric(NA), be)
  )
```
We keep only the last available information for each firm-year group. Note that `datadate` refers to the time to which the corresponding financial refers to, not the date when it was made available to the public. If you wonder why there can be multiple observations for a firm in the same year, check out the exercises.
```{r}
compustat <- compustat %>%
  mutate(year = year(datadate)) %>%
  group_by(gvkey, year) %>%
  filter(datadate == max(datadate)) %>%
  ungroup()
```
We are already done preparing the firm fundamentals, so we can store them in our local database. 
```{r}
compustat %>%
  dbWriteTable(tidy_finance, "compustat", ., overwrite = TRUE)
```
Before we move on to the next data source, let us look at an interesting descriptive of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is actually interesting to know how many of our stocks even have that piece of information. The next figure hence plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX and NYSE listed stocks in the 60s, but hovers around 80% for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there are only a handful of them anyway in our sample. 
```{r}
crsp_monthly %>%
  group_by(permno, year = year(month)) %>%
  filter(date == max(date)) %>%
  ungroup() %>%
  left_join(compustat, by = c("gvkey", "year")) %>%
  group_by(exchange, year) %>%
  summarize(share = n_distinct(permno[!is.na(be)]) / n_distinct(permno)) %>%
  ggplot(aes(x = year, y = share, color = exchange)) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "End-of-year share of securities with book equity values by exchange"
  ) +
  scale_y_continuous(labels = percent) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1))
```

## Managing SQLite Databases

When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves it for the future uses. As a result, the size of the database file always grows in size.

To optimize the database file, you can run the `VACUUM` command in the database which will rebuild the database and free up unused space. You can execute the command in the database using the `dbSendQuery()` function. 
```{r}
dbSendQuery(tidy_finance, "VACUUM")
```
The `VACUUM` command actually does a couple of more clean up steps, which you can read up in [this tutorial](https://www.sqlitetutorial.net/sqlite-vacuum/). 

<!--chapter:end:20_data.Rmd-->

# Estimating Beta

## Estimate Beta Using Monthly Returns

## Estimate Beta Using Daily Returns

<!--chapter:end:31_beta.Rmd-->

# Univariate Sorts

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

In this section, we start to dive into portfolio sorts - one of most widely used statistical methodologies in empirical asset pricing. The key application of portfolio analysis is to examine whether one or more variables are able to predict future excess returns. In general, the idea is to sort individual stocks into portfolios of stocks, where the stocks within each portfolio exhibit similar characteristics of a characteristic, such as firm size. The different portfolios then represent well-diversified investments that differ only in the level of the sorting variable. Differences in performance are then attributed to the sorting variable. 
We start by introducing univariate portfolio sorts (which means sorting based on only one characteristic), the most basic type of portfolio analysis. In the subsequent section we tackle bivariate sorting. 

A univariate portfolio sort considers only one sort variable $x_{t-1,i}$. Here, $i$ denotes the stock and $t-1$ indicates that the characteristic is known at point in time $t$.  
The objective is to asses the cross-sectional relation between $x_{t-1,i}$ and, typically, stock returns $r_{t,i}$ at time $t$ as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas, which we computed in the previous section, as our sorting variable.

The current section relies on the following set of packages. 
```{r}
library(tidyverse)
library(RSQLite)
library(lubridate)
library(sandwich)
library(lmtest)
library(scales)
```

We start with loading the required data from the database. In particular we use the monthly CRSP database as our asset universe. We use the Fama-French factor returns to compute the risk-adjusted performance (alpha) of the portfolios we create. `beta` is the tibble with asset betas, computed in the section before. 
```{r}
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)

crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>% 
  collect() 

factors_ff_monthly <- tbl(tidy_finance, "factors_ff_monthly") %>% 
  collect()

beta <- tbl(tidy_finance, "beta") %>% 
  collect()
```
We keep only relevant data from the CRSP sample.
```{r}
crsp_monthly <- crsp_monthly %>% 
  left_join(factors_ff_monthly, by = "month") %>% 
  select(permno, month, ret_excess, mkt_excess, mktcap_lag)
crsp_monthly
```
Next, we want to add our sorting variable to the return data: estimated market betas. We actually want to use *lagged* betas as a sorting variable to ensure that we use information that is available when we form the portfolios and before the month where the return realizes. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month $t$ information is available in month $t+1$. (You may be tempted to simply use a call such as `crsp_monthly %>% group_by(permno) %>% mutate(beta_lag = lag(beta)))` instead. This procedure, however, does not work if there are non-explicit missing values in the time-series).
```{r}
beta_lag <- beta %>%
  mutate(month = month %m+% months(1)) %>% 
  select(permno, month, beta_lag = beta_daily) %>%
  drop_na()

data_beta <- crsp_monthly %>%
  inner_join(beta_lag, by = c("permno", "month"))
```
The first step of the portfolio analysis is to calculate periodic breakpoints that are used to group the stocks into portfolios. For simplicity, we start with using the median as the single breakpoint. We then compute the value-weighted returns in each of the two resulting portfolios using lagged market capitalizations as the weight.  
```{r}
beta_portfolios <- data_beta %>%
  group_by(month) %>%
  mutate(breakpoint = median(beta_lag),
         portfolio = case_when(beta_lag <= breakpoint ~ "low",
                               beta_lag > breakpoint ~ "high")) %>%
  group_by(month, portfolio) %>%
  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = "drop")
```
The following figure shows the monthly excess returns of the two portfolios. 
```{r}
beta_portfolios %>% 
  ggplot(aes(x = month, y = ret, fill = portfolio)) +
  geom_col() +
  facet_wrap(~portfolio, ncol = 1) +
  scale_y_continuous(labels = percent) + 
  labs(x = NULL, y = NULL, 
       title = "Monthly beta portfolio excess returns using median as breakpoint") +
  theme_bw() +
  theme(legend.position = "none")
```
Using the two portfolios, we can easily construct a long-short strategy: buy the high-beta portfolio and at the same time short the low-beta portfolio. 
```{r}
beta_longshort <- beta_portfolios %>%
  pivot_wider(month, names_from = portfolio, values_from = ret) %>%
  mutate(long_short = high - low) %>% 
  left_join(factors_ff_monthly, by = "month") 
```
To test whether the long-short portfolio yields on average positive or negative excess returns, we compute the average return and the corresponding standard error. In an academic context, one typically uses Newey-West (1987) $t$-statistics (using six lags) to test the null hypothesis of average portfolio excess returns being equal to zero. To implement this test, we compute the average return via `lm()` and then employ the `coeftest` function.
```{r}
fit <- lm(long_short ~ 1, data = beta_longshort)
coeftest(fit, vcov = NeweyWest, lag = 6)
```
The results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint hence does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM yields that the high beta stocks should yield higher expected returns. Our portfolio sort implicitly mimicks an investment strategy that finances high beta stocks by shorting low beta stocks. One should therefore expect that the average excess returns yield a risk adjusted return that is above the risk-free rate.

Let us take the portfolio construction to the next level. We now want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming becomes very handy: we employ the [curly-curly](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/#a-simpler-interpolation-pattern-with-) operator to give us flexibility with respect to which variable to use for the sorting, denoted by `var`. We use `quantile()` to compute breakpoints for `n_portfolios`. We then assign portfolios to stocks using the `findInterval()` function. The output of the following function is hence a new column that contains the number of the portfolio in which a stock ends up.

```{r}
assign_portfolio <- function(data, var, n_portfolios) {
  # Calculate breakpoints
  breakpoints <- data %>%
    summarize(breakpoint = quantile({{ var }}, probs = seq(0, 1, length.out = n_portfolios + 1),
                                    na.rm = TRUE)) %>%
    pull(breakpoint) %>% 
    as.numeric()
  
  # Sort stocks into portfolios based on breakpoints
  data %>%
    mutate(portfolio = findInterval({{ var }}, breakpoints, all.inside = TRUE)) %>% 
    pull(portfolio)
}
```
We can use the above function to easily sort stocks into 10 portfolios each month using lagged betas and then again compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.

```{r}
beta_portfolios <- data_beta %>%
  group_by(month) %>%
  mutate(portfolio = assign_portfolio(data = cur_data(), 
                                      var = beta_lag, 
                                      n_portfolios = 10),
         portfolio = as.factor(portfolio)) %>%
  group_by(portfolio, month) %>% 
  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = "drop")
```
In the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns. 
```{r}
beta_portfolios_summary <- beta_portfolios %>%
  left_join(factors_ff_monthly, by = "month") %>%
  group_by(portfolio) %>%
  summarise(alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),
            beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),
            ret = mean(ret))
```
The next figure illustrates the CAPM alphas of beta-sorted portfolios. It shows that low beta portfolios tend to exhibit positive alphas, while high beta portfolios exhibit negative alphas.
```{r}
beta_portfolios_summary %>% 
  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Alphas of beta-sorted portfolios",
       x = "Portfolio",
       y = "CAPM Alpha",
       fill = "Portfolio") +
  theme_bw()
```
This results suggest a negative relation between beta and future stock returns, which contradicts the predictions of the CAPM. According to the CAPM, returns should increase with beta across the portfolios and risk-adjusted returns should be statistically indistinguishable from zero.

In fact, the CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time.
```{r}
sml_capm <- lm(ret ~ 1 + beta, data = beta_portfolios_summary)$coefficients 

beta_portfolios_summary %>% 
  ggplot(aes(x = beta, y = ret, color = portfolio)) +
  geom_point() +
  geom_abline(intercept = 0, slope = mean(factors_ff_monthly$mkt_excess)) +
  geom_abline(intercept = sml_capm[1], slope = sml_capm[2], color = "green") +
  scale_y_continuous(labels = percent, limit = c(0, mean(factors_ff_monthly$mkt_excess)*2)) +
  scale_x_continuous(limits = c(0,2)) +
  theme_bw() +
  labs(x = "Beta", y = "Excess return", color = "Portfolio",
       title = "Average portfolio excess returns and average beta estimates")
```
To provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio. 
```{r}
beta_longshort <- beta_portfolios %>%
  ungroup() %>%
  mutate(portfolio = case_when(portfolio == max(as.numeric(portfolio)) ~ "high",
                               portfolio == min(as.numeric(portfolio)) ~ "low")) %>% 
  filter(portfolio %in% c("low", "high")) %>%
  pivot_wider(month, names_from = portfolio, values_from = ret) %>%
  mutate(long_short = high - low) %>% 
  left_join(factors_ff_monthly, by = "month") 
```
Again, the resulting long-short strategy does not exhibit statistically significant returns. 
```{r}
coeftest(lm(long_short ~ 1, data = beta_longshort), vcov = NeweyWest)
```
However, the long-short portfolio yields a statistically significant negative CAPM-adjusted alpha although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM. The negative value has been documented as the so-called betting against beta factor. Betting-against-beta corresponds to a strategy that shorts high beta stocks and takes a (levered) long position in low beta stocks. If borrowing constraints prevent investors from taking positions on the security market line they are instead incentivized to buy high beta stocks which yields to a relative higher price (and therefore lower expected returns than implied by the CAPM) for such high beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital constraint investors with lower risk aversion. 
```{r}
coeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort), vcov = NeweyWest)
```
The last plot shows annual returns of the beta portfolios we are mainly interested in. The figure illustrates that there are no striking consistent patterns over the last years - each portfolio exhibits periods with positive as well as negative annual returns. 
```{r}
beta_longshort %>% 
  group_by(year = year(month)) %>% 
  summarize(low = prod(1 + low),
            high = prod(1 + high),
            long_short = prod(1 + long_short)) %>% 
  pivot_longer(cols = -year) %>% 
  ggplot(aes(x = year, y = 1 - value, fill = name)) +
  geom_col(position = "dodge") + 
  facet_wrap(~name, ncol = 1) +
  theme_bw() + theme(legend.position = "none") + 
  scale_y_continuous(labels = percent) + 
  labs(title = "Annual returns of beta portfolios",
       x = NULL, y = NULL)
```
Overall, this section shows how functional programming can be leveraged to form an arbitrary number of portfolios using any sorting variable and how to evaluate the performance of the resulting portfolios. In the next section, we dive deeper into the many degrees of freedom that arise in the context of portfolio analysis. 

<!--chapter:end:32_univariate_sorts.Rmd-->

# Univariate Sorts: Firm Size
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

In this section, we continue with portfolio sorts in a univariate setting. Yet we consider firm size as a sorting variable, which gives rise to a well-known return factor - the size premium. The size premium arises from buying small stocks and selling large stocks, and Fama and French (1993) include it as a factor in their three-factor model - which we will describe later. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.

We will also introduce new choices in the formation of portfolios. In particular, we will discuss exchanges, industries, weighting regimes, and time periods. We will see that these choices matter for the portfolio returns and result in different size premiums. If anybody exploits these ideas, they will be p hacking. Hence, we want to emphasize that these alternative specifications are supposed to be robustness tests.

The current section relies on this set of packages. 
```{r}
library(tidyverse)
library(RSQLite)
library(lubridate)
library(sandwich)
library(lmtest)
library(scales)
library(furrr)
```

First, we need to load some data from our database. Firms size is equal to market equity in most asset pricing applications, which means we can compute it using the CRSP database known from previous chapters.
```{r}
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)

crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>% 
  collect() 

factors_ff_monthly <- tbl(tidy_finance, "factors_ff_monthly") %>% 
  collect()
```

Before we start building our size portfolios, we investigate the distribution of the variable `firm size`. Visualizing the data is a valuable starting point for understanding the input to the analysis. The figure below shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x% of the firms by setting the indicator's value to one. Then, we aggregate the firms within each bucket and compute the buckets share of total market capitalization. 

The figure shows that the largest 1% of firms cover up to 50% of the total market capitalization, and holding just the quarter consisting of the largest firms basically replicates the market portfolio. The size distribution means that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.

```{r}
crsp_monthly %>%
  group_by(month) %>%
  mutate(top01 = if_else(mktcap >= quantile(mktcap, 0.99), 1L, 0L),
         top05 = if_else(mktcap >= quantile(mktcap, 0.95), 1L, 0L),
         top10 = if_else(mktcap >= quantile(mktcap, 0.90), 1L, 0L),
         top25 = if_else(mktcap >= quantile(mktcap, 0.75), 1L, 0L),
         total_market_cap = sum(mktcap)) %>%
  summarize(`Largest 1% of Stocks` = sum(mktcap[top01 == 1]) / total_market_cap,
            `Largest 5% of Stocks` = sum(mktcap[top05 == 1]) / total_market_cap,
            `Largest 10% of Stocks` = sum(mktcap[top10 == 1]) / total_market_cap,
            `Largest 25% of Stocks` = sum(mktcap[top25 == 1]) / total_market_cap) %>%
  pivot_longer(cols = -month) %>% 
  mutate(name = factor(name, levels = c("Largest 1% of Stocks", "Largest 5% of Stocks", 
                                        "Largest 10% of Stocks", "Largest 25% of Stocks"))) %>% 
  ggplot(aes(x = month, y = value, color = name)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) + 
  labs(x = NULL, y = NULL, color = NULL,
       title = "Percentage of total market capitalization in largest stocks") +
  theme_bw()
```

Second, the issue of firm size is also relevant across exchanges. Stocks' primary listings were important in the past and are potentially still relevant today. The graph below shows that the New York Stock Exchange (`NYSE`) was and still is the largest exchange in terms of market capitalization. Nasdaq has gained size over time. Do you know what the small peak in Nasdaq's market cap around 2000 was about?

```{r}
crsp_monthly %>% 
  group_by(month, exchange) %>% 
  summarize(mktcap = sum(mktcap)) %>% 
  mutate(share = mktcap / sum(mktcap)) %>% 
  ggplot(aes(x = month, y = share, fill = exchange)) +
  geom_col() +
  theme_bw() +
  scale_y_continuous(labels = percent) + 
  labs(x = NULL, y = NULL,
       title = "Share of total market capitalization per exchange")
```

Finally, we consider the distribution of firm size across exchanges by creating summary statistics. Notice that the pre-build function `summary()` does not include all statistics we are interested in, which is why we create the function `create_summary()` that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our CRSP data on each exchange. We also add a row with `add_row()` with the overall summary statistics.

The resulting table shows that firms listed on NYSE are significantly larger on average than firms listed on the other exchanges. Moreover, Nasdaq has the largest number of firms. This discrepancy between firm sizes across exchanges motivated researchers to form breakpoints exclusively on the NYSE sample and apply those breakpoints to all stocks. We will use this to update our portfolio building procedure.

```{r}
create_summary <- function(data, column_name) {
  data %>% 
    select(value = {{column_name}}) %>%
    summarize(mean = mean(value),
              sd = sd(value),
              min = min(value),
              q05 = quantile(value, 0.05),
              q25 = quantile(value, 0.25),
              q50 = quantile(value, 0.50),
              q75 = quantile(value, 0.75),
              q95 = quantile(value, 0.95),
              max = max(value),
              n = n())
  }

crsp_monthly %>%
  filter(month == max(month)) %>%
  group_by(exchange) %>% 
  create_summary(mktcap) %>%
  add_row(crsp_monthly %>%
            filter(month == max(month)) %>% 
            create_summary(mktcap) %>% 
            mutate(exchange = "Overall"))
```

## Univariate Size Portfolios

In the previous section, we constructed portfolios with a varying number of portfolios and different sorting variables. Here, we introduce the concept of computing breakpoints on a subset of exchanges. In published articles, many scholars compute their breakpoints only on NYSE-listed stocks. These NYSE-specific breakpoints are then applied to the entire universe of stocks. Therefore, we introduce `exchanges` as an argument in our `assign_portfolio()` function. The exchange-specific argument then enters in the filter `filter(grepl(exchanges, exchange))`. The function `grepl()` is part of a family of functions on *regular expressions*, which allow you to work with character strings in various ways. For our case, we replace the character string stored in the column `exchange` with a logical constant that indicates if the string matches the pattern specified in the argument `exchanges`. For example, if `exchanges = 'NYSE'` is specified, only stocks from NYSE are stored in breakpoints, which are used to compute the breakpoints. Alternatively, you could specify `exchanges = 'NYSE|NASDAQ|AMEX'`, which keeps all stocks listed on either of these exchanges. Overall, regular expressions are a very powerful tool, and we only touch on a specific case here.

```{r}
assign_portfolio <- function(n_portfolios, 
                                  exchanges, 
                                  data) {
  breakpoints <- data %>%
    filter(grepl(exchanges, exchange)) %>%
    summarize(breakpoint = quantile(
      mktcap_lag, 
      probs = seq(0, 1, length.out = n_portfolios + 1), 
      na.rm = TRUE)) %>%
    pull(breakpoint) %>% 
    as.numeric()
  
  data %>%
    mutate(portfolio = findInterval(mktcap_lag, breakpoints, all.inside = TRUE)) %>% 
    pull(portfolio)
}
```

Apart from computing breakpoints on different samples, researchers also often use different return weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called *value-weighting* and most projects rely on it. The alternative protocol is *equal-weighting*, which assigns each stock's return the same weight, i.e., a simple average of the constituents' returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly.

We implement the two weighting schemes in a new function `compute_portfolio_returns()` that takes a logical argument to weight the returns by firm value. It affects the `if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess))` line, which is value-weighted returns if the argument is `TRUE`. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, in line with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in the previous section.

```{r}
compute_portfolio_returns <- function(n_portfolios = 10, 
                                      exchanges = "NYSE|NASDAQ|AMEX", 
                                      value_weighted = TRUE, 
                                      data = crsp_monthly) {
  
  data %>%
    group_by(month) %>%
    mutate(portfolio = assign_portfolio(n_portfolios = n_portfolios,
                                        exchanges = exchanges, 
                                        data = cur_data())) %>%
    group_by(month, portfolio) %>% 
    summarise(ret = if_else(value_weighted, weighted.mean(ret_excess, mktcap_lag), mean(ret_excess)),
              .groups = "drop_last") %>%
    summarise(size_premium = ret[portfolio == min(portfolio)] - ret[portfolio == max(portfolio)]) %>%
    summarise(size_premium = mean(size_premium))
}
```

To see how the function `compute_portfolio_returns()` works, we consider a simple median breakpoint example with value-weighted returns. We are then interested in seeing what difference the choice of exchanges makes for the portfolio returns. In the first function call, we compute returns based on breakpoints from all exchanges. Then, we check how breakpoints computed only from NYSE stocks change the returns.

```{r}
ret_all <- compute_portfolio_returns(n_portfolios = 2,
                          exchanges = "NYSE|NASDAQ|AMEX",
                          value_weighted = TRUE,
                          data = crsp_monthly)

ret_nyse <- compute_portfolio_returns(n_portfolios = 2,
                          exchanges = "NYSE",
                          value_weighted = TRUE,
                          data = crsp_monthly)

tibble(Exchanges = c("all", "NYSE"), Premium = as.numeric(c(ret_all, ret_nyse))*100)
```

The table shows that the size premium is more than 60% larger if we consider only stocks from NYSE to form the breakpoint each month. The NYSE-specific breakpoints are larger, and there are more than 50% of the stocks in the entire universe in the resulting small portfolio because NYSE firms are larger on average. The impact of this choice is not negligible.  

## P hacking

Since the choice of the exchange had a significant impact, the next step is to investigate the effect of other choices. In particular, we consider the effects of the number of portfolios, the exchanges to form breakpoints, equal- and value-weighting, the inclusion of the finance industry, and dropping part of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. 

The reason behind the exercise here is to show that the different ways to form portfolios result in different premiums. Despite the effects of this multitude of choices, there is no correct way. If a researcher was hunting for a significant result to publish, these different ways give them multiple chances at finding statistical significance. Yet this is considered *p-hacking* and it is important to highlight that p-hacking is considered scientific fraud. Moreover, the statistical inference from multiple testing is itself invalid.

Nevertheless, this creates a problem since there is no single correct way of sorting portfolios; how should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as *robustness tests* and report multiple approaches to show that a result does not depend on a single choice. The main point here is to show how different premiums can be and that robustness of premiums is a key feature. 

Below we conduct a series of robustness tests or a p-hacking exercise. We do so by examining the size premium in different specifications presented in the table `p_hacking_setup`. The function `expand_grid()` produces a table of all combinations of its arguments and makes it easier to create all of the combinations. Notice that we use the argument `data` to exclude financial firms and truncate the time series. Next, we again employ a parallelization approach introduced in a previous section. The resulting premiums are then tabulated in descending order.

```{r}
p_hacking_setup <- expand_grid(
  n_portfolios = c(2, 5, 10),
  exchanges = c("NYSE", "NYSE|NASDAQ|AMEX"),
  value_weighted = c(TRUE, FALSE),
  data = rlang::parse_exprs('crsp_monthly; crsp_monthly %>% filter(industry != "Finance");
                             crsp_monthly %>% filter(month < "1990-06-01");
                             crsp_monthly %>% filter(month >="1990-06-01")'))
p_hacking_setup


plan(multisession, workers = availableCores())

p_hacking_setup <- p_hacking_setup %>% 
  mutate(size_premium = future_pmap(.l = list(n_portfolios,
                                       exchanges,
                                       value_weighted,
                                       data), 
                             .f = ~compute_portfolio_returns(n_portfolios = ..1,
                                                             exchanges = ..2,
                                                             value_weighted = ..3,
                                                             data = rlang::eval_tidy(..4))))
p_hacking_setup %>% 
  mutate(data = map_chr(data, deparse)) %>%
  unnest(size_premium) %>% 
  arrange(desc(size_premium))
```

Additionally, we provide a graph that shows the different premiums. This plot also shows the relation to the Fama-French SMB (small minus big) premium used in the literature. 

```{r}
p_hacking_setup %>% 
  mutate(data = map_chr(data, deparse)) %>%
  unnest(size_premium) %>% 
  ggplot(aes(x = size_premium*100)) + 
  geom_histogram(bins = 20) +
  labs(x = NULL, y = NULL, 
       title = "Size premium over different sorting choices",
       subtitle = "The dotted vertical line indicates the average size premium (in %)") + 
  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb)*100),
             color = "red", 
             linetype = "dashed") +
  theme_bw()
```




<!--chapter:end:33_size_and_portfolio_building.Rmd-->

# Bivariate Sorts: Value
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

In the previous section, we introduced the concept of portfolio sorts to quantify the effect of a sorting variable on expected stock returns. This section extends the univariate framework to bivariate sorts (based on two characteristics). The academic literature predominantly employes these bivariate sorts. Yet some scholars also use sorts with three grouping variables, but the concepts presented are also applicable in higher dimensions.

We form our portfolios on firm size and the book-to-market ratio. We also encounter accounting variables for calculating book-to-market ratios, which necessitates additional steps during portfolio formation. In the end, we will demonstrate how to form portfolios on two sorting variables using independent and dependent portfolio sorts.

The current section relies on this set of packages. 
```{r}
library(tidyverse)
library(RSQLite)
library(lubridate)
library(sandwich)
library(lmtest)
library(scales)
library(furrr)
```

As usual, we need some data and only keep the necessary columns in our memory. We use the same data sources for firm size as in the previous section.

```{r}
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)

crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>% 
  collect()

factors_ff_monthly <- tbl(tidy_finance, "factors_ff_monthly") %>% 
  collect()

# Keep only relevant data
crsp_monthly <- crsp_monthly %>% 
  left_join(factors_ff_monthly, by = "month") %>% 
  select(permno, gvkey, month, ret_excess, mkt_excess, mktcap, mktcap_lag, exchange) %>% 
  drop_na()
```

However, we also need accounting data. We use the most common source of accounting data, namely *compustat*. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable `datadate` to its monthly value, as we only consider monthly returns here and do not need to account for the exact date. To achieve this, we use the function `floor_date()`.

```{r}
compustat <- tbl(tidy_finance, "compustat") %>% 
  collect()

# Book-equity data
be <- compustat %>%
  select(gvkey, datadate, be) %>%
  drop_na() %>%
  mutate(month = floor_date(ymd(datadate), "month"))
```

A fundamental problem in handling accounting data is the *look-ahead bias* - we must not include data in forming a portfolio that is not public knowledge at the time. Of course, researchers have more information when looking into the past than agents had at that moment. However, a return differential from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents' trades. Hence, we have to lag accounting information.

We have already made time adjustments before handling market capitalization and firm size. We continue to lag these CRSP variables by one month. Then, we compute the book-to-market ratio, which relates a firm's book equity to its market equity. Firms with high (low) book-to-market are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. In fact, this is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the literature, even longer lags have been common. Regarding the p-hacking / robustness tests discussed in the previous section, the definition of a time lag is another choice a researcher has to make.

Having both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the `sorting_date`-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the `fill()`-function after sorting by date and firm (which we identify by permno and gvkey) and on a firm basis (which we do by `group_by()` as usual). As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.

```{r}
me <- crsp_monthly %>%
  mutate(sorting_date = month %m+% months(1)) %>%
  select(permno, sorting_date, me = mktcap)

# Book-to-market
bm <- be %>%
  inner_join(crsp_monthly %>% 
               select(month, permno, gvkey, mktcap), by = c("gvkey", "month")) %>%
  mutate(bm = be / mktcap,
         sorting_date = month %m+% months(6)) %>%
  select(permno, gvkey, sorting_date, bm) %>%
  arrange(permno, gvkey, sorting_date)
  
# Merged data
ccm <- crsp_monthly %>%
  left_join(bm, by = c("permno", "gvkey", "month" = "sorting_date")) %>%
  left_join(me, by = c("permno", "month" = "sorting_date")) %>%
  select(permno, gvkey, month, ret_excess, mktcap_lag, me, bm, exchange)

# Fill accounting data
ccm <- ccm %>%
  arrange(permno, gvkey, month) %>%
  group_by(permno, gvkey) %>%
  fill(bm) %>%
  drop_na()
```

The last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function allowing for the specification of exchanges to use for the breakpoints. Additionally, we reintroduce the argument `var` into the function for defining different sorting variables via `curly-curly` (as discussed in a previous section).

```{r}
assign_portfolio <- function(data, var, n_portfolios, exchanges) {
  breakpoints <- data %>%
    filter(exchange %in% exchanges) %>%
    summarize(breakpoint = quantile(
      {{ var }}, 
      probs = seq(0, 1, length.out = n_portfolios + 1), 
      na.rm = TRUE)) %>%
    pull(breakpoint) %>% 
    as.numeric()
  
  data %>%
    mutate(portfolio = findInterval({{ var }}, breakpoints, all.inside = TRUE)) %>% 
    pull(portfolio)
}
```

After these data preparation steps, we will present bivariate portfolio sorts on an independent and dependent basis.

## Independent sorts

Bivariate sorts will create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the *value premium* (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.

To implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables `portfolio_bm` and `portfolio_bm`, respectively. Then, these separate portfolios are combined to the final stored in `portfolio_combined`. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier - as the alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.

```{r}
ccm_portfolios <- ccm %>%
  group_by(month) %>%
  mutate(portfolio_bm = assign_portfolio(data = cur_data(),
                                         var = bm, 
                                         n_portfolios = 5,
                                         exchanges = c("NYSE")),
         portfolio_me = assign_portfolio(data = cur_data(),
                                         var = me, 
                                         n_portfolios = 5,
                                         exchanges = c("NYSE")),
         portfolio_combined = paste0(portfolio_bm, portfolio_me)) %>%
  group_by(month, portfolio_combined) %>% 
  summarize(ret = weighted.mean(ret_excess, mktcap_lag), 
            portfolio_bm = unique(portfolio_bm),
            .groups = "drop") 

# TODO: Should we create a table with average returns?
```

Equipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher's choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.

```{r}
value_premium <- ccm_portfolios %>%
  group_by(month, portfolio_bm) %>%
  summarise(ret = mean(ret), .groups = "drop_last") %>%
  summarise(value_premium = ret[portfolio_bm == max(portfolio_bm)] - ret[portfolio_bm == min(portfolio_bm)])

mean(value_premium$value_premium*100)
```

## Dependent sorts

In the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, create portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts ensure an equal amount of stocks within each portfolio.

To implement the dependent sorts, we first create the size portfolios by calling `assign_portfolio()` with `var = me`. Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, the value premium is again computed at the end.

```{r}
ccm_portfolios <- ccm %>%
  group_by(month) %>%
  mutate(portfolio_me = assign_portfolio(data = cur_data(),
                                         var = me, 
                                         n_portfolios = 5,
                                         exchanges = c("NYSE"))) %>%
  group_by(month, portfolio_me) %>%
  mutate(portfolio_bm = assign_portfolio(data = cur_data(),
                                         var = bm, 
                                         n_portfolios = 5,
                                         exchanges = c("NYSE")),
         portfolio_combined = paste0(portfolio_bm, portfolio_me)) %>%
  group_by(month, portfolio_combined) %>% 
  summarize(ret = weighted.mean(ret_excess, mktcap_lag), 
            portfolio_bm = unique(portfolio_bm),
            .groups = "drop") 

value_premium <- ccm_portfolios %>%
  group_by(month, portfolio_bm) %>%
  summarise(ret = mean(ret), .groups = "drop_last") %>%
  summarise(value_premium = ret[portfolio_bm == max(portfolio_bm)] - ret[portfolio_bm == min(portfolio_bm)])

mean(value_premium$value_premium*100)
```

<!--chapter:end:34_value_and_bivariate.Rmd-->

# Factor selection via machine learning

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

This section has two aims: From a data science perspective, we introduce `tidymodels`,  a collection of packages for modeling and machine learning using tidyverse principles which comes with a handy workflow for all sorts of typical prediction tasks. From a finance perspective, we address the *factor zoo* Cochrane (2011). In previous chapters we illustrated that stock-characteristics such as size provide valuable pricing information in addition to the stock beta which questions the usefulness of the Capital Asset Pricing Model. In fact, during the last decades, financial economists "discovered" a plethora of additional factors which may be correlated with the marginal utility of consumption (and would thus deserve a prominent role for pricing applications). Therefore, the challenge these days rather is: *Do we really believe in the relevance of 300+ risk factors?*. 

We introduce Lasso and Ridge Regression as a special case of penalized regression models. Then, we explain the concept of cross-validation for model **tuning** with elastic net regularization as a popular example. We implement and showcase the entire cycle from model specification, training and forecast evaluation within the `tidymodels` universe, a unified framework for machine learning applications in R. While the tools can in general be applied to an abundance of interesting asset pricing problems, we apply penalized regressions to identify macro-economic variables and asset pricing factors that help to explain a cross-section of industry portfolios.

## Brief theoretical background

This is a book about *doing* empirical work in a tidy manner and we refer to any of the many excellent textbook treatments of machine learning methods and especially penalized regressions for some deeper discussion. Instead, we now just briefly summarize the idea of Lasso and Ridge regressions as well as the more general elastic net before we turn to the fascinating question on *how* to implement, tune and use such models with the `tidymodels` workflows. 

To set the stage, we start with the definition of a linear model: suppose we have data $(y_t, X_t), t = 1,\ldots, T$ where $X_t$ is a $(K \times 1)$ vector of regressors and $y_t$ is the response for observation $t$. 
The linear model takes the form $y_t = \beta X_t + \varepsilon_t$ with some error term $\varepsilon_t$ and has been studied in abundance. The well-known ordinary-least square (OLS) estimator minimizes the sum of squared residuals and is then $\hat{\beta} = \left(\sum\limits_{t=1}^T X_t'X_t\right)^{-1} \sum\limits_{t=1}^T X_t'y_t$. 
While often we are interested in the estimated value of $\hat\beta$, machine learning most of the time is about the predictive performance. The linear model generates predictions $\hat y_t = E\left(y|X_t = \tilde X_t\right) = \hat\beta \tilde X_t$. 
Is this the best we can do? 
Not really: Instead of minimizing the sum of squared residuals, penalized linear models can improve predictive performance by reducing the variance of the estimator $\hat\beta$. At the same time, it seems appealing to restrict the set of regressors to few meaningful ones if possible. In other words, if $K$ is large such as for the number of proposed factors in the finance literature, it may be a desirable feature to *select* reasonable factors and to set $\hat\beta_k = 0$ for some redundant factors. 

It should be clear that the promised benefits of penalized regressions come at a cost. In most cases, reducing the variance of the estimator introduces a bias such that $E\left(\hat\beta\right) \neq \beta$. What is the effect of such a bias-variance trade-off? To see the benefit, assume the following data-generating process for $y$: $$y = f(x) + \varepsilon, \quad \varepsilon \sim (0, \sigma_\varepsilon^2)$$ While the properties of $\hat\beta$ as an unbiased estimator may be desirable under some circumstances, they are certainly not if we consider predictive accuracy. Consider the mean-squared error (MSE) which depends on our model choice: $$\begin{aligned}
E\left(\hat{\epsilon}^2\right)&=E((y-\hat{f}(\textbf{x}))^2)=E((f(\textbf{x})+\epsilon-\hat{f}(\textbf{x}))^2)\\
&= \underbrace{E((f(\textbf{x})-\hat{f}(\textbf{x}))^2)}_{\text{total quadratic error}}+\underbrace{E(\epsilon^2)}_{\text{irreducible error}} \\
&= E\left(\hat{f}(\textbf{x})^2\right)+E\left(f(\textbf{x})^2\right)-2E\left(f(\textbf{x})\hat{f}(\textbf{x})\right)+\sigma_\varepsilon^2\\
&=E\left(\hat{f}(\textbf{x})^2\right)+f(\textbf{x})^2-2f(\textbf{x})E\left(\hat{f}(\textbf{x})\right)+\sigma_\varepsilon^2\\
&=\underbrace{\text{Var}\left(\hat{f}(\textbf{x})\right)}_{\text{variance of model}}+ \underbrace{E\left((f(\textbf{x})-\hat{f}(\textbf{x}))\right)^2}_{\text{squared bias}} +\sigma_\varepsilon^2 
\end{aligned}$$ As a result, we see that a biased estimator with small variance may have a lower mean squared error than an unbiased estimator.

### Ridge Regression

One such biased estimator is known as Ridge regression. Hoerl and Kennard (1970a, 1970b) propose to minimize the sum of squared errors *while simultaneously imposing a penalty on the $L_2$ norm of the parameters* $\hat\beta$. Formally, this means that for a penalty factor $\lambda\geq 0$ the minimization takes the form $\min_\beta \left(y - X\beta\right)'\left(y - X\beta\right)\text{ s.t. } \beta'\beta \leq \lambda$. A closed-form solution for the resulting regression coefficient vector $\beta^\text{ridge}$ exists: $$\hat{\beta}^\text{ridge} = \left(X'X + \lambda I\right)^{-1}X'y.$$ A couple of observations are worth noting: $\hat\beta^\text{ridge} = \hat\beta$ for $\lambda = 0$ and $\hat\beta^\text{ridge} \rightarrow 0$ for $\lambda\rightarrow \infty$. Also for $\lambda > 0$, $\left(X'X + \lambda I\right)$ is non-singular even if $X'X$ is which means that $\hat\beta^\text{ridge}$ exists even if $\hat\beta$ is not defined. But note also that the Ridge estimator requires careful choice of the hyperparameter $\lambda$ which controls the *amount of regularization*.
Usually, $X$ contains an intercept column with ones. As a general rule, the associated intercept coefficient is not penalized. In practice, this often implies that $y$ is simply demeaned before computing $\hat\beta^\text{ridge}$.

What about the statistical properties of the Ridge estimator?  First, the bad news is that $\hat\beta^\text{ridge}$ is a biased estimator of $\beta$. However, the good news is that (under homoscedastic error terms) the variance of the Ridge estimator is **smaller** than the variance of the ordinary least square estimator. We encourage you to verify these two statements in the exercises. As a result, we face a trade-off: Ridge regression sacrifice some bias to achieve a smaller variance than the OLS estimator.

### Lasso

An alternative to Ridge regression is the Lasso (*l*east *a*bsolute *s*hrinkage and *s*election *o*perator). Similar to Ridge regression, the Lasso (Tibshirani, 1996) is a penalized and hence biased estimator. 
The main difference to Ridge regression is that the Lasso does not only *shrink* coefficients but effectively selects variables by setting coefficients for *irrelevant* variables to zero. Lasso implements a $L_1$ penalization on the parameters such that: $$\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda\sum\limits_{k=1}^K|\beta_k|.$$ There is no closed form solution for $\hat\beta^\text{Lasso}$ in the above maximization problem but efficient algorithms exist (e.g., the R package `glmnet`).

### Elastic Net

The elastic net (Zhou and Hastie, 2005) combines $L_1$ and $L_2$ penalization and encourages a grouping effect where strongly correlated predictors tend to be in or out of the model together. This more general framework considers the following optimization problem: $$\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda(1-\rho)\sum\limits_{k=1}^K|\beta_k| +\frac{1}{2}\lambda\rho\sum\limits_{k=1}^K\beta_k^2$$ Now, we have to chose 2 hyperparameters: the *shrinkage* factor $\lambda$ and the *weighting parameter* $\rho$. The elastic net resembles Lasso for $\rho = 1$ and Ridge regression for $\rho = 0$.
While the R package `glmnet` provides efficient algorithms to compute the coefficients of penalized regressions, it is a good exercise to implement Ridge and Lasso estimation on your own before you use the `glmnet` package or the `tidymodels` back-end.

## Data Preparation

To get started, we load the required packages and data. The main focus is going on the workflows behind the amazing `tidymodels` package collection. We load few additional packages for specific purposes once they become relevant. 
```{r}
library(RSQLite) # To gather the data
library(tidyverse)
library(tidymodels) # For ML applications
library(furrr) # For parallelization
library(broom)
library(timetk)
library(kableExtra)
library(scales)
```

In this analysis we use 4 different data sources. We start with two different set of factor portfolio returns which have been suggested as representing useful risk factor exposure and thus should be relevant when it comes to asset pricing applications. 

-   The standard workhorse: monthly Fama-French 3 factor returns (market, small-minus-big and high-minus-low book-to-market valuation sorts)
-   Monthly q-factor returns from Hou, Xue and Zhang (2015). The factors contain the size factor, the investment factor, the return-on-equity factor and the expected growth factor

Next, we include macroeconomic predictors which may predict the general stock market economy. Macroeconomic variables effecticely serve as conditioning information such that their inclusion hints at the relevance of conditional models instead of unconditional asset pricing. We refer the interested reader to Cochrane (2005) on the role of conditioning information.

- Our set of macroeconomic predictors comes from the paper "A Comprehensive Look at The Empirical Performance of Equity Premium Prediction" [@Goyal2008]. The data has been updated by the authors until 2020 and comprises of monthly variables that have been suggested as good predictors for the equity premium. Some of the variables are the Dividend Price Ratio, Earnings Price Ratio, Stock Variance, Net Equity Expansion, Treasury Bill rate and inflation

Finally, we need a set of *test assets*. The aim is to understand which of the plenty factors and macroeconomic variable combinations proof useful to explain the cross-section of returns of our test assets. 

- In line with many existing papers we use monthly portfolio returns from 10 different industries according to the definition from Kenneth French's homepage as test assets.

```{r}
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)

# Load factor returns
factors_ff_monthly <- tbl(tidy_finance, "factors_ff_monthly") %>%
  collect() %>%
  rename_with(~ paste0("factor_ff_", .), -month)

factors_q_monthly <- tbl(tidy_finance, "factors_q_monthly") %>%
  collect() %>%
  rename_with(~ paste0("factor_q_", .), -month)

# Load macroeconomic variables
macro_predictors <- tbl(tidy_finance, "macro_predictors") %>%
  collect() %>%
  rename_with(~ paste0("macro_", .), -month) %>%
  select(-macro_rp_div)

# Load test assets
industries_ff_monthly <- tbl(tidy_finance, "industries_ff_monthly") %>%
  collect() %>%
  pivot_longer(-month, names_to = "industry", values_to = "return") %>%
  mutate(industry = as_factor(industry))

```

We combine all observations into one data frame.

```{r}
data <- industries_ff_monthly %>%
  left_join(factors_ff_monthly, by = "month") %>%
  left_join(factors_q_monthly, by = "month") %>%
  left_join(macro_predictors, by = "month") %>%
  mutate(
    return = return - factor_ff_rf
  ) %>% # Compute excess returns
  select(month, industry, return, everything()) %>%
  drop_na()
```

Our data contains `r ncol(data)-2` columns of regressors with the `r ncol(macro_predictors) - 1` macro variables and `r ncol(factors_ff_monthly) + ncol(factors_q_monthly)-2` factor returns for each month. The panel ranges from `r data %>% pull(month) %>% min() %>% format('%B %Y')` until `r data %>% pull(month) %>% max() %>% format('%B %Y')`. Table \@ref(tab:industryreturns) provides summary statistics for the `r industries_ff_monthly %>% count(industry) %>% nrow()` industries such as the sample standard deviation and the minimum and maximum monthly excess returns.

```{r industryreturns}
data %>%
  group_by(industry) %>%
  mutate(return = 100 * return) %>%
  summarise(
    mean = mean(return),
    sd = max(return),
    min = min(return),
    median = median(return),
    max = max(return)
  ) %>%
  kable(caption = "Summary statistics: Industry excess returns in percent.")
```

## The tidymodels workflow

To illustrate penalized linear regressions, we employ the `tidymodels` collection of packages for modeling and machine learning using `tidyverse` principles. You can simply use `install.packages("tidymodels")` to get access to all the related packages. We recommend to check out the work of Max Kuhn and Julia Silge: They  continuously write on a great book '[Tidy Modeling with R](https://www.tmwr.org/)' using tidy principles.

The `tidymodels` workflow encompasses the main stages of the modeling process: pre-processing of data, model fitting, and post-processing of results. As we demonstrate below, `tidymodels` provides efficient workflows that can be updated with low effort.

Using the ideas of Ridge and Lasso regression, the following example guides you through (i) preprocessing the data (data split and variable mutation), (ii) building models, (iii) fitting models, and (iv) tuning models to create the "best" possible predictions.

To start, we restrict our analysis to just one industry: Manufacturing. We first split the sample into a *training* and a *test* set. For that purpose, `tidymodels` provides the function `initial_time_split` from the `rsample` package. The split takes the last 20% of the data as test set which is not used for any model tuning. We use this test set to evaluate the predictive accuracy in an out-of-sample scenario.

```{r}
split <- initial_time_split(
  data %>%
    filter(industry == "Manuf") %>%
    select(-industry),
  prop = 4 / 5
)
split
```

The object `split` simply takes track of the observation index which belongs to the training and the test set. We can call the training set with `training(split)`, while we can extract the test set with `testing(split)`.

### Preprocess Data

Recipes help you preprocess your data before training your model. Recipes are a series of preprocessing steps such as variable selection, transformation or converting qualitative predictors to indicator variables. Each recipe starts with a `formula` which defines the general structure of the dataset and the role of each variable (regressor or dependent variable). For our dataset, our recipe contains the following steps before we fit any model:

-   We want to explain excess returns with all available predictors
-   We exclude the column *month* from the analysis
-   We include all interaction terms between factors and macro economic predictors
-   We demean and scale each regressor such that the standard deviation is one

```{r}
rec <- recipe(return ~ ., data = training(split)) %>%
  step_rm(month) %>% # remove date variable
  step_interact(terms = ~ contains("factor"):contains("macro")) %>% # interaction terms
  step_normalize(all_predictors()) %>%
  step_center(return, skip = TRUE)
```

A table of all available recipe steps can be found [here](https://www.tidymodels.org/find/recipes/). More than 100 different processing steps are available! One important point: the definition of a recipe does not imply any calculations yet but rather provides a *description* of the tasks which are applied later. As a result, it is very easy to *reuse* recipes for different models and thus make sure that the outcomes are comparable as they are based on the same input. 
In the example above, it does not make a difference whether the input `data = training(split)` or `data = testing(split)` is used. 
All that matters at this early stage are the column names and types.

We can apply the recipe to any data with suitable structure. The code below combines two different functions: `prep` estimates the required parameters from a training set that can be later applied to other data sets. `bake` applies the processed computations to new data.

```{r}
tmp_data <- bake(prep(rec, training(split)), new_data = testing(split))
tmp_data
```

Note that the resulting data contains the `r nrow(tmp_data)` observations from the test set and `r ncol(tmp_data)` columns. Why so many? Recall that the recipe states to compute every possible interaction term between the factors and predictors which increases the dimension of the data matrix substantially. 

You may ask at this stage: Why should I use a recipe instead of simply using the data wrangling commands such as `mutate` or `select`? `tidymodels` beauty is that there is a lot happening under the hood. Recall, that for the simple scaling step you actually have to compute the standard deviation of each column, then *store* this value and apply the identical transformation to the new dataset, e.g. `testing(split)`. A prepped `recipe` stores these values and hands them on once you `bake` a novel dataset. Easy as pie with `tidymodels`, isn't it?

### Build a Model

Next we can build an actual model based on our pre-processed data. In line with the definition from above, we estimate regression coefficients of a Lasso regression such that we get\
$$\hat\beta_\lambda^\text{Lasso} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda\sum\limits_{k=1}^K|\beta_k|.$$ We want to emphasize that the `tidymodels` workflow for **any** model is very similar, irrespective of the specific model. As you will see further below, it is a piece of cake to fit Ridge regression coefficients and - later - Neural networks or Random forests with basically the same code. The structure is always as follows: create a so-called `workflow` and use the `fit` function. A table with all available model APIs is available [here](https://www.tidymodels.org/find/parsnip/).
For now, we start with the linear regression model with a given value for the penalty factor $\lambda$. In the setup below, `mixture` denotes the value of $\rho$, hence setting `mixture = 1` implies the Lasso.
```{r}
lm_model <- linear_reg(
  penalty = 0.0001,
  mixture = 1
) %>%
  set_engine("glmnet", intercept = FALSE)
```

That's it - we are done! The object `lm_model` contains the definition of our model with all required information. Note that `set_engine("glmnet")` indicates the API character of the `tidymodels` workflow: Under the hood, the package `glmnet` is doing the heavy lifting, while `linear_reg` provides a unified framework to collect the inputs. The `workflow` ends with combining everything necessary for the (serious) data science workflow: a recipe and a model. Now we are ready to use `fit`.

```{r}
lm_fit <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model)
lm_fit
```

### Fit a Model

We use the training data to fit the model. The training data is pre-processed according to our recipe steps and the Lasso regression coefficients are computed. First, we focus on the predicted values $\hat{y}_t = X_t\hat\beta^\text{Lasso}.$ Figure @ref(industrypremia) illustrates the projections for the **entire** time series of the Manufacturing industry portfolio returns. The grey area indicates the out-of-sample period which has not been used to fit the model.

```{r industrypremia}
predicted_values <- lm_fit %>%
  fit(data = training(split)) %>%
  predict(data %>% filter(industry == "Manuf")) %>%
  bind_cols(data %>% filter(industry == "Manuf")) %>%
  select(month, .pred, return) %>%
  pivot_longer(-month, names_to = "Variable") %>%
  mutate(Variable = case_when(
    Variable == ".pred" ~ "Fitted Value",
    Variable == "return" ~ "Realization"
  )) 

predicted_values %>%
  ggplot(aes(x = month, y = value, color = Variable)) +
  geom_line() +
  theme_minimal() +
  labs(
    x = NULL,
    y = NULL,
    color = NULL,
    title = "Monthly Realized and Fitted Manufacturing Industry Risk Premia"
  ) +
  scale_x_date(
    breaks = function(x) seq.Date(from = min(x), to = max(x), by = "5 years"),
    minor_breaks = function(x) seq.Date(from = min(x), to = max(x), by = "1 years"),
    expand = c(0, 0),
    labels = date_format("%Y")
  ) +
  scale_y_continuous(
    labels = percent
  ) +
  geom_rect(aes(
    xmin = testing(split) %>% pull(month) %>% min(),
    xmax = testing(split) %>% pull(month) %>% max(),
    ymin = -Inf, ymax = Inf
  ),
  alpha = 0.005
  ) +
  theme(legend.position = "bottom")
```

How do the estimated coefficients look like? To analyse these values and illustrate the difference between the `tidymodels` workflow and the underlying `glmnet` package, it is worth to compute the coefficients $\hat\beta^\text{Lasso}$ directly. The code below estimates the coefficients for the Lasso and Ridge regression for the processed training data sample. Note that `glmnet` actually takes a vector `y` and the matrix of regressors $X$ as input. Moreover, `glmnet` requires choosing the penalty parameter $\alpha$ which corresponds to $\rho$ in the notation above. Such details do not need consideration when using the `tidymodels` model API.

```{r}
library(glmnet) # For penalized regressions
x <- tmp_data %>%
  select(-return) %>%
  as.matrix()
y <- tmp_data %>% pull(return)

fit_lasso <- glmnet(
  x = x,
  y = y,
  alpha = 1, intercept = FALSE, standardize = FALSE,
  lambda.min.ratio = 0
) # Lasso

fit_ridge <- glmnet(
  x = x,
  y = y,
  alpha = 0, intercept = FALSE, standardize = FALSE,
  lambda.min.ratio = 0
) # Ridge
```

The objects `fit_lasso` and `fit_ridge` contain an entire sequence of estimated coefficients for multiple values of the penalty factor $\lambda$. The figure below illustrates how the trajectory of the regression coefficients as a function of the penalty factor $\lambda$. As the penalty factor increases, both Lasso and Ridge coefficients converge to zero.

```{r}
bind_rows(
  tidy(fit_lasso) %>% mutate(Model = "Lasso"),
  tidy(fit_ridge) %>% mutate(Model = "Ridge")
) %>%
  rename("Variable" = term) %>%
  ggplot(aes(x = lambda, y = estimate, color = Variable)) +
  geom_line() +
  scale_x_log10() +
  facet_wrap(~Model, scales = "free_x") +
  labs(
    x = "Lambda", y = "",
    title = "Estimated Coefficients paths as a function of the penalty factor Lambda"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

::: {.rmdnote}
One word of caution: The package `glmnet` computes estimates of the coefficients $\hat\beta$ based on numerical optimization procedures. As a result the estimated coefficients for the special case with no regularization ($\lambda = 0$) can deviate from the standard OLS estimates.
:::


### Tune a Model

To compute $\hat\beta_\lambda^\text{Lasso}$ , we simply imposed a value for the penalty hyperparameter $\lambda$. Model tuning is the process of optimally selecting such hyperparameters. `tidymodels` provides extensive tuning options based on so-called **cross validation**. Again, we refer to any treatment of cross-validation to get a more detailed  discussion of the statistical underpinnings, here we focus on the general idea and the implementation with `tidymodels`. 

The goal for choosing $\lambda$ (or any other hyperparameter, for instance $\rho$) is to find a way to produce predictors $\hat{Y}$ for an outcome $Y$ that minimizes the mean squared prediction error $\mbox{MSPE} = \mbox{E}\left( \frac{1}{T}\sum_{t=1}^T (\hat{y}_t - y_t)^2 \right). $ Unfortunately, $\mbox{MSPE}$ is not directly observable and we can only compute an estimate because our data is random and because we do not observe the entire population.

Obviously, if we train an algorithm on the same data that we use to compute the error, our estimate $\hat{\mbox{MSPE}}$ would indicate way better predictive accuracy than what we can expect in a real out-of-sample data. As a result, we would overfit.

Cross validation is a technique that allows us to alleviate this problem. We approximate the true MSPE as the average of many mean squared prediction errors obtained by creating predictions for $K$ new random samples of the data, none of them used to train the algorithm $\frac{1}{K} \sum_{k=1}^K \frac{1}{T}\sum_{t=1}^T \left(\hat{y}_t^k - y_t^k\right)^2$. In practice, this is done by carving out a piece of our data and pretend it is an independent sample. We again divide the data into a training set and a test set. The MSPE on the test set is our measure for actual predictive ability, while we use the training set to fit models with the aim to find the **optimal** hyperparameter values. To do so, we further divide our training sample into (several) subsets, fit our model for a grid of potential hyperparameter values (e.g. $\lambda$) and evaluate the predictive accuracy on an *independent* sample. This works as follows:

1.  Specify a grid of hyperparameters
2.  Obtain predictors $\hat{y}_i(\lambda)$ to denote the predictors for the used parameters $\lambda$
3.  Compute $$
    \mbox{MSPE}(\lambda) = \frac{1}{K} \sum_{k=1}^K \frac{1}{T}\sum_{t=1}^T \left(\hat{y}_t^k(\lambda) - y_t^k\right)^2 
    $$ With K-fold cross validation, we do this computation $K$ times. Simply pick a validation set with $M=T/K$ observations at random and think of these as random samples $y_1^k, \dots, y_\tilde{T}^k$, with $k=1$.

How should you pick $K$? Large values of $K$ are preferable because the training data better imitates the original data. However, larger values of $K$ will have much higher computation time.
`tidymodels` provides all required tools to conduct $K$-fold cross validation. We just have to update our model specification and let `R` know which parameters to tune. In our case we specify both the penalty factor $\lambda$ as well as the mixing factor $\rho$ as *free* parameters. 
```{r}
lm_model <- linear_reg(
  penalty = tune(),
  mixture = tune()
) %>%
  set_engine("glmnet")

# Update the existing model
lm_fit <- lm_fit %>%
  update_model(lm_model)
```

For our sample, we consider time-series cross validation sample. That is, we tune our models with 20 random samples of length 5 years with a validation period of 4 years. For a grid of possible hyperparameters, we then fit the model for each fold and evaluate $MSPE$ in the corresponding validation set. Finally, we select the model specification with the lowest MSPE in the validation set. First, we define the cross-validation folds based on our training data only.

```{r}
data_folds <- time_series_cv(
  data        = training(split),
  date_var    = month,
  initial     = "5 years",
  assess      = "48 months",
  cumulative  = FALSE,
  slice_limit = 20
)
```
Then, we evaluate the performance for a grid of different penalty values. `tidymodels` provides functionalities to construct a suitable grid of hyperparameters with `grid_regular`. The code chunk below creates a grid of $10 \times 3$ hyperparameters and the function `tune_grid` does the tasks of evaluating all the models for each fold.
```{r}
lm_tune <- lm_fit %>%
  tune_grid(
    resample = data_folds,
    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),
    metrics = metric_set(rmse)
  )
```
After the tuning process, we collect the evaluation metrics (RMSE in our example) to identify the *optimal* model. Figure \@ref(fig:cvplot) illustrates the average validation set the root mean-squared error for each value of $\lambda$ and $\rho$.
```{r cvplot}
autoplot(lm_tune) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(y = "Root mean-squared prediction error",
       title = "MSPE for manufacturing excess returns",
       subtitle = "Lasso (1.0), Ridge (0.0) and Elastic net (0.5) with different levels of regularization.")
```
The figure shows that the cross-validated mean squared prediction error drops for both, Lasso and the Elastic Net and spike afterwards. For Ridge regression, the MSPE simply increases above a certain threshold. Recall, that the larger the regularization, the more restricted the model becomes. Thus, we would chooose the model with the lowest MSPE which happens to exhibit some intermediate level of regularization.

### Parallelized Workflow

Our starting point was the question: which factors determine industry returns? To illustrate the entire workflow, we now run the penalized regressions for all 10 industries. We want to identify relevant variables by fitting Lasso models for each of the time-series of industry returns. More specifically, for each industry, we perform cross-validation to identify the optimal penalty factor $\lambda$. Then, we use the set of `finalize_*` functions which take a list or tibble of tuning parameter values and update objects with those values. After determining the best model, we compute the final fit on the entire training set and analyse the estimated coefficients.

First we define the Lasso model with one tuning parameter:

```{r}
lasso_model <- linear_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")

lm_fit <- lm_fit %>%
  update_model(lasso_model)
```

The following task can be easily parallelized to substantially reduce computing time. We use the parallelization capabilities of `furrr` just as in chapter ???. Note that we can also just recycle all the steps from above and collect them in a function.

```{r}
select_variables <- function(input) {
  # Split into training and testing dataset
  split <- initial_time_split(input, prop = 4 / 5)

  # Data folds for cross validation
  data_folds <- time_series_cv(
    data = training(split),
    date_var = month,
    initial = "5 years",
    assess = "48 months",
    cumulative = FALSE,
    slice_limit = 20
  )

  # Model tuning with the Lasso model
  lm_tune <- lm_fit %>%
    tune_grid(
      resample = data_folds,
      grid = grid_regular(penalty(), levels = c(10)),
      metrics = metric_set(rmse)
    )

  # Finalizing: Identify the best model and fit with the training data
  lasso_lowest_rmse <- lm_tune %>% select_by_one_std_err("rmse")
  lasso_final <- finalize_workflow(lm_fit, lasso_lowest_rmse)
  lasso_final_fit <- last_fit(lasso_final, split, metrics = metric_set(rmse))

  # Extract the estimated coefficients
  lasso_final_fit %>%
    extract_fit_parsnip() %>%
    tidy() %>%
    mutate(
      term = gsub("factor_|macro_|industry_", "", term)
    )
}

plan(multisession, workers = availableCores()) # Parallelization

selected_factors <- data %>%
  nest(data = -industry) %>% # Computation by industry
  mutate(selected_variables = future_map(data, select_variables,
    .options = furrr_options(seed = TRUE)
  )) # Seed is required to make the cross-validation process reproducible
```

What has just happened? In principle, exactly the same as before but instead of computing the Lasso coefficients for one industry we did it for 10 in parallel. Now we just have to do some housekeeping and keep only variables which Lasso does *not* set to zero. We illustrate the results in a heat map.

```{r fig.heigt=10}
selected_factors %>%
  unnest(selected_variables) %>%
  filter(
    term != "(Intercept)",
    estimate != 0
  ) %>%
  add_count(term) %>%
  mutate(
    term = gsub("NA|ff_|q_", "", term),
    term = gsub("_x_", " ", term),
    term = fct_reorder(as_factor(term), n),
    term = fct_lump_min(term, min = 2),
    selected = 1
  ) %>%
  filter(term != "Other") %>%
  mutate(term = fct_drop(term)) %>%
  complete(industry, term, fill = list(selected = 0)) %>%
  ggplot(aes(industry,
    term,
    fill = as_factor(selected)
  )) +
  geom_tile() +
  scale_fill_manual(values = c("white", "cornflowerblue")) +
  theme_minimal() +
  theme(legend.position = "None") +
  labs(
    x = NULL, y = NULL,
    title = "Selected variables for different industries"
  )
```

The heat map conveys two main insights: First, we see a lot of white which simply means that a lot of the factors, macroeconomic variables and also the interaction terms are not relevant when it comes to explain the cross-section of returns across the industry portfolios. In fact, only the market factor and the return-on-equity factor play a role for several industries. Second, there seems to be quite some heterogeneity across different industries. While not even the market factor is selected by Lasso for Utilities (which means the proposed model essentially just contains an intercept), quite a number of factors are selected for, e.g., High-Tech and Energy but they do not coincide at all. In other words, there seems to be a clear picture that we do not need a lot of factors, but Lasso does not provide consens across industries when it comes to pricing abilities.

<!--chapter:end:40_factor_selection_with_machine_learning.Rmd-->

# Option Pricing via Machine learning methods

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

Machine learning is seen as a part of artificial intelligence. 
Machine learning algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so.
While Machine learning can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests and neural networks to Option pricing. More specifically, we are going to create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for Call options. Then, we train different models to *learn* how to price Call options without prior knowledge of the theoretical underpinnings of the famous Option pricing equation. 

The roadmap is as follows: We first provide a very brief introduction into regression trees, random forests and neural networks. As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues.
We show how to implement random forests and deep neural networks with tidy principles using `tidymodels` or `tensorflow` for more complicated network structures. 

Note: in order to replicate the analysis regarding neural networks in this chapter, you have to install `TensorFlow` on your system which requires administrator rights on your machine. Parts of this can be done from within R, just follow [these quick start instructions](https://tensorflow.rstudio.com/installation/).

Throughout this chapter we need the following packages.
```{r}
library(tidyverse)
library(tidymodels)
library(keras)
library(hardhat)
library(ranger)
```

## Regression trees and random forests

Regression trees have become a popular machine learning approach for incorporating multiway predictor interactions. Trees are fully nonparametric and possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each. A tree “grows” in a sequence of steps. At each step, a new “branch” sorts the data which is left over from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into rectangular partitions, and approximates the unknown function $f(x)$ with the average value of the outcome variable within each partition

We partition the  predictor space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$. For any predictor $x$ that falls within region $R_j$ we estimate $f(x)$ with the average of the training observations $y_i$ for which the associated predictor $x_i$ is also in $R_j$. Once we select a partition $\mathbf{x}$ to split in order to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, which we will call $R_1(j,s)$ and $R_2(j,s)$, that split our observations in the current partition by asking if $x_j$ is bigger than $s$:
$$
R_1(j,s) = \{\mathbf{x} \mid x_j < s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
$$
To pick $j$ and $s$ we find the pair that minimizes the residual sum of square (RSS):
$$
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
$$
Note: We don't scale by $\# R_k(j, s)$! As in the chapter on penalized regressions, we next should ask: What are the hyperparameters decisions? Instead of a regularization parameter, trees are fully determined by the number branches used to generate the partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).

Single tree models suffer from high variance. Pruning the tree helps reduce this variance. Random forests address the shortcomings of decision trees. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). A forest basically implies to create many regression trees and average their predictions. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. More specifically, we build $B$ decision trees $T_1, \ldots, T_B$ using the training sample. For that purpose we randomly select features to be included in the building of each tree. For each observation in the test set we then form a prediction $\hat{y} = \frac{1}{B}\sum\limits_{i=1}^B\hat{y}_{T_i}$.

## Neural Networks

TODO for Stefan: add brief description of a neural network 

## Option Pricing

To apply Machine Learning methods in a relevant field of finance we focus on option pricing. In its most basic form, Call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price $K$) at a specific date (the exercise date $T$). The Black–Scholes price of a call option for a non-dividend-paying underlying stock is given by
$$
\begin{aligned}
  C(S, T) &= \Phi(d_1)S - \Phi(d_1 - \sigma\sqrt{T})Ke^{-r T} \\
     d_1 &= \frac{1}{\sigma\sqrt{T}}\left[\ln\left(\frac{S}{K}\right) + \left(r_f + \frac{\sigma^2}{2}\right)T\right]
\end{aligned}
$$
where $C(S, T)$ is the price of the option as a function of today's stock price of the underlying, $S$, with time to maturity$T$, $r_f$ is the risk-free interest rate, and $\sigma$ is the volatility of the underlying stock return. $\Phi$ is the cumulative distribution function of a standard normal random variable.

The Black-Scholes equation provides an easy way to compute the arbitrage-free price of a Call option once the parameters $S, K, r_f, T$ and $\sigma$ are specified (arguably, all parameters are easily to specify except for $\sigma$ which has to be estimated). A simple `R` function allows to compute the price as we do below. 
```{r}
black_scholes_price <- function(S = 50, K = 70, r = 0, T = 1, sigma = 0.2) {
  # Arbitrage-free price of a Call option
  d1 <- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))
  value <- S * pnorm(d1) - K * exp(-r * T) * pnorm(d1 - sigma * sqrt(T))
  return(value)
}
```

## Learning Black-Scholes

We illustrate the concept of machine learning by showing how machine learning methods *learn* the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation. 

To that end we start with simulated data. We compute option prices for Call options for a grid of different combinations of times to maturity (`T`), risk-free rate (`r`), volatility (`sigma`), strike prices (`K`) and current stock prices (`S`). In the code below we add an idiosyncratic error term to each observation such that the prices which are considered observed do not exactly reflect the values implied by the Black-Scholes equation.

```{r}
option_prices <- expand_grid(
  S = 40:60, # stock price
  K = 20:90, # strike price
  r = seq(from = 0, to = 0.05, by = 0.01), # risk-free rate
  T = seq(from = 3 / 12, to = 2, by = 1 / 12), # Time to maturity
  sigma = seq(from = 0.1, to = 0.8, by = 0.1)
) %>%   mutate(
    black_scholes = black_scholes_price(S, K, r, T, sigma), # Option price in theory
    observed_price = map(black_scholes, function(x) x + rnorm(2, sd = 0.15))
  ) %>% # Add some random deviations to each option price
  unnest(observed_price)
```

The code above generates `r option_prices %>% nrow() / 2` random parameter constellations and for each of these values two *observed* prices which reflect the Black-Scholes prices and a random innovation term which *pollutes* the observed prices. 

Next, we split the data into a training set (which contains 1\% of all the observed option prices) and a test set which is only going to be used for the final evaluation. Note that the entire grid of possible combinations contains more than `r option_prices %>% nrow()` different specifications, thus the sample to learn the Black-Scholes price contains only `r option_prices %>% nrow() / 100` and therefore is relatively small.
In order to keep the analysis reproducible, we use `set.seed()`. A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines. 

```{r}
set.seed(301088) # Ensure the analysis can be reproduced
split <- initial_split(option_prices, prop = 1 / 100)
```

We process the training dataset further before we fit the different Machine learning models. For that purpose we define a `recipe` which defines all processing steps. For our specific case we want to explain the observed price by the 5 variables that enter the Black-Scholes equation. The **true** price should obviously not be used to fit the model. The recipe also reflect that we standardize all predictors to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.  
```{r}
rec <- recipe(observed_price ~ .,
  data = option_prices
) %>%
  step_rm(black_scholes) %>% # Exclude the true price
  step_normalize(all_predictors())
```

Next, we propose two ways to fit a neural network to the data. Note that both require that `keras` is installed on your local machine. The function `mlp` from the package `parsnip` provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with 20 hidden units. We set the number of training iterations to `epochs = 75`. The option `set_mode("regression")` specifies a linear activation function for the output layer. 

```{r}
# Single layer neural network
nnet_model <- mlp(
  epochs = 75,
  hidden_units = 20
) %>%
  set_mode("regression") %>%
  set_engine("keras", verbose = 0) # `verbose=0` argument prevents logging the results
```

We can follow the straightforward `tidymodel` workflow as in the chapter before: Define a workflow, equip it with the recipe and the associated model. Finally, fit the model with the training data. 
```{r}
nn_fit <- workflow() %>%
  add_recipe(rec) %>%
  add_model(nnet_model) %>%
  fit(data = training(split))
```

Once you are familiar the `tidymodel` workflow, it is a piece of cake to fit other models from the `parsnip` family. For instance, the model below initializes a random forest with 50 trees contained in the ensemble where we require at least 20 observations in a node. 

```{r}
rf_model <- rand_forest(
  trees = 50,
  min_n = 20
) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

Fitting the model follows exactly the same convention as for the neural network before.
```{r}
rf_fit <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = training(split))
```

Note that while the `tidymodels` workflow is extremely convenient, more sophisticated *deep* neural networks are not supported yet (as of January 2022). For that reason, the code snippet below illustrates how to initialize a sequential model with 3 hidden layers with 20 units per layer. The `keras` package provides a convenient interface and is flexible enough to handle different activation functions. The `compile` command defines the loss function with which the model predictions are evaluated. 

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 20, activation = "sigmoid", input_shape = 5) %>%
  layer_dense(units = 20, activation = "sigmoid") %>%
  layer_dense(units = 20, activation = "sigmoid") %>%
  layer_dense(units = 1, activation = "linear") %>%
  compile(
    loss = "mean_absolute_error"
  )
model
```

To train the neural network, we simply provide the inputs (`x`) and the variable to predict (`y`) and then fit the parameters. Note the slightly tedious use of the method `extract_mold(nn_fit)`: instead of simply using the **raw** data, we fit the neural network with the same processed data that is used for the single-layer feed-forward network. What is the difference to simply calling `x = training(data) %>% select(-observed_price, -black_scholes)`? Recall, that the recipe standardizes the variables such that all columns have unit standard deviation and zero mean. Further, it adds consistency if we ensure that all models are trained using the same recipe such that a change in the recipe is reflected in the performance of any model. A final note on a potentially irritating observation: Note that `fit()` alters the `keras` model: this is one of the few instances where a function in `R` alters the *input* such that after calling the function the object `model` is not going to be same anymore!

```{r}
model %>%
  fit(
    x = extract_mold(nn_fit)$predictors %>% as.matrix(),
    y = extract_mold(nn_fit)$outcomes %>% pull(observed_price),
    epochs = 75, verbose = 0
  )
```

Before it comes to evaluation we implement one more final model: In principle, any non-linear function can also be approximated by a linear model that contains polynomial expansions of the input variables. To illustrate this we first define a new recipe, `rec_linear`, which processes the training data even further: We include polynomials up to the tenth degree of each predictor and then add all possible pairwise interaction terms. The final recipe step `step_lincomb` removes potentially redundant variables (for instance, the interaction between $r^4$ and $r^5$ is the same as the term $r^9$). We fit a Lasso regression model with a pre-specified penalty term (consult the chapter on factor selection on how to tune the model hyperparameters).

```{r} 
rec_linear <- rec %>%
  step_poly(all_predictors(), degree = 10, options = list(raw = T)) %>%
  step_interact(terms = ~ all_predictors():all_predictors()) %>%
  step_lincomb(all_predictors())

lm_model <- linear_reg(penalty = 0.01) %>%
  set_engine("glmnet")

lm_fit <- workflow() %>%
  add_recipe(rec_linear) %>%
  add_model(lm_model) %>%
  fit(data = training(split))
```

## Evaluating predictions

Finally, we collect all predictions to compare the **out-of-sample** prediction error. 
Note, that for the evaluation we use, again, the call to `extract_mold` to ensure that we use the same pre-processing steps for the testing data across each model. We make also use of the somewhat advanced functionality in `hardhat::forge` which provides an easy, consistent, and robust pre-processor at prediction time. 

```{r}
out_of_sample_data <- testing(split) %>% slice_sample(n = 10000) # We evaluate the predictions based on 100k new data points

predictive_performance <- model %>%
  predict(forge(out_of_sample_data, extract_mold(nn_fit)$blueprint)$predictors %>% as.matrix()) %>%
  as.vector() %>%
  tibble("Deep NN" = .) %>%
  bind_cols(nn_fit %>%
    predict(out_of_sample_data)) %>%
  rename("Single Layer" = .pred) %>%
  bind_cols(lm_fit %>% predict(out_of_sample_data)) %>%
  rename("Lasso" = .pred) %>%
  bind_cols(rf_fit %>% predict(out_of_sample_data)) %>%
  rename("Random Forest" = .pred) %>%
  bind_cols(out_of_sample_data) %>%
  pivot_longer("Deep NN":"Random Forest", names_to = "Model") %>%
  mutate(
    moneyness = (S - K),
    pricing_error = sqrt((value - black_scholes)^2) # mean squared prediction error
  ) 
```
In the lines above we use each of the fitted models to generate predictions for the entire test data set of option prices. As one possible measure of pricing accuracy we evaluate the absolute pricing error, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model. 

```{r}
predictive_performance %>%
  ggplot(aes(x = moneyness, y = pricing_error, color = Model)) +
  geom_jitter(alpha = 0.05) +
  geom_smooth(se = FALSE) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(x = "Moneyness (S - K)", color = NULL, 
       y = "Mean Squared Prediction Error (USD)", 
       title = "Prediction Errors: Call option prices") 
```

The results can be summarized as follow: i) All machine learning methods seem to be able to *price* Call options after observing the training test set. ii) The average prediction errors increase for far out-of-the money options, especially for the Single Layer neural network. ii) Random forest seems to perform consistently better in prediction option prices than the Single Layer network. iii) The deep neural network yields the best out-of-sample predictions.

<!--chapter:end:41_option_pricing_via_machine_learning.Rmd-->

# Parametric Portfolio Policies

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

In this section, we introduce different portfolio performance measures to evaluate and compare different allocation strategies. For this purpose, we introduce a direct (and probably simplest) way to estimate optimal portfolio weights when stock characteristics are related to the stock’s expected return, variance, and covariance with other stocks: We parametrize weights as a function of the characteristics such that we maximize expected utility. The approach is feasible for large portfolio dimensions (such as the entire CRSP universe) and has been proposed by Brand et al. (2009) in their influential paper *Parametric Portfolio Policies: Exploiting Characteristics in the Cross Section of Equity Returns*. 

To get started, we load the monthly CRSP file which forms our investment universe. 
```{r}
library(tidyverse)
library(lubridate)
library(RSQLite)
```

```{r}
# Load data from database
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)

crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>%
  collect()
```

For the purpose of performance evaluation, we will need the Fama French 3 factor model monthly returns in order to be able to compute CAPM alphas. 
```{r}
factors_ff_monthly <- tbl(tidy_finance, "factors_ff_monthly") %>%
  collect()
```

Next we create some characteristics that have been proposed in the literature to have an effect on the expected returns or expected variances (or even higher moments) of the return distribution: We record for each firm the lagged one-year return (mom) defined as the compounded return between months $t − 13$ and $t − 2$ and the firm's market equity (me), defined as the log of the price per share times the number of shares outstanding

```{r}
crsp_monthly_lags <- crsp_monthly %>% 
  transmute(permno, 
            month_12 = month %m+% months(12), 
            month_1 = month %m+% months(1),
            altprc = abs(altprc))

crsp_monthly <- crsp_monthly %>% 
  inner_join(crsp_monthly_lags %>% select(-month_1), 
             by = c("permno", "month" = "month_12"), suffix = c("", "_12")) %>% 
  inner_join(crsp_monthly_lags %>% select(-month_12), 
             by = c("permno", "month" = "month_1"), suffix = c("", "_1"))

# Create characteristics for portfolio tilts
crsp_monthly <- crsp_monthly %>%
  group_by(permno) %>%
  mutate(
    momentum_lag = altprc_1 / altprc_12, # Gross returns TODO for SV: Do we need slider?
    size_lag = log(mktcap_lag)
  ) %>%
  drop_na(contains("lag"))
```

The basic idea of parametric portfolio weights is easy to explain: Suppose that at each date $t$ we have $N_t$ stocks in the investment universe, where each stock $i$ has return of $r_{i, t+1}$ and is associated with a vector of firm characteristics $x_{i, t}$ such as time-series momentum or the market capitalization. The investors problem is to choose portfolio weights $w_{i,t}$ to maximize the expected utility of the portfolio return:
$$\begin{align}
\max_{w} E_t\left(u(r_{p, t+1})\right) = E_t\left[u\left(\sum\limits_{i=1}^{N_t}w_{i,t}r_{i,t+1}\right)\right].
\end{align}$$
Where do the stock characteristics show up? We parametrize the optimal portfolio weights as a function of $x_{i,t}$ with the following linear specification for the portfolio weights:
$$w_{i,t} = \bar{w}_{i,t} + \frac{1}{N_t}\theta'\hat{x}_{i,t}$$ where $\bar{w}_{i,t}$ is the weight of a benchmark portfolio (we use the value-weighted or naive portfolio in the application below), $\theta$ is a vector of coefficients which we are going to estimate and $\hat{x}_{i,t}$ are the characteristics of stick $i$, cross-sectionally standardized to have zero mean and unit standard deviation. Think of the portfolio strategy as a form of active portfolio management relative to a performance benchmark: Deviations from the benchmark portfolio are derived from the individual stock characteristics. Note that by construction the weights sum up to one as $\sum_{i=1}^{N_t}\hat{x}_{i,t} = 0$ due to the standardization. Note that the coefficients are constant across assets and through time. The implicit assumption is that the characteristics fully capture all aspects of the joint distribution of returns that are relevant for forming optimal portfolios.       

To get started, we first implement the cross-sectional standardization for the entire CRSP universe. We also keep track of (lagged) relative market capitalization which will represent the value-weighted benchmark portfolio.
```{r}
crsp_monthly <- crsp_monthly %>%
  group_by(month) %>%
  mutate(
    n = n(), # number of traded assets N_t (benchmark for naive portfolio)
    relative_mktcap = mktcap_lag / sum(mktcap_lag), # Value weighting benchmark
    across(contains("lag"), ~ (. - mean(.)) / sd(.)), # standardization. Note: Code handles every column with "lag" as a characteristic)
  ) %>%
  ungroup() %>%
  select(-mktcap_lag, -altprc)
```

Next we can move to optimal choices of $\theta$. We rewrite the optimization problem together with the weight parametrisation and can then estimate $\theta$ to maximize the objective function based on our sample 
$$\begin{align}
E_t\left(u(r_{p, t+1})\right) = \frac{1}{T}\sum\limits_{t=0}^{T-1}u\left(\sum\limits_{i=1}^{N_t}\left(\bar{w}_{i,t} + \frac{1}{N_t}\theta'\hat{x}_{i,t}\right)r_{i,t+1}\right).
\end{align}$$
The allocation strategy is simple because the number of parameters to estimate is small. Instead of a tedious specification of the $N_t$ dimensional vector of expected returns and the $N_t(N_t+1)/2$ free elements of the variance covariance, all we need to focus on in our application is the vector $\theta$ which contains only 2 elements - the relative deviation from the benchmark due to size and due to past returns. 

To get a feeling on the performance of such an allocation strategy we start with an arbitrary vector $\theta$ - the next step is then to choose $\theta$ in an optimal fashion to maximize the objective function. 

```{r}
# Automatic detection of parameters and initialization of parameter vector theta
number_of_param <- sum(grepl("lag", crsp_monthly %>% colnames()))
theta <- rep(1.5, number_of_param) # We start with some arbitrary initial values for theta
names(theta) <- colnames(crsp_monthly)[grepl("lag", crsp_monthly %>% colnames())]
```

The following function computes the portfolio weights $\bar{w}_{i,t} + \frac{1}{N_t}\theta'\hat{x}_{i,t}$ according to our parametrization for a given value of $\theta$ - everything in one pipeline, thus here goes a quick walk-through what is happening:
The weights are computed for each month by first computing the tilting values $\frac{1}{N_t}\theta'\hat{x}_{i, t}$ away from the benchmark portfolio. Then, we compute the benchmark portfolio which can in principle be any reasonable set of weights, in our case we either choose the value- or equal-weighted allocation. `weight_tilt` completes the picture and contains the final portfolio weights which deviate from the benchmark portfolio depending on the tilting. The final few lines go a bit further and implement a simple version of a no-short sale constraint: To ensure portfolio constraints via the parametrization is not straightforward but in our case we simply renormalize the portfolio weights before returning them by computing 
$$w_{i,t}^+ = \frac{\max(0, w_{i,t})}{\sum\limits_{j=1}^{N_t}\max(0, w_{i,t})}.$$
Here we go to optimal portfolio weights in 20 lines. 
```{r}
compute_portfolio_weights <- function(theta,
                                      data,
                                      value_weighting,
                                      allow_short_selling) {
  data %>%
    group_by(month) %>% # Aggregate to compute portfolio returns
    bind_cols(
      characteristic_tilt = data %>% # Computes theta'x / N_t
        transmute(across(contains("lag"), ~ . / n)) %>%
        as.matrix() %*% theta %>% as.numeric()
    ) %>%
    mutate(
      weight_benchmark = case_when( # Definition of the benchmark weight
        value_weighting == TRUE ~ relative_mktcap,
        value_weighting == FALSE ~ 1 / n
      ),
      weight_tilt = weight_benchmark + characteristic_tilt, # Parametric Portfolio Weights
      weight_tilt = case_when( # Extension as of Brandt, Santa-Clara and Valkanoff: Short-sell constraint
        allow_short_selling == TRUE ~ weight_tilt,
        allow_short_selling == FALSE ~ pmax(0, weight_tilt)
      ),
      weight_tilt = weight_tilt / sum(weight_tilt) # Weights sum up to 1 (even if no-short selling)
    ) %>%
    ungroup()
}
```

Done! Next step is to compute the optimal portfolio weights at your convenience.
```{r}
weights_crsp <- compute_portfolio_weights(theta,
  crsp_monthly,
  value_weighting = TRUE,
  allow_short_selling = TRUE
)
```

But are these weights optimal in any way? Most likely not as we chose $\theta$ arbitrarily but in order to evaluate the performance of an allocation strategy, one can think of many different approaches. One simple setup as above could be to simply evaluate the hypothetical utility of an agent equipped with a power utility function $u_\gamma(r) = \frac{(1 + r)^\gamma}{1-\gamma}$, implement in R in the function below. 
```{r}
power_utility <- function(r, gamma = 5) { # gamma is the risk aversion factor
  (1 + r)^(1 - gamma) / (1 - gamma)
}
```

No doubt, there are many more ways to evaluate a portolio. The function below provides an exhaustive summary of interesting measures which all can be considered relevant. 
Do we need all these evaluation measures? It depends: The original paper only cares about
expected utility in order to choose $\theta$. But if you want to choose optimal values that achieve the highest performance while putting some constraints on your portfolio weights it is helpful to have everything in one function.

```{r}
evaluate_portfolio <- function(weights_crsp,
                               full_evaluation = TRUE) {

  evaluation <- weights_crsp %>%
    group_by(month) %>%
    # Compute monthly portfolio returns
    summarise(
      return_tilt = weighted.mean(ret_excess, weight_tilt),
      return_benchmark = weighted.mean(ret_excess, weight_benchmark)
    ) %>%
    pivot_longer(-month, values_to = "portfolio_return", names_to = "Model") %>%
    group_by(Model) %>%
    left_join(factors_ff_monthly, by = "month") %>% # FF data to compute alpha
    summarise(tibble(
      "Expected utility" = mean(power_utility(portfolio_return)),
      "Average return" = 100 * mean(12 * portfolio_return),
      "SD return" = 100 * sqrt(12) * sd(portfolio_return),
      "Sharpe ratio" = mean(portfolio_return) / sd(portfolio_return),
      "CAPM alpha" = coefficients(lm(portfolio_return ~ mkt_excess))[1],
      "Market beta" = coefficients(lm(portfolio_return ~ mkt_excess))[2]
    )) %>%
    mutate(Model = gsub("return_", "", Model)) %>%
    pivot_longer(-Model, names_to = "measure") %>%
    pivot_wider(names_from = Model, values_from = value)

  if (full_evaluation) { # additional values based on the portfolio weights
    weight_evaluation <- weights_crsp %>%
      select(month, contains("weight")) %>%
      pivot_longer(-month, values_to = "weight", names_to = "Model") %>%
      group_by(Model, month) %>%
      transmute(tibble(
        "Absolute weight" = abs(weight),
        "Max. weight" = max(weight),
        "Min. weight" = min(weight),
        "Avg. sum of negative weights" = -sum(weight[weight < 0]),
        "Avg. fraction of negative weights" = sum(weight < 0) / n()
      )) %>%
      group_by(Model) %>%
      summarise(across(-month, ~ 100 * mean(.))) %>%
      mutate(Model = gsub("weight_", "", Model)) %>%
      pivot_longer(-Model, names_to = "measure") %>%
      pivot_wider(names_from = Model, values_from = value)
    evaluation <- bind_rows(evaluation, weight_evaluation)
  }
  return(evaluation)
}
```

Let's take a look at the different portfolio strategies and evaluation measures.
```{r}
evaluate_portfolio(weights_crsp)
```
The value weighted portfolio delivers an annualized return of above 6 percent and clearly outperforms the tilted portfolio, irrespective of whether we evaluate expected utility, the Sharpe ratio or the CAPM alpha. We can conclude the the market beta is close to one for both strategies (naturally almost identically 1 for the value-weighted benchmark portfolio). When it comes to the distribution of the portfolio weights, we see that the benchmark portfolio weight takes less extreme positions (lower average absolute weights and lower maximum weight). By definition, the value-weighted benchmark does not take any negative positions, while the tilted portfolio also takes short positions. 

Next we move to a choice of $\theta$ that actually aims to to improve some (or all) of the performance measures. We first define a helper function `compute_objective_function` which is then passed to R's optimization schemes. 
```{r}
compute_objective_function <- function(theta,
                                       data,
                                       objective_measure = "Expected utility",
                                       value_weighting,
                                       allow_short_selling) {
  processed_data <- compute_portfolio_weights(
    theta,
    data,
    value_weighting,
    allow_short_selling
  )

  objective_function <- evaluate_portfolio(processed_data, full_evaluation = FALSE) %>%
    filter(measure == objective_measure) %>%
    pull(tilt)

  return(-objective_function)
}
```
You may wonder why we return the negative value of the objective function. This is simply due to the common convention for optimization procedures to search minima as a default. By minimizing the negative value of the objective function we will get the maximum value as a result.
Optimization in R in its most basic form is done by `optim`. As main inputs, the function requires an initial "guess" of the parameters, and the function to minimize. Next, we are therefore equipped to compute the optimal values of $\theta$ which maximize hypothetical expected utility of the investor. 

```{r}
optimal_theta <- optim(
  par = theta, # Initial vector of thetas (can be any value)
  compute_objective_function,
  objective_measure = "Expected utility",
  data = crsp_monthly,
  value_weighting = TRUE,
  allow_short_selling = TRUE
)

optimal_theta$par # Optimal values
```
The chosen values of $\theta$ are easy to interpret on an intuitive basis: Expected utility increases by tilting weights from the value weighted portfolio towards smaller stocks (negative coefficient for size) and towards past winners (positive value for momentum). 

A final open question is then: How does the portfolio perform for different model specifications?

```{r}
full_model_grid <- expand_grid(
  value_weighting = c(TRUE, FALSE),
  allow_short_selling = c(TRUE, FALSE),
  data = list(crsp_monthly)
) %>%
  mutate(optimal_theta = pmap(
    .l = list(
      data,
      value_weighting,
      allow_short_selling
    ),
    .f = ~ optim(
      par = rep(0, number_of_param),
      compute_objective_function,
      data = ..1,
      objective_measure = "Expected utility",
      value_weighting = ..2,
      allow_short_selling = ..3,
    )$par
  ))
```

Finally, let's get to the comparison. The table below shows summary statistics for all possible combinations: Equal- or Value-weighted benchmark portfolio, with or without short-selling constraints. 
```{r}
table <- full_model_grid %>%
  mutate(
    processed_data = pmap(
      .l = list(optimal_theta, data, value_weighting, allow_short_selling),
      .f = ~ compute_portfolio_weights(..1, ..2, ..3, ..4)
    ),
    portfolio_evaluation = map(processed_data, evaluate_portfolio, full_evaluation = TRUE)
  ) %>%
  select(value_weighting, allow_short_selling, portfolio_evaluation) %>%
  unnest(portfolio_evaluation)

table %>%
  rename(
    " " = benchmark,
    Optimal = tilt
  ) %>%
  mutate(
    value_weighting = case_when(
      value_weighting == TRUE ~ "VW",
      value_weighting == FALSE ~ "EW"
    ),
    allow_short_selling = case_when(
      allow_short_selling == TRUE ~ "",
      allow_short_selling == FALSE ~ "(no s.)"
    )
  ) %>%
  pivot_wider(
    names_from = value_weighting:allow_short_selling,
    values_from = " ":Optimal,
    names_glue = "{value_weighting} {allow_short_selling} {.value} "
  ) %>%
  select(measure, `EW    `, `VW    `, sort(contains("Optimal"))) %>%
  kableExtra::kable()
``` 

<!--chapter:end:50_parametric_portfolio_policies.Rmd-->

# Constraint Optimization and Portfolio Backtesting

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = TRUE, 
                      fig.width = 8, 
                      fig.height = 4, 
                      fig.align = "center")
```

In this section we conduct portfolio backtesting procedures in more realistic settings with transaction costs and investment constraints such as no-short selling rules. 
We start with *standard* mean-variance efficient portfolios and then introduce further constraints step-by-step. Numerical constrained optimization is done with the packages `quadprog` (for quadratic objective functions such as in typical mean-variance framework) and `alabama` (for more general, non-linear objectives and constraints).

```{r}
library(tidyverse)
library(RSQLite)
library(quadprog) # Optimization (mean-variance)
library(alabama)  # Advanced optimization (non-linear objective or constraints)
```

We start by loading the required data. In our application we will restrict our investment universe to the monthly Fama-French industry portfolio returns.

```{r}
tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite", extended_types = TRUE)

# Load industry data
industry_returns <- tbl(tidy_finance, "industries_ff_monthly") %>% 
  collect() 

industry_returns <- industry_returns %>% 
  select(-month)

N <- ncol(industry_returns) # Number of assets
```

First a brief recap. A common objective for portfolio optimization is to choose mean-variance efficient portfolio, that is the allocation which delivers the lowest possible return variance for a given minimum level of expected returns. In the most extreme case where the investor is only concerned about portfolio variance, she may choose to implement the minimum variance portfolio weights which are given by the solution to 
$$\arg\min w'\Sigma w \text{ s.t. } w'\iota = 1$$
where $\Sigma$ is the $(N \times N)$ variance covariance matrix of the returns. The optimal weights $\omega_\text{mvp}$ can be found analytically and are $\omega_\text{mvp} = \frac{\Sigma^{-1}\iota}{\iota'\Sigma^{-1}\iota}$. In code, this is equivalent to the following:
```{r}
Sigma <- cov(industry_returns)

w_mvp <- solve(Sigma) %*% rep(1, ncol(Sigma))
w_mvp <- as.vector(w_mvp / sum(w_mvp))
```

Next, consider now an investor who aims to achieve minimum variance *given a minimum desired expected return* $\bar{\mu}$ such that she chooses
$$\arg\min w'\Sigma w \text{ s.t. } w'\iota = 1 \text{ and } \omega'\mu \geq \bar{\mu}.$$
The Lagrangian reads $$ \mathcal{L}(\omega) = w'\Sigma w - \lambda(w'\iota - 1) - \tilde{\lambda}(\omega'\mu - \bar{\mu}) $$. Solving the first order conditions yields
$$
\begin{aligned}
2\Sigma w &= \lambda\iota + \tilde\lambda \mu\\
\omega &= \frac{\lambda}{2}\Sigma^{-1}\iota + \frac{\tilde\lambda}{2}\Sigma^{-1}\mu
\end{aligned}
$$
The constraints imply 
$$
\begin{aligned}
1 &= \iota'\omega = \frac{\lambda}{2}\underbrace{\iota'\Sigma^{-1}\iota}_{C} + \frac{\tilde\lambda}{2}\underbrace{\iota'\Sigma^{-1}\mu}_D
\Rightarrow \lambda = \frac{2 - \tilde\lambda D}{C}\\
\bar\mu &= \frac{\lambda}{2}\underbrace{\mu'\Sigma^{-1}\iota}_{D} + \frac{\tilde\lambda}{2}\underbrace{\mu'\Sigma^{-1}\mu}_E = \frac{1}{2}\left(\frac{2 - \tilde\lambda D}{C}\right)D+\frac{\tilde\lambda}{2}E  \\&=\frac{D}{C}+\frac{\tilde\lambda}{2}\left(E - \frac{D^2}{C}\right)\\
\Rightarrow \tilde\lambda &= 2\frac{\bar\mu - D/C}{E-D^2/C}\\
\end{aligned}
$$
As a result, the efficient portfolio weight takes the form
$$w_\text{eff} = \omega_\text{mvp} + \frac{\tilde\lambda}{2}\left(\Sigma^{-1}\mu -\frac{D}{C}\Sigma^{-1}\iota \right)$$
Note that $$\iota'\left(\Sigma^{-1}\mu -\frac{D}{C}\Sigma^{-1}\iota \right) = D - D = 0\text{ so }\iota'\omega_\text{eff} = \iota'\omega_\text{mvp} = 1$$ and $$\mu'\omega_\text{eff} = \frac{D}{C} + \bar\mu - \frac{D}{C} = \bar\mu$$. As a result, the efficient portfolio allocates wealth in the minimum variance portfolio $\omega_\text{mvp}$ and a levered (self-financing) portfolio to increase the expected return.

You can rewrite the optimal portfolio weights as follows:
$$\begin{aligned}
 \omega^*_{t+1} & = \frac{1}{\gamma}\left(\Sigma_t^{-1} - \frac{1}{\iota' \Sigma_t^{-1}\iota }\Sigma_t^{-1}\iota\iota' \Sigma_t^{-1} \right) \mu_t  + \frac{1}{\iota' \Sigma_t^{-1} \iota }\Sigma_t^{-1} \iota
 \end{aligned}$$
Empirically this classical solution imposes many problems: Especially the estimates of $\mu_t$ are noisy over short horizons, the ($N \times N$) matrix $\Sigma_t$ contains $N(N-1)/2$ distinct elements and thus, estimation error is huge. Even worse, if the asset universe contains more assets ($N$) than available time periods ($T$), the sample variance covariance matrix is not longer positive definite such that the inverse $\Sigma^{-1}$ does not exist anymore. 
The empirical evidence regarding the performance of a mean-variance optimization procedure in which you simply plugin some sample estimates $\hat \mu_t$ and $\hat \Sigma_t$ can be summarised rather easily: Mean-variance optimization performs badly! The literature brought forward a lot of proposal to overcome these empirical issues, for instance, impose some form of regularization of $\Sigma$, to rely on Bayesian priors inspired by theoretical asset pricing models, to use high-frequency data to improve forecasting (which are not captured in this book). One unifying framework that works easily, effective (even for large dimensions) and is purely inspired by economic arguments is an ex-ante adjustment for transaction costs: 

Assume that returns are multivariate normal distributed such that: $p_t({r}_{t+1}|\mathcal{M})=N(\mu,\Sigma)$. Additionally we assume quadtratic transaction costs which penalize rebalancing such that $$\begin{aligned}
\nu\left(\omega_{t+1},\omega_{t^+}, \beta\right) :=\nu_t\left(\omega_{t+1}, \beta\right) = \frac{\beta}{2} \left(\omega_{t+1} - \omega_{t^+}\right)'\left(\omega_{t+1}- \omega_{t^+}\right),
\end{aligned}$$
with cost parameter $\beta>0$ and $\omega_{t^+} := {\omega_t \circ  (1 +r_{t})}/{\iota' (\omega_t \circ (1 + r_{t}))}$. Note: $\omega_{t^+}$ differs mechanically from $\omega_t$ due to the returns in the past period.  

Then, the optimal portfolio choice is
$$\begin{aligned}
\omega_{t+1} ^* :=&  \arg\max_{\omega \in \mathbb{R}^N,  \iota'\omega = 1} \omega'\mu - \nu_t (\omega,\omega_{t^+}, \beta) - \frac{\gamma}{2}\omega'\Sigma\omega \\
=&\arg\max_{\omega\in\mathbb{R}^N,\text{ }  \iota'\omega=1}
\omega'\color{red}{\mu^*} - \frac{\gamma}{2}\omega'\color{red}{\Sigma^*} \omega ,
\end{aligned}$$
where
$$\begin{aligned}
\color{red}{\mu^*:=\mu+\beta \omega_{t^+}} \quad  \text{and} \quad \color{red}{\Sigma^*:=\Sigma + \frac{\beta}{\gamma} I_N}.
\end{aligned}$$
As a result, adjusting for transaction costs implies a standard mean-variance optimal portfolio choice with adjusted return parameters $\Sigma^*$ and $\mu^*$: $$\begin{aligned}
 \omega^*_{t+1} & = \frac{1}{\gamma}\left(\Sigma^{*-1} - \frac{1}{\iota' \Sigma^{*-1}\iota }\Sigma^{*-1}\iota\iota' \Sigma^{*-1} \right) \mu^*  + \frac{1}{\iota' \Sigma^{*-1} \iota }\Sigma^{*-1} \iota.
 \end{aligned}$$

An alternative formulation of the optimal portfolio can be derived as follows: 
$$\begin{aligned}
\omega_{t+1} ^*=\arg\max_{\omega\in\mathbb{R}^N,\text{ }  \iota'\omega=1}
\omega'\left(\mu+\beta\left(\omega_{t^+} - \frac{1}{N}\iota\right)\right) - \frac{\gamma}{2}\omega'\Sigma^* \omega .
\end{aligned}$$
The optimal weights correspond to a mean-variance portfolio where the vector of expected returns is such that assets that currently exhibit a higher weight are considered as delivering an higher expected return. 

The function below implements the efficient portfolio weight in its general form which also allows to reflect transaction costs (conditional on the holdings *before* reallocation):
```{r}
compute_efficient_weight <- function(Sigma,
                                     mu,
                                     gamma = 2, # risk-aversion
                                     beta = 0, # transaction costs
                                     w_prev = 1/ncol(Sigma) * rep(1, ncol(Sigma))){ # weights before rebalancing

  iota <- rep(1, ncol(Sigma))
  Sigma_processed <- Sigma + beta / gamma * diag(ncol(Sigma))
  mu_processed <- mu + beta * w_prev
  
  Sigma_inverse <- solve(Sigma_processed)
  
  w_mvp <- Sigma_inverse %*% iota
  w_mvp <- as.vector(w_mvp / sum(w_mvp))
  w_opt <- w_mvp  + 1/gamma * (Sigma_inverse - 1 / sum(Sigma_inverse) * Sigma_inverse %*% iota %*% t(iota) %*% Sigma_inverse) %*% mu_processed
  return(as.vector(w_opt))
}

mu <- colMeans(industry_returns)
compute_efficient_weight(Sigma, mu)
```

What is the effect of transaction costs or different levels of risk aversion on the optimal portfolio choice? The following few lines of code analyse the distance between the minimum variance portfolio and the portfolio implemented by the investor for different values of the transaction cost parameter $\beta$ and risk aversion $\gamma$. 
```{r}
transaction_costs <- expand_grid(gamma = c(2, 4, 8, 20),
                                 beta = 20 * qexp((1:99)/100)) %>% # transaction costs in basis points
  mutate(weights = map2(.x = gamma, 
                        .y = beta,
                        ~compute_efficient_weight(Sigma,
                                                        mu,
                                                        gamma = .x,
                                                        beta = .y / 10000,
                                                  w_prev = w_mvp)),
         concentration = map_dbl(weights, ~sum(abs(. - w_mvp))))

transaction_costs %>% 
  mutate(`Risk aversion` = as_factor(gamma)) %>% 
  ggplot(aes(x = beta, y = concentration, color = `Risk aversion`)) + 
  geom_line() +
  scale_x_sqrt() +
  labs(x = "Transaction cost parameter", 
       y = "Distance from MVP",
       title = "Optimal portfolio weights for different risk aversion and transaction cost values",
       caption = "Initial portfolio is always the (sample) minimum variance portfolio. Distance is measured as the sum of absolute deviations. ") + 
  theme_minimal() +
  theme(legend.position = "bottom")
```
The figure illustrates: the higher the transaction costs parameter $\beta$, the smaller rebalancing from the initial portfolio (always set to the minimum variance portfolio weights in this example). Further, if risk aversion $\gamma$ increases, the efficient portfolio is closer to the minimum variance portfolio weights such that the investor desires less rebalancing from the initial holdings.

Next we introduce constrained optimization. Very often, typical constraints such as no-short selling rules prevent analytical solutions for optimal portfolio weights. However, numerical optimization allows to compute the solutions to such constrained problems. For the purpose of mean-variance optimization we rely on `solve.QP` from `quadprog`. First, we start with an *unconstrained* problem to replicate the analytical solutions for the minimum variance and efficient portfolio weights from above. 

```{r}
# (Constrained) optimization in R ----
# To get started: replicate minimum variance portfolio as numerical solution
w_mvp_numerical <- solve.QP(Dmat = Sigma,
                            dvec = rep(0, N), # no vector of expected returns for MVP 
                            Amat = cbind(rep(1, N)), # Matrix A has one column which is a vector of ones
                            bvec = 1, # bvec is 1 and enforces the constraint that weights sum up to one
                            meq = 1) # there is one (out of one) equality constraint

# Check that w and w_numerical are the same (up to numerical instabilities)
cbind(w_mvp, w_mvp_numerical$solution)

w_efficient_numerical <- solve.QP(Dmat = 2 * Sigma,
                            dvec = mu, # no vector of expected returns for MVP 
                            Amat = cbind(rep(1, N)), # Matrix A has one column which is a vector of ones
                            bvec = 1, # bvec is 1 and enforces the constraint that weights sum up to one
                            meq = 1) # there is one (out of one) equality constraint

cbind(compute_efficient_weight(Sigma, mu), w_efficient_numerical$solution)
```

The function `solve.QP` from package `quadprog` delivers numerical solution to quadratic programming problem of the form 
$$\min(-\mu \omega + 1/2 \omega' \Sigma \omega) \text{ s.t. } A' \omega >= b_0.$$
The function takes one argument (`meq`) for the number of equality constraints. Therefore, above matrix $A$ is simply a vector of ones to ensure that the weights sum up to one. In the case of no-short selling, the matrix $A$ is of the form 
$$\begin{aligned}A = \begin{pmatrix}1 & 1& \ldots&1 \\1 & 0 &\ldots&0\\0 & 1 &\ldots&0\\\vdots&&\ddots&\vdots\\0&0&\ldots&1\end{pmatrix}'\qquad b_0 = \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}\end{aligned}.$$
For more complex optimization routines, [this link (optimization task view)](https://cran.r-project.org/web/views/Optimization.html) helps

Next (here is where analytical solutions stop): we additionally impose no short-sale constraints which implies $N$ additional inequality constraints: $w_i >=0$. 

```{r}
A <- cbind(1, diag(N)) # Matrix of constraints t(A) >= bvec = c(1, rep(0, N))

# Introduce short-selling constraint: no element of w is allowed to be negative
w_no_short_sale <- solve.QP(Dmat = 2 * Sigma, # Efficient portfolio with risk aversion gamma = 2
                            dvec = mu, 
                            Amat = A, 
                            bvec = c(1, rep(0, N)), 
                            meq = 1)$solution

```

`solve.QP` is fast because it benefits from a very clear structure with a quadratic objective and linear constraints. Often, however, optimization requires more flexibility. As one example, we show below how to compute optimal weights which are subject to the so-called regulation T-constraint which requires that the sum of all absolute portfolio weights is smaller than 1.5. This clearly is a non-linear objective function, thus we cannot longer rely on `solve.QP`. Instead, we rely on the package `alabama` which requires us to define the objective and constraint functions individually. 
```{r}
fn <- function(w, gamma = 2) -t(w) %*% mu + gamma / 2 * t(w)%*%Sigma%*%w # Objective function which we minimize
hin <- function(w, reg_t = 1.5) return(reg_t - sum(abs(w))) # inequality constraints such that reg_t > sum(abs(w))
heq <- function(w) return(sum(w) - 1) # equality constraint

w_reg_t <- constrOptim.nl( # from alabama package
  par = 1/N * rep(1, N),# Initial set of weights
  hin = hin,
  fn = fn, 
  heq = heq,
  control.outer = list(trace = FALSE))$par # omit optimization output
```

The figure below shows the optimal allocation weights across all `r ncol(industry_returns)` industries for the four different strategies considered so far: minimum variance, efficient portfolio with $\gamma$ = 2, efficient portfolio with a no-short sale constraint and the Reg-T constrained portfolio.
```{r}
tibble(`No short-sale` = w_no_short_sale, 
       `Minimum Variance` = w_mvp, 
       `Efficient portfolio` = compute_efficient_weight(Sigma, mu),
       `Regulation-T` = w_reg_t,
       Industry = colnames(industry_returns)) %>%
  pivot_longer(-Industry, 
               names_to = "Strategy") %>% 
ggplot(aes(fill = Strategy, 
                 y = 100 * value, 
                 x = Industry)) + 
  geom_bar(position="dodge", stat="identity") +
  coord_flip() + 
  theme_minimal() +
  labs(y = "Allocation weight (in percent)",
       title ="Optimal allocations for different investment rules") +
  theme(legend.position = "bottom") 
```

Before moving on, we propose a final allocation strategy which reflects a somewhat more realistict structure of transaction costs instead of the quadratic specification used above. The function below computes efficient portfolio weights while adjusting for $L_1$ transaction costs $\beta\sum\limits_{i=1}^N |(w_{i, t+1} - w_{i, t^+})|$. No closed-form solution exists, thus we rely on non-linear optimization procedures (however, the interpretation of transaction costs as a form of shrinkage is still valid, see Hautsch and Voigt (2019)).
```{r}
compute_efficient_weight_L1_TC <- function(mu,
                                          Sigma, 
                                          gamma = 2, 
                                          beta = 0, # in basis points
                                          w_prev = 1 / ncol(sigma) * rep(1, ncol(sigma))) {
  
  # Define objective function 
  fn <- function(w) -t(w) %*% mu + gamma / 2* t(w) %*% Sigma %*% w + (beta / 10000) / 2 * sum(abs(w - w_prev))

  w_optimal <- constrOptim.nl(
    par = w_prev,# Initial set of weights
    fn = fn, 
    heq = function(w){sum(w) - 1},
    control.outer = list(trace = FALSE))$par # To omit optimization output
  return(w_optimal)
}
```

You may have noticed that, for the sake of keeping things easy, we committed one fundamental error in computing portfolio weights above: We used the full sample of the data to determine the optimal allocation. In other words, in order to implement this strategy in the beginning of the 2000's, you will need to know in advance how the returns will evolve until 2020. Instead, while interesting from a methodological point of view, we cannot evaluate the performance of the portfolios in a reasonable out-of-sample fashion. We will do so next in a backtesting exercise. For the backtest we recompute optimal weights just based on past available data. 

```{r}
window_length <- 120 # Estimation window (length of past available data)
periods <- nrow(industry_returns) - window_length # total number of out-of-sample periods

beta <- 50 # Transaction costs in basis points
gamma <- 2 # Risk aversion

performance_values <- matrix(NA, 
                     nrow = periods, 
                     ncol = 3) # A matrix to collect all returns
colnames(performance_values) <- c("raw_return", "turnover", "net_return") # we implement 3 strategies

performance_values <- list("MV (TC)" = performance_values, 
                           "Naive" = performance_values, 
                           "MV" = performance_values)

w_prev_1 <- w_prev_2 <- w_prev_3 <- rep(1/N ,N) # Every strategy starts with naive portfolio

# Two small helper functions: Weight adjustments due to returns and evaluation
adjust_weights <- function(w, next_return){
  w_prev <- 1 + w * next_return
  return(as.numeric(w_prev / sum(as.vector(w_prev))))
}

evaluate_performance <- function(w, w_previous, next_return, beta = 50){
  raw_return <- as.matrix(next_return) %*% w
  turnover <- sum(abs(w - w_previous))
  # Realized returns net of TC
  net_return <- raw_return - beta / 10000 * turnover
  return(c(raw_return, turnover, net_return))
}
```
The lines above define the general setup: We consider `r window_length` periods from the past to update the parameter estimates before recomputing portfolio weights. Then, portfolio weights are updated which is costly and affects the net performance. The portfolio weights determine the portfolio return. A period later, the current portfolio weights have changed and form the foundation for transaction costs incurred in the next period. We consider three different competing strategies: the mean-variance efficient portfolio, mean-variance efficient portfolio with ex-ante adjustment for transaction costs and the naive portfolio which simply allocates wealth equally across the different assets.
```{r}
for(i in 1:periods){ # Rolling window
  
  # Estimation window
  returns_window <- industry_returns[i : (i + window_length - 1),] # the last X returns available up to date t
  next_return <- industry_returns[i + window_length, ] # Out-of-sample return in the next period
  
  # Sample moments (in practice: replace mu or sigma with more advanced methods) 
  Sigma <- cov(returns_window) 
  mu <- 0 * colMeans(returns_window) # Note: We essentially perform MV optimization 
  
  # TC robust portfolio
  w_1 <- compute_efficient_weight_L1_TC(mu = mu, 
                                     Sigma = Sigma, 
                                     beta = beta, 
                                     gamma = gamma,
                                     w_prev = w_prev_1)

  # Evaluation
  performance_values[[1]][i, ] <- evaluate_performance(w_1, 
                                               w_prev_1, 
                                               next_return, 
                                               beta = beta)
  
  #Computes adjusted weights based on the weights and next period returns
  w_prev_1 <- adjust_weights(w_1, next_return)

  # Naive Portfolio 
  w_2 <- rep(1/N, N)
  
  # Evaluation
  performance_values[[2]][i, ] <- evaluate_performance(w_2, 
                                               w_prev_2, 
                                               next_return)
  
  #Computes adjusted weights based on the weights and next period returns
  w_prev_2 <- adjust_weights(w_2, next_return)
  
  # Mean-variance efficient portfolio (ignoring transaction costs)
  w_3 <- compute_efficient_weight(Sigma = Sigma,
                                  mu = mu, 
                                  gamma = gamma)
  # Evaluation
  performance_values[[3]][i, ] <- evaluate_performance(w_3, 
                                               w_prev_3, 
                                               next_return)
  
  #Computes adjusted weights based on the weights and next period returns
  w_prev_3 <- adjust_weights(w_3, next_return)
}
```

Finally we get to the evalation of the portfolio strategies *net-of-transaction costs*.
```{r}
performance <- lapply(performance_values, as_tibble) %>% 
  bind_rows(.id = "strategy")

performance %>%
  group_by(strategy) %>%
  summarise(Mean = 12 * mean(100 * net_return), # Annualized returns
            SD = sqrt(12) * sd(100 * net_return), # Annualized standard deviation
            Sharpe = if_else(Mean > 0, Mean/SD, NA_real_),
            Turnover = 100 * mean(turnover)) %>% 
  knitr::kable(digits = 4)
``` 

<!--chapter:end:51_constrained_optimization_and_backtesting.Rmd-->

