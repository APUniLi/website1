{
  "hash": "12a811a004c31cfa0f6060ed32106de1",
  "result": {
    "markdown": "---\ntitle: Option Pricing via Machine Learning\n---\n\n::: {.callout-note}\nYou are reading the work-in-progress edition of **Tidy Finance with Python**. Code chunks and text might change over the next couple of months. We are always looking for feedback via [contact\\@tidy-finance.org](mailto:contact@tidy-finance.org). Meanwhile, you can find the complete R version [here](../r/index.qmd).\n:::\n\nThis chapter covers machine learning methods in option pricing. \nFirst, we briefly introduce regression trees, random forests, and neural networks - these methods are advocated as highly flexible *universal approximators*, capable of recovering highly nonlinear structures in the data.\\index{Universal approximator} As the focus is on implementation, we leave a thorough treatment of the statistical underpinnings to other textbooks from authors with a real comparative advantage on these issues.\nWe show how to implement random forests and deep neural networks with tidy principles using `scikit-learn`. \n\n\\index{Machine learning} Machine learning (ML) is seen as a part of artificial intelligence. \nML algorithms build a model based on training data in order to make predictions or decisions without being explicitly programmed to do so.\nWhile ML can be specified along a vast array of different branches, this chapter focuses on so-called supervised learning for regressions. \\index{Supervised learning} The basic idea of supervised learning algorithms is to build a mathematical model for data that contains both the inputs and the desired outputs. In this chapter, we apply well-known methods such as random forests \\index{Random forests} and neural networks \\index{Neural network} to a simple application in option pricing. More specifically, we create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to *learn* how to price call options \\index{Option pricing} without prior knowledge of the theoretical underpinnings of the famous option pricing equation by @Black1976.    \n\n::: {.callout-note}\nIn order to replicate the analysis regarding neural networks in this chapter, you have to install `TensorFlow` on your system, which requires administrator rights on your machine. Parts of this can be done from within R. Just follow [these quick-start instructions.](https://tensorflow.rstudio.com/installation/)\n:::\n\nThroughout this chapter, we need the following packages.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd \nimport numpy as np\n\nfrom itertools import product\n\nfrom plotnine import *\n\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Lasso\n```\n:::\n\n\n## Regression Trees and Random Forests\n\nRegression trees are a popular ML approach for incorporating multiway predictor interactions. In Finance, regression trees are gaining popularity, also in the context of asset pricing [see, e.g. @Bryzgalova2022].\nTrees possess a logic that departs markedly from traditional regressions. Trees are designed to find groups of observations that behave similarly to each other. A tree *grows* in a sequence of steps. At each step, a new *branch* sorts the data leftover from the preceding step into bins based on one of the predictor variables. This sequential branching slices the space of predictors into partitions and approximates the unknown function $f(x)$ which yields the relation between the predictors $x$ and the outcome variable $y$ with the average value of the outcome variable within each partition. For a more thorough treatment of regression trees, we refer to @Coqueret2020.\n\nFormally, we partition the  predictor space into $J$ non-overlapping regions, $R_1, R_2, \\ldots, R_J$. For any predictor $x$ that falls within region $R_j$, we estimate $f(x)$ with the average of the training observations, $\\hat y_i$, for which the associated predictor $x_i$ is also in $R_j$. Once we select a partition $x$ to split in order to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, called $R_1(j,s)$ and $R_2(j,s)$, which split our observations in the current partition by asking if $x_j$ is bigger than $s$:\n$$R_1(j,s) = \\{x \\mid x_j < s\\} \\mbox{  and  } R_2(j,s) = \\{x \\mid x_j \\geq s\\}.$$\nTo pick $j$ and $s$, we find the pair that minimizes the residual sum of square (RSS):\n$$\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2$$\nAs in Chapter 14 in the context of penalized regressions, the first relevant question is: what are the hyperparameter decisions? Instead of a regularization parameter, trees are fully determined by the number of branches used to generate a partition (sometimes one specifies the minimum number of observations in each final branch instead of the maximum number of branches).\n\nModels with a single tree may suffer from high predictive variance. Random forests address these shortcomings of decision trees. The goal is to improve the predictive performance and reduce instability by averaging multiple decision trees. A forest basically implies creating many regression trees and averaging their predictions. To assure that the individual trees are not the same, we use a bootstrap to induce randomness. More specifically, we build $B$ decision trees $T_1, \\ldots, T_B$ using the training sample. For that purpose, we randomly select features to be included in the building of each tree. For each observation in the test set we then form a prediction $\\hat{y} = \\frac{1}{B}\\sum\\limits_{i=1}^B\\hat{y}_{T_i}$.\n\n## Neural Networks\n\nRoughly speaking, neural networks propagate information from an input layer, through one or multiple hidden layers, to an output layer. While the number of units (neurons) in the input layer is equal to the dimension of the predictors, the output layer usually consists of one neuron (for regression) or multiple neurons for classification. The output layer predicts the future data, similar to the fitted value in a regression analysis. Neural networks have theoretical underpinnings as *universal approximators* for any smooth predictive association [@Hornik1991]. Their complexity, however, ranks neural networks among the least transparent, least interpretable, and most highly parameterized ML tools.\nIn finance, applications of neural networks can be found in the context of many different contexts, e.g. @Avramov2022, @Chen2019, and @Gu2020.\n\nEach neuron applies a nonlinear *activation function* $f$ to its aggregated signal before\nsending its output to the next layer\n$$x_k^l = f\\left(\\theta^k_{0} + \\sum\\limits_{j = 1}^{N ^l}z_j\\theta_{l,j}^k\\right)$$\nHere, $\\theta$ are the parameters to fit, $N^l$ denotes the number of units (a hyperparameter to tune), and $z_j$ are the input variables which can be either the raw data or, in the case of multiple chained layers, the outcome from a previous layers $z_j = x_k-1$.\nWhile the easiest case with $f(x) = \\alpha + \\beta x$ resembles linear regression, typical activation functions are sigmoid (i.e., $f(x) = (1+e^{-x})^{-1}$) or ReLu (i.e., $f(x) = max(x, 0)$).\n\nNeural networks gain their flexibility from chaining multiple layers together. Naturally, this imposes many degrees of freedom on the network architecture for which no clear theoretical guidance exists. The specification of a neural network requires, at a minimum, a stance on depth (number of hidden layers), the activation function, the number of neurons, the connection structure of the units (dense or sparse), and the application of regularization techniques to avoid overfitting. Finally, *learning* means to choose optimal parameters relying on numerical optimization, which often requires specifying an appropriate learning rate. Despite these computational challenges, implementation in R is not tedious at all because we can use the API to `TensorFlow`. \n\n## Option Pricing\n\nTo apply ML methods in a relevant field of finance, we focus on option pricing. The application in its core is taken from @Hull2020. In its most basic form, call options give the owner the right but not the obligation to buy a specific stock (the underlying) at a specific price (the strike price $K$) at a specific date (the exercise date $T$). The Blackâ€“Scholes price [@Black1976] of a call option for a non-dividend-paying underlying stock is given by\n$$\n\\begin{aligned}\n  C(S, T) &= \\Phi(d_1)S - \\Phi(d_1 - \\sigma\\sqrt{T})Ke^{-r T} \\\\\n     d_1 &= \\frac{1}{\\sigma\\sqrt{T}}\\left(\\ln\\left(\\frac{S}{K}\\right) + \\left(r_f + \\frac{\\sigma^2}{2}\\right)T\\right)\n\\end{aligned}\n$$\nwhere $C(S, T)$ is the price of the option as a function of today's stock price of the underlying, $S$, with time to maturity $T$, $r_f$ is the risk-free interest rate, and $\\sigma$ is the volatility of the underlying stock return. $\\Phi$ is the cumulative distribution function of a standard normal random variable.\n\nThe Black-Scholes equation provides a way to compute the arbitrage-free price of a call option once the parameters $S, K, r_f, T$, and $\\sigma$ are specified (arguably, in a realistic context, all parameters are easy to specify except for $\\sigma$ which has to be estimated). A simple R function allows computing the price as we do below. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef black_scholes_price(S, K, r, T, sigma):\n  \n    d1 = (np.log(S / K) + (r + sigma ** 2 / 2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n    \n    return price\n```\n:::\n\n\n## Learning Black-Scholes\n\nWe illustrate the concept of ML by showing how ML methods *learn* the Black-Scholes equation after observing some different specifications and corresponding prices without us revealing the exact pricing equation. \n\n### Data simulation\n\nTo that end, we start with simulated data. We compute option prices for call options for a grid of different combinations of times to maturity (`T`), risk-free rates (`r`), volatilities (`sigma`), strike prices (`K`), and current stock prices (`S`). In the code below, we add an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\n\nIn order to keep the analysis reproducible, we use `np.random.seed()`. A random seed specifies the start point when a computer generates a random number sequence and ensures that our simulated data is the same across different machines. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nrandom_state = 420\nnp.random.seed(random_state)\n\nS = np.arange(40, 61)\nK = np.arange(20, 91)\nr = np.arange(0, 0.051, 0.01)\nT = np.arange(3 / 12, 2.01, 1 / 12)\nsigma = np.arange(0.1, 0.81, 0.1)\n\noption_prices = pd.DataFrame(product(S, K, r, T, sigma), \n                             columns=[\"S\", \"K\", \"r\", \"T\", \"sigma\"])\n\noption_prices[\"black_scholes\"] = black_scholes_price(\n  option_prices[\"S\"].values, \n  option_prices[\"K\"].values, \n  option_prices[\"r\"].values, \n  option_prices[\"T\"].values, \n  option_prices[\"sigma\"].values\n)\n\noption_prices = (option_prices\n  .assign(\n    observed_price = lambda x: x[\"black_scholes\"] + np.random.normal(scale=0.15)\n  )\n)\n```\n:::\n\n\nThe code above generates more than 1.5 million random parameter constellations. For each of these values, two *observed* prices reflecting the Black-Scholes prices are given and a random innovation term *pollutes* the observed prices. The intuition of this application is simple: the simulated data provides many observations of option prices - by using the Black-Scholes equation we can evaluate the actual predictive performance of a ML method, which would be hard in a realistic context were the actual arbitrage-free price would be unknown. \n\nNext, we split the data into a training set (which contains 1\\% of all the observed option prices) and a test set that will only be used for the final evaluation. Note that the entire grid of possible combinations contains `python len(option_prices.columns)` different specifications. Thus, the sample to learn the Black-Scholes price contains only 31,489 observations and is therefore relatively small.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntrain_data, test_data = train_test_split(\n  option_prices, \n  test_size=0.01, random_state=random_state\n)\n```\n:::\n\n\nWe process the training dataset further before we fit the different ML models. We define a `ColumnTransformer()` that defines all processing steps for that purpose. For our specific case, we want to explain the observed price by the five variables that enter the Black-Scholes equation. The *true* price (stored in column `black_scholes`) should obviously not be used to fit the model. The recipe also reflects that we standardize all predictors via `StandardScaler()` to ensure that each variable exhibits a sample average of zero and a sample standard deviation of one.  \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\npreprocessor = ColumnTransformer(\n  transformers=[\n    (\"normalize_predictors\", StandardScaler(), [\"S\",\"K\",\"r\",\"T\",\"sigma\"]),\n  ],\n  remainder=\"drop\"\n)\n```\n:::\n\n\n### Single layer networks and random forests\n\nNext, we show how to fit a neural network to the data. The function `MLPRegressor()` from the package `scikit-learn` provides the functionality to initialize a single layer, feed-forward neural network. The specification below defines a single layer feed-forward neural network with 10 hidden units. We set the number of training iterations to `max_iter=500`. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnnet_model = MLPRegressor(\n  hidden_layer_sizes=(10, ), \n  max_iter=500, \n  random_state=random_state\n)\n```\n:::\n\n\nWe can follow the straightforward workflow as in the chapter before: define a workflow, equip it with the recipe, and specify the associated model. Finally, fit the model with the training data. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nnnet_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", nnet_model)\n])\n\nnnet_fit = nnet_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]), \n  train_data.get(\"observed_price\")\n)\n```\n:::\n\n\nOnce you are familiar with the `scikit-learn` workflow, it is a piece of cake to fit other models. \nFor instance, the model below initializes a random forest with 50 trees contained in the ensemble, where we require at least 2000 observations in a node.\nThe random forests are trained using the function `RandomForestRegressor()`. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nrf_model = RandomForestRegressor(\n  n_estimators=50, \n  min_samples_leaf=2000, \n  random_state=random_state\n)\n```\n:::\n\n\nFitting the model follows exactly the same convention as for the neural network before.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nrf_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", rf_model)\n])\n\nrf_fit = rf_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]), \n  train_data.get(\"observed_price\")\n)\n```\n:::\n\n\n### Deep neural networks\n\nA deep neural network is a neural network with multiple layers between the input and output layers. By chaining multiple layers together, more complex structures can be represented with fewer parameters than simple shallow (one-layer) networks as the one implemented above. For instance, image or text recognition are typical tasks where deep neural networks are used [for applications of deep neural networks in finance, see, for instance, @Jiang2022; @Jensen2022].\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndeepnnet_model = MLPRegressor(\n  hidden_layer_sizes=(10, 10, 10),\n  activation=\"logistic\", \n  solver=\"lbfgs\",\n  max_iter=500, \n  random_state=random_state\n)\n                              \ndeepnnet_pipeline = Pipeline([\n  (\"preprocessor\", preprocessor),\n  (\"regressor\", deepnnet_model)\n])\n\ndeepnnet_fit = deepnnet_pipeline.fit(\n  train_data.drop(columns=[\"observed_price\"]),\n  train_data.get(\"observed_price\")\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\christoph.scheuch\\.conda\\envs\\tidy-finance\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n```\n:::\n:::\n\n\n### Universal approximation\n\nBefore we evaluate the results, we implement one more model. In principle, any non-linear function can also be approximated by a linear model containing the input variables' polynomial expansions. To illustrate this, we include polynomials up to the fifth degree of each predictor and then add all possible pairwise interaction terms. We fit a Lasso regression model with a pre-specified penalty term (consult Chapter 14 on how to tune the model hyperparameters).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nlm_pipeline = Pipeline([\n  (\"polynomial\", PolynomialFeatures(degree=5, \n                                    interaction_only=False, \n                                    include_bias=True)),\n  (\"scaler\", StandardScaler()),\n  (\"regressor\", Lasso(alpha=0.01))\n])\n\nlm_fit = lm_pipeline.fit(\n  train_data.get([\"S\", \"K\", \"r\", \"T\", \"sigma\"]),\n  train_data.get(\"observed_price\")\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\christoph.scheuch\\.conda\\envs\\tidy-finance\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+05, tolerance: 1.552e+04\n```\n:::\n:::\n\n\n## Prediction Evaluation\n\nFinally, we collect all predictions to compare the *out-of-sample* prediction error evaluated on ten thousand new data points. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntest_X = test_data.get([\"S\", \"K\", \"r\", \"T\", \"sigma\"])\ntest_y = test_data.get(\"observed_price\")\n\npredictive_performance = (pd.concat(\n  [test_data.reset_index(drop = True), \n   pd.DataFrame({\"Random forest\": rf_fit.predict(test_X),\n                 \"Single layer\": nnet_fit.predict(test_X),\n                 \"Deep NN\": deepnnet_fit.predict(test_X),\n                 \"Lasso\": lm_fit.predict(test_X)})\n  ], axis = 1)\n  .melt(\n    id_vars=[\"S\", \"K\", \"r\", \"T\", \"sigma\", \"black_scholes\", \"observed_price\"],\n    var_name=\"Model\",\n    value_name=\"Predicted\"\n  )\n  .assign(\n    moneyness = lambda x: x[\"S\"] - x[\"K\"],\n    pricing_error = lambda x: np.abs(x[\"Predicted\"] - x[\"black_scholes\"])\n  )\n)\n```\n:::\n\n\nIn the lines above, we use each of the fitted models to generate predictions for the entire test data set of option prices. We evaluate the absolute pricing error as one possible measure of pricing accuracy, defined as the absolute value of the difference between predicted option price and the theoretical correct option price from the Black-Scholes model.  We show the results graphically in Figure 15.1.\\index{Graph!Prediction error}\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\npredictive_performance_plot = (\n  ggplot(predictive_performance, \n         aes(x=\"moneyness\", y=\"pricing_error\", \n             color=\"Model\", linetype=\"Model\")) +\n  geom_jitter(alpha=0.05) +\n  geom_smooth(se=False) +\n  labs(\n    x=\"Moneyness (S - K)\", y=\"Absolut prediction error (USD)\",\n    color=None, linetype=None,\n    title=\"Prediction errors of call option prices for different models\",\n  )\n)\npredictive_performance_plot.draw()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](option-pricing-via-machine-learning_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nThe results can be summarized as follows:\n\n1. All ML methods seem to be able to price call options after observing the training test set.\n1. The average prediction errors increase for far in-the-money options. \n1. Random forest and the Lasso seem to perform consistently worse in prediction option prices than the neural networks.\n1. The complexity of the deep neural network relative to the single-layer neural network does not result in better out-of-sample predictions.\n\n## Exercises\n\n1. Write a function that takes `y` and a matrix of predictors `X` as inputs and returns a characterization of the relevant parameters of a regression tree with 1 branch. \n1. Create a function that creates predictions for a new matrix of predictors `newX` based on the estimated regression tree. \n<!-- 1. Use the package `rpart` to *grow* a tree based on the training data and use the illustration tools in `rpart` to understand which characteristics the tree deems relevant for option pricing. -->\n<!-- 1. Make use of a training and a test set to choose the optimal depth (number of sample splits) of the tree. -->\n<!-- 1. Use `keras` to initialize a sequential neural network that can take the predictors from the training data set as input, contains at least one hidden layer, and generates continuous predictions. *This sounds harder than it is: *see a simple [regression example here.](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/) How many parameters does the neural network you aim to fit have?  -->\n<!-- 1. Compile the object from the previous exercise. It is important that you specify a loss function. Illustrate the difference in predictive accuracy for different architecture choices. -->\n\n",
    "supporting": [
      "option-pricing-via-machine-learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}