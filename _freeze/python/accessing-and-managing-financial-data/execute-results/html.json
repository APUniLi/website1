{
  "hash": "9798a2b9f515514ea85a33db6d1ec7ea",
  "result": {
    "markdown": "---\ntitle: Accessing and Managing Financial Data\n---\n\n\n\n::: {.callout-note}\nYou are reading the work-in-progress edition of **Tidy Finance with Python**. Code chunks and text might change over the next couple of months. We are always looking for feedback via [contact\\@tidy-finance.org](mailto:contact@tidy-finance.org). Meanwhile, you can find the complete R version [here](../r/index.qmd).\n:::\n\nIn this chapter, we suggest a way to organize your financial data. Everybody, who has experience with data, is also familiar with storing data in various formats like CSV, XLS, XLSX, or other delimited value storage. Reading and saving data can become very cumbersome in the case of using different data formats, both across different projects and across different programming languages. Moreover, storing data in delimited files often leads to problems with respect to column type consistency. For instance, date-type columns frequently lead to inconsistencies across different data formats and programming languages. \n\nThis chapter shows how to import different open source data sets. Specifically, our data comes from the application programming interface (API) of Yahoo!Finance, a downloaded standard CSV file, an XLSX file stored in a public Google Drive repository, and other macroeconomic time series.\\index{API} We store all the data in a *single* database, which serves as the only source of data in subsequent chapters. We conclude the chapter by providing some tips on managing databases.\\index{Database}\n\nFirst, we load the global packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\nMoreover, we initially define the date range for which we fetch and store the financial data, making future data updates tractable. In case you need another time frame, you can adjust the dates below. Our data starts with 1960 since most asset pricing studies use data from 1962 on.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nstart_date = \"1960-01-01\"\nend_date = \"2022-12-31\"\n```\n:::\n\n\n## Fama-French Data\n\nWe start by downloading some famous Fama-French factors [e.g., @Fama1993] and portfolio returns commonly used in empirical asset pricing. Fortunately, the `pandas-datareader` package provides a simple interface to read data from Ken French's Data Library.\\index{Data!Fama-French factors} \\index{Kenneth French homepage}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport pandas_datareader as pdr\n```\n:::\n\n\nWe can use the `pdr.DataReader()` function of the package to download monthly Fama-French factors. The set *Fama/French 3 Factors* contains the return time series of the market `mkt_excess`, size `smb` and value `hml` alongside the risk-free rates `rf`. Note that we have to do some manual work to correctly parse all the columns and scale them appropriately, as the raw Fama-French data comes in a very unpractical data format. For precise descriptions of the variables, we suggest consulting Prof. Kenneth French's finance data library directly. If you are on the website, check the raw data files to appreciate the time you can save thanks to `frenchdata`.\\index{Factor!Market}\\index{Factor!Size}\\index{Factor!Value}\\index{Factor!Profitability}\\index{Factor!Investment}\\index{Risk-free rate}\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfactors_ff3_monthly_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_Factors\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nfactors_ff3_monthly = (factors_ff3_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(\n    month = lambda x: pd.to_datetime(x[\"month\"].astype(str))\n  )\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns = {\"mkt-rf\" : \"mkt_excess\"})\n)\n```\n:::\n\n\nWe also download the set *5 Factors (2x3)*, which additionally includes the return time series of the profitability `rmw` and investment `cma` factors. We demonstrate how the  monthly factors are constructed in the chapter [Replicating Fama and French Factors](replicating-fama-and-french-factors.qmd).\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfactors_ff5_monthly_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_5_Factors_2x3\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nfactors_ff5_monthly = (factors_ff5_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(\n    month = lambda x: pd.to_datetime(x[\"month\"].astype(str))\n  )\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns = {\"mkt-rf\" : \"mkt_excess\"})\n)\n```\n:::\n\n\nIt is straightforward to download the corresponding *daily* Fama-French factors with the same function. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfactors_ff3_daily_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_Factors_daily\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nfactors_ff3_daily = (factors_ff3_daily_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns = {\"mkt-rf\" : \"mkt_excess\"})\n)\n```\n:::\n\n\nIn a subsequent chapter, we also use the 10 monthly industry portfolios, so let us fetch that data, too.\\index{Data!Industry portfolios}\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nindustries_ff_monthly_raw = pdr.DataReader(\n  name=\"10_Industry_Portfolios\",\n  data_source=\"famafrench\", \n  start=start_date, \n  end=end_date)[0]\n\nindustries_ff_monthly = (industries_ff_monthly_raw\n  .divide(100)\n  .reset_index(names=\"month\")\n  .assign(\n    month = lambda x: pd.to_datetime(x[\"month\"].astype(str))\n  )\n  .rename(str.lower, axis=\"columns\")\n)\n```\n:::\n\n\nIt is worth taking a look at all available portfolio return time series from Kenneth French's homepage. You should check out the other sets by calling `pdr.famafrench.get_available_datasets()`.\n\n## q-Factors\n\nIn recent years, the academic discourse experienced the rise of alternative factor models, e.g., in the form of the @Hou2015 *q*-factor model. We refer to the [extended background](http://global-q.org/background.html) information provided by the original authors for further information. The *q* factors can be downloaded directly from the authors' homepage from within `pd.read_csv()`.\\index{Data!q-factors}\\index{Factor!q-factors}\n\nWe also need to adjust this data. First, we discard information we will not use in the remainder of the book. Then, we rename the columns with the \"R_\"-prescript using regular expressions and write all column names in lowercase. You should always try sticking to a consistent style for naming objects, which we try to illustrate here - the emphasis is on *try*. You can check out style guides available online, e.g., [Hadley Wickham's `tidyverse` style guide.](https://style.tidyverse.org/index.html)\\index{Style guide}\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfactors_q_monthly_link = (\n  \"https://global-q.org/uploads/1/2/2/6/122679606/\" +\n  \"q5_factors_monthly_2022.csv\"\n)\nfactors_q_monthly=(pd.read_csv(factors_q_monthly_link)\n  .assign(\n    month = lambda x: (\n      pd.to_datetime(x[\"year\"].astype(str) + \"-\" + \n        x[\"month\"].astype(str) + \"-01\"))\n  )\n  .drop(columns=[\"R_F\", \"R_MKT\", \"year\"])\n  .rename(columns = lambda x: x.replace(\"R_\", \"\").lower())\n  .query(\"month >= @start_date and month <= @end_date\")\n  .assign(\n    **{col: lambda x: x[col] / 100 \n                        for col in [\"me\", \"ia\", \"roe\", \"eg\"]}\n  )\n)\n```\n:::\n\n\n## Macroeconomic Predictors\n\nOur next data source is a set of macroeconomic variables often used as predictors for the equity premium. @Goyal2008 comprehensively reexamine the performance of variables suggested by the academic literature to be good predictors of the equity premium. The authors host the data updated to 2022 on [Amit Goyal's website.](https://sites.google.com/view/agoyal145) Since the data is an XLSX-file stored on a public Google drive location, we need additional packages to access the data directly from our Python session. Usually, you need to authenticate if you interact with Google drive directly in Python. Since the data is stored via a public link, we can proceed without any authentication.\\index{Google Drive}\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsheet_id = \"1g4LOaRj4TvwJr9RIaA_nwrXXWTOy46bP\"\nsheet_name = \"macro_predictors.xlsx\"\nmacro_predictors_link = (\n  \"https://docs.google.com/spreadsheets/d/\" + sheet_id + \n  \"/gviz/tq?tqx=out:csv&sheet=\" + sheet_name\n)\n```\n:::\n\n\nNext, we read in the new data and transform the columns into the variables that we later use:\n\n1. The dividend price ratio (`dp`), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices [@Campbell1988; @Campbell2006]. \n1. Dividend yield (`dy`), the difference between the log of dividends and the log of lagged prices [@Ball1978]. \n1. Earnings price ratio (`ep`), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index [@Campbell1988]. \n1. Dividend payout ratio (`de`), the difference between the log of dividends and the log of earnings [@Lamont1998]. \n1. Stock variance (`svar`), the sum of squared daily returns on the S&P 500 index [@Guo2006].\n1. Book-to-market ratio (`bm`), the ratio of book value to market value for the Dow Jones Industrial Average [@Kothari1997] \n1. Net equity expansion (`ntis`), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks [@Campbell2008].\n1. Treasury bills (`tbl`), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis [@Campbell1987].\n1. Long-term yield (`lty`), the long-term government bond yield from Ibbotson's Stocks, Bonds, Bills, and Inflation Yearbook [@Goyal2008].\n1. Long-term rate of returns (`ltr`), the long-term government bond returns from Ibbotson's Stocks, Bonds, Bills, and Inflation Yearbook [@Goyal2008].\n1. Term spread (`tms`), the difference between the long-term yield on government bonds and the Treasury bill [@Campbell1987].\n1. Default yield spread (`dfy`), the difference between BAA and AAA-rated corporate bond yields [@Fama1989]. \n1. Inflation (`infl`), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics [@Campbell2004].\n\t\t\t\nFor variable definitions and the required data transformations, you can consult the material on [Amit Goyal's website](https://sites.google.com/view/agoyal145).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nmacro_predictors = (\n  pd.read_csv(macro_predictors_link, thousands=\",\")\n  .assign(\n    month = lambda x: pd.to_datetime(x[\"yyyymm\"], \n                                     format=\"%Y%m\"),\n    IndexDiv = lambda x: x[\"Index\"] + x[\"D12\"],\n    logret = lambda x: (np.log(x[\"IndexDiv\"]) - \n                        np.log(x[\"IndexDiv\"].shift(1))),\n    Rfree = lambda x: np.log(x[\"Rfree\"] + 1),\n    rp_div = lambda x: x[\"logret\"] - x[\"Rfree\"].shift(-1),\n    dp = lambda x: np.log(x[\"D12\"]) - np.log(x[\"Index\"]),\n    dy = lambda x: (np.log(x[\"D12\"]) - \n                      np.log(x[\"D12\"].shift(1))),\n    ep = lambda x: np.log(x[\"E12\"]) - np.log(x[\"Index\"]),\n    de = lambda x: np.log(x[\"D12\"]) - np.log(x[\"E12\"]),\n    tms = lambda x: x[\"lty\"] - x[\"tbl\"],\n    dfy = lambda x: x[\"BAA\"] - x[\"AAA\"]\n  )\n  .get([\"month\", \"rp_div\", \"dp\", \"dy\", \"ep\", \"de\", \"svar\",\n        \"b/m\", \"ntis\", \"tbl\", \"lty\", \"ltr\", \"tms\", \"dfy\", \n        \"infl\"])\n  .query(\"month >= @start_date and month <= @end_date\")\n  .dropna()\n)\n```\n:::\n\n\n## Other Macroeconomic Data\n\nThe Federal Reserve bank of St. Louis provides the Federal Reserve Economic Data (FRED), an extensive database for macroeconomic data. In total, there are 817,000 US and international time series from 108 different sources. As an illustration, we use the already familiar `pandas-datareader` package to fetch consumer price index (CPI) data that can be found under the [CPIAUCNS](https://fred.stlouisfed.org/series/CPIAUCNS) key.\\index{Data!FRED}\\index{Data!CPI}\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ncpi_monthly = (pdr.DataReader(\n  name=\"CPIAUCNS\", \n  data_source=\"fred\", \n  start=start_date, \n  end=end_date\n  )\n  .reset_index(names=\"month\")\n  .rename(columns = {\"CPIAUCNS\" : \"cpi\"})\n  .assign(cpi=lambda x: x[\"cpi\"] / x[\"cpi\"].iloc[-1])\n)\n```\n:::\n\n\nTo download other time series, we just have to look it up on the FRED website and extract the corresponding key from the address. For instance, the producer price index for gold ores can be found under the [PCU2122212122210](https://fred.stlouisfed.org/series/PCU2122212122210) key.\n\n## Setting Up a Database\n\nNow that we have downloaded some (freely available) data from the web into the memory of our R session let us set up a database to store that information for future use. We will use the data stored in this database throughout the following chapters, but you could alternatively implement a different strategy and replace the respective code. \n\nThere are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an [SQLite](https://SQLite.org/) database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that [SQL](https://en.wikipedia.org/wiki/SQL) (Structured Query Language) is a standard language for accessing and manipulating databases.\\index{Database!SQLite}\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nimport sqlite3\n```\n:::\n\n\nAn SQLite database is easily created - the code below is really all there is. You do not need any external software. Otherwise, date columns are stored and retrieved as integers.\\index{Database!Creation}  We will use the resulting file `tidy_finance.db` in the subfolder `data` for all subsequent chapters to retrieve our data. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\n```\n:::\n\n\nNext, we create a remote table with the monthly Fama-French factor data. We do so with the function `to_sql()`, which copies the data to our SQLite-database. Before we copy the data to the database, we convert the date to UNIX integers, which allows us to smoothly share the data between R and Python. We follow the [approach recommended by `pandas`](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#from-timestamps-to-epoch) for this conversion.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n(factors_ff3_monthly\n  .assign(\n    month = lambda x: \n      ((x[\"month\"]- pd.Timestamp(\"1970-01-01\")) \n          // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(\n    name=\"factors_ff3_monthly\", \n    con=tidy_finance, \n    if_exists=\"replace\",\n    index = False)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n756\n```\n:::\n:::\n\n\nIf we want to have the whole table in memory, we need to call `pd.read_sql_query()` with the corresponding query. You will see that we regularly load the data into the memory in the next chapters.\\index{Database!Fetch}\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n(pd.read_sql_query(\n  sql=\"SELECT month, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>month</th>\n      <th>rf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1960-01-01</td>\n      <td>0.0033</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1960-02-01</td>\n      <td>0.0029</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1960-03-01</td>\n      <td>0.0035</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1960-04-01</td>\n      <td>0.0019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1960-05-01</td>\n      <td>0.0027</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>2022-08-01</td>\n      <td>0.0019</td>\n    </tr>\n    <tr>\n      <th>752</th>\n      <td>2022-09-01</td>\n      <td>0.0019</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>2022-10-01</td>\n      <td>0.0023</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>2022-11-01</td>\n      <td>0.0029</td>\n    </tr>\n    <tr>\n      <th>755</th>\n      <td>2022-12-01</td>\n      <td>0.0033</td>\n    </tr>\n  </tbody>\n</table>\n<p>756 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\nThe last couple of code chunks is really all there is to organizing a simple database! You can also share the SQLite database across devices and programming languages. \n\nBefore we move on to the next data source, let us also store the other five tables in our new SQLite database. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n(factors_ff5_monthly\n  .assign(\n    month = lambda x: \n      ((x[\"month\"]- pd.Timestamp(\"1970-01-01\")) \n        // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(name=\"factors_ff5_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n          \n(factors_ff3_daily\n  .assign(\n    date = lambda x: \n      ((x[\"date\"]- pd.Timestamp(\"1970-01-01\")) \n        // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(name=\"factors_ff3_daily\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(industries_ff_monthly\n  .assign(\n    month = lambda x: \n      ((x[\"month\"]- pd.Timestamp(\"1970-01-01\")) \n        // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(name=\"industries_ff_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(factors_q_monthly\n  .assign(\n    month = lambda x: \n      ((x[\"month\"]- pd.Timestamp(\"1970-01-01\")) \n        // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(name=\"factors_q_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(macro_predictors\n  .assign(\n    month = lambda x: \n      ((x[\"month\"]- pd.Timestamp(\"1970-01-01\")) \n        // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(name=\"macro_predictors\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n(cpi_monthly\n  .assign(\n    month = lambda x: \n      ((x[\"month\"]- pd.Timestamp(\"1970-01-01\")) \n        // pd.Timedelta(\"1d\"))\n  )\n  .to_sql(name=\"cpi_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n```\n:::\n\n\nFrom now on, all you need to do to access data that is stored in the database is to follow two steps: (i) Establish the connection to the SQLite database and (ii) execute the query to fetch the data. For your convenience, the following steps show all you need in a compact fashion.\\index{Database!Connection}\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nimport pandas\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\"data/tidy_finance.sqlite\")\nfactors_q_monthly = (pd.read_sql_query(\n    sql=\"SELECT * FROM factors_q_monthly\",\n    con=tidy_finance,\n    parse_dates={\"month\": {\"unit\": \"D\", \"origin\": \"unix\"}})\n)\n```\n:::\n\n\n## Managing SQLite Databases\n\nFinally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.\\index{Database!Management}\n\nTo optimize the database file, you can run the `VACUUM` command in the database, which rebuilds the database and frees up unused space. You can execute the command in the database using the `execute()` function. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ntidy_finance.execute(\"VACUUM\")\n```\n:::\n\n\nThe `VACUUM` command actually performs a couple of additional cleaning steps, which you can read up in [this tutorial.](https://SQLite.org/docs/sql/statements/vacuum.html) \\index{Database!Cleaning}\n\n## Exercises\n\n1. Download the monthly Fama-French factors manually from [Ken French's data library](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) and read them in via `pd.read_csv()`. Validate that you get the same data as via the `pandas-datareader` package. \n1. Download the daily Fama-French 5 factors using the `pdr.DataReader()` package. After the successful download and conversion to the column format that we used above, compare the `rf`, `mkt_excess`, `smb`, and `hml` columns of `factors_ff3_daily` to `factors_ff5_daily`. Discuss any differences you might find. \n\n",
    "supporting": [
      "accessing-and-managing-financial-data_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}