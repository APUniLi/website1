{
  "hash": "891180ee15a5f2f94922c9b93d8be999",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: WRDS Dummy Data\nmetadata:\n  pagetitle: WRDS Dummy Data with Python\n  description-meta: Use this Python code to generate dummy data to run the code chunks in Tidy Finance with Python without access to WRDS.\n---\n\nIn this appendix chapter, we alleviate the constraints of readers who donâ€™t have access to WRDS and hence cannot run the code that we provide. We show how to create a dummy database that contains the WRDS tables and corresponding columns such that all code chunks in this book can be executed with this dummy database. We do not create dummy data for macro tables because they can be freely downloaded from the original sources - check out [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd).\\index{WRDS}\n\nWe deliberately use the dummy label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does not contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books.\n\nTo generate the dummy database, we use the following packages:\n\n::: {#067a926c .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport string\n```\n:::\n\n\nLet us initialize a SQLite database (`tidy_finance_python.sqlite`) or connect to your existing one. Be careful, if you already downloaded the data from WRDS, then the code in this chapter will overwrite your data!\n\n::: {#b2becdef .cell execution_count=2}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n```\n:::\n\n\nSince we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over ten years that we then use to create yearly, monthly, and daily data, respectively. \n\n::: {#a7074ff3 .cell execution_count=3}\n``` {.python .cell-code}\nnp.random.seed(1234)\n\nstart_date = pd.Timestamp(\"2003-01-01\")\nend_date = pd.Timestamp(\"2022-12-31\")\n\ndummy_years = np.arange(start_date.year, end_date.year+1, 1)\ndummy_months = pd.date_range(start_date, end_date, freq=\"MS\") \ndummy_days = pd.date_range(start_date, end_date, freq=\"D\")\n```\n:::\n\n\n## Create Stock Dummy Data\n\nLet us start with the core data used throughout the book: stock and firm characteristics. We first generate a table with a cross-section of stock identifiers with unique `permno` and `gvkey` values, as well as associated `exchcd`, `exchange`, `industry`, and `siccd` values. The generated data is based on the characteristics of stocks in the `crsp_monthly` table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the `permno`-`gvkey` combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data.\n\n::: {#0be06c7c .cell execution_count=4}\n``` {.python .cell-code}\nnumber_of_stocks = 100\n\nindustries = pd.DataFrame({\n  \"industry\": [\"Agriculture\", \"Construction\", \"Finance\",\n               \"Manufacturing\", \"Mining\", \"Public\", \"Retail\", \n               \"Services\", \"Transportation\", \"Utilities\", \"Wholesale\"],\n  \"n\": [81, 287, 4682, 8584, 1287, 1974, 1571, 4277, 1249, 457, 904],\n  \"prob\": [0.00319, 0.0113, 0.185, 0.339, 0.0508, 0.0779, \n           0.0620, 0.169, 0.0493, 0.0180, 0.03451]\n})\n\nexchanges = pd.DataFrame({\n  \"exchange\": [\"AMEX\", \"NASDAQ\", \"NYSE\"],\n  \"n\": [2893, 17236, 5553],\n  \"prob\": [0.113, 0.671, 0.216]\n})\n\nstock_identifiers_list = []\nfor x in range(1, number_of_stocks+1):\n  exchange = np.random.choice(exchanges[\"exchange\"], p=exchanges[\"prob\"])\n  industry = np.random.choice(industries[\"industry\"], p=industries[\"prob\"])\n\n  exchcd_mapping = {\n    \"NYSE\": np.random.choice([1, 31]),\n    \"AMEX\": np.random.choice([2, 32]),\n    \"NASDAQ\": np.random.choice([3, 33])\n  }\n\n  siccd_mapping = {\n    \"Agriculture\": np.random.randint(1, 1000),\n    \"Mining\": np.random.randint(1000, 1500),\n    \"Construction\": np.random.randint(1500, 1800),\n    \"Manufacturing\": np.random.randint(1800, 4000),\n    \"Transportation\": np.random.randint(4000, 4900),\n    \"Utilities\": np.random.randint(4900, 5000),\n    \"Wholesale\": np.random.randint(5000, 5200),\n    \"Retail\": np.random.randint(5200, 6000),\n    \"Finance\": np.random.randint(6000, 6800),\n    \"Services\": np.random.randint(7000, 9000),\n    \"Public\": np.random.randint(9000, 10000)\n  }\n\n  stock_identifiers_list.append({\n    \"permno\": x,\n    \"gvkey\": str(x+10000),\n    \"exchange\": exchange,\n    \"industry\": industry,\n    \"exchcd\": exchcd_mapping[exchange],\n    \"siccd\": siccd_mapping[industry]\n  })\n\nstock_identifiers = pd.DataFrame(stock_identifiers_list)\n```\n:::\n\n\nNext, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the `stock_panel_yearly` panel. To achieve this, we combine the `stock_identifiers` table with a new table containing the variable `year` from `dummy_years`. The `expand_grid()` function ensures that we get all possible combinations of the two tables. After combining, we select only the `gvkey` and `year` columns for our final yearly panel.\n\nNext, we construct the `stock_panel_monthly` panel. Similar to the yearly panel, we use the `expand_grid()` function to combine `stock_identifiers` with a new table that has the `month` variable from `dummy_months`. After merging, we select the columns `permno`, `gvkey`, `month`, `siccd`, `industry`, `exchcd`, and `exchange` to form our monthly panel.\n\nLastly, we create the `stock_panel_daily` panel. We combine `stock_identifiers` with a table containing the `date` variable from `dummy_days`. After merging, we retain only the `permno` and `date` columns for our daily panel.\n\n::: {#e54df718 .cell execution_count=5}\n``` {.python .cell-code}\nstock_panel_yearly = pd.DataFrame({\n  \"gvkey\": np.tile(stock_identifiers[\"gvkey\"], len(dummy_years)),\n  \"year\": np.repeat(dummy_years, len(stock_identifiers))\n})\n\nstock_panel_monthly = pd.DataFrame({\n  \"permno\": np.tile(stock_identifiers[\"permno\"], len(dummy_months)),\n  \"gvkey\": np.tile(stock_identifiers[\"gvkey\"], len(dummy_months)),\n  \"month\": np.repeat(dummy_months, len(stock_identifiers)),\n  \"siccd\": np.tile(stock_identifiers[\"siccd\"], len(dummy_months)),\n  \"industry\": np.tile(stock_identifiers[\"industry\"], len(dummy_months)),\n  \"exchcd\": np.tile(stock_identifiers[\"exchcd\"], len(dummy_months)),\n  \"exchange\": np.tile(stock_identifiers[\"exchange\"], len(dummy_months))\n})\n\nstock_panel_daily = pd.DataFrame({\n  \"permno\": np.tile(stock_identifiers[\"permno\"], len(dummy_days)),\n  \"date\": np.repeat(dummy_days, len(stock_identifiers))\n})\n```\n:::\n\n\n### Dummy `beta` table\n\nWe then proceed to create dummy beta values for our `stock_panel_monthly` table. We generate monthly beta values `beta_monthly` using the `rnorm()` function with a mean and standard deviation of 1. For daily beta values `beta_daily`, we take the dummy monthly beta and add a small random noise to it. This noise is generated again using the `rnorm()` function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.\n\n::: {#2c4e9a0a .cell execution_count=6}\n``` {.python .cell-code}\nbeta_dummy = (stock_panel_monthly\n  .assign(\n    beta_monthly=np.random.normal(\n      loc=1, scale=1, size=len(stock_panel_monthly)\n    ),\n    beta_daily=lambda x: (\n      x[\"beta_monthly\"]+np.random.normal(scale=0.01, size=len(x))\n    )\n  )\n)\n\n(beta_dummy\n  .to_sql(name=\"beta\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index = False)\n)\n```\n:::\n\n\n### Dummy `compustat` table\n\nTo create dummy firm characteristics, we take all columns from the `compustat` table and create random numbers between 0 and 1. For simplicity, we set the `datadate` for each firm-year observation to the last day of the year, although it is empirically not the case. \\index{Data!Compustat}\n\n::: {#9ffd2bcd .cell execution_count=7}\n``` {.python .cell-code}\nrelevant_columns = [\n  \"seq\", \"ceq\", \"at\", \"lt\", \"txditc\", \"txdb\", \"itcb\", \n  \"pstkrv\", \"pstkl\", \"pstk\", \"capx\", \"oancf\", \"sale\", \n  \"cogs\", \"xint\", \"xsga\", \"be\", \"op\", \"at_lag\", \"inv\"\n]\n\ncommands = {\n  col: np.random.rand(len(stock_panel_yearly)) for col in relevant_columns\n}\n\ncompustat_dummy = (\n  stock_panel_yearly\n  .assign(\n    datadate=lambda x: pd.to_datetime(x[\"year\"].astype(str)+\"-12-31\")\n  )\n  .assign(**commands)\n)\n\n(compustat_dummy\n  .to_sql(name=\"compustat\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n### Dummy `crsp_monthly` table\n\nThe `crsp_monthly` table only lacks a few more columns compared to `stock_panel_monthly`: the returns `ret` drawn from a normal distribution, the excess returns `ret_excess` with small deviations from the returns, the shares outstanding `shrout` and the last price per month `altprc` both drawn from uniform distributions, and the market capitalization `mktcap` as the product of `shrout` and `altprc`. \\index{Data!CRSP}\n\n::: {#d39a6bd4 .cell execution_count=8}\n``` {.python .cell-code}\ncrsp_monthly_dummy = (stock_panel_monthly\n  .assign(\n    date=lambda x: x[\"month\"]+pd.offsets.MonthEnd(1),\n    ret=lambda x: np.fmax(np.random.normal(size=len(x)), -1),\n    ret_excess=lambda x: (\n      np.fmax(x[\"ret\"]-np.random.uniform(0, 0.0025, len(x)), -1)\n    ),\n    shrout=1000*np.random.uniform(1, 50, len(stock_panel_monthly)),\n    altprc=np.random.uniform(0, 1000, len(stock_panel_monthly)))\n  .assign(mktcap=lambda x: x[\"shrout\"]*x[\"altprc\"])\n  .sort_values(by=[\"permno\", \"month\"])\n  .assign(\n    mktcap_lag=lambda x: (x.groupby(\"permno\")[\"mktcap\"].shift(1))\n  )\n  .reset_index(drop=True)\n)\n\n(crsp_monthly_dummy\n  .to_sql(name=\"crsp_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n### Dummy `crsp_daily` table\n\nThe `crsp_daily` table only contains a `month` column and the daily excess returns `ret_excess` as additional columns to `stock_panel_daily`.  \n\n::: {#50ef8548 .cell execution_count=9}\n``` {.python .cell-code}\ncrsp_daily_dummy = (stock_panel_daily\n  .assign(\n    month=lambda x: x[\"date\"].dt.to_period('M').dt.start_time,\n    ret_excess=lambda x: np.fmax(np.random.normal(size=len(x)), -1)\n  )\n  .reset_index(drop=True)\n)\n\n(crsp_daily_dummy\n  .to_sql(name=\"crsp_daily\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n## Create Bond Dummy Data\n\nLastly, we move to the bond data that we use in our books. \n\n### Dummy `fisd` data\n\nTo create dummy data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: `maturity`, `offering_amt`, `interest_frequency`, `coupon`, and `sic_code`. We use these probabilities to sample a small cross-section of bonds with completely made up `complete_cusip`, `issue_id`, and `issuer_id`.\\index{Data!FISD}\n\n::: {#e75273ce .cell execution_count=10}\n``` {.python .cell-code}\nnumber_of_bonds = 100\n\ndef generate_cusip():\n  \"\"\"Generate cusip.\"\"\"\n  \n  characters = list(string.ascii_uppercase+string.digits)  # Convert to list\n  cusip = (\"\".join(np.random.choice(characters, size=12))).upper()\n    \n  return cusip\n\nfisd_dummy = (pd.DataFrame({\n    \"complete_cusip\": [generate_cusip() for _ in range(number_of_bonds)]\n  })\n  .assign(\n    maturity=lambda x: np.random.choice(dummy_days, len(x), replace=True),\n    offering_amt=lambda x: np.random.choice(\n      np.arange(1, 101)*100000, len(x), replace=True\n    )\n  )\n  .assign(\n    offering_date=lambda x: (\n      x[\"maturity\"]-pd.to_timedelta(\n        np.random.choice(np.arange(1, 26)*365, len(x), replace=True), \n        unit=\"D\"\n      )\n    )\n  )\n  .assign(\n    dated_date=lambda x: (\n      x[\"offering_date\"]-pd.to_timedelta(\n        np.random.choice(np.arange(-10, 11), len(x), replace=True), \n        unit=\"D\"\n      )\n    ),\n    interest_frequency=lambda x: np.random.choice(\n      [0, 1, 2, 4, 12], len(x), replace=True\n    ),\n    coupon=lambda x: np.random.choice(\n      np.arange(0, 2.1, 0.1), len(x), replace=True\n    )\n  )\n  .assign(\n    last_interest_date=lambda x: (\n      x[[\"maturity\", \"offering_date\", \"dated_date\"]].max(axis=1)\n    ),\n    issue_id=lambda x: x.index+1,\n    issuer_id=lambda x: np.random.choice(\n      np.arange(1, 251), len(x), replace=True\n    ),\n    sic_code=lambda x: (np.random.choice(\n      np.arange(1, 10)*1000, len(x), replace=True)\n    ).astype(str)\n  )\n)\n\n(fisd_dummy\n  .to_sql(name=\"fisd\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n### Dummy `trace_enhanced` data\n\nFinally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy `fisd` data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book. \\index{Data!TRACE}\n\n::: {#e6cb7ab3 .cell execution_count=11}\n``` {.python .cell-code}\nnumber_of_bonds = 100\nstart_date = pd.Timestamp(\"2014-01-01\")\nend_date = pd.Timestamp(\"2016-11-30\")\n\nbonds_panel = pd.DataFrame({\n  \"cusip_id\": np.tile(\n    fisd_dummy[\"complete_cusip\"], \n    (end_date-start_date).days+1\n  ),\n  \"trd_exctn_dt\": np.repeat(\n    pd.date_range(start_date, end_date), len(fisd_dummy)\n  )\n})\n\ntrace_enhanced_dummy = (pd.concat([bonds_panel]*5)\n  .assign(\n    trd_exctn_tm = lambda x: pd.to_datetime(\n      x[\"trd_exctn_dt\"].astype(str)+\" \" +\n      np.random.randint(0, 24, size=len(x)).astype(str)+\":\" +\n      np.random.randint(0, 60, size=len(x)).astype(str)+\":\" +\n      np.random.randint(0, 60, size=len(x)).astype(str)\n    ),\n    rptd_pr=np.random.uniform(10, 200, len(bonds_panel)*5),\n    entrd_vol_qt=1000*np.random.choice(\n      range(1,21), len(bonds_panel)*5, replace=True\n    ),\n    yld_pt=np.random.uniform(-10, 10, len(bonds_panel)*5),\n    rpt_side_cd=np.random.choice(\n      [\"B\", \"S\"], len(bonds_panel)*5, replace=True\n    ),\n    cntra_mp_id=np.random.choice(\n      [\"C\", \"D\"], len(bonds_panel)*5, replace=True\n    )\n  )\n  .reset_index(drop=True)\n)\n\n(trace_enhanced_dummy\n  .to_sql(name=\"trace_enhanced\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\nAs stated in the introduction, the data does *not* contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout this book.\n\n",
    "supporting": [
      "wrds-dummy-data_files"
    ],
    "filters": [],
    "includes": {}
  }
}