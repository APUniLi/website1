{
  "hash": "621b4163492c4a3e926e67beb89c1edf",
  "result": {
    "markdown": "---\ntitle: \"Non-standard errors in portfolio sorts\"\nauthor: \"Patrick Weiss\"\ndate: \"2023-05-10\"\ndescription: An all-in-one implementation of non-standard errors in portfolio sorts.\nimage: thumbnail.jpg\ncategories: \n  - Replications\n---\n\n\n# What does this post cover?\n\nWelcome to our latest blog post where we delve into the intriguing world of non-standard errors[^1], a crucial aspect of academic research, which also is addressed in financial economics. These non-standard errors often stem from the methodological decisions we make, adding an extra dimension of uncertainty to the estimates we report. I had the pleasure of working with Stefan Voigt on a contribution to Menkveld et al. (2023), which was an exciting opportunity to shape the first discussions on non-standard errors.\n\nOne of the goals of Tidy Finance has always been focused on promoting reproducibility in finance research. We began this endeavor by introducing a chapter on Size sorts and p-hacking, which initiated some analysis of portfolio sorting choices. Recently, my fellow authors, [Dominik Walter](https://sites.google.com/view/dominikwalter/startseite) and [RÃ¼diger Weber](https://sites.google.com/site/ruedigercweber/), and I published an update to our working paper, [Non-Standard Errors in Portfolio Sorts](http://dx.doi.org/10.2139/ssrn.4164117)[^2]. This paper delves into the impact of methodological choices on return differentials derived from portfolio sorts. Our conclusion? We need to accept non-standard errors in empirical asset pricing. By reporting the entire range of return differentials, we simultaneously deepen our economic understanding of return anomalies and improve trust.\n\nThis blog post will guide you through conducting portfolio sorts which keep non-standard errors in perspective. We explore the multitude of possible decisions that can impact return differentials, allowing us to estimate a premium's distribution rather than a single return differential. The code is inspired by Tidy Finance with R and the replication code for Walter, Weber, and Weiss (2023), which can be found in this [Github repository](https://github.com/patrick-weiss/PortfolioSorts_NSE). By the end of this post, you will have the knowledge to sort portfolios based on *asset growth* in nearly 70,000 different ways.\n\nWhile this post is detailed, it is not overly complex. However, if you are new to R or portfolio sorts, I recommend first checking out our chapter on [Size sorts and p-hacking](https://www.tidy-finance.org/size-sorts-and-p-hacking.html). Due to the length constraints, we will be skipping over certain details related to implementation and the economic background (you can find these in the WWW paper). If there are particular aspects you would like to delve into further, please feel free to reach out.\n\n# Data\n\nFirst, we need some data. On the one hand, we need the monthly return time series for the CRSP universe. On the other hand, we need some accounting data from Compustat for constructing the sorting variable itself. To save space and because there is a chapter in Tidy Finance on it, I refer you to our chapter on [WRDS, CRSP, and Compustat](https://www.tidy-finance.org/wrds-crsp-compustat.html) for the details downloading the data. Here, we only read the data from my preprepared SQLite database.\n\nWe first need a few packages. The `tidyverse` (of course) and the `RSQLite`-package for the database. Additionally, we connect to my database, which contains all the necessary data. The prefix `../` in the path argument moves one directory up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(RSQLite)\n\n# Database\ndata_tidy_nse <- dbConnect(SQLite(),\n  \"../../data/data_nse.sqlite\",\n  extended_types = TRUE\n)\n```\n:::\n\n\nNext, I load the necessary stock market (CRSP) and accounting (Compustat) data from my SQLite database.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_monthly <- tbl(data_tidy_nse, \"crsp_monthly\") |>\n  collect()\n\ncompustat <- tbl(data_tidy_nse, \"compustat\") |>\n  collect()\n```\n:::\n\n\nThen, we need to construct the sorting variable. As an example for this post, we will use *asset growth*, suggested as a predictor of the cross-section of stock prices by Cooper, Gulen, and Schill (2008)[^3]. Asset growth is measured as the relative change in *total assets* of a firm. The naming convention `sv_ag` for the sorting variable asset growth reflects the practice of assigning the prefix `sv_` to all sorting variables in Walter, Weber, and Weiss (2023). This convention also allows you to test multiple specifications of the sorting variable (e.g., considering a winsorized version, etc.). Next to the main variable, we have to compute three *filters* relating to the firm's stock price, book equity, and earnings. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lag variable\ncompustat_lag <- compustat |>\n  select(gvkey, year, at) |>\n  mutate(year = year + 1) |>\n  rename_with(.cols = at, ~ paste0(.x, \"_lag\"))\n\n# Compute asset growth\ncompustat <- compustat |>\n  left_join(compustat_lag, by = c(\"gvkey\", \"year\")) |>\n  mutate(sv_ag = (at - at_lag) / at_lag)\n\n# Compute filters\ncompustat <- compustat |>\n  mutate(\n    filter_be = coalesce(seq, ceq + pstk, at - lt) +\n      coalesce(txditc, txdb + itcb, 0) -\n      coalesce(pstkrv, pstkl, pstk, 0),\n    filter_price = prcc_f,\n    filter_earnings = ib\n  )\n\n# Select required variables\ncompustat <- compustat |>\n  select(\n    gvkey, month, datadate,\n    starts_with(\"filter_\"), starts_with(\"sv_\")\n  ) |>\n  drop_na()\n```\n:::\n\n\nFor the CRSP data, we also construct a variable that we will need below. We compute the stock age filter as the time in years between the stock's first appearance in CRSP and the current month. To do this reliably, we again leverage `group_by()`'s power. We also lag the filter by one month to avoid inducing a look-ahead bias before merging it back to our main stock market data. Following our main data construction, market capitalization is already in the CRSP database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_monthly_filter <- crsp_monthly |>\n  group_by(permno) |>\n  arrange(month) |>\n  mutate(\n    filter_stock_age = as.numeric(difftime(month,\n      min(month),\n      units = \"days\"\n    )) / 365,\n    month = month %m+% months(1)\n  ) |>\n  ungroup() |>\n  select(permno, month, filter_stock_age)\n\ncrsp_monthly <- crsp_monthly |>\n  select(\n    permno, gvkey, month, industry,\n    exchange, mktcap, mktcap_lag, ret_excess\n  ) |>\n  left_join(crsp_monthly_filter,\n    by = c(\"permno\", \"month\")\n  ) |>\n  drop_na()\n```\n:::\n\n\nAt the moment, we only have panels of stock returns and characteristics. These panels still need to be matched together yet, because this also constitutes a decision. Hence, let us move to discuss these decisions.\n\n# The decision nodes\n\nIn Walter, Weber, and Weiss (2023), we identify 14 methodological choices that must be made to estimate a premium from portfolio sorts. We split these into decisions on the sample construction and the portfolio construction. The table below illustrates the choices, and for further reference, you can refer to Walter, Weber, and Weiss (2023) for details on these nodes. Note that the three nodes referring to *breakpoints* control the number of portfolios (in the main and secondary dimensions) or the exchanges considered for computing the breakpoints.\n\nNode | Choices\n:------|:------\nSize restriction | none, NYSE 5%, NYSE 20%\nFinancials | include, exclude\nUtilities | include, exclude\nPos. book equity | include, exclude\nPos. earnings | include, exclude\nStock-age restriction | none, >2 years\nPrice restriction | none, >USD 1, >USD 5 \nSorting variable lag | 3 months, 6 months, Fama-French\nRebalancing | monthly, annually\nBreakpoint quantiles main | 5, 10\nDouble sort | single, double dependent, double independent\nBreakpoint quantiles secondary | 2, 5\nBreakpoint exchanges | NYSE, all\nWeighting scheme | equal-weighting, value-weighting\n\nIn principle, there are more decisions to be made. However, this set of 14 choices appears in published, peer-reviewed articles and covers different aspects. If you think another choice is essential, please reach out.\n\nLet us now create all possible combinations of choices that are feasible. We use `expand_grid()` on the tibble of individual nodes and their branches. Note that single sorts do not use the node regarding the number of secondary portfolios, i.e., we remove these paths after combining all choices. This leaves us with 69,120 choices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sorting grid\nsetup_grid <- expand_grid(\n  sorting_variable = \"sv_ag\",\n  drop_smallNYSE_at = c(0, 0.05, 0.2),\n  include_financials = c(TRUE, FALSE),\n  include_utilities = c(TRUE, FALSE),\n  drop_bookequity = c(TRUE, FALSE),\n  drop_earnings = c(TRUE, FALSE),\n  drop_stock_age_at = c(0, 2),\n  drop_price_at = c(0, 1, 5),\n  sv_lag = c(\"3m\", \"6m\", \"FF\"),\n  formation_time = c(\"monthly\", \"FF\"),\n  n_portfolios_main = c(5, 10),\n  sorting_method = c(\"single\", \"dbl_ind\", \"dbl_dep\"),\n  n_portfolios_secondary = c(2, 5),\n  exchanges = c(\"NYSE\", \"NYSE|NASDAQ|AMEX\"),\n  value_weighted = c(TRUE, FALSE)\n)\n\n# Remove information on double sorting for univariate sorts\nsetup_grid <- setup_grid |>\n  filter(!(sorting_method == \"single\" & n_portfolios_secondary > 2)) |>\n  mutate(n_portfolios_secondary = case_when(\n    sorting_method == \"single\" ~ NA_real_,\n    TRUE ~ n_portfolios_secondary\n  ))\n```\n:::\n\n\n## Merge data\n\nOne key decision node is the sorting variable lag. However, merging data is an expensive operation, and doing it repeatedly is unnecessary. Hence, we merge the data in the three possible lag configurations and store them as separate tibbles. Thereby, we can later reference the correct table instead of merging the desired output.\n\nFirst, let us consider the Fama-French (FF) lag. Here, we consider accounting information published in year $t-1$ starting from July of year $t$. That is, we use the accounting information published 6 to 18 months ago. We first match the accounting data to the stock market data before we fill in the missing observations. A few pitfalls exist when using the `fill()`-function. First, one might easily forget to order and group the data. Second, the function does not care how outdated the information becomes. In principle, you can end up with data that is decades old. Therefore, we ensure that these filled data points are not older than 12 months. This is achieved with the new variable `sv_age_check`, which serves as a filter for outdated data. Finally, notice that this code provides much flexibility. All variables with prefixes *sv_* and *filter_* get filled. So you can easily adapt my code to your needs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_FF <- crsp_monthly |>\n  mutate(sorting_date = month) |>\n  left_join(\n    compustat |>\n      mutate(\n        sorting_date = floor_date(datadate, \"year\") %m+% months(18),\n        sv_age_check = sorting_date\n      ) |>\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n# Fill variables and ensure timeliness of data\ndata_FF <- data_FF |>\n  arrange(permno, month) |>\n  group_by(permno, gvkey) |>\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |>\n  ungroup() |>\n  filter(sv_age_check > month %m-% months(12)) |>\n  select(-sv_age_check, -sorting_date, -gvkey)\n```\n:::\n\n\nNext, we create the basis with lags of three and six months. The process is exactly the same as above for the FF lag, but without the `floor_date()` as we apply a constant lag to all observations. Again, we make sure that after the call to `fill()` our information does not become too old.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3 months\n## Merge data\ndata_3m <- crsp_monthly |>\n  mutate(sorting_date = month) |>\n  left_join(\n    compustat |>\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(3),\n        sv_age_check = sorting_date\n      ) |>\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_3m <- data_3m |>\n  arrange(permno, month) |>\n  group_by(permno, gvkey) |>\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |>\n  ungroup() |>\n  filter(sv_age_check > month %m-% months(12)) |>\n  select(-sv_age_check, -sorting_date, -gvkey)\n\n# 6 months\n## Merge data\ndata_6m <- crsp_monthly |>\n  mutate(sorting_date = month) |>\n  left_join(\n    compustat |>\n      mutate(\n        sorting_date = floor_date(datadate, \"month\") %m+% months(6),\n        sv_age_check = sorting_date\n      ) |>\n      select(-month, -datadate),\n    by = c(\"gvkey\", \"sorting_date\")\n  )\n\n## Fill variables and ensure timeliness of data\ndata_6m <- data_6m |>\n  arrange(permno, month) |>\n  group_by(permno, gvkey) |>\n  fill(starts_with(\"sv_\"), starts_with(\"filter_\")) |>\n  ungroup() |>\n  filter(sv_age_check > month %m-% months(12)) |>\n  select(-sv_age_check, -sorting_date, -gvkey)\n```\n:::\n\n\n#  Portfolio sorts\n\nWe are equipped with the necessary data and the set of decisions we consider. Next, we implement our decisions into actual portfolio sorts. Well. First, we have to define a few functions to make the implementation feasible. Thinking in functions is an important aspect that enables you to accomplish the task set for this blog post. Then, we will apply these functions.\n\n## Functions\n\nWe write functions that complete specific tasks and then combine them to generate the desired output. Breaking it up into smaller steps makes the whole process more tractable and easier to test.\n\n### Select the sample\n\nThe first function gets the name `handle_data()` because it is intended to select the sample according to the sample construction choices. The function first selects the data based on the desired sorting variable lag (specified in `sv_lag`). Then, we apply the various filters we discussed above. As you see, this is relatively simple, but it already covers our sample construction nodes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhandle_data <- function(include_financials, include_utilities,\n                        drop_smallNYSE_at, drop_price_at, drop_stock_age_at,\n                        drop_earnings, drop_bookequity,\n                        sv_lag) {\n  # Select dataset\n  if (sv_lag == \"FF\") data_all <- data_FF\n  if (sv_lag == \"3m\") data_all <- data_3m\n  if (sv_lag == \"6m\") data_all <- data_6m\n\n  # Size filter based on NYSE percentile\n  if (drop_smallNYSE_at > 0) {\n    data_all <- data_all |>\n      group_by(month) |>\n      mutate(NYSE_breakpoint = quantile(\n        mktcap_lag[exchange == \"NYSE\"],\n        drop_smallNYSE_at\n      )) |>\n      ungroup() |>\n      filter(mktcap_lag >= NYSE_breakpoint) |>\n      select(-NYSE_breakpoint)\n  }\n\n  # Exclude industries\n  data_all <- data_all |>\n    filter(if (include_financials) TRUE else !grepl(\"Finance\", industry)) |>\n    filter(if (include_utilities) TRUE else !grepl(\"Utilities\", industry))\n\n  # Book equity filter\n  if (drop_bookequity) {\n    data_all <- data_all |>\n      filter(filter_be > 0)\n  }\n\n  # Earnings filter\n  if (drop_earnings) {\n    data_all <- data_all |>\n      filter(filter_earnings > 0)\n  }\n\n  # Stock age filter\n  if (drop_stock_age_at > 0) {\n    data_all <- data_all |>\n      filter(filter_stock_age >= drop_stock_age_at)\n  }\n\n  # Price filter\n  if (drop_price_at > 0) {\n    data_all <- data_all |>\n      filter(filter_price >= drop_price_at)\n  }\n\n  # Define ME\n  data_all <- data_all |>\n    mutate(me = mktcap_lag) |>\n    drop_na(me) |>\n    select(-starts_with(\"filter_\"), -industry)\n\n  # Return\n  return(data_all)\n}\n```\n:::\n\n\n### Assign portfolios\n\nNext, we define a function that assigns portfolios based on the specified sorting variable, the number of portfolios, and the exchanges. The function only works on a single cross-section of data, i.e., it has to be applied to individual months of data. The central part of the function is to compute the $n$ breakpoints based on the exchange filter. Then, `findInterval()` assigns the respective portfolio number.\n\nThe function also features two sanity checks. First, it does not assign portfolios if there are too few stocks in the cross-section. Second, sometimes the sorting variable creates scenarios where some portfolios are overpopulated. For example, if the variable in question is bounded from below by 0. In such a case, an unexpectedly large number of firms might end up in the lowest bucket, covering multiple quantiles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassign_portfolio <- function(data, sorting_variable,\n                             n_portfolios, exchanges) {\n  # Escape small sets (i.e., less than 10 firms per portfolio)\n  if (nrow(data) < n_portfolios * 10) {\n    return(NA)\n  }\n\n  # Compute breakpoints\n  breakpoints <- data |>\n    filter(grepl(exchanges, exchange)) |>\n    pull(all_of(sorting_variable)) |>\n    quantile(\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE,\n      names = FALSE\n    )\n\n  # Assign portfolios\n  portfolios <- data |>\n    mutate(portfolio = findInterval(\n      pick(everything()) |>\n        pull(all_of(sorting_variable)),\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n\n  # Check if breakpoints are well defined\n  if (length(unique(breakpoints)) == n_portfolios + 1) {\n    return(portfolios)\n  } else {\n    print(breakpoints)\n    cat(paste0(\n      \"\\n Breakpoint issue! Month \",\n      as.Date(as.numeric(cur_group())),\n      \"\\n\"\n    ))\n    stop()\n  }\n}\n```\n:::\n\n\n### Single and double sorts\n\nOur goal is to construct portfolios for single sorts, independent double sorts, and dependent double sorts. Hence, our next three functions do exactly that. The double sorts considered always take a first sort on market equity (the variable `me`) before sorting on the actual sorting variable.\n\nLet us start with single sorts. As you see, we group by month as the function `assign_portfolio()` we wrote above handles one cross-section at a time. The rest of the function just passes the arguments to the portfolio assignment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort_single <- function(data, sorting_variable,\n                        exchanges, n_portfolios_main) {\n  data |>\n    group_by(month) |>\n    mutate(portfolio = assign_portfolio(\n      data = pick(all_of(sorting_variable), exchange),\n      sorting_variable = sorting_variable,\n      n_portfolios = n_portfolios_main,\n      exchanges = exchanges\n    )) |>\n    drop_na(portfolio) |>\n    ungroup()\n}\n```\n:::\n\n\nFor double sorts, things are more interesting. First, we have the issue of independent and dependent double sorts. An independent sort considers the two sorting variables (*size* and *asset growth*) independently. In contrast, dependent sorts are, in our case, first sorting on *size* and within these buckets on *asset growth*. We group by the secondary portfolio to achieve the dependent sort before generating the main portfolios. Second, we need to generate an overall portfolio of the two sorts - we will see this later. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort_double_independent <- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |>\n    group_by(month) |>\n    mutate(\n      portfolio_secondary = assign_portfolio(\n        data = pick(me, exchange),\n        sorting_variable = \"me\",\n        n_portfolios = n_portfolios_secondary,\n        exchanges = exchanges\n      ),\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |>\n    drop_na(portfolio_main, portfolio_secondary) |>\n    ungroup()\n}\n\nsort_double_dependent <- function(data, sorting_variable, exchanges, n_portfolios_main, n_portfolios_secondary) {\n  data |>\n    group_by(month) |>\n    mutate(portfolio_secondary = assign_portfolio(\n      data = pick(me, exchange),\n      sorting_variable = \"me\",\n      n_portfolios = n_portfolios_secondary,\n      exchanges = exchanges\n    )) |>\n    drop_na(portfolio_secondary) |>\n    group_by(month, portfolio_secondary) |>\n    mutate(\n      portfolio_main = assign_portfolio(\n        data = pick(all_of(sorting_variable), exchange),\n        sorting_variable = sorting_variable,\n        n_portfolios = n_portfolios_main,\n        exchanges = exchanges\n      ),\n      portfolio = paste0(portfolio_main, \"-\", portfolio_secondary)\n    ) |>\n    drop_na(portfolio_main) |>\n    ungroup()\n}\n```\n:::\n\n\n### Annual vs monthly rebalancing\n\nNow, we still have one decision node to cover: Rebalancing. We can either rebalance annually in July or monthly. To achieve this, we write two more functions - the last functions before finishing up. Let us start with monthly rebalancing because it is much easier. All we need to do is to use the assigned portfolio numbers to generate portfolio returns. Inside the function, we use three `if()` calls to decide the sorting method. Notice that the double sorts use the simple average for aggregating the extreme portfolios of the size buckets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrebalance_monthly <- function(data, sorting_variable, sorting_method,\n                              n_portfolios_main, n_portfolios_secondary,\n                              exchanges, value_weighted) {\n  # Single sort\n  if (sorting_method == \"single\") {\n    data_rets <- data |>\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |>\n      group_by(month, portfolio) |>\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    data_rets <- data |>\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |>\n      group_by(month, portfolio) |>\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |>\n      group_by(month, portfolio_main) |>\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |>\n      rename(portfolio = portfolio_main)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    data_rets <- data |>\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |>\n      group_by(month, portfolio) |>\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |>\n      group_by(month, portfolio_main) |>\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |>\n      rename(portfolio = portfolio_main)\n  }\n\n  return(data_rets)\n}\n```\n:::\n\n\nNow, let us move to the annual rebalancing. Here, we first assign a portfolio on the data in July based on single or independent/dependent double sorts. Then, we fill the remaining months forward before computing returns. Hence, we need one extra step for each sort.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrebalance_annually <- function(data, sorting_variable, sorting_method,\n                               n_portfolios_main, n_portfolios_secondary,\n                               exchanges, value_weighted) {\n  data_sorting <- data |>\n    filter(month(month) == 7) |>\n    group_by(month)\n\n  # Single sort\n  if (sorting_method == \"single\") {\n    # Assign portfolios\n    data_sorting <- data_sorting |>\n      sort_single(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main\n      ) |>\n      select(permno, month, portfolio) |>\n      mutate(sorting_month = month)\n  }\n\n  # Double independent sort\n  if (sorting_method == \"dbl_ind\") {\n    # Assign portfolios\n    data_sorting <- data_sorting |>\n      sort_double_independent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |>\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |>\n      mutate(sorting_month = month)\n  }\n\n  # Double dependent sort\n  if (sorting_method == \"dbl_dep\") {\n    # Assign portfolios\n    data_sorting <- data_sorting |>\n      sort_double_dependent(\n        sorting_variable = sorting_variable,\n        exchanges = exchanges,\n        n_portfolios_main = n_portfolios_main,\n        n_portfolios_secondary = n_portfolios_secondary\n      ) |>\n      select(permno, month, portfolio, portfolio_main, portfolio_secondary) |>\n      mutate(sorting_month = month)\n  }\n\n  # Compute portfolio return\n  if (sorting_method == \"single\") {\n    data |>\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |>\n      group_by(permno) |>\n      arrange(month) |>\n      fill(portfolio, sorting_month) |>\n      filter(sorting_month >= month %m-% months(12)) |>\n      drop_na(portfolio) |>\n      group_by(month, portfolio) |>\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        .groups = \"drop\"\n      )\n  } else {\n    data |>\n      left_join(data_sorting, by = c(\"permno\", \"month\")) |>\n      group_by(permno) |>\n      arrange(month) |>\n      fill(portfolio_main, portfolio_secondary, portfolio, sorting_month) |>\n      filter(sorting_month >= month %m-% months(12)) |>\n      drop_na(portfolio_main, portfolio_secondary) |>\n      group_by(month, portfolio) |>\n      summarize(\n        ret = if_else(value_weighted,\n          weighted.mean(ret_excess, mktcap_lag),\n          mean(ret_excess)\n        ),\n        portfolio_main = unique(portfolio_main),\n        .groups = \"drop\"\n      ) |>\n      group_by(month, portfolio_main) |>\n      summarize(\n        ret = mean(ret),\n        .groups = \"drop\"\n      ) |>\n      rename(portfolio = portfolio_main)\n  }\n}\n```\n:::\n\n\n### Combining the functions\n\nNow, we have everything to compute our 69,120 portfolio sorts for *asset growth* to understand the variation our decisions induce. To achieve this, our function considers all choices as arguments and passes them to the sample selection and portfolio construction functions.\n\nFinally, the function computes the return differential for each month. Since we are only interested in the mean here, we simply take the mean of these time series and call it our premium estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexecute_sorts <- function(sorting_variable, drop_smallNYSE_at,\n                          include_financials, include_utilities,\n                          drop_bookequity, drop_earnings,\n                          drop_stock_age_at, drop_price_at,\n                          sv_lag, formation_time,\n                          n_portfolios_main, sorting_method,\n                          n_portfolios_secondary, exchanges,\n                          value_weighted) {\n  # Select data\n  data_sorts <- handle_data(\n    include_financials = include_financials,\n    include_utilities = include_utilities,\n    drop_smallNYSE_at = drop_smallNYSE_at,\n    drop_price_at = drop_price_at,\n    drop_stock_age_at = drop_stock_age_at,\n    drop_earnings = drop_earnings,\n    drop_bookequity = drop_bookequity,\n    sv_lag = sv_lag\n  )\n\n  # Rebalancing\n  ## Monthly\n  if (formation_time == \"monthly\") {\n    data_return <- rebalance_monthly(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  ## Annually\n  if (formation_time == \"FF\") {\n    data_return <- rebalance_annually(\n      data = data_sorts,\n      sorting_variable = sorting_variable,\n      sorting_method = sorting_method,\n      n_portfolios_main = n_portfolios_main,\n      n_portfolios_secondary = n_portfolios_secondary,\n      exchanges = exchanges,\n      value_weighted = value_weighted\n    )\n  }\n\n  # Compute return differential\n  data_return |>\n    group_by(month) |>\n    summarize(\n      premium = ret[portfolio == max(portfolio)] - ret[portfolio == min(portfolio)],\n      .groups = \"drop\"\n    ) |>\n    pull(premium) |>\n    mean() * 100\n}\n```\n:::\n\n\n## Applying the functions\n\nFinally, we have data, decisions, and functions. Indeed, we are now ready to implement the portfolio sort, right? Yes! Just let me briefly discuss how the implementation works.\n\nWe have a grid with 69,120 specifications. For each of these specifications, we want to estimate a return differential. This is most easily achieved with a `pmap()` call. However, we want to parallelize the operation to leverage the multiple cores of our device. Hence, we have to use the package `furrr` and a `future_pmap()` instead. As a side note, in most cases we also have to increase the maximum memory for each worker, which can be done with `options()`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr)\n\noptions(future.globals.maxSize = 891289600)\n\nplan(multisession, workers = availableCores())\n```\n:::\n\n\nWith the parallel environment set and ready to go, we map the arguments our function needs into the final function `execute_sorts()` from above. Then, we go and have some tea. And some lunch, breakfast, second breakfast, and so on. In short, it takes a while - depending on your device even more than a day. Each result requires roughly 13 seconds, but you must remember that you are computing 69,120 results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_premia <- setup_grid |>\n  mutate(premium_estimate = future_pmap(\n    .l = list(\n      sorting_variable, drop_smallNYSE_at, include_financials,\n      include_utilities, drop_bookequity, drop_earnings,\n      drop_stock_age_at, drop_price_at, sv_lag,\n      formation_time, n_portfolios_main, sorting_method,\n      n_portfolios_secondary, exchanges, value_weighted\n    ),\n    .f = ~ execute_sorts(\n      sorting_variable = ..1,\n      drop_smallNYSE_at = ..2,\n      include_financials = ..3,\n      include_utilities = ..4,\n      drop_bookequity = ..5,\n      drop_earnings = ..6,\n      drop_stock_age_at = ..7,\n      drop_price_at = ..8,\n      sv_lag = ..9,\n      formation_time = ..10,\n      n_portfolios_main = ..11,\n      sorting_method = ..12,\n      n_portfolios_secondary = ..13,\n      exchanges = ..14,\n      value_weighted = ..15\n    )\n  ))\n```\n:::\n\n\nNow you have all the estimates for the premium. However, one last step has to be considered when you actually investigate the premium. The portfolio sorting algorithm we constructed here is always long in the firms with a high value for the sorting variable and short in firms with low characteristics. This provides a very general way of doing it. Hence, you simply correct for this effect at the end by multiplying the column by `-1` if your sorting variable predicts an inverse relation between the sorting variable and expected returns. Otherwise, you can ignore the following chunk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_premia <- data_premia |>\n  mutate(premium_estimate = premium_estimate * -1)\n```\n:::\n\n\n\n\n# The premium distribution\n\nGiven all the estimates for the premium, we can now take a look at their distribution with a call to `geom_density()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_premia |>\n  unnest(premium_estimate) |>\n  ggplot() +\n  geom_density(aes(x = premium_estimate),\n    alpha = 0.25,\n    linewidth = 1.2,\n    color = \"#3B9AB2\",\n    fill = \"#3B9AB2\"\n  ) +\n  labs(\n    x = \"Premium (in %, p.m.)\",\n    y = NULL\n  )\n```\n\n::: {.cell-output-display}\n![Asset growth return differentials with non-standard errors.](index_files/figure-html/fig-nse-1-1.png){#fig-nse-1 fig-alt='Title: Asset growth return differentials with non-standard errors. The figure shows a density of estimated return differentials in percent per month. There is significant variation in the estimates.' width=2100}\n:::\n:::\n\n\n\n\nYou can immediately see one of the results in Walter, Weber, and Weiss (2023): There is a lot of variation depending on your choices. However, despite the variation, the premium is always positive. I would argue that this is a pretty strong sign.\n\n# Conclusion\n\nIn summary, there are many potential ways of sorting the cross-section of stocks into portfolios. We explored 69,120 different possibilities based on decisions taken in published research articles. These choices induce variation that we embrace by considering all the possible paths. \n\nIn practice, most research papers introducing a new predictor usually present one of the specifications we have explored here, followed by a few robustness checks. These checks are commonly limited by space constraints. However, our analysis could be viewed as nearly 70,000 robustness tests condensed into an easily interpretable graph or two (if we include t-statistics). Personally, I find this approach both compelling and efficient.\n\nNaturally, we still have to scrutinize the time series of return differentials against a factor model to demonstrate that existing factors cannot capture the suggested premium. I did not include this step in our discussion as it is a straightforward extension of the code we have used. I encourage you to give it a try.\n\nFor those interested in delving deeper into non-standard errors in portfolio sorts, I recommend reading [Walter, Weber, and Weiss (2023)](http://dx.doi.org/10.2139/ssrn.4164117). It provides a comprehensive overview of numerous variables and a more profound analysis of the variation itself.\n\n\n[^1]: Menkveld, A. J. et al. (2023). âNon-standard Errorsâ, Journal of Finance (forthcoming). http://dx.doi.org/10.2139/ssrn.3961574\n\n[^2]:  Walter, D., Weber, R., and Weiss, P. (2023). \"Non-Standard Errors in Portfolio Sorts\". [http://dx.doi.org/10.2139/ssrn.4164117](http://dx.doi.org/10.2139/ssrn.4164117)\n\n[^3]: Cooper, M. J., Gulen, H., and Schill, M. J. (2008). \"Asset growth and the crossâsection of stock returns\", The Journal of Finance, 63(4), 1609-1651.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}