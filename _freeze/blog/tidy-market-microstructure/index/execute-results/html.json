{
  "hash": "d3b0ac42d68c9ddd360aeb8edd9c2e12",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tidy Market Microstructure\"\nauthor:\n  - name: Björn Hagströmer\n    url: https://hagstromer.org/\n    affiliations:\n      - name: Stockholm Business School\n  - name: Niklas Landsberg\n    url: https://www.kuleuven.be/wieiswie/en/person/00167120\n    affiliations:\n      - name: KU Leuven \ndate: \"2023-02-15\"\ndescription: A beginner's guide to market quality measurement in high-frequency data\nimage: thumbnail.png\nimage-alt: Cartoon of robots and humans trying to make sense of green stock price charts on black background. Created with DALL-E 3.\ncategories: \n  - Market microstructure\n  - R\n  - data.table\n---\n\n\nAnyone active in market microstructure research *knows* that the devil is in the details. When clocks tick in microseconds and prices move in cents, a brief delay or a small fee discount can make a huge difference for traders. In fact, they can even be the business model of an exchange. But as much as such institutional detail is fascinating, the empirical implementation of even the most conventional microstructure concepts can be frustrating. Referring to the *interest of brevity*, many journal articles often defer the implementation details to an appendix, and even there they tend to be vague or incomplete. With the additional challenge of vast data sets, new entrants into this field face a steep challenge. \n\nWe provide a beginner's guide to market quality measurement in high-frequency data, aiming to lower the barriers to entry into empirical market microstructure. We discuss economic considerations and show step-by-step how to code the most common measures of market liquidity and market efficiency. Because virtually all securities now trade at multiple venues, we also emphasize how market quality can account for market fragmentation. \n\nIs this guide really needed? Well, a recent paper by [Menkveld et al., (2023)](http://dx.doi.org/10.2139/ssrn.3961574)[^1] shows in full clarity that even small variations in methodology can lead to large differences in market quality measures. The authors assigned the same set of market microstructure hypotheses and the same data to 164 research teams. They found that the variation in results across teams, the *non-standard error*, was of a magnitude similar to the standard error. Many teams included seasoned professors, but past publication performance and seniority did not reduce the non-standard error. \n\nAnother question is if the cumbersome high-frequency data analysis is really worth the effort? After all, there are numerous liquidity proxies based on daily data. The answer depends on the research question. First, low-frequency proxies are designed for low-frequency applications. For example, [Amihud's (2002)](https://doi.org/10.1016/S1386-4181(01)00024-6)[^2] popular proxy was originally proposed to be measured as an annual average. Most microstructure applications require liquidity measures at higher frequencies than that. Furthermore, recent evidence by [Jahan-Parvar and Zikes (2023)](https://doi.org/10.1093/rfs/hhad028)[^3] show that many low-frequency proxies capture volatility rather than liquidity.\n\nIf we convinced you to take on the high-frequency data, here's what we offer. Table 1 lists the market quality measures that we cover, as well as their underlying data type. For some measures, we include several versions and discuss the differences between them. We organize the text by the data type, as we think that is a natural work flow. We start with liquidity measures based on tick-by-tick quote data, followed by measures based on both trade and quote data. Finally, we look into a set of measures of efficiency and volatility that require equispaced quote data. \n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Market quality variables and data types</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable name </th>\n   <th style=\"text-align:left;\"> Variable type </th>\n   <th style=\"text-align:left;\"> Data type </th>\n   <th style=\"text-align:right;\"> Section </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Quoted bid-ask spread </td>\n   <td style=\"text-align:left;\"> Liquidity </td>\n   <td style=\"text-align:left;\"> Quote tick data </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Quoted depth </td>\n   <td style=\"text-align:left;\"> Liquidity </td>\n   <td style=\"text-align:left;\"> Quote tick data </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Effective bid-ask spread </td>\n   <td style=\"text-align:left;\"> Liquidity </td>\n   <td style=\"text-align:left;\"> Quote and trade tick data </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Trade volume </td>\n   <td style=\"text-align:left;\"> Volume </td>\n   <td style=\"text-align:left;\"> Trade tick data </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Price impact </td>\n   <td style=\"text-align:left;\"> Liquidity </td>\n   <td style=\"text-align:left;\"> Quote and trade tick data </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Realized spread </td>\n   <td style=\"text-align:left;\"> Liquidity </td>\n   <td style=\"text-align:left;\"> Quote and trade tick data </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Return autocorrelation </td>\n   <td style=\"text-align:left;\"> Efficiency </td>\n   <td style=\"text-align:left;\"> Equispaced quote data </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Realized volatility </td>\n   <td style=\"text-align:left;\"> Volatility </td>\n   <td style=\"text-align:left;\"> Equispaced quote data </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Variance ratio </td>\n   <td style=\"text-align:left;\"> Efficiency </td>\n   <td style=\"text-align:left;\"> Equispaced quote data </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nOur focus is on the *practice* of empirical market microstructure. As such, we often motivate the coding choices with economic concepts. Other than that, however, readers interested in the economics underlying each metric are referred to introductory texts such as Campbell, Lo and MacKinlay (1997)[^4], Foucault, Pagano & Röell (2013)[^5] and Hasbrouck (2007)[^6]. \n\n**Programming.**\nThe coding here is in R.\n\nThe choice of language is of course a matter of taste. When deciding on the tools to use, what is important to consider is that the data files in this field are often huge. How can we process them without crashing our laptops? The example data used in this guide are tiny, but we wrote the code to be efficient when the number of observations per day go from thousands to millions.^[A previous guide to microstructure programming (in SAS) is provided by Boehmer, Broussard and Kallunki (2002): Boehmer, E., Broussard, J. P., & Kallunki, J. P. (2002). *Using SAS in financial research*. SAS Publishing.]\n\nEven within the world of R, there are of course different packages available to pursue the same goal. We primarily rely on `data.table`, which is a very fast package for working with the high dimensional data that characterizes microstructure. The speed benefits show when reading and sorting data, and really shine when applying functions to several groups.^[Part of the `data.table` speed advantage is that it uses parallel processing by default. Consequently, one has to be cautious to use it in parallelization, as improper specification may lead to [over-parallelization](https://towardsdatascience.com/parallelization-caveats-in-r-1-the-basics-multiprocessing-and-multithreading-performance-eb584b7e850e) and undermine the performance benefits.] \n\nThe basic `data.table` syntax takes the form `DT[i, j, by]`, where `i` and `j` can be used to subset or reorder rows and columns much in the same way as in a `data.frame`. In addition, `j` can be used to define new variables either for the full data set or group-wise as specified in the `by` argument. We will give examples and introduce more `data.table` syntax as we go along. A more complete intro to the package can be found [here](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).\n\nFor readers following the Tidy Finance project of [Scheuch et al. (2023)](https://tidy-finance.org/)[^7], the `data.table` syntax can become a friction. For this reason, this guide comes in two flavors. As an alternative to `data.table`, we also show how to achieve essentially the same virtues in speed, but using the `tidyverse` syntax. The latter is based on `dtplyr`, which provides a `data.table` backend for the familiar `dplyr` package. \n\n**Data.**\nWe use one week of quote and trade data for one stock (PRU, [*Prudential Financial Inc.*](https://en.wikipedia.org/wiki/Prudential_plc)). There is nothing special about that stock -- it is chosen to be representative of large-cap stocks in general. As is typical these days, PRU is traded at several exchanges as well as off-exchange. In addition to the listing venue (London Stock Exchange, LSE), the data includes trades and quotes from the competing venue Turquoise (TQE), as well as some dark pool trades. \n\nThe data is loaded automatically in the scripts provided. It can also be downloaded manually [here](http://tinyurl.com/tidymicrostructure).\n\nThe data are extracted from the *Tick History* database, available from the London Stock Exchange Group (LSEG). We are grateful to LSEG for giving us permission to post the example data in the public domain. **It is to be used for educational purposes only.** The data set is incomplete in that it excludes quotes and trades from numerous venues where the same stock is traded. For the illustrative purpose here, however, two exchanges suffice.\n\n# Quote-based liquidity\nThe most well-known liquidity measure is probably the quoted bid-ask spread. It measures the cost of a hypothetical roundtrip trade, where an investor buys one share only to immediately sell it again. Even though such a trade is not economically sensible, the quoted spread offers a liquidity snapshot that is accessible at any time when the market is open. All that is required is the best bid and ask prices, , the highest bid price and the lowest ask price. Such data typically also come with the number of shares that is available at each price, allowing for a similar snapshot of market depth. That is, the maximum size that can be traded at the best price. \n\nQuotes are expressions of interests to buy or sell securities, observable before a trading decision is made. The quotes that we access in the examples below come from limit order book (LOB) markets, but they may just as well be posted by dealers in request for quote systems, or shouted by specialists in a trading pit. \n\n## Quote data inspection and preparation\nLet's dive into it. After loading the required packages, the following code shows how to import and preview the data. We get an overview of the data by simply typing the name of the data frame in the console. It automatically abbreviates the content to show only a subset of the data. \n\nThe data can be downloaded directly from within `R`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes_url <- \"http://tinyurl.com/pruquotes\"\n```\n:::\n\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\nTo load the data we use the function 'fread' which is similar to `read.csv`, but much faster.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the `data.table` package\n# install.packages(\"data.table\")\nlibrary(data.table)\n\n# Load the view the quote data\nquotes <- fread(quotes_url)\n```\n:::\n\n\nThe raw data looks as follows.\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           #RIC       Domain           Date-Time GMT Offset  Type\n     1:   PRU.L Market Price 2021-06-07 04:00:03          1 Quote\n     2:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote\n     3:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote\n     4:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote\n     5:   PRU.L Market Price 2021-06-07 06:50:00          1 Quote\n    ---                                                          \n263969: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote\n263970: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote\n263971: PRUl.TQ Market Price 2021-06-11 15:29:59          1 Quote\n263972: PRUl.TQ Market Price 2021-06-11 15:30:00          1 Quote\n263973: PRUl.TQ Market Price 2021-06-11 15:30:00          1 Quote\n        Bid Price Bid Size Ask Price Ask Size          Exch Time\n     1:      1450      300      1550      700 04:00:02.983920000\n     2:      1450     1028      1550      700 06:50:00.016246000\n     3:      1450     1028      1510     1000 06:50:00.251314000\n     4:      1450     1028      1504     1000 06:50:00.402760000\n     5:      1450     1028      1504     1009 06:50:00.547739000\n    ---                                                         \n263969:      1488      476      1496      472 15:29:59.623000000\n263970:      1488      476      1600      425 15:29:59.798000000\n263971:        NA       NA      1600      425 15:29:59.833000000\n263972:        NA       NA      1600      325 15:30:00.104000000\n263973:        NA       NA        NA       NA 15:30:00.104000000\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"data.table\")\n# install.packages(\"dtplyr\")\n# install.packages(\"tidyverse\")\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(tidyverse)\n```\n:::\n\n\nWe read the data as a ´tibble´ using the (very fast) function ´vroom´. The call ´lazy_dt´ converts the tibble to a lazy ´data.table´ object. Lazy means that all following commands are not executed immediately but translated to ´data.table´ syntax first and then executed at a final stage. As a result, there should be almost no difference in execution time of the code in ´tidyverse´ syntax versus the ´data.table´ implementation. \n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- vroom::vroom(quotes_url, \n                       col_types = list(`Exch Time` = col_character()))\n\ntv_quotes <- lazy_dt(tv_quotes)\n```\n:::\n\n\nThe raw data looks as follows.\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [263,973 x 10]\nCall:   `_DT1`\n\n  `#RIC` Domain    `Date-Time`         `GMT Offset` Type  `Bid Price`\n  <chr>  <chr>     <dttm>                     <dbl> <chr>       <dbl>\n1 PRU.L  Market P… 2021-06-07 04:00:03            1 Quote        1450\n2 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450\n3 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450\n4 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450\n5 PRU.L  Market P… 2021-06-07 06:50:00            1 Quote        1450\n6 PRU.L  Market P… 2021-06-07 06:50:05            1 Quote        1450\n# ℹ 263,967 more rows\n# ℹ 4 more variables: `Bid Size` <dbl>, `Ask Price` <dbl>,\n#   `Ask Size` <dbl>, `Exch Time` <chr>\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\nThe data contain four variables that describe the state of the order book (`Bid Price`, `Bid Size`, `Ask Price`, and `Ask Size`). The LOB holds orders at numerous different prices, but we only see the best price level on each side. This is sufficient for most quote-based market quality measures. \nWe also get three variables conveying time stamps and time zone information (`Date-Time`, `GMT Offset`, and `Exch time`; discussed in detail below), and three categorical variables (`#RIC`, `Domain`, and `Type`). \n\nBefore processing the data, we rename the variables. Names containing spaces, hashes, and dashes may make sense for data vendors, but they are impractical to work with in R. Furthermore, for the order book variables, we prefer to use the term *depth* to refer to the number of shares quoted, reserving the term *size* to the number of shares changing hands in a trade. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\nThe command ´setnames´ replaces the existing variables with our desired column names. \n\n::: {.cell}\n\n```{.r .cell-code}\nraw_quote_variables <- c(\"#RIC\", \"Date-Time\", \"GMT Offset\", \"Domain\", \"Exch Time\",\n                         \"Type\", \"Bid Price\", \"Bid Size\", \"Ask Price\", \"Ask Size\")\n\nnew_quote_variables <- c(\"ticker\", \"date_time\", \"gmt_offset\", \"domain\", \"exchange_time\", \n                         \"type\", \"bid_price\", \"bid_depth\", \"ask_price\", \"ask_depth\")\n\nsetnames(quotes, raw_quote_variables, new_quote_variables)\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes|> \n  rename(ticker = `#RIC`,\n            domain = Domain,\n            date_time = `Date-Time`,\n            gmt_offset = `GMT Offset`,\n            type = Type,\n            bid_price = `Bid Price`, \n            bid_depth = `Bid Size`, \n            ask_price = `Ask Price`,\n            ask_depth = `Ask Size`,\n            exchange_time = `Exch Time`\n  ) |>\n  lazy_dt()\n```\n:::\n\n:::\n\nFrom the output above, the categorical variables look like constants, with the same value in every line. If so, they occupy way more memory than necessary. The `table` function is great to gauge the variation in categorical variables. In this case, it shows us that there is variation in the `ticker` variable. The two tickers are for the same stock, `PRU`, traded at two different exchange, LSE and TQE. The former is more active, with 165,261 quote updates. \n\nThe other two categorical variables, `domain` and `type`, are indeed constants. We delete them to save memory.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Output a table of sample tickers and values of `domain` and `type`\ntable(quotes$ticker, quotes$type, quotes$domain)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n, ,  = Market Price\n\n         \n           Quote\n  PRU.L   165261\n  PRUl.TQ  98712\n```\n\n\n:::\n\n```{.r .cell-code}\n# Delete variables\nquotes[, c(\"domain\", \"type\") := NULL]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes |> \n  count(ticker, type, domain)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [2 x 4]\nCall:   `_DT2`[, .(n = .N), keyby = .(ticker, type, domain)]\n\n  ticker  type  domain            n\n  <chr>   <chr> <chr>         <int>\n1 PRU.L   Quote Market Price 165261\n2 PRUl.TQ Quote Market Price  98712\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |>\n  select(-type, -domain)\n```\n:::\n\nWe can see that `type` and `Domain` only have unique values (`Quote` and `Market Price`). Going forward, we delete these variables.\n:::\n\n**Dates.**\nIn the raw data, dates and times are embedded in the same variable, `date_time`, but for us it is useful to have them in separate variables. Accordingly, we now define the variable `date`.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\nNote here that the operator `:=` is used to define a new variable (`date`) within an existing `data.table`, such as `quotes` in this example. Within a `data.table`, it suffices to refer to the variable name, `date_time`, when defining the new variable. This is different to a `data.frame`, for which we would have to write `quotes$date_time`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain dates\nquotes[, date := as.Date(date_time)]\n\n# Output a table of sample dates and tickers\ntable(quotes$ticker, quotes$date)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         \n          2021-06-07 2021-06-08 2021-06-09 2021-06-10 2021-06-11\n  PRU.L        27415      38836      30962      39639      28409\n  PRUl.TQ      16424      26040      18207      21170      16871\n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |> \n  mutate(date = as.Date(date_time)) |>\n  lazy_dt()\n\ntv_quotes |> \n  count(ticker, date) |> \n  pivot_wider(names_from = ticker, \n              values_from = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [5 x 3]\nCall:   dcast(`_DT3`[, .(n = .N), keyby = .(ticker, date)], formula = date ~ \n    ticker, value.var = \"n\")\n\n  date       PRU.L PRUl.TQ\n  <date>     <int>   <int>\n1 2021-06-07 27415   16424\n2 2021-06-08 38836   26040\n3 2021-06-09 30962   18207\n4 2021-06-10 39639   21170\n5 2021-06-11 28409   16871\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\nThe table output shows that there are five trading days in the sample. The number of quote observations per stock-date varies between roughly 16,000 and 40,000. The tendency that LSE has more quotes than TQE is consistent across trading days.\n\n**Timestamps.**\nThe accuracy of timestamps is important in microstructure data. Timestamps are often matched between quote and trade data (that are not necessarily generated in the same systems), or between data from exchanges in different locations. It is thus essential to be aware of latencies that may arise due to geography or hardware, for example. \n\nWe have two timestamps for each observation. The `exchange_time` variable is assigned by the exchange at the time an event is recorded in the exchange matching engine. The `date_time` variable is the timestamp assigned on receipt at the data vendor, which is by definition later than the `exchange_time`. Exchanges that are located at different distances from the vendor are likely to have different reporting delays. It is then up to the researcher to determine which timestamp to rely on, and the choice may depend on the research question. In our setting, as we measure liquidity across venues, it is important that the time stamps across venues are comparable. Based on that each exchanges has strong incentives to assign accurate time stamps (to cater for low-latency participants), we choose to work with the `exchange_time` variable.  \n\nFor US equity markets, the timestamp may reflect the matching engine time, the time when the national best bid and offer updates, or the participant timestamp. For discussions about which of these to use, see [Bartlett & McCrary (2019)](https://doi.org/10.1016/j.finmar.2019.06.003)^[Bartlett, R. P., & McCrary, J. (2019). How rigged are stock markets? Evidence from microsecond timestamps. *Journal of Financial Markets*, *45*, 37-60.], [Holden, Pierson & Wu (2023)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441422)^[Holden, C. W., Pierson, M., & Wu, J. (2023). In the blink of an eye: Exchange-to-SIP latency and trade classification accuracy. *Working paper*.], and [Schwenk-Nebbe (2021)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3984827)^[Schwenk-Nebbe, S. (2022). The participant timestamp: Get the most out of TAQ data. *Working paper*.].\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\nWhen working with timestamps in microstructure applications, it is useful to convert them into a numeric format. Dedicated time formats (e.g., `xts`) are imprecise when it comes to sub-second units (see [here](https://stackoverflow.com/questions/62366490/timestamp-r-sequence-milliseconds) and [here](https://lubridate.tidyverse.org/reference/round_date.html)). We thus convert the timestamps to the number of seconds elapsed since midnight. For example, 8:30 am becomes 8.5 x 3,600 = 30,600, because there are 3,600 seconds per hour.\n\nThe code below converts `exchange_time` to numeric and adjusts it for daylight saving using the `gmt_offset` variable (which is measured in hours). Note the use of curly brackets `{...}` in the definition of the time variable, which allows us to temporarily define the variable `time_elements` within the call. Once the operation is complete, the temporary variable is automatically deleted. The variable that is retained should always be returned as a list, hence `list(time)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert time stamps to numeric format, expressed in seconds past midnight\n# The function `strsplit` splits a character string at the point defined by `split`\n# `do.call` is a way to call a function, which in this case calls `rbind` to convert a \n# list of vectors to a matrix, where each vector forms one row\nquotes[, time := {\n\ttime_elements = strsplit(exchange_time, split = \":\")\n\ttime_elements = do.call(rbind, time_elements)\n\ttime = as.numeric(time_elements[, 1]) * 3600 + \n\t\t   as.numeric(time_elements[, 2]) * 60 + \n\t\t   as.numeric(time_elements[, 3]) +\n\t\t   gmt_offset * 3600\n\tlist(time)}]\n```\n:::\n\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables. \n\n::: {.cell}\n\n```{.r .cell-code}\nquotes[, c(\"date_time\", \"exchange_time\", \"gmt_offset\") := NULL]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |> \n  separate(exchange_time, \n           into = c(\"hour\", \"minute\", \"second\"), \n           sep=\":\", \n           convert = TRUE) |> \n  mutate(time = hour * 3600 + minute * 60 + second + gmt_offset * 3600)\n```\n:::\n\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes|> \n  select(-c(\"date_time\", \"gmt_offset\",\"hour\",\"minute\",\"second\"))\n```\n:::\n\n:::\n\n**Prices and depths.**\nAn important feature of LOB quotes is that they remain valid until cancelled, executed or modified. Whenever there is a change to the prevailing quotes, a new quote observation is added to the data. It is irrelevant if the latest quote is from the previous millisecond or from the previous minute -- it remains valid until updated. It is thus economically meaningful to forward-fill quotes that prevailed in the previous period. Trades, in contrast, are agreed upon at a fixed point in time and do not convey any information about future prices or sizes. They should not be forward-filled, see [Hagströmer and Menkveld (2023)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4356262)^[Hagströmer, B., & Menkveld, A. J. (2023). Trades, quotes, and information shares. Working paper.].\n\nWhen forward-filling quote data, it is important to restrict the procedure to the same date, stock and trading venue. For example, quotes should never be forward-filled from one day to the next, and not from one venue to another. This is ensured with the `by` argument (in ´data.table´) or the ´groupby´ argument (in ´tidyverse´), which specifies that the operation is to be done *within* each combination of tickers and dates. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\nWe use the `nafill` function to forward-fill, with the option `type = \"locf\"` (last observation carried forward) specifying the type of filling. The `.SD` inside the `lapply` command tells `data.table` to repeat the same operation for the set of variables specified by the option `.SDcols`. \n\nIn summary, whereas the `.SD` applies the same function across a set of variables (columns), the `by` applies it across categories of observations (rows). The same outcome could be achieved with `for` loops, but in R, that would be much slower. We discuss that further below. \n\nNote here how the `:=` notation can be used to define multiple variables, using a vector of variable names on the left-hand-side and a function (in this case `lapply`) that returns a list of variables on the right-hand-side. Note also that when referring to multiple variable names within the `data.table`, they are specified as a character vector. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Forward-fill quoted prices and depths\nlob_variables <- c(\"bid_price\", \"bid_depth\", \"ask_price\", \"ask_depth\")\n\nquotes[, \n  (lob_variables) := lapply(.SD, nafill, type = \"locf\"),\t.SDcols = (lob_variables), \n  by = c(\"ticker\", \"date\")]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |>\n  group_by(ticker, date) |>\n  fill(matches(\"bid|ask\")) |> \n  ungroup()\n```\n:::\n\n:::\n\nWhen measuring market quality in continuous trading, it is common to filter out periods that may be influenced by call auctions. The LSE opens for trading with a call auction at 08:00 am, local time, and closes with another call at 4:30 pm. There is also an intraday call auction at noon, 12:00 pm. To avoid the impact of the auctions, we exclude quotes before 8:01 am and after 4:29 pm. We do *not* exclude quotes recorded around the intraday call auction, but set them as missing (`NA`).  If they were instead deleted, it would give the false impression that the last observation before the excluded quotes was still valid.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_time <- 8 * 3600\nclose_time <- 16.5 * 3600\nintraday_auction_time <- 12 * 3600\n```\n:::\n\n\nFirst, we exclude quotes around the opening and closing of continuous trading.\nNext, we set quotes around the intraday auction to missing. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes <- quotes[time > (open_time + 60) & time < (close_time - 60)]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes[time > (intraday_auction_time - 60) & time < (intraday_auction_time + 3 * 60), \n  (lob_variables)] <-  NA\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |>\n  filter(time > hms::as_hms(\"08:01:00\"), \n         time < hms::as_hms(\"16:29:00\"))\n\ntv_quotes <- tv_quotes |>\n  mutate(across(matches(\"bid|ask\"), \n                ~if_else(time > hms::as_hms(\"11:59:00\") & \n                         time < hms::as_hms(\"12:03:00\"), NA_real_, .))) |>\n  lazy_dt()\n```\n:::\n\n::: \n\n**Screening.**\nBefore turning to the market quality measurement, it is a good habit to check that the quote observations make economic sense. One way to do that is to study the variation in the bid-ask spread. The *nominal* bid-ask spread is defined as the difference between the ask price, $P^A$, and the bid price, $P^B$, $quoted\\_spread^{nom}= P^A - P^B$. A histogram offers a quick overview of the variation (a line plot of the prices is also useful, see Section 2.1).\n\nIn the output below, note that the x-axis is in units of pence (0.01 British Pounds, GBP). All quoted prices in this example data follow that convention. Note also that the bid-ask spread is strictly positive, as it should be whenever the market is open. The TQE occasionally has wider spreads than the LSE, but there are no extraordinarily large spreads. The maximum spread, GBP 0.11, corresponds to around 0.7% of the stock price. \n\nAlso, it is clear from the histogram that the tick size, the minimum price increment that is allowed when quoting prices, is 0.5 pence (that is, GBP 0.005). Most spreads are quoted at one or two ticks.\n\nWe use the package ´ggplot2´ to plot an histogram of the nominal quoted bid-ask spreads.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(quotes, \n       aes(x = ask_price - bid_price, fill = ticker)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Histogram of nominal bid-ask spread\",\n       x = \"Nominal bid-ask spread (pence)\") +\n  scale_x_continuous(breaks=1:12)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1576 rows containing non-finite values\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=2100}\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes |> \n  as_tibble() |>\n  ggplot(aes(x = ask_price - bid_price, fill = ticker)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Histogram of nominal bid-ask spread\",\n       x = \"Nominal bid-ask spread (pence)\") +\n  scale_x_continuous(breaks=1:12)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1576 rows containing non-finite values\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=2100}\n:::\n:::\n\n:::\n\nR produces a warning when plotting the nominal bid-ask spread. It mentions 1,576 rows containing \"non-finite values\". The non-finite values refer either `NA`, `Inf` (inifinite) or `-Inf` (negative inifinite). In the timestamp section, we imposed `NA` for LOB variables during midday auction. To see if those are the cause of the warning, let's create a histogram of the time stamps of the missing values. \n\nIndeed, all missing values are around ~43,150 and ~43,350 seconds of the trading day which is the time of the midday auction (noon is $12\\times3,600=43,200$ seconds past midnight). Accounting for missing spreads by plotting the histogram without `NA` removes the warning. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot a histogram of missing quoted spreads\nggplot(quotes[is.na(ask_price - bid_price)], \n       aes(x = time, fill = ticker)) +\n  geom_histogram(bins = 100) + \n  labs(title = \"Histogram of missing spreads\",\n       x = \"Time of Day (seconds past midnight)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=2100}\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes |> \n         filter(is.na(ask_price) | is.na(bid_price)) |>\n         mutate(time = hms::hms(time)) |>\n         as_tibble() |>\nggplot(aes(x = time, fill = ticker)) +\n  geom_histogram(bins = 100) + \n  labs(title = \"Histogram of missing spreads\",\n       x = \"Time of Day\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=2100}\n:::\n:::\n\n:::\n\n## Liquidity measures\n\nWith all the data preparation done, we are ready for the actual liquidity measurement. For comparisons across stocks, it is useful to relate the nominal spread to the fundamental value of the security. This is done by the *relative* quoted bid-ask spread, defined as $quoted\\_spread^{rel} = (P^A - P^B)/M$, where $M$ is the midpoint (also known as the midprice; defined as the average of the best bid and the best ask prices). One can argue that the midpoint is not always representative of the fundamental value, but it has the strong advantage that it is continuously available in the quote data. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fundamental value\nquotes[, midpoint := (bid_price + ask_price) / 2]\n```\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |> \n  mutate(midpoint = (bid_price + ask_price) / 2)\n```\n:::\n\n:::\n\nThe quoted spread can also be measured relative to the tick size. In an open market, the spread can never be below one tick. A tick refers to the tick size of a security. It is the minimum price increment a security can be quoted and traded. The tick size in the example data is half a cent at both exchanges. We refer to the average number of ticks in the bid-ask spread as the *tick* spread, $quoted\\_spread^{tic} = (P^A - P^B) / tick\\_size$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntick_size <- 0.5\n```\n:::\n\n\nAnother dimension of quoted liquidity is the market depth. We measure the average depth quoted at the best bid and ask prices. It is defined as $quoted\\_depth = (Q^A + Q^B)/2$, where $Q^A$ and $Q^B$ are the depths available at the bid and ask prices. \n\nIn the code below, we store the liquidity measures in a new `data.table` named `quotes_liquidity`. This is because the new variables are averages, observed on a ticker-date frequency, as opposed to the tick-by-tick frequency of the `quotes` object. We multiply the quoted spread by 10,000 to express it in basis points, and divide the quoted depth by 100,000 to express it in thousand GBP.\n\nThe output shows that the liquidity is higher at the LSE than at the TQE. Both in nominal and relative terms, the spreads are somewhat tighter at the LSE, and there is more than three times more depth posted at the LSE.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure the average quote-based liquidity\n# This step calculates the mean of each of the market quality variables, for each \n# ticker-day (as indicated in `by = c(\"ticker\", \"date\")`)\n\nquotes_liquidity <- quotes[, {\n\tquoted_spread = ask_price - bid_price\n\tlist(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),\n\t     quoted_spread_relative = mean(quoted_spread / midpoint, na.rm = TRUE),\n\t     quoted_spread_tick = mean(quoted_spread / tick_size, na.rm = TRUE),\n\t     quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, \n\t                         na.rm = TRUE) / 2)},\n\tby = c(\"ticker\", \"date\")]\n\n# Output the liquidity measures, averaged across the five trading days for each ticker. \nquotes_liquidity[, \n\tlist(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n\t     quoted_spread_relative = round(mean(quoted_spread_relative) * 1e4, digits = 2),\n\t     quoted_spread_tick = round(mean(quoted_spread_tick), digits = 2),\n\t     quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), \n\tby = \"ticker\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ticker quoted_spread_nom quoted_spread_relative\n1:   PRU.L              0.83                   5.65\n2: PRUl.TQ              1.02                   6.94\n   quoted_spread_tick quoted_depth\n1:               1.67        24.72\n2:               2.05         6.65\n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_liquidity <- tv_quotes |>\n  mutate(quoted_spread = ask_price - bid_price) |>\n  group_by(ticker, date) |> \n  summarise(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),\n            quoted_spread_relative = mean(quoted_spread / midpoint, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = mean(quoted_spread / tick_size, na.rm = TRUE),\n            quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, \n                                        na.rm = TRUE) / 2 * 1e-5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'ticker'. You can override using\nthe `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_quotes_liquidity |> \n  group_by(ticker) |>\n  summarise(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |>\n  pivot_longer(-ticker) |>\n  as_tibble() |>\n  pivot_wider(names_from = name, values_from = value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  ticker  quoted_spread_nom quoted_spread_relative quoted_spread_tick\n  <chr>               <dbl>                  <dbl>              <dbl>\n1 PRU.L                0.83                   5.65               1.67\n2 PRUl.TQ              1.02                   6.94               2.05\n# ℹ 1 more variable: quoted_depth <dbl>\n```\n\n\n:::\n:::\n\n:::\n\n**Duration-weighted liquidity.**\nThe output above are straight averages, implying an assumption that all quote observations are equally important. But whereas some quotes remain valid for several minutes, many don't last longer than a split-second. For this reason, it is common to either sample the quote data in fixed time intervals (such as at the end of each second), or to weight the observations by their duration. The duration is the time that a quote observation is in force. That is, the time elapsed until the next quote update arrives. We show the duration-weighted approach in the code below (for guidance on how to get the quotes at the end of each second, see Section 3.1).\n\nNote that the `duration` variable is obtained separately for each ticker and date. Even if we are interested in the average liquidity across dates, it is important to partition by each ticker and date to avoid that duration is calculated overnight (resulting in a huge weight with negative sign, because it will be roughly the opening time minus the closing time). Except for replacing the `mean` function with the `weighted.mean`, the code below is very similar to that above.\n\nIn the output, we note that the differences between the duration-weighted and the equal-weighted liquidity averages are small. Nevertheless, we consider the duration-weighted average more appropriate because it is not sensitive to shortlived price and depth fluctuations.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate quote durations\nquotes[, duration := c(diff(time), 0), by = c(\"ticker\", \"date\")]\n\n# Measure the duration-weighted average quote-based liquidity\n# The specified subset excludes quotes for which no duration can be calculated\nquotes_liquidity_dw <- quotes[!is.na(duration), {\n\tquoted_spread = ask_price - bid_price\n\tlist(quoted_spread_nom = weighted.mean(quoted_spread, \n\t                                       w = duration, na.rm = TRUE),\n\t     quoted_spread_rel = weighted.mean(quoted_spread / midpoint, \n\t                                       w = duration, na.rm = TRUE),\n\t     quoted_spread_tic = weighted.mean(quoted_spread / tick_size, \n\t                                       w = duration, na.rm = TRUE),\n\t     quoted_depth = weighted.mean(bid_depth * bid_price + ask_depth * ask_price, \n\t                                  w = duration, na.rm = TRUE) / 2)},\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker \nquotes_liquidity_dw[, \n\tlist(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n\t\t quoted_spread_rel = round(mean(quoted_spread_rel) * 1e4, digits = 2),\n\t\t quoted_spread_tic = round(mean(quoted_spread_tic), digits = 2),\n\t\t quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), \n\tby = \"ticker\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ticker quoted_spread_nom quoted_spread_rel quoted_spread_tic\n1:   PRU.L              0.84              5.70              1.68\n2: PRUl.TQ              0.99              6.69              1.97\n   quoted_depth\n1:        25.65\n2:         7.06\n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |>\n  group_by(ticker, date) |>\n  mutate(duration = c(diff(time), 0)) |>\n  ungroup()\n\ntv_quotes_liquidity <- tv_quotes |>\n  mutate(quoted_spread = ask_price - bid_price) |>\n  group_by(ticker, date) |>\n  summarise(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, na.rm = TRUE),\n            quoted_spread_relative = weighted.mean(quoted_spread / midpoint, w = duration, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = weighted.mean(quoted_spread / tick_size, w = duration, na.rm = TRUE),\n            quoted_depth = weighted.mean(bid_depth * bid_price + ask_depth * ask_price, w = duration, na.rm = TRUE) / 2 * 1e-5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'ticker'. You can override using\nthe `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_quotes_liquidity |> \n  group_by(ticker) |>\n  summarise(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) |>\n  pivot_longer(-ticker) |>\n  as_tibble() |>\n  pivot_wider(names_from = name, values_from = value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  ticker  quoted_spread_nom quoted_spread_relative quoted_spread_tick\n  <chr>               <dbl>                  <dbl>              <dbl>\n1 PRU.L                0.84                   5.7                1.68\n2 PRUl.TQ              0.99                   6.69               1.97\n# ℹ 1 more variable: quoted_depth <dbl>\n```\n\n\n:::\n:::\n\n:::\n\n## Consolidated liquidity in fragmented markets\nWith the competition between exchanges, liquidity is dispersed across venues. For example, if there is a change to the market structure at the LSE, it is typically not sufficient to analyze liquidity at LSE alone. If liquidity is reduced at the LSE, it may simultaneously be boosted at the TQE. To assess the overall market quality, which may be most relevant for welfare, it is often necessary to consider the *consolidated* liquidity. \n\nIn Europe, the consolidated liquidity is sometimes referred to as the European Best Bid and Offer (EBBO). The terminology follows in the footsteps of the US market, where the *National* Best Bid and Offer (NBBO) is transmitted to the market on continuous basis. To obtain the EBBO, one needs to merge the LOB data from each relevant venue, and then determine the EBBO prices and depths. In the code below, we show step-by-step how to do that.\n\n**Retaining only the last quote update in each interval.**\nQuote updates tend to cluster and it is common that several observations have identical timestamps. Multiple observations at one timestamp can be due to several investors responding to the same events, or that one market order leads to several LOB updates as it is executed against multiple limit orders. When matching quotes across venues, we need to restrict the number of observations per unit of time to one. There is no sensible way to distinguish observations with identical timestamps. In lack of a better approach, we retain the last observation in each interval. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Retain only the last observation per unit of time\n# The function `duplicated` returns `TRUE` if the observation is a duplicate of another \n# observation based on the columns given in the `by` option, and `FALSE` otherwise.\n# The option `fromLast = TRUE` ensures that the last rather than the first observation \n# in each millisecond that returns `FALSE`.\nquotes <- quotes[!duplicated(quotes, fromLast = TRUE, by = c(\"ticker\", \"date\", \"time\"))]\n```\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes <- tv_quotes |> \n  group_by(ticker, date, time) |>\n  slice(n()) |> \n  ungroup() |>\n  lazy_dt()\n```\n:::\n\n:::\n\n**Merging quotes from different venues.**\nWe are now ready to match the quotes from the two exchanges. First, we create separate quote data sets for the two exchanges. Second, we merge the two by matching on date and time. Third, we forward-fill quotes from both venues, such that for each LSE quote we know the prevailing TQE quote, and vice versa. The validity of this is ensured by the option `sort = TRUE` in the `merge` function, which returns a data.table that is sorted on the matching variables.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Merge quotes from two venues trading the same security\n# In the `merge` function, we add exchange suffixes to the variable names to keep track of \n# which quote comes from which exchange, using the option `suffixes`. \n# The option `all = TRUE` specifies that unmatched observations from both sets of quotes \n# should be retained (known as an outer join). \nvenues <- c(\"_lse\", \"_tqe\")\n\nquotes_lse <- quotes[ticker == \"PRU.L\", .SD, .SDcols = c(\"date\", \"time\", lob_variables)]\nquotes_tqe <- quotes[ticker == \"PRUl.TQ\", .SD, .SDcols = c(\"date\", \"time\", lob_variables)]\n\nquotes_ebbo <- merge(quotes_lse, quotes_tqe, \n                     by = c(\"date\", \"time\"), \n                     suffixes = venues, \n                     all = TRUE, sort = TRUE)\n```\n:::\n\n\nNext, we forward-fill the quoted prices and depth for each exchange. \n\n::: {.cell}\n\n```{.r .cell-code}\nlocal_lob_variables <- paste0(lob_variables, rep(venues, each = 4))\n\nquotes_ebbo[, (local_lob_variables) := lapply(.SD, nafill, type = \"locf\"), \n  .SDcols = (local_lob_variables),\n\tby = \"date\"]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes |> \n  select(-midpoint, -duration) |>\n  mutate(ticker = case_when(ticker == \"PRUl.TQ\" ~ \"tqe\",\n                            ticker == \"PRU.L\" ~ \"lse\")) |>\n  pivot_wider(names_from = ticker, \n              values_from = matches(\"bid|ask\")) |>\n  arrange(date, time)\n\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  group_by(date) |>\n  fill(matches(\"bid|ask\")) |> \n  ungroup()\n```\n:::\n\n:::\n\nThe best bid price at each point in time is the *maximum* of the best bid at the LSE and the best bid at the TQE. Similarly, the best ask is the *minimum* of the best ask prices at the two venues. We calculate the best bid using the parallel maxima function, `pmax`, which returns the highest value in each row. The best ask is obtained in the same way, using the parallel minima function, `pmin`. \n\nNote that it would also be possible to obtain the EBBO using a `for` loop, checking row-wise which is the highest bid and lowest ask. When working with large data sets in R, however, loops become extremely slow. It is strongly encouraged to run vectorised operations for the whole column at once (like we do here), or to apply functions repeatedly to blocks of data (like we have done several times above).\n\nWe obtain the depth at the best prices by summing the depth of the individual venues. When doing this, we should only consider both venues at times when they are both at the best price. When the two venues have the same best bid, for example, we calculate the consolidated bid depth as the sum of the two. To code this, we use the feature that a logical variable (with values `FALSE` or `TRUE`; such as `bid_price_lse  == best_bid_price`) works as a binary variable (with values `0` or `1`) when used in multiplication.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain the EBBO prices and depths\nquotes_ebbo[, best_bid_price := pmax(bid_price_lse, bid_price_tqe, na.rm = TRUE)]\nquotes_ebbo[, best_ask_price := pmin(ask_price_lse, ask_price_tqe, na.rm = TRUE)]\nquotes_ebbo[, best_bid_depth := bid_depth_lse * (bid_price_lse == best_bid_price) + \n              bid_depth_tqe * (bid_price_tqe == best_bid_price)]\nquotes_ebbo[, best_ask_depth := ask_depth_lse * (ask_price_lse == best_ask_price) +\n              ask_depth_tqe * (ask_price_tqe == best_ask_price)]\n```\n:::\n\n\nFinally, we drop local exchange variables and objects\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes_ebbo[, (local_lob_variables) := NULL]\nrm(quotes_lse, quotes_tqe, quotes, local_lob_variables)\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  mutate(best_bid_price = pmax(bid_price_lse, bid_price_tqe, na.rm = TRUE),\n         best_ask_price = pmin(ask_price_lse, ask_price_tqe, na.rm = TRUE),\n         best_bid_depth = bid_depth_lse * (bid_price_lse == best_bid_price) + \n           bid_depth_tqe * (bid_price_tqe == best_bid_price),\n         best_ask_depth = ask_depth_lse * (ask_price_lse == best_ask_price) + \n           ask_depth_tqe * (ask_price_tqe == best_ask_price)\n  )\n```\n:::\n\n\nFinally, we drop local exchange variables and objects\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  select(date, time, contains(\"best\")) |>\n  lazy_dt()\n```\n:::\n\n:::\n\n**Fundamental value.**\nWe can now obtain EBBO midpoints, as a proxy of fundamental value that factors in liquidity posted at multiple exchanges.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate EBBO midpoints\nquotes_ebbo[, midpoint := (best_bid_price + best_ask_price) / 2]\n```\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |> \n  mutate(midpoint = (best_bid_price + best_ask_price) / 2)\n```\n:::\n\n:::\n\n**Screening.**\nAs above, we check that the EBBO quotes are economically meaningful by tabulating the counts of nominal spread levels. This exercise shows us that the consolidated spread is not strictly positive. There are numerous cases of zeroes, known as *locked* quotes, and also many negatives, referred to *crossed* quotes. This is possible because orders at the LSE and the TQE are never executed against each other -- it takes arbitrageurs to step in and act on crossed markets. Locked and crossed spreads are not uncommon in consolidated data. For an analysis of the incidence in US markets, see [Shkilko, van Ness & van Ness, 2008](https://www.sciencedirect.com/science/article/pii/S1386418107000031?casa_token=GIqJQiPKyisAAAAA:ZVbaf4SPC0IzxFFLWqgI2papSw2MrEf_lPXCL9OlT_ZnKgA6-pGTl4EYisKuIUaFdx8JY1p3d7o)^[Shkilko, A. V., Van Ness, B. F., & Van Ness, R. A. (2008). Locked and crossed markets on NASDAQ and the NYSE. *Journal of Financial Markets*, *11*(3), 308-337.].\n\nIt is also notable from the table that the maximum consolidated spread is 2.5, as compared to the spreads of up to 11 recorded in the single-venue analysis. By definition, the EBBO quoted spread is never wider than at the single venues. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Output an overview of the EBBO nominal quoted bid-ask spread \ntable(quotes_ebbo$best_ask_price - quotes_ebbo$best_bid_price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    -1   -0.5      0    0.5      1    1.5      2    2.5    3.5 \n     2    127   6367 104351 103265   4668    637     51      1 \n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo |> \n  transmute(ebbo_nominal_spread = best_ask_price - best_bid_price) |>\n  as_tibble() |>\n  count(ebbo_nominal_spread)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 2\n  ebbo_nominal_spread      n\n                <dbl>  <int>\n1                -1        2\n2                -0.5    127\n3                 0     6367\n4                 0.5 104351\n5                 1   103265\n# ℹ 4 more rows\n```\n\n\n:::\n:::\n\n:::\n\nObservations with locked or crossed quotes are usually excluded when measuring market quality. It is also common to exclude bid-ask spread observations that are unrealistically high. We have no such cases in this sample, but, for illustration, we include a filter that would capture spreads that relative to the share price are wider than 5%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.05\n```\n:::\n\n\nIn the procedure below, we flag the problematic quotes, but we do not exclude them. If they were deleted, it would imply that the last observation before the excluded spread was still in force, which may mislead subsequent analysis. \n\nThe output shows that 2.90% of the quote observations are locked, while 0.06% are crossed. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Flag problematic consolidated quotes\n\nquotes_ebbo[, c(\"crossed\", \"locked\", \"large\") := {\n\tquoted_spread = (best_ask_price - best_bid_price)\n    list(quoted_spread < 0, quoted_spread == 0, quoted_spread / midpoint > threshold)}]\n\n# Count the incidence of the consolidated quote flags\nquotes_ebbo_filters  <- quotes_ebbo[, \n    list(crossed = mean(crossed, na.rm = TRUE),\n         locked = mean(locked, na.rm = TRUE),\n         large = mean(large, na.rm = TRUE))]\n\n# Output the fraction of quotes that is flagged\nquotes_ebbo_filters[, \n\tlapply(.SD * 100, round, digits = 2), .SDcols = c(\"crossed\", \"locked\", \"large\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   crossed locked large\n1:    0.06    2.9     0\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  mutate(quoted_spread = best_ask_price - best_bid_price,\n         crossed = quoted_spread < 0,\n         locked = quoted_spread == 0,\n         large = quoted_spread / midpoint > threshold) |>\n  lazy_dt()\n\ntv_quotes_ebbo |>\n  summarise(across(c(crossed, locked, large), \n                   ~round(100 * mean(.),2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [1 x 3]\nCall:   `_DT13`[, .(crossed = round(100 * mean(crossed), 2), locked = round(100 * \n    mean(locked), 2), large = round(100 * mean(large), 2))]\n\n  crossed locked large\n    <dbl>  <dbl> <dbl>\n1    0.06    2.9     0\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\n**Consolidated liquidity measures.**\nWe obtain duration-weighted measures of consolidated liquidity in the same way as above. The only difference here is that we subset the quotes to filter out crossed and locked markets. \n\nThe consolidated relative quoted bid-ask spread is 5.59 basis points, as compared to 5.70 and 6.69 basis points at LSE and TQE locally. The consolidated depth, 3.14 million GBP, is somewhat lower than the sum of the local depths seen above. This is to be expected, as some of the local depth is posted at price levels that are inferior to the EBBO.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure the duration-weighted consolidated quotes liquidity\n# Because this is the EBBO, there is no variation across tickers, but different averages \n# across dates are considered\nquotes_ebbo[, duration := c(diff(time), 0), by = \"date\"]\n\n# Note that the subset used here excludes crossed and locked quotes\nquotes_liquidity_ebbo_dw <- quotes_ebbo[!crossed & !locked & !large, {\n\tquoted_spread = best_ask_price - best_bid_price\n\t\n\tlist(quoted_spread_nom = weighted.mean(quoted_spread, \n\t                                       w = duration, na.rm = TRUE),\n\t     quoted_spread_relative = weighted.mean(quoted_spread / midpoint, \n\t                                       w = duration, na.rm = TRUE),\n\t     quoted_spread_tick = weighted.mean(quoted_spread / tick_size,\n\t                                       w = duration, na.rm = TRUE),\n\t     quoted_depth = weighted.mean(best_bid_depth * best_bid_price + \n\t                                  best_ask_depth * best_ask_price, \n\t                                  w = duration, na.rm = TRUE) / 2)}, \n\tby = \"date\"]\n\n# Output the liquidity measures, averaged across the five trading days \nquotes_liquidity_ebbo_dw[, \n\tlist(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),\n\t     quoted_spread_relative = round(mean(quoted_spread_relative * 1e4), digits = 2),\n\t     quoted_spread_tick = round(mean(quoted_spread_tick), digits = 2),\n\t     quoted_depth = round(mean(quoted_depth * 1e-6), digits = 2))]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   quoted_spread_nom quoted_spread_relative quoted_spread_tick\n1:              0.82                   5.59               1.65\n   quoted_depth\n1:         3.14\n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  group_by(date)|>\n  mutate(duration = c(diff(time), 0)) |>\n  ungroup()\n\ntv_quotes_liquidity_ebbo_dw <- tv_quotes_ebbo |>\n  group_by(date) |>\n  mutate(quoted_spread = best_ask_price - best_bid_price) |>\n  filter(!crossed, !locked, !large) |>\n  summarise(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, na.rm = TRUE),\n            quoted_spread_relative = weighted.mean(quoted_spread / midpoint, w = duration, na.rm = TRUE) * 1e4,\n            quoted_spread_tick = weighted.mean(quoted_spread / tick_size, w = duration, na.rm = TRUE),\n            quoted_depth = weighted.mean(best_bid_depth * best_bid_price + best_ask_depth * best_ask_price, w = duration, na.rm = TRUE) / 2 * 1e-5)\n\ntv_quotes_liquidity_ebbo_dw |>\n  summarise(across(contains(\"quoted\"), \n                   ~round(mean(.), digits = 2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [1 x 4]\nCall:\n  _DT15 <- copy(`_DT13`)[, `:=`(duration = c(diff(time), 0)), by = .(date)][,\n  _DT15 <-   `:=`(quoted_spread = best_ask_price - best_bid_price), by = .(date)]\n  `_DT15`[`_DT15`[, .I[!crossed & !locked & !large], by = .(date)]$V1, \n    .(quoted_spread_nom = weighted.mean(quoted_spread, w = duration, \n        na.rm = TRUE), quoted_spread_relative = weighted.mean(quoted_spread/midpoint, \n        w = duration, na.rm = TRUE) * 10000, quoted_spread_tick = weighted.mean(quoted_spread/..tick_size, \n        w = duration, na.rm = TRUE), quoted_depth = weighted.mean(best_bid_depth * \n        best_bid_price + best_ask_depth * best_ask_price, w = duration, \n        na.rm = TRUE)/2 * 1e-05), keyby = .(date)][, .(quoted_spread_nom = round(mean(quoted_spread_nom), \n    digits = 2), quoted_spread_relative = round(mean(quoted_spread_relative), \n    digits = 2), quoted_spread_tick = round(mean(quoted_spread_tick), \n    digits = 2), quoted_depth = round(mean(quoted_depth), digits = 2))]\n\n  quoted_spread_nom quoted_spread_relative quoted_spread_tick\n              <dbl>                  <dbl>              <dbl>\n1              0.82                   5.59               1.65\n# ℹ 1 more variable: quoted_depth <dbl>\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n## Data export\nAs we will reuse the consolidated quotes in the applications below, we save the `quotes_ebbo` object to disk. \n\n\n::: {.cell}\n\n:::\n\n\n::: {.panel-tabset group=\"language\"}\n## data.table\nWe export the consolidated quotes using ´fwrite´. \n\n::: {.cell}\n\n```{.r .cell-code}\nfwrite(quotes_ebbo, file = \"quotes_ebbo.csv\")\n```\n:::\n\n## tidyverse (using dtplyr)\nWe export the consolidated quotes using ´write_csv´. \n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(as_tibble(tv_quotes_ebbo), file = \"tv_quotes_ebbo.csv\")\n```\n:::\n\n:::\n\n# Trade-based liquidity\nA shortcoming of the quoted bid-ask spread is that it can only be measured for liquidity that is visible in the quote data. That is not always the full picture. For example, limit order books typically allow hidden liquidity, and dealers often offer price improvements on their quotes. Outside the exchanges, trades execute extensively at venues without quotes, such as dark pools. The effective bid-ask spread is a good alternative because it uses the trade price (the *effective* price) instead of the quoted price, and benchmarks it to the spread midpoint holding at the time of the trade. \n\nWhenever there is a trade, the price tends to move in the direction of the trade. This is known as *price impact*, and is an aspect of market depth. Whereas the quoted depth measure above captures depth in a mechanic sense (how much can you trade without changing the price), *price impact* also captures other traders' response to a trade. The response may be either that the LOB is refilled (if the trade is viewed as uninformative), or that liquidity is withdrawn (if the trade is viewed as a signal of more to come).\n\nFor the market makers, who makes a living from the bid-ask spread, price impact undermines the profits. A more relevant measure for them may be the *realized* spread, which accounts for price impact by evaluating the trade price relative to the midpoint some time later. The three trade-based liquidity measures are closely related: the realized spread equals the effective spread minus the price impact.\n\n## Trade data inspection and preparation\nWe load the trade data and view the data structure. The ticker, date, and time variables follow the same structure as in quotes data, so we can proceed with the same date and time transformations as above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrades_url <- \"http://tinyurl.com/prutrades\"\n```\n:::\n\n\nThere are three more variables: `Price`, `Volume`, and `MMT Classification`. As discussed above, we refer to the number of shares executed in a trade as \"size\" (we reserve the term \"volume\" to the sum of trade sizes in a given interval). We rename the `Volume` variable accordingly, and also alter the other variable names to make them easier to work with in R.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the trade data\ntrades <- fread(trades_url)\n\n# View the trade data\ntrades\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          #RIC       Domain           Date-Time GMT Offset  Type\n    1:   PRU.L Market Price 2021-06-07 07:00:09          1 Trade\n    2:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade\n    3:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade\n    4:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade\n    5:   PRU.L Market Price 2021-06-07 07:00:12          1 Trade\n   ---                                                          \n34513: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade\n34514: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade\n34515: PRUl.TQ Market Price 2021-06-11 15:28:56          1 Trade\n34516: PRUl.TQ Market Price 2021-06-11 15:28:58          1 Trade\n34517: PRUl.TQ Market Price 2021-06-11 15:29:06          1 Trade\n       Price Volume          Exch Time MMT Classification\n    1:  1482  11379 07:00:09.478177000     1O-------P----\n    2:  1478    703 07:00:12.859753000     12-------PH---\n    3:  1478    327 07:00:12.859753000     12-------PH---\n    4:  1478    650 07:00:12.859753000     12-------PH---\n    5:  1478    327 07:00:12.860655000     12-------PH---\n   ---                                                   \n34513:  1492    315 15:28:56.202000000     12-------PH---\n34514:  1492     50 15:28:56.218000000     12-------PH---\n34515:  1492    170 15:28:56.413000000     12-------PH---\n34516:  1492      6 15:28:58.062128000     32D---S--PH---\n34517:  1492      9 15:29:06.141081000     32D---S--PH---\n```\n\n\n:::\n\n```{.r .cell-code}\n# Rename the variables\nraw_trade_variables <- c(\"#RIC\", \"Date-Time\", \"Exch Time\", \"GMT Offset\", \n                         \"Price\", \"Volume\", \"MMT Classification\")\nnew_trade_variables <- c(\"ticker\", \"date_time\", \"exchange_time\", \"gmt_offset\",\n                         \"price\", \"size\", \"mmt\")\nsetnames(trades,\n         old = raw_trade_variables, \n         new = new_trade_variables)\n```\n:::\n\n\nFinally, we remove the columns ´Domain´ and ´Type´.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrades[, c(\"Domain\", \"Type\") := NULL]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- vroom::vroom(trades_url, \n                       col_types = list(`Exch Time` = col_character()))\n\ntv_trades <- lazy_dt(tv_trades)\n```\n:::\n\n\nThe raw data looks as follows.\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [34,517 x 9]\nCall:   `_DT17`\n\n  `#RIC` Domain   `Date-Time`         `GMT Offset` Type  Price Volume\n  <chr>  <chr>    <dttm>                     <dbl> <chr> <dbl>  <dbl>\n1 PRU.L  Market … 2021-06-07 07:00:09            1 Trade 1482.  11379\n2 PRU.L  Market … 2021-06-07 07:00:12            1 Trade 1478.    703\n3 PRU.L  Market … 2021-06-07 07:00:12            1 Trade 1478.    327\n4 PRU.L  Market … 2021-06-07 07:00:12            1 Trade 1478.    650\n5 PRU.L  Market … 2021-06-07 07:00:12            1 Trade 1478     327\n6 PRU.L  Market … 2021-06-07 07:00:12            1 Trade 1478     201\n# ℹ 34,511 more rows\n# ℹ 2 more variables: `Exch Time` <chr>, `MMT Classification` <chr>\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\nWe rename the variables.\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  rename(ticker = `#RIC`,\n            date_time = `Date-Time`,\n            gmt_offset = `GMT Offset`,\n            price = Price, \n            size = Volume, \n            exchange_time = `Exch Time`,\n            mmt = `MMT Classification`\n  ) |>\n  select(-c(\"Domain\", \"Type\")) \n```\n:::\n\n:::\n\n**Dates and timestamps.**\nAs above, we filter out the first and last minute of continuous trading, as well as the minutes surrounding the intraday auction. Note here that filtered trades are excluded, not just set as missing. This is OK, because we won't do any forward-filling for the trade data.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract dates\ntrades[, date := as.Date(date_time)]\n\n# Convert time stamps to numeric format, expressed in seconds past midnight\ntrades[, time := {\n\ttime_elements = strsplit(exchange_time, split = \":\")\n\ttime_elements = do.call(rbind, time_elements)\n\ttime = as.numeric(time_elements[, 1]) * 3600 + \n\t  as.numeric(time_elements[, 2]) * 60 + \n\t  as.numeric(time_elements[, 3]) +\n\t  gmt_offset * 3600\n\tlist(time)}]\n\n# Delete raw time variables\ntrades[, c(\"date_time\", \"exchange_time\", \"gmt_offset\") := NULL]\n\n# Retain trades from the continuous trading sessions\ntrades <- trades[time > (open_time + 60) & time < (close_time - 60) & \n\t\t\t\t    (time < (intraday_auction_time - 60) | \n\t\t\t\t     time > (intraday_auction_time + 3 * 60))]\n```\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |> \n  separate(exchange_time, \n           into = c(\"hour\", \"minute\", \"second\"), \n           sep=\":\", \n           convert = TRUE) |> \n  mutate(date = as.Date(date_time),\n         time = hour * 3600 + minute * 60 + second + gmt_offset * 3600)\n```\n:::\n\n\nHaving made sure that dates and times are in the desired format, we can save space by dropping the raw time and date variables. Further, we only retain trades from the continuous trading sessions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades|> \n  select(-c(\"date_time\", \"gmt_offset\",\"hour\",\"minute\",\"second\"))\n\ntv_trades <- tv_trades |>\n  filter(time > hms::as_hms(\"08:01:00\"), \n         time < hms::as_hms(\"16:29:00\"),\n         time < hms::as_hms(\"11:59:00\") | time > hms::as_hms(\"12:03:00\")) |>\n  lazy_dt()\n```\n:::\n\n:::\n\n**Trade variables.**\nA line plot offers a good overview of price data. In the figure below, we see that the trade prices are plagued by outliers that seem to be close to zero. The out-of-sequence prices are recorded on all trading dates and at virtually all times of the day. These outliers need to be addressed before we can proceed with the analysis.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the trade prices\nggplot(trades, \n       aes(time, price)) + \n       geom_line() + \n       facet_wrap(.~date, ncol= 1) + \n       labs(title = \"Trade prices\",  y = \"Price\", x = \"Time (seconds past midnight)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Trade price plot-1.png){width=2100}\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tv_trades |>as_tibble(), \n       aes(hms::hms(time), price)) + \n       geom_line() + \n       facet_wrap(.~date, ncol= 1) + \n       labs(title = \"Trade prices\",  y = \"Price\", x = \"Time\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-46-1.png){width=2100}\n:::\n:::\n\n:::\n\nThere are various ways to handle outliers, but the best way is to understand them. In trade data sets, there is often information provided about the trade circumstances (for quote observations, such information is often sparse). In the current data set, the best piece of supporting information is the MMT code. MMT, short for Market Model Typology, is a rich set of flags reported for trades in Europe in recent years. For details, see the website of the [Fix Trading Community](https://www.fixtrading.org/mmt/).\n\nThe MMT code is a 14-character string, where each position corresponds to one flag. The first character specifies the type of market mechanism. For example, \"1\" tells us that the trade was recorded in an LOB market, \"3\" indicates dark pools, \"4\" is for off-book trading, and \"5\" is for periodic auctions. The second character indicates the trading mode, where, for example, continuous trading is indicated by \"2\". \n\nAn overview of the populated values shows in the first column that the LOB market with continuous trading (\"12\") is by far the most common combination remaining after applying the filters above, followed by dark pool continuous trading (\"32\"). \n\nThe low-priced trades are captured in the second column. All those trades are off-book, as indicated by the first digit being \"4\". The second digit holds information about how the off-book trades are reported (\"5\" is for on-exchange, \"6\" is for off-exchange, and \"7\" indicates systematic internalisers).  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Output an overview of the MMT codes\n# The function `substr` is used here to extract the first two characters of the MMT code\ntable(substr(trades[, mmt], start = 1, stop = 2), trades[, price] < 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    \n     FALSE  TRUE\n  12 27737     0\n  32  3383     0\n  3U   114     0\n  45  1221    28\n  46     0    41\n  47  1023   236\n  5U    88     0\n```\n\n\n:::\n:::\n\n\nIn this analysis we are focusing on liquidity at the exchanges. Accordingly, we use the MMT codes to filter out trades from other trading venues. Restricting the trades to continuous trading at the exchanges, we obtain price plots that are free from outliers.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a subset with continuous trades only\nLOB_continuous_trades <- substr(trades[, mmt], start = 1, stop = 2) == \"12\"\n\n# Plot the prices of continuous trades\nggplot(trades[LOB_continuous_trades], \n       aes(time, price)) + \n  geom_line() + \n  facet_wrap(.~date, ncol = 1) + \n  labs(title = \"Trade prices\", y = \"Price\", x = \"Time (seconds past midnight)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Trade filtering MMT plot-1.png){width=2100}\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades |>\n  filter(str_extract(mmt, \"^.{2}\") == \"12\") |>\n  as_tibble() |>\nggplot(aes(hms::hms(time), price)) + \n  geom_line() + \n  facet_wrap(.~date, ncol = 1) + \n  labs(title = \"Trade prices\", y = \"Price\", x = \"Time\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-47-1.png){width=2100}\n:::\n:::\n\n:::\n\nFurther detective work reveals that the trade price outliers are not erroneous, they are just stated in pounds rather than in pence. This is clear because the outliers are priced 100 times lower than the other trades. Apparently, some off-exchange trades follow a different price reporting convention. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View trades with low prices\ntrades[price < 100]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     ticker price size            mmt       date  time\n  1:  PRU.L  14.8  635 47------MP---- 2021-06-07 29103\n  2:  PRU.L  14.9  400 45------MP---- 2021-06-07 31868\n  3:  PRU.L  14.9  204 45------MP---- 2021-06-07 32331\n  4:  PRU.L  14.9  540 45------MP---- 2021-06-07 32466\n  5:  PRU.L  14.8 1042 45------MP---- 2021-06-07 39063\n ---                                                  \n301:  PRU.L  14.9    1 47------MP---- 2021-06-11 57966\n302:  PRU.L  14.9  434 47------MP---- 2021-06-11 58540\n303:  PRU.L  14.9  870 47------MP---- 2021-06-11 59286\n304:  PRU.L  14.9 1463 46------MP---- 2021-06-11 59336\n305:  PRU.L  14.9 1463 46------MP---- 2021-06-11 59336\n```\n\n\n:::\n:::\n\n\nQuirks in the data are not unusual, and if they go unnoticed they can have strong impact on the market quality measures. The take-away from the outlier analysis is that there is often an explanation for why their prices are off. It is not always as straightforward as here, but it is worthwhile to try to find out what the cause of the deviations is. Other potential explanations are that the time stamps are off (possibly due to delayed reporting) or that the pricing is not done at the market (but in accordance to some derivative contract). \n\nFor the analysis below, we want to focus on exchange trades. Accordingly, we filter out all trades that are not from the on-exchange continuous trading sessions. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Retain continuous LOB trades only\ntrades <- trades[LOB_continuous_trades]\n```\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  filter(str_extract(mmt, \"^.{2}\") == \"12\")\n```\n:::\n\n:::\n\n**Matching trades to quotes.**\nTo evaluate the cost of trading, we want to compare the trade price to the fundamental value at the time of trade, as implied by the bid-ask quotes.\n\nThe objective of matching trades and quotes is to obtain the quotes that prevailed just before the trade. This is straightforward in settings where the trades and quotes are recorded at the same point, such that they are correctly sequenced. In other settings, the timestamps may need to be adjusted due to reporting latencies, or the trade size needs to be matched to changes in quoted depth ([Jurkatis, 2021](https://doi.org/10.1016/j.finmar.2021.100635))^[Jurkatis, S. (2022). Inferring trade directions in fast markets. *Journal of Financial Markets*, *58*, 100635.]. \n\nFor US data, the most common approach is to match trades to the last quotes available in the millisecond or microsecond before the trade, as prescribed by [Holden and Jacobsen (2014)](https://doi.org/10.1111/jofi.12127)^[Holden, C. W., & Jacobsen, S. (2014). Liquidity measurement problems in fast, competitive markets: Expensive and cheap solutions. *Journal of Finance*, 69(4), 1747-1785.]. There is, however, an active debate *which* timestamp to use. Several recent papers advocate the use of *participant* time stamps in trade and quote matching, see references about US timestamps above.\n\nIn lack of specific guidance for stocks traded in the UK, we match trades to quotes prevailing just before the trade. Based on the assumption that the combined liquidity from LSE and TQE offers the best fundamental value approximation, we match trades from all venues to the EBBO.\n\nThe merge function in `data.table` can be called as above by `merge(dt1, dt2)` (for two data.tables named `dt1` and `dt2`), or simply `dt1[dt2]`. We use the latter approach here because it allows us to specify what to do when the timestamps do not match exactly. The option `roll = TRUE` specifies that each observation in `trades` should be matched to the `quotes_ebbo` observation with the latest timestamp that is equal or earlier than the trade timestamp. However, we don't want *equal* matches, because the quote observation should always be *before* the trade. To avoid matching to contemporaneous quotes, which may be updated to reflect the impact of the trade itself, we add one microsecond to the quote timestamps before running the merge function.\nFor further understanding and illustration of the rolling join, we refer to the blog post by [R-Bloggers](https://www.r-bloggers.com/2016/06/understanding-data-table-rolling-joins/).\n\n::: {.panel-tabset group=\"language\"}\n## data.table\nFor the sake of completeness, you can load the EBBO quote data which we stored at an intermediate step above as follows.\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes_ebbo <- fread(file = \"quotes_ebbo.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adjust quote time stamps by one microsecond\nsetnames(quotes_ebbo, old = \"time\", new = \"quote_time\")\nquotes_ebbo[, time := quote_time + 0.000001]\n\n# Sort trades and quotes (this specifies the matching criteria for the merge function)\nsetkeyv(trades, cols = c(\"date\", \"time\"))\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\n\n# Match trades to quotes prevailing at the time of trade \n# The rolling is done only for the last of the matching variables, in this case \"time\"\n# `mult = \"last\"` specifies that if there are multiple matches with identical timestamps, \n# the last match is retained\ntrades <- quotes_ebbo[trades, roll = TRUE, mult = \"last\"]\n```\n:::\n\n\n## tidyverse (using dtplyr)\nFor the sake of completeness, you can load the EBBO quote data which we stored at an intermediate step above as follows.\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- read_csv(\"tv_quotes_ebbo.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 219469 Columns: 12\n── Column specification ─────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (8): time, best_bid_price, best_ask_price, best_bid_depth, b...\nlgl  (3): crossed, locked, large\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  mutate(time = time + 0.000001) |>\n  arrange(date, time) |> \n  as_tibble()\n\ntv_trades <- tv_trades |>\n  arrange(date, time) |> \n  as_tibble()\n\ntv_trades <- tv_trades |>\n  left_join(tv_quotes_ebbo, join_by(date, closest(time >= time)), suffix = c(\"\", \"_quotes\")) |>\n  lazy_dt()\n```\n:::\n\n:::\n\n**Further screening.**\nAs some trades may be matched to crossed or locked quotes, another round of data screening is required. Because such quotes are not considered reliable, we do not include those trades in the liquidity measurement. Furthermore, if there are trades that could not be matched to any quotes, or that lack information on price or size, they should be excluded too. \n\nThe output shows that 88.5% of the trades at LSE are eligible for the liquidity analysis, and 96.8% of the trades at TQE. The criterion that drives virtually all exclusions in the sample is the locked quotes.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Flag trades that should be included\ntrades[, include := !crossed & !locked & !large & !is.na(size) & size > 0 &  \n\t   \t            !is.na(price) & price > 0 & !is.na(midpoint) & midpoint > 0]\n\n# Report trade filtering stats\ntrades_filters <- trades[, \n\tlist(crossed = mean(crossed, na.rm = TRUE),\n\t     locked = mean(locked, na.rm = TRUE),\n    \t large = mean(large, na.rm = TRUE),\n    \t no_price = mean(is.na(price) | price == 0),\n    \t no_size = mean(is.na(size) | size == 0),\n    \t no_quotes = mean(is.na(midpoint) | midpoint <= 0),\n    \t included = mean(include)), \n\tby = \"ticker\"]\n\ntrades_filters[,\n  lapply(.SD * 100, round, digits = 2), \n  .SDcols = c(\"crossed\", \"locked\", \"large\", \"no_price\", \"no_size\", \"no_quotes\", \"included\"),\n  by = \"ticker\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ticker crossed locked large no_price no_size no_quotes included\n1:   PRU.L    0.30  11.20     0        0       0      0.00     88.5\n2: PRUl.TQ    0.07   3.05     0        0       0      0.02     96.8\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  mutate(include = !crossed & !locked & !large & !is.na(size) & size > 0 &\n           !is.na(price) & price > 0 & !is.na(midpoint) & midpoint > 0)\n\ntrades_filters <- tv_trades |>\n  group_by(ticker) |>\n  summarise(\n    crossed = mean(crossed, na.rm = TRUE),\n    locked = mean(locked, na.rm = TRUE),\n    large = mean(large, na.rm = TRUE),\n    no_price = mean(is.na(price) | price == 0),\n    no_size = mean(is.na(size) | size == 0),\n    no_quotes = mean(is.na(midpoint) | midpoint <= 0),\n    included = mean(include)\n  )\n\n# Round percentages and display\ntrades_filters |>\n  mutate(across(crossed:included, ~round(. * 100, digits = 2))) |>\n  select(ticker, crossed, locked, large, no_price, no_size, no_quotes, included)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [2 x 8]\nCall:   copy(`_DT19`)[, `:=`(include = !crossed & !locked & !large & \n    !is.na(size) & size > 0 & !is.na(price) & price > 0 & !is.na(midpoint) & \n    midpoint > 0)][, .(crossed = mean(crossed, na.rm = TRUE), \n    locked = mean(locked, na.rm = TRUE), large = mean(large, \n        na.rm = TRUE), no_price = mean(is.na(price) | price == \n        0), no_size = mean(is.na(size) | size == 0), no_quotes = mean(is.na(midpoint) | \n        midpoint <= 0), included = mean(include)), keyby = .(ticker)][, \n    `:=`(crossed = round(crossed * 100, digits = 2), locked = round(locked * \n        100, digits = 2), large = round(large * 100, digits = 2), \n        no_price = round(no_price * 100, digits = 2), no_size = round(no_size * \n            100, digits = 2), no_quotes = round(no_quotes * 100, \n            digits = 2), included = round(included * 100, digits = 2))]\n\n  ticker  crossed locked large no_price no_size no_quotes included\n  <chr>     <dbl>  <dbl> <dbl>    <dbl>   <dbl>     <dbl>    <dbl>\n1 PRU.L      0.3   11.2      0        0       0      0        88.5\n2 PRUl.TQ    0.07   3.05     0        0       0      0.02     96.8\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\n**Direction of trade.**\nIn empirical market microstructure, we often need to determine the direction of trade. If a trade happens following a buy market order, it is said to be buyer-initiated, and vice versa. \n\nThe most common tool to determine the direction of trade is the algorithm prescribed by [Lee and Ready (1991)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.1991.tb02683.x)^[Lee, C. M., & Ready, M. J. (1991). Inferring trade direction from intraday data. *Journal of Finance*, 46(2), 733-746.]. They primarily recommend the *quote rule*, saying that a trade is buyer-initiated if the trade price is above the prevailing midpoint, and seller-initiated if it is below. When the price equals the midpoint, Lee and Ready propose the *tick rule*. It specifies that a trade is buyer-initiated (seller-initiated) if the price is higher (lower) than the closest previous trade with a different price.\n\nThe quote rule is straightforward to implement using the `sign` function, which returns `+1` when the price deviation from the midpoint is positive and `-1` if it is negative. The tick rule, in contrast, requires several steps of code. We create a new `data.table`, named `price_tick`, which in addition to date and time observations for each trade, indicates whether a trade is priced higher (`+1`), lower (`-1`), or the same (`0`) as the previous one. We then exclude all trades that don't imply a price change. Finally, we merge the `price_tick` and the `trades` objects, such that each trade is associated with the latest previous price change.\n\nThe direction of trade can now be determined, using primarily the quote rule, and secondarily the tick rule. The output shows that, in this sample, seller-initiated trades are somewhat more common than buyer-initiated.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quote rule (the trade price is compared to the midpoint at the time of the trade)\ntrades[, quote_diff := sign(price - midpoint)]\n\n# Tick rule (each trade is matched to the closest preceding trade price change)\nprice_tick <- data.table(date = trades$date,\n                         time = trades$time,\n                         price_change = c(NA, sign(diff(trades$price))))\n\n# Retain trades that imply a trade price change\nprice_tick <- price_tick[price_change != 0]\n\n# Merge trades and trade price changes\nsetkeyv(trades, c(\"date\", \"time\"))\nsetkeyv(price_tick, c(\"date\", \"time\"))\ntrades <- price_tick[trades, roll = TRUE, mult = \"last\"]\n\n# Apply the Lee-Ready (1991) algorithm\ntrades[, dir := {\n  # 1st step: quote rule\n  direction = quote_diff\n  # 2nd step: tick rule\n  no_direction = is.na(direction) | direction == 0\n\tdirection[no_direction] = price_change[no_direction] \n\t\n\tlist(direction)},\n\tby = \"date\"]\n\ntable(trades$ticker, trades$dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         \n             -1     1\n  PRU.L   10854  8821\n  PRUl.TQ  4222  3840\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\nFirst, we compute the sign of the price change based on the quote rule (the trade price is compared to the midpoint at the time of the trade).\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  mutate(quote_diff = sign(price - midpoint))\n```\n:::\n\n\nSecond, each trade is matched to the closest preceding trade price change.\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_tick <- tv_trades |>\n  transmute(date, time, price_change = c(NA, sign(diff(price)))) |>\n  filter(price_change != 0) |> \n  group_by(date,time)|> \n  slice(n()) |> \n  ungroup() |>\n  as_tibble()\n```\n:::\n\n\nWe merge the `price_tick` and the `trades` objects, such that each trade is associated with the latest previous price change.\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  as_tibble() |>\n  left_join(price_tick, join_by(date, closest(time>=time)), suffix = c(\"\", \"_tick\")) |>\n  fill(price_change, .direction = \"up\") |>\n  lazy_dt()\n```\n:::\n\n\nFinally, we apply the Lee-Ready (1991) algorithm\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  group_by(date) |>\n  mutate(dir = case_when(!is.na(quote_diff) & quote_diff != 0 ~ quote_diff,\n                         TRUE ~ price_change)\n  ) |>\n  ungroup() |>\n  lazy_dt()\n\ntv_trades |> \n  as_tibble() |> \n  count(ticker,dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  ticker    dir     n\n  <chr>   <dbl> <int>\n1 PRU.L      -1 10854\n2 PRU.L       1  8821\n3 PRUl.TQ    -1  4222\n4 PRUl.TQ     1  3840\n```\n\n\n:::\n:::\n\n:::\n\n## Liquidity measures\n\n**Effective spread.**\nThe relative effective spread is defined as $effective\\_spread^{rel} = 2 D(P - M)/M$, where $P$ is the trade price and $D$ is the direction of trade. The multiplication by two is to make the effective spread comparable to the quoted spread. As is common in the literature, we use trade size weights when calculating the average effective spread. We multiply the effective spread by 10,000 to express it in basis points.\n\nWe also calculate the dollar volume, which is simply $dollar\\_volume = P \\times size$, with $size$ denoting the trade size. The trading volume is commonly referred to as liquidity in popular press, but in academic papers it rarely used as a liquidity measure. One reason for that is that spikes in volume are usually due to news rather than liquidity shocks. We still include trading volume here, because it often included as a control variable in microstructure event studies. To express the dollar volume in million GBP, we multiply it by $10^{-8}$.\n\nConsistent with the quote-based measures, the effective spread indicates that PRU is more liquid at LSE than at TQE. The LSE effective spread is 4.18 bps, more than 25% lower than the LSE quoted spread at 5.70 bps. The large difference may be due to trades executed at prices better than visible in the LOB, the effective spreads benchmarked to the consolidated midpoint rather than the respective venue midpoint, or the calculation of the effective spread at times of trade rather than continuously throughout the day. If investors time their trades to reduce trading costs, it makes sense that the effective spread is on average lower than the quoted spread. The interested reader can find out which of these differences drive the wedge between the two measures, by altering the way the average quoted spread is obtained.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure average liquidity for each stock-day\n\n# First order the table\nsetorder(trades, ticker, date, time)\n\ntrades_liquidity <- trades[include == TRUE, {             \n  list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n  \t\t\t\t\t\t\t\t\t  w = size),\n       volume = sum(price * size))\n  },\n\tby = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntrades_liquidity[, \n\tlist(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n\t     volume = round(mean(volume * 1e-8), digits = 2)), \n\tby = \"ticker\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ticker effective_spread volume\n1:   PRU.L             4.18  13.64\n2: PRUl.TQ             4.52   3.19\n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades <- tv_trades |>\n  arrange(ticker, date, time)\n\ntv_trades_liquidity <- tv_trades |>\n  filter(include == TRUE) |>\n  group_by(ticker, date) |>\n  summarise(\n    effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, w = size),\n    volume = sum(price * size)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'ticker'. You can override using\nthe `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Output liquidity measures, averaged across the five trading days for each ticker\ntv_trades_liquidity |>\n  group_by(ticker) |>\n  summarise(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    volume = round(mean(volume * 1e-8), digits = 2)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [2 x 3]\nCall:   `_DT23`[order(ticker, date, time)][include == TRUE][, .(effective_spread = weighted.mean(2 * \n    dir * (price - midpoint)/midpoint, w = size), volume = sum(price * \n    size)), keyby = .(ticker, date)][, .(effective_spread = round(mean(effective_spread * \n    10000), digits = 2), volume = round(mean(volume * 1e-08), \n    digits = 2)), keyby = .(ticker)]\n\n  ticker  effective_spread volume\n  <chr>              <dbl>  <dbl>\n1 PRU.L               4.18  13.6 \n2 PRUl.TQ             4.52   3.19\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\nIs the midpoint really a good proxy for the fundamental value of the security? [Hagströmer (2021)](https://doi.org/10.1016/j.jfineco.2021.04.018)^[Hagströmer, B. (2021). Bias in the effective bid-ask spread. *Journal of Financial Economics*, 142(1), 314-337.] shows that the reliance on the midpoint introduces bias in the effective spread. The bias is because traders are more likely to buy the security when the fundamental value is closer to the ask price, and more inclined to sell when the true value is close to the bid price. To capture this, we also consider the weighted midpoint, which is a proxy that allows the true value to lie anywhere between the best bid and ask prices. It is defined as $M_w=(P^BQ^A+P^AQ^B) / (Q^A+Q^B)$, and denoted `midpoint_w` in the code below. The weighted midpoint equals the midpoint when $Q^A=Q^B$.\n\nIn line with Hagströmer (2021), we find that the weighted midpoint version of the effective spread is lower than the conventional measure. At TQE, the difference is about 15\\%. This is noteworthy, as it overturns the previous finding that the TQE is less liquid than the LSE. Although the quoted spread at the TQE is wider, the results indicate that the traders at TQE are better at timing their trades in accordance to the fundamental value. \n\nThe reason for that the choice of effective spread measure matters for PRU is that its trading is constrained by the tick size (the quoted spread is almost always one or two ticks). Stocks that are not tick-constrained tend to show smaller differences between the effective spread measures.   \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure average liquidity for each stock-day, including the effective spread \n# relative to the weighted midpoint\ntrades_liquidity <- trades[include == TRUE, {\n  midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) / \n\t\t\t     (best_ask_depth + best_bid_depth)\n\tlist(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n\t\t\t\t\t\t\t\t          w = size),\n  \t\t effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint, \n  \t\t \t\t\t\t\t\t\t        w = size))},\n    by = c(\"ticker\", \"date\")]\n\n# Output liquidity measures, averaged across the five trading days for each ticker\ntrades_liquidity[, \n\tlist(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n\t     effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)), \n\tby = \"ticker\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ticker effective_spread effective_spread_w\n1:   PRU.L             4.18                 NA\n2: PRUl.TQ             4.52               3.94\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_trades_liquidity <- tv_trades |>\n  filter(include == TRUE) |>\n  group_by(ticker, date) |>\n  mutate(    midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) /\n                                (best_ask_depth + best_bid_depth)) |>\n  summarise(\n    effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, w = size),\n    effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint_w, w = size)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'ticker'. You can override using\nthe `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_trades_liquidity |>\n  group_by(ticker) |>\n  summarise(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [2 x 3]\nCall:   `_DT23`[order(ticker, date, time)][include == TRUE][, `:=`(midpoint_w = (best_bid_price * \n    best_ask_depth + best_ask_price * best_bid_depth)/(best_ask_depth + \n    best_bid_depth)), by = .(ticker, date)][, .(effective_spread = weighted.mean(2 * \n    dir * (price - midpoint)/midpoint, w = size), effective_spread_w = weighted.mean(2 * \n    dir * (price - midpoint_w)/midpoint_w, w = size)), keyby = .(ticker, \n    date)][, .(effective_spread = round(mean(effective_spread * \n    10000), digits = 2), effective_spread_w = round(mean(effective_spread_w * \n    10000), digits = 2)), keyby = .(ticker)]\n\n  ticker  effective_spread effective_spread_w\n  <chr>              <dbl>              <dbl>\n1 PRU.L               4.18              NA   \n2 PRUl.TQ             4.52               3.94\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\n**Price impact and realized spreads.**\nThe price impact is defined as $price\\_impact^{rel} = 2 D(M_{t+\\Delta}-M)/M$, where $M_{t+\\Delta}$ is the midpoint holding $\\Delta$ seconds after the trade. It thus captures the signed price change following a trade. \n\nThe realized spread is defined as $realized\\_spread^{rel} = 2 D(P - M_{t+\\Delta})/M$. Note that the price impact and the realized spread may be viewed as two components of the effective spread, as $effective\\_spread^{rel}=price\\_impact^{rel}+realized\\_spread^{rel}$.\n\nThe code below prepares the variables needed to calculate the realized spread and the price impact at a 60 second horizon (setting $\\Delta=60$). It loads the consolidated quotes, subtracts 60 seconds to their timestamps, and merges them with the trades. In this way, we have trades that are matched to quotes prevailing just before the trade (from the previous matching) as well as to future quotes.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adjust quote time stamps by 60 seconds\nquotes_ebbo <- quotes_ebbo[, time := quote_time - 60]\n\n# Rename variables to indicate that they correspond to quotes 1 minute after the trade\n# The function `paste0` adds the suffix \"_1min\" to each variable\nsetnames(quotes_ebbo, \n         old = c(\"midpoint\", \"crossed\", \"locked\", \"large\"), \n         new = paste0(c(\"midpoint\", \"crossed\", \"locked\", \"large\"), \"_1min\"))\n\n# Merge trades and quotes\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\nsetkeyv(trades, cols = c(\"date\", \"time\"))\ntrades <- quotes_ebbo[trades, roll = TRUE, mult = \"last\"]\n\n# Flag valid future quotes\ntrades[, include_1min := !crossed_1min & !locked_1min & !large_1min]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- tv_quotes_ebbo |>\n  mutate(time = time - 60)\n\n# Rename variables to indicate that they correspond to quotes 1 minute after the trade\n# The function `paste0` adds the suffix \"_1min\" to each variable\ntv_quotes_ebbo <- tv_quotes_ebbo |> \n  mutate(across(c(midpoint, crossed, locked, large), \n                ~., \n                .names=\"{col}_1min\"))\n\n# Merge trades and quotes\ntv_trades <- tv_trades |>\n  as_tibble() |>\n  left_join(tv_quotes_ebbo |> select(date, time, contains(\"_1min\")), \n            join_by(date, closest(time>=time)), suffix = c(\"\", \"_quotes\")) |>\n  arrange(ticker, date, time) |>\n  group_by(ticker, date) |>\n  fill(midpoint_1min, crossed_1min, locked_1min, large_1min, .direction = \"down\")\n\n# Flag valid future quotes\ntv_trades <- tv_trades |>\n  mutate(include_1min = !crossed_1min & !locked_1min & !large_1min) |>\n  lazy_dt()\n```\n:::\n\n:::\n\nThe next step calculates the liquidity measures. The results show that the price impact exceeds the effective spread. The realized spread is then negative, implying that liquidity providers on average lose money. How can that be? Wouldn't the market makers who incur losses simply refrain from trading? It could be that the 60-second evaluation does not reflect the market makers' trading horizon. More likely, perhaps, is that not all traders who post limit orders are in the market to earn the bid-ask spread. It could also be liquidity traders or informed investors who use limit orders to save on transaction costs. Relative to paying the effective spread, earning a negative realized spread may well be an attractive alternative.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure trade-based liquidity\neffective_spread_decomposition <- trades[include & include_1min, {\n\tlist(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n\t\t\t\t\t\t\t\t  w = size),\n\t     price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, \n  \t\t \t\t\t\t\t\t w = size),\n  \t\t realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, \n  \t\t \t\t\t\t\t\t w = size))}, \n\tby = c(\"ticker\", \"date\")]\n\n# Output the average liquidity measures\neffective_spread_decomposition[, \n\tlist(effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n\t     price_impact = round(mean(price_impact * 1e4), digits = 2),\n\t     realized_spread = round(mean(realized_spread * 1e4), digits = 2)), \n\tby = \"ticker\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ticker effective_spread price_impact realized_spread\n1:   PRU.L             4.18         4.75           -0.57\n2: PRUl.TQ             4.52         6.13           -1.61\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_effective_spread_decomposition <- tv_trades |>\n  filter(include == TRUE,\n         include_1min == TRUE) |>\n  group_by(ticker, date) |>\n  summarise(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, \n\t\t\t\t\t\t\t\t  w = size),\n\t     price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, \n  \t\t \t\t\t\t\t\t w = size),\n  \t\t realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, \n  \t\t \t\t\t\t\t\t w = size))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'ticker'. You can override using\nthe `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_effective_spread_decomposition |>\n  group_by(ticker) |>\n  summarise(\n    effective_spread = round(mean(effective_spread * 1e4), digits = 2),\n    price_impact = round(mean(price_impact * 1e4), digits = 2),\n    realized_spread = round(mean(realized_spread * 1e4), digits = 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [2 x 4]\nCall:   `_DT24`[`_DT24`[, .I[include == TRUE & include_1min == TRUE], \n    by = .(ticker, date)]$V1][, .(effective_spread = weighted.mean(2 * \n    dir * (price - midpoint)/midpoint, w = size), price_impact = weighted.mean(2 * \n    dir * (midpoint_1min - midpoint)/midpoint, w = size), realized_spread = weighted.mean(2 * \n    dir * (price - midpoint_1min)/midpoint, w = size)), keyby = .(ticker, \n    date)][, .(effective_spread = round(mean(effective_spread * \n    10000), digits = 2), price_impact = round(mean(price_impact * \n    10000), digits = 2), realized_spread = round(mean(realized_spread * \n    10000), digits = 2)), keyby = .(ticker)]\n\n  ticker  effective_spread price_impact realized_spread\n  <chr>              <dbl>        <dbl>           <dbl>\n1 PRU.L               4.18         4.75           -0.57\n2 PRUl.TQ             4.52         6.13           -1.61\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\nThe accuracy of the effective spread decomposition may be improved by replacing the midpoint by the weighted midpoint. We leave the implementation of that to the interested reader.\n\n# Market efficiency\n\nIn efficient markets, future price moves are unpredictable, but it is well-known that frictions can cause deviations from that view. This insight is the starting point for measurement of the degree of market efficiency. We consider two measures, the autocorrelation of returns and the variance ratio. We also include the *realized volatility*, the mean squared returns, which is a common control variable in microstructure research. \n\nFor each of these measures, rather than the tick data that we worked with above, we need *equispaced returns*. That is, we want to calculate price changes between fixed points in time, such as second by second. As this does not correspond to the LOB updates, we need to transform the data to the desired frequency.\n\n## Equispaced returns\nTo get started, we again rely on the consolidated quote data. For the applications below, we drop all LOB variables except the midpoint and the filtering flags.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\nquotes_ebbo <- fread(file = \"quotes_ebbo.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Delete variables\nquotes_ebbo[, c(\"best_bid_price\", \"best_bid_depth\", \"best_ask_price\", \n                \"best_ask_depth\", \"duration\") := NULL]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_ebbo <- read_csv(\"tv_quotes_ebbo.csv\") |> \n  select(date, time, midpoint, crossed, locked , large) |>\n  lazy_dt()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 219469 Columns: 12\n── Column specification ─────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (8): time, best_bid_price, best_ask_price, best_bid_depth, b...\nlgl  (3): crossed, locked, large\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n:::\n\nTo obtain equispaced observations, we create a time grid with one observation per second. As above, we exclude the first and last minute when setting the opening and closing times. We use the `expand.grid` function to create a grid of second-by-second observations for each sample date, and then convert it to a `data.table`.  \n\nFirst we create an equispaced time grid.\n\n::: {.cell}\n\n```{.r .cell-code}\nsampling_freq  <- 1\nopen_time <- 8 * 3600\nclose_time <- 16.5 * 3600\n```\n:::\n\n\nThe function `seq` creates a sequence of discrete numbers the `by` option defines the increment of the sequence, which is here 1 second\n\n::: {.cell}\n\n```{.r .cell-code}\ntime_grid <- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)\n```\n:::\n\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Repeat the time grid for each date and sort it by date and time\ndates <- unique(quotes_ebbo$date)\nquotes_1sec <- expand.grid(date = dates, time = time_grid)\n\n# Make it a data.table\nquotes_1sec <- data.table(quotes_1sec, key = c(\"date\", \"time\"))\n\n# View the time grid\nquotes_1sec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              date  time\n     1: 2021-06-07 28860\n     2: 2021-06-07 28861\n     3: 2021-06-07 28862\n     4: 2021-06-07 28863\n     5: 2021-06-07 28864\n    ---                 \n152401: 2021-06-11 59336\n152402: 2021-06-11 59337\n152403: 2021-06-11 59338\n152404: 2021-06-11 59339\n152405: 2021-06-11 59340\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_1sec <- expand_grid(date = tv_quotes_ebbo |> pull(date) |> unique(), \n            time = time_grid)\n```\n:::\n\n:::\n\nFor each point in the grid, we want to find the prevailing quote. That is, the last quote update preceding or coinciding with the time in question. Because quotes are valid until cancelled, the same quote may be matched to several consecutive seconds. To avoid bid-ask bounce in the market efficiency measures, we use the midpoint rather than the bid or ask prices.\n\nIn the same way as we matched trades to quotes above, we use the rolling merge to match the grid times to quotes. Once the merge is done, it is straightforward to set prices that are flagged as problematic to `NA`, and to calculate returns. We use log-diff returns expressed in basis points (i.e., multiplied by 10,000).\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort the quotes\nsetkeyv(quotes_ebbo, cols = c(\"date\", \"time\"))\n\n# Merge the time grid with the quotes\nquotes_1sec <- quotes_ebbo[quotes_1sec, roll = TRUE]\n\n# Set problematic quotes to NA\nquotes_1sec$midpoint[quotes_1sec$crossed|quotes_1sec$locked| quotes_1sec$large] <- NA\n\n# Calculate returns, expressed in basis points\n# The function `diff` returns the first difference of a time series, which in this case \n# is the log of the midpoint. \n# For each date, a leading `NA` is added to make the resulting vector fit the number of \n# observations in `quotes_1sec`. \nquotes_1sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = \"date\"]\n```\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_1sec <- tv_quotes_1sec |> \n  left_join(tv_quotes_ebbo |> as_tibble(), join_by(date, closest(time>=time)), suffix = c(\"\", \"_quotes\")) |>\n  mutate(midpoint = na_if(midpoint, locked|crossed|large)) |>\n  group_by(date) |>\n  mutate(return = 1e4 * c(NA, diff(log(midpoint)))) |>\n  lazy_dt()\n```\n:::\n\n:::\n\n## Efficiency and volatility measures\n**Return autocorrelation and realized volatility.**\nWe obtain the return autocorrelation by applying the `cor` function to returns and lagged returns. The latter are generated using the `shift` function with one lag. We account for missing values by specifying the option `use = \"complete.obs\"`. The return autocorrelation comes out at 0.02, a very low number. This is not surprising, as the sample stock is a large firm with high trading activity -- two characteristics associated with high market efficiency.\n\nRealized volatility is defined as the mean of squared returns. We also obtain the return variance to be able to calculate the variance ratio below. Although their definitions differ, the realized volatility and the return variance are almost identical in this data set, 0.32. This is because the mean return is close to zero.\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure market efficiency and volatility \nefficiency_1sec <- quotes_1sec[order(date, time), \n  list(return_corr_1sec = cor(return, shift(return, n = 1, type = \"lag\"), \n                              use = \"complete.obs\"),\n\t     realized_vol_1sec = mean(return^2, na.rm = TRUE),\n\t     return_var_1sec = var(return, na.rm = TRUE)), \n\tby = \"date\"]\n\n# Output an overview of the average efficiency and volatility \nefficiency_1sec[, \n  list(return_corr_1sec = round(mean(return_corr_1sec), digits = 2),\n       realized_vol_1sec = round(mean(realized_vol_1sec), digits = 2),\n       return_var_1sec = round(mean(return_var_1sec), digits = 2))]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   return_corr_1sec realized_vol_1sec return_var_1sec\n1:             0.02              0.32            0.32\n```\n\n\n:::\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_efficiency_1sec <- tv_quotes_1sec |>\n  arrange(date, time) |>\n  group_by(date) |>\n  summarise(return_corr_1sec = cor(return, lag(return), use = \"complete.obs\"),\n            vol_1sec = mean(return^2, na.rm = TRUE),\n            return_var_1sec = var(return, na.rm = TRUE))\n\ntv_efficiency_1sec |>\n  summarise(across(everything(), \n                   ~round(mean(.), digits = 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [1 x 4]\nCall:   `_DT26`[order(date, time)][, .(return_corr_1sec = cor(return, \n    shift(return, type = \"lag\"), use = \"complete.obs\"), vol_1sec = mean(return^2, \n    na.rm = TRUE), return_var_1sec = var(return, na.rm = TRUE)), \n    keyby = .(date)][, .(date = round(mean(date), digits = 2), \n    return_corr_1sec = round(mean(return_corr_1sec), digits = 2), \n    vol_1sec = round(mean(vol_1sec), digits = 2), return_var_1sec = round(mean(return_var_1sec), \n        digits = 2))]\n\n  date       return_corr_1sec vol_1sec return_var_1sec\n  <date>                <dbl>    <dbl>           <dbl>\n1 2021-06-09             0.02     0.32            0.32\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\n**Variance ratios.**\nThe variance ratio is defined as $var\\_ratio_{\\tau_{1}, \\tau_{2}} = (Var(R_{\\tau_{1}}) \\tau_2) / (Var(R_{\\tau_{2}})  \\tau_1)$, where $\\tau_{1}$ and $\\tau_{2}$ are two return sampling frequencies, and $Var(R_{\\tau_{i}})$  is the variance of returns sampled at frequency $\\tau_{i}$. Under market efficiency, the variance ratio should be equal to one. Negative and positive deviations from unity are due to frictions. It is usually the absolute deviation from unity that is used as an efficiency measure.\n\nWe measure the variance ratio for 1-second and 10-second returns. To get the 10-second return variance, the first step is to obtain the 10-second price grid. To do so, we simply take a subset of the grid obtained for the 1-second frequency. We then proceed with the calculation of returns and the return variance in the same way as above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsampling_freq <- 10 # 10 second grid\ntime_grid <- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)\n```\n:::\n\n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset the 1-second price grid to get the 10-second price grid \nquotes_10sec <- quotes_1sec[time %in% time_grid,]\n\n# Calculate returns at the 10-second frequency, expressed in basis points\nquotes_10sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = \"date\"]\n\n# Calculate the return variance at the 10-second frequency, daily\nefficiency_10sec <- quotes_10sec[, \n                      list(return_var_10sec = var(return, na.rm = TRUE)), \n                      by = \"date\"]\n```\n:::\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_quotes_10sec <- tv_quotes_1sec |>\n  inner_join(tibble(time = time_grid)) |>\n  group_by(date) |>\n  mutate(return = 1e4 * c(NA, diff(log(midpoint))))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining, by = \"time\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_efficiency_10sec <- tv_quotes_10sec |>\n  group_by(date) |>\n  summarise(return_var_10sec = var(return, na.rm = TRUE))\n```\n:::\n\n:::\n\nFinally, we merge the 10-second return variance with the 1-second frequency efficiency measures and calculate the variance ratio. The output shows that the 10-second return variance is slightly higher than ten times the 1-second return variance, resulting in a variance ratio that exceeds the efficiency benchmark (unity) by 8%. \n\n::: {.panel-tabset group=\"language\"}\n## data.table\n\n::: {.cell}\n\n```{.r .cell-code}\n# Merge the efficiency measures of different return sampling frequencies\nefficiency <- efficiency_1sec[efficiency_10sec]\n\n# Obtain the variance ratio\nefficiency[, var_ratio := return_var_10sec / (10 * return_var_1sec)]\n\n# Output an overview of the average variance ratio\nefficiency[, \n  list(return_var_1sec = round(mean(return_var_1sec), digits = 3),\n       return_var_10sec = round(mean(return_var_10sec), digits = 3),\n       var_ratio = round(mean(var_ratio), digits = 3))]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   return_var_1sec return_var_10sec var_ratio\n1:           0.318             3.44      1.08\n```\n\n\n:::\n:::\n\n\n## tidyverse (using dtplyr)\n\n::: {.cell}\n\n```{.r .cell-code}\ntv_efficiency <- tv_efficiency_1sec |>\n  inner_join(tv_efficiency_10sec) |>\n  mutate(var_ratio = return_var_10sec / (10 * return_var_1sec)) |>\n  lazy_dt()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining, by = \"date\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntv_efficiency |>\n  summarise(across(c(return_var_1sec, return_var_10sec, var_ratio), ~round(mean(.), digits = 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSource: local data table [1 x 3]\nCall:   `_DT29`[, .(return_var_1sec = round(mean(return_var_1sec), digits = 3), \n    return_var_10sec = round(mean(return_var_10sec), digits = 3), \n    var_ratio = round(mean(var_ratio), digits = 3))]\n\n  return_var_1sec return_var_10sec var_ratio\n            <dbl>            <dbl>     <dbl>\n1            0.32             3.44      1.08\n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n```\n\n\n:::\n:::\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n# Conclusion\nThis blog post offers a glimpse into the world of empirical market microstructure. A world characterized by intriguing questions, vast data and implementation details that matter greatly -- both for real-world outcomes of market structure reforms and for market quality measures.\nIf the blog post lowers the threshold and triggers your interest to enter this fascinating world, its aim is fulfilled.\n\nA key point throughout this guide is to try to monitor and understand the data. It is a good habit to inspect, plot, and summarize the data for each step of the processing. Outliers can often be understood with the aid of flags in the data in combination with institutional knowledge. If we know why they appear, we can make an informed decision about how to handle them. \n\nThe format necessarily imposes limitations. We don't cover all market quality measures and not all variations in methodology, but aim to convey what we consider to be good practice when calculating the most common measures. If you disagree, find bugs, or have other feedback, don't hesitate to send us an email at [bjh@sbs.su.se](mailto:bjh@sbs.su.se) and [niklas.landsberg@kuleuven.be](mailto:niklas.landsberg@kuleuven.be). \n\nTo cite our work, please use the following reference information:\nHagströmer, B. and Landsberg, N. (2023). Tidy Market Microstructure. URL: [www.tidy-finance.org/blog/tidy-market-microstructure](www.tidy-finance.org/blog/tidy-market-microstructure)\n\n[^1]: Menkveld, A. J. et al. (2023). Non-standard errors, *Journal of Finance (forthcoming)*. http://dx.doi.org/10.2139/ssrn.3961574\n[^2]: Amihud, Y. (2002). Illiquidity and stock returns: Cross-section and time-series effects. *Journal of Financial Markets*, 5(1), 31-56. https://doi.org/10.1016/S1386-4181(01)00024-6\n[^3]: Jahan-Parvar, M. R., & Zikes, F. (2023). When do low-frequency measures really measure effective spreads? Evidence from equity and foreign exchange markets. *Review of Financial Studies*, 36(10), 4190-4232. https://doi.org/10.1093/rfs/hhad028\n[^4]: Campbell, J. Y., Lo, A. W., & MacKinlay, A. C. (1998). *The econometrics of financial markets*. Princeton University Press.\n[^5]: Foucault, T., Pagano, M., & R?ell, A. (2013). *Market liquidity: Theory, evidence, and policy*. Oxford University Press, USA.\n[^6]: Hasbrouck, J. (2007). *Empirical market microstructure: The institutions, economics, and econometrics of securities trading*. Oxford University Press.\n[^7]: Scheuch, C., Voigt, S., & Weiss P. (2023). *Tidy finance with {R}*. Chapman and Hall/CRC. https://doi.org/10.1201/b23237\n}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}