{
  "hash": "6d2190f9c735ad9a5a836418c5ddb39c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using DuckDB with WRDS Data\"\nauthor:\n  - name: Ian Gow\n    url: https://www.linkedin.com/in/iangow/\n    affiliations:\n      - name: University of Melbourne\ndate: \"2023-12-22\"\ndescription: Demonstrate the power of DuckDB and dbplyr with WRDS data.\nimage: thumbnail.png\nimage-alt: image alt text\ncategories: \n  - Data\n  - R\n---\n\n\nIn this short note, I show how one can use DuckDB with WRDS data stored in the PostgreSQL database provided by WRDS.\nI then use some simple benchmarks to show how DuckDB offers a powerful, fast analytical engine for researchers in accounting and finance.\n\nTo make the analysis concrete, I focus on data used in the excellent recent book [\"Tidy Finance with R\"](../../r/index.qmd).\nEssentially, I combine data from CRSP's daily stock return file (`crsp.dsf`) with data on factor returns from Ken French's website and then run an aggregate query.\n\n# Summary of findings\n\nWhile using DuckDB simplifies the process of collecting data from WRDS (and results in a shorter download time), the real differences come after the data sets are on your computer.\nUsing DuckDB to load the data and run an aggregate query reduces the time taken from over two minutes using `dplyr` to well under one second.\nDuckDB from disk is faster than `dplyr` from RAM.\nAdditionally, DuckDB is faster than SQLite.\nIn fact, for many queries DuckDB would be faster collecting data from WRDS than `dplyr` is with data in a local SQLite database.\nWhile performance isn't everything, gains like these likely deliver real quality-of-life benefits to data analysts.\n\nI also show that almost all the performance benefits of DuckDB are realized even if the data are stored in parquet files.\nThis is useful information because, while the format of DuckDB database files remains in flux, parquet files are regarded by many as the \"modern CSV\" and can be read by many software systems, including R and Python.\nI describe how one could maintain a local library of parquet files including copies of WRDS tables [here](https://iangow.github.io/far_book/parquet-wrds.html).\n\nThis note illustrates the power of the core Tidy Finance approach.\nWith a few tweaks, one can springboard from the SQLite-and-`dplyr` approach of the book to the very cutting-edge of data science tools and approaches.\n\n# Databases and tidy data\n\nA popular way to manage and store data is with SQL databases.\n[Tidy Finance with R](../../r/index.qmd) uses SQLite, which \"implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine.\"\nIn this note, I use DuckDB, which has been [described](https://medium.com/short-bits/duckdb-sqlite-for-analytics-1273f7267298) as offering \"SQLite for Analytics\".\nDuckDB is like SQLite in not requiring a server process, but like server-based databases such as PostgreSQL in terms of support for advanced SQL features and data types.\n\nWhile storing data in a DuckDB database offers some benefits of SQLite (e.g., data compression), the real benefits of using DuckDB come from using the database engine for data analytics.\nFor the most part, [Tidy Finance with R](../../r/index.qmd) uses SQLite for storage and uses `dplyr` and in-memory data frames for analysis.\nFor example, in the [chapter on beta estimation](../../r/beta-estimation.qmd), the data are read into memory immediately using `collect()` before any analysis is conducted.\nHowever, the `dbplyr` package allows many analytical tasks to be performed in the database.\nIn this note, I demonstrate how using DuckDB and `dbplyr` can lead to significant performance gains. \n\n# Getting data\n\nThere are two data sets that we need to collect.\nThe first is the factor returns, which we collect from Ken French's website using the `frenchdata` package.\nThe second is from CRSP's daily stock file, which we get from WRDS.\n\nWe start by loading three packages.\nNote that we load `DBI` rather than the underlying database driver package.^[This is how it's done in [\"R for Data Science\"](https://r4ds.hadley.nz). I have read comments by Hadley Wickham that this is the right way to do it, but I can't find those comments.]\nIn addition to these three packages, you should have the `duckdb` and `RSQLite` packages installed. \nUse `install.packages()` in R to install any missing packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(frenchdata)\nlibrary(arrow)\n```\n:::\n\n\nNext we set up a DuckDB database file in the `data` directory, creating this directory if need be.\nWe set `read_only = FALSE` because we will want to write to this database connection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!dir.exists(\"data\")) dir.create(\"data\")\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = FALSE\n)\n```\n:::\n\n\n## Fama-French factor returns\n\nWe use the same `start_date` and `end_date` values used in [\"Tidy Finance with R\"](../../r/index.qmd) and the code below also is adapted from that book.\nHowever, we use the `copy_to()` function from `dplyr` to save the table to our database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_date <- ymd(\"1960-01-01\")\nend_date <- ymd(\"2021-12-31\")\n\nfactors_ff_daily_raw <- \n  download_french_data(\"Fama/French 3 Factors [Daily]\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n• `` -> `...1`\n```\n\n\n:::\n\n```{.r .cell-code}\nfactors_ff_daily <- \n  factors_ff_daily_raw$subsets$data[[1]] |>\n  mutate(\n    date = ymd(date),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |>\n  rename_with(str_to_lower) |>\n  rename(mkt_excess = `mkt-rf`) |>\n  filter(date >= start_date & date <= end_date) |>\n  copy_to(tidy_finance,\n          df = _,\n          name = \"factors_ff_daily\",\n          temporary = FALSE,\n          overwrite = TRUE)\n```\n:::\n\n\n## Getting daily returns from WRDS\n\nNext, I specify the connection details as follows.\nI recommend using environment variables (e.g., set using `Sys.setenv()`), as this facilitates sharing code with others.\nYou should not include this chunk of code in your code, rather run it before executing your other code.\nIn addition to setting these environment variables, you may want to set `PGPASSWORD` too.\n(Hopefully it is obvious that your should use *your* WRDS ID and password, not mine.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(PGHOST = \"wrds-pgdata.wharton.upenn.edu\",\n           PGPORT = 9737L,\n           PGDATABASE = \"wrds\",\n           PGUSER = Sys.getenv(\"WRDS_USER\"),\n           PGPASSWORD = Sys.getenv(\"WRDS_PASSWORD\"))\n```\n:::\n\n\nThird, we connect to the CRSP daily stock file in the WRDS PostgreSQL database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npg <- dbConnect(RPostgres::Postgres())\ndsf_db <- tbl(pg, Id(schema = \"crsp\", table = \"dsf\"))\n```\n:::\n\n\nAs we can see, we have access to data in `crsp.dsf`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndsf_db\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   table<\"crsp\".\"dsf\"> [?? x 20]\n# Database: postgres  [pweiss@wrds-pgdata.wharton.upenn.edu:9737/wrds]\n  cusip    permno permco issuno hexcd hsiccd date       bidlo askhi\n  <chr>     <dbl>  <dbl>  <dbl> <dbl>  <dbl> <date>     <dbl> <dbl>\n1 68391610  10000   7952  10396     3   3990 1986-01-07  2.38  2.75\n2 68391610  10000   7952  10396     3   3990 1986-01-08  2.38  2.62\n3 68391610  10000   7952  10396     3   3990 1986-01-09  2.38  2.62\n4 68391610  10000   7952  10396     3   3990 1986-01-10  2.38  2.62\n5 68391610  10000   7952  10396     3   3990 1986-01-13  2.5   2.75\n# ℹ more rows\n# ℹ 11 more variables: prc <dbl>, vol <dbl>, ret <dbl>, bid <dbl>,\n#   ask <dbl>, shrout <dbl>, cfacpr <dbl>, cfacshr <dbl>,\n#   openprc <dbl>, numtrd <dbl>, retx <dbl>\n```\n\n\n:::\n:::\n\n\nBefore proceeding with our first benchmark, we will make a version of `system.time()` that works with assignment.^[If we put `system.time()` at the end of this pipe, then `crsp_daily` would hold the value returned by that function rather than the result of the pipeline preceding it.\nAt first, the `system_time()` function may seem like magic, but Hadley Wickham explained to me that this works because of **lazy evaluation**, which is discussed in \"Advanced R\" [here](https://adv-r.hadley.nz/environments.html?q=lazy#lazy-call-stack).\nEssentially, `x` is evaluated just once---inside `system.time()`---and its value is returned in the next line.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem_time <- function(x) {\n  print(system.time(x))\n  x\n}\n```\n:::\n\n\nThe following code is adapted from the Tidy Finance code [here](../../r/wrds-crsp-and-compustat.qmd#daily-crsp-data).\nBut the original code is much more complicated and takes slightly longer to run.^[Performance will vary according to the speed of your connection to WRDS. Note that this query does temporarily use a significant amount of RAM on my machine, it is not clear that DuckDB will use as much RAM if this is more constrained. \nIf necessary, you can run (say) `dbExecute(tidy_finance, \"SET memory_limit='1GB'\")` to constrain DuckDB's memory usage; doing so has little impact on performance for this query.]\n\n\n::: {.cell freeze='auto'}\n\n```{.r .cell-code}\nrs <- dbExecute(tidy_finance, \"DROP TABLE IF EXISTS crsp_daily\")\n\ncrsp_daily <- \n  dsf_db |>\n  filter(between(date, start_date, end_date),\n         !is.na(ret)) |>\n  select(permno, date, ret) |>\n  mutate(month = as.Date(floor_date(date, \"month\"))) |>\n  copy_to(tidy_finance, df = _, name = \"dsf_temp\") |>\n  left_join(factors_ff_daily |>\n              select(date, rf), by = \"date\") |>\n  mutate(\n    ret_excess = ret - rf,\n    ret_excess = pmax(ret_excess, -1, na.rm = TRUE)\n  ) |>\n  select(permno, date, month, ret_excess) |>\n  compute(name = \"crsp_daily\", temporary = FALSE, overwrite = TRUE) |>\n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   52.5     6.3   270.3 \n```\n\n\n:::\n:::\n\n\n## Saving data to SQLite\n\nIf you have been working through \"Tidy Finance\", you may already have an SQLite database containing `crsp_daily`.\nIf not, we can easily create one now and copy the table from our DuckDB database to SQLite.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_finance_sqlite <- dbConnect(\n  RSQLite::SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncopy_to(tidy_finance_sqlite,\n        crsp_daily,\n        name = \"crsp_daily\",\n        overwrite = TRUE,\n        temporary = FALSE)\n\ndbExecute(tidy_finance_sqlite, \"VACUUM\")\n```\n:::\n\n\nWe can also save the data to a parquet file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(tidy_finance, \n          \"COPY crsp_daily TO 'data/crsp_daily.parquet' \n          (FORMAT 'PARQUET')\")\n```\n:::\n\n\nHaving created our two databases, we disconnect from them.\nThis mimics the most common \"write-once, read-many\" pattern for using databases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(tidy_finance_sqlite)\ndbDisconnect(tidy_finance, shutdown = TRUE)\n```\n:::\n\n\n# Benchmarking a simple aggregation query\n\nThe following is a simple comparison of several different ways of doing some basic data analysis with R.\nAfter running the code above, we have the table `crsp_daily` as described in [Tidy Finance](https://www.tidy-finance.org/wrds-crsp-and-compustat.html#daily-crsp-data) in two separate databases---a SQLite database and a DuckDB database---and in a parquet file.\n\nThe following examines the same query processed in three different ways.\n\n  1. Using `dplyr` on an in-memory data frame\n  2. Using `dbplyr` with an SQLite database\n  3. Using `dbplyr` with a DuckDB database \n  4. Using `dbplyr` with DuckDB and a parquet file.\n  5. Using `dbplyr` with the `arrow` library and a parquet file.\n  \n## dplyr\n\nWe first need to load the data into memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_finance <- dbConnect(\n  RSQLite::SQLite(),\n  \"data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\")\n```\n:::\n\n\nWhat takes most time is simply loading nearly 2GB of data into memory.\n\n\n::: {.cell freezer='auto'}\n\n```{.r .cell-code}\ncrsp_daily_local <- \n  crsp_daily |> \n  collect() |>\n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n 204.89    2.05  212.82 \n```\n\n\n:::\n:::\n\n\nOnce the data are in memory, it is *relatively* quick to run a summary query.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily_local |> \n  group_by(month) |> \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |> \n  collect() |> \n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   3.05    1.14    4.26 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 744 × 2\n  month            ret\n  <date>         <dbl>\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(crsp_daily_local)\n```\n:::\n\n\n## dbplyr with SQLite\n\nThings are faster with SQLite, though there's no obvious way to split the time between reading the data and performing the aggregation.\nNote that we have a `collect()` at the end.\nThis will not take a noticeable amount of time, but seems to be a reasonable step if our plan is to analyse the aggregated data in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily |> \n  group_by(month) |> \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |> \n  collect() |>\n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  21.12    7.36   37.08 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 744 × 2\n  month            ret\n  <date>         <dbl>\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(tidy_finance)\n```\n:::\n\n\n## dbplyr with DuckDB\n\nLet's consider DuckDB. \nNote that we are only reading the data here, so we set `read_only = TRUE` in connecting to the database.\nApart from the connection, there is no difference between the code here and the code above using SQLite.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily |> \n  group_by(month) |> \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |> \n  collect() |>\n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   0.55    0.36    0.47 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 744 × 2\n  month             ret\n  <date>          <dbl>\n1 2019-07-01 -0.000265 \n2 2021-06-01  0.00107  \n3 2021-12-01  0.0000748\n4 1984-11-01 -0.00178  \n5 1985-08-01 -0.000240 \n# ℹ 739 more rows\n```\n\n\n:::\n:::\n\n\nHaving done our benchmarks, we can take a quick peek at the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily |> \n  group_by(month) |> \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |> \n  arrange(month) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 744 × 2\n  month            ret\n  <date>         <dbl>\n1 1960-01-01 -0.00213 \n2 1960-02-01  0.000325\n3 1960-03-01 -0.00115 \n4 1960-04-01 -0.00106 \n5 1960-05-01  0.00114 \n# ℹ 739 more rows\n```\n\n\n:::\n:::\n\n\nFinally, we disconnect from the database.\nThis will happen automatically if we close R, etc., and is less important if we have `read_only = TRUE` (so there is no lock on the file), but we keep things tidy here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(tidy_finance, shutdown = TRUE)\n```\n:::\n\n\n## dbplyr with DuckDB and a parquet file\n\nLet's do the benchmark using the parquet data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndb <- dbConnect(duckdb::duckdb())\ncrsp_daily <- tbl(db, \"read_parquet('data/crsp_daily.parquet')\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily |> \n  group_by(month) |> \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |> \n  collect() |>\n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   2.69    1.28    0.46 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 744 × 2\n  month            ret\n  <date>         <dbl>\n1 2017-06-01  0.00106 \n2 2017-08-01 -0.000327\n3 2017-10-01  0.000169\n4 2018-12-01 -0.00539 \n5 2019-01-01  0.00525 \n# ℹ 739 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(db, shutdown = TRUE)\n```\n:::\n\n\n## The `arrow` library with a parquet file\n\nLet's do one more benchmark using the parquet data with the `arrow` library.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily <- open_dataset(\"data/crsp_daily.parquet\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrsp_daily |> \n  group_by(month) |> \n  summarize(ret = mean(ret_excess, na.rm = TRUE)) |> \n  collect() |>\n  system_time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   0.46    0.03    0.70 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 744 × 2\n  month            ret\n  <date>         <dbl>\n1 1986-11-01 -0.000261\n2 1986-12-01 -0.00146 \n3 1987-01-01  0.00527 \n4 1987-03-01  0.00152 \n5 1987-02-01  0.00370 \n# ℹ 739 more rows\n```\n\n\n:::\n:::\n\n\n## Conclusion\n\n...\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}