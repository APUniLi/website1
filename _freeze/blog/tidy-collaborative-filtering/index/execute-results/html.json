{
  "hash": "66c6d0b7c41bd4e474233b037d379340",
  "result": {
    "markdown": "---\ntitle: \"Tidy Collaborative Filtering: Building A Stock Recommender\"\nauthor: \"Christoph Scheuch\"\ndate: \"2023-05-14\"\ndescription: A simple implementation for prototyping multiple collaborative filtering algorithms.\nimage: thumbnail.png\ncategories: \n  - Recommender System\n---\n\n\nRecommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kinds of product recommendations. A recommender system aims to efficiently deliver relevant information to users by automatically searching through a large volume of dynamically generated information to provide users with personalized content. In this blog post, I illustrate the concept of recommender systems in building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks for investors who already have their own portfolio and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide useful inspiration for other investors. \n\nwikifolio.com is the leading social trading platform in Europe, where anyone can publish and monetize their trading strategies through virtual portfolios, which are called wikifolios. The community of wikifolio traders includes full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers and financial editors. All traders share their trading ideas through fully transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender. \n\nThere are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the the recommender approaches and refer to [Ricci et al. (2011)](https://link.springer.com/book/10.1007/978-0-387-85820-3) for a comprehensive exposition. \n\n# A Primer on Recommender Systems\n\n## Collaborative Filtering \n\nIn collaborative filtering, recommendations are purely based on past interactions between users and items to produce new recommendations. The main notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two sub-classes of collaborative filtering: the memory-based approach which essentially searches nearest neighbors based on recorded transactions and is hence model-free; and the model-based approach where new representations of users and items are built based on some generative pre-estimated model. Theoretically speaking, the memory-based approach has a low bias (since there is no latent model assumed), but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model and has hence a relatively higher bias, but a lower variance, i.e., recommendations are more stable since they come out of a model. \n\nAdvantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem). \n\n## Content-Based Filering\n\nContent-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (user will like the item or not) or more generally a regression problem (which rating will user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalized (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has few interactions then the resulting model becomes easily instable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors and putting them into a neural network. \n\nThe main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filtering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics.\n\n## Hybrid Recommenders\n\nHybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each individual approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), choosing among different compontent (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc.\n\n# Train Collaborative Filtering Recommenders in R\n\nFor this post, we rely on the `tidyverse` [@tidyverse] family of packages, `scales` [@scales] for scale functions for visualization, and `[recommenderlab](https://github.com/mhahsler/recommenderlab)` - a package that provides an infrastructure to develop and test collaborative filerting recommender algorithms.  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ───────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ─────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(scales)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n:::\n\n```{.r .cell-code}\nlibrary(recommenderlab)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: arules\n\nAttaching package: 'arules'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\nLoading required package: proxy\n\nAttaching package: 'proxy'\n\nThe following object is masked from 'package:Matrix':\n\n    as.matrix\n\nThe following objects are masked from 'package:stats':\n\n    as.dist, dist\n\nThe following object is masked from 'package:base':\n\n    as.matrix\n\nRegistered S3 methods overwritten by 'registry':\n  method               from \n  print.registry_field proxy\n  print.registry_entry proxy\n```\n:::\n:::\n\n\n\n\nI load a dataset with stock holdings of investable wikifolios at the beginning of 2023. Reach out if you want to get access to the data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(wikifolio_portfolios)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 149,916\nColumns: 6\n$ wikifolio    <chr> \"DE000LS9AAB3\", \"DE000LS9AAB3\", \"DE000LS9AAB3\"…\n$ stock        <chr> \"AU000000CSL8\", \"AU0000193666\", \"CA0679011084\"…\n$ quantity     <dbl> 3, 20, 5, 3, 5, 2, 2, 3, 1, 15, 5, 5, 4, 2, 3,…\n$ price        <dbl> 182.89, 7.26, 16.25, 30.15, 33.40, 299.00, 59.…\n$ weight       <dbl> 0.01975, 0.00523, 0.00292, 0.00326, 0.00601, 0…\n$ in_portfolio <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n:::\n:::\n\n\nFirst, I convert the long data to a binary rating matrix where only non-NA values are stored explicitly and NA values are represented by dots for efficiency. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinary_rating_matrix <- wikifolio_portfolios |>\n  pivot_wider(id_cols = wikifolio,\n              names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |>\n  select(-wikifolio) |>\n  as.matrix() |>\n  as(\"binaryRatingMatrix\")\nbinary_rating_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n```\n:::\n:::\n\n\nAs in our book chapter on factor selection via machine learning, I perform cross-validation and split the data into training and test data. The training sample constitute 80% of the data and I perform $k$-times recursive estimation of the cross validation with $k=5$. Note that the parameter `given=-1` means that all but one randomly selected item is ignored for the evaluation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscheme <- binary_rating_matrix |>\n  evaluationScheme(\n    method = \"cross\",\n    k      = 5,\n    train  = 0.8,\n    given  = -1\n)\nscheme\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvaluation scheme using all-but-1 items\nMethod: 'cross-validation' with 5 run(s).\nGood ratings: NA\nData set: 6544 x 5069 rating matrix of class 'binaryRatingMatrix' with 149916 ratings.\n```\n:::\n:::\n\n\nHere is the list of recommenders that I consider for the backtest:\n\n* Random Items: the benchmark case because it just stupidly chooses random stocks from all possible choices.\n* Popular Items: just recommended the most popular stocks to measured by the number of wikifolios that hold the stock.\n* Association Rules: each wikifolio and its portfolio is considered as a transaction. Association rule mining finds similar portfolios across all traders (if a traders has x, y and z in his/her portfolio, then he/she is X% likely of also including w). \n* Item-Based Filtering: the algorithm calculates a similarity matrix across stocks. Recommendations are then based on the list of most similar stocks to the ones the wikifolio already has in its portfolio. \n* User-Based Filtering: the algorithm finds a neighborhood of similar wikifolios for each wikifolio (for this exercise it is set to 100 most similar wikifolios). Recommendations are then based on what the most similar wikifolios have in their portfolio. \n\nFor each algorithm, I base the evaluation on 1, 3, 5, and 10 recommendations. Note that the evaluation takes a couple of hours, in particular because IBCF and UBCF are quite time-consuming. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgorithms <- list(\n  \"Random Items\"         = list(name  = \"RANDOM\",  param = NULL),\n  \"Popular Items\"        = list(name  = \"POPULAR\", param = NULL),\n  \"Association Rules\"    = list(name  = \"AR\", param = list(supp = 0.01, conf = 0.1)),\n  \"Item-Based Filtering\" = list(name  = \"IBCF\", param = list(k = 10)),\n  \"User-Based Filtering\" = list(name  = \"UBCF\", param = list(method = \"Cosine\", nn = 100))\n)\n\nnumber_of_recommendations <- c(1, 3, 5, 10)\nresults <- evaluate(scheme,\n                    algorithms,\n                    type = \"topNList\",\n                    progress = TRUE,\n                    n = number_of_recommendations\n)\n```\n:::\n\n\n\n\n\n\n# Evaluate Recommenders\n\nThe output of `evaluate()` already provides evaluation metrics in a structured way. I can simply average the metrics over the cross-validation folds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_tbl <- results |>\n  avg() |>\n  map(as_tibble) |>\n  bind_rows(.id = \"model\")\n```\n:::\n\n\nNow, for each recommender, we are interested the following numbers:\n\n* True Negative (TN) = number of not predicted items that do not correspond to withheld items\n* False Positive (FP) = number of incorrect predictions that do not correspond to withheld items\n* False negative (FN) = number of not predicted items that correspond to withheld items\n* True Positive (TP) = number of correct predictions that correspond to withheld items\n\nThe two figures below present the most common evaluation techniques for the performance of recommender algorithms in backtest settings like mine. \n\n## ROC curves\n\nThe first visualization approach comes from signal-detection and is called \"Receiver Operating Characteristic\" (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) against the probability of false alarm (FPR).\n\n* TPR = TP / (TP + FN) (i.e. share of true positive recommendations relative to all known portfolios)\n* FPR = FP / (FP + TN) (i.e. share of incorrect recommendations relative to )\n\nIntuitively, the bigger the area under the ROC curve, the better is the corresponding algorithm.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_tbl |>\n  ggplot(aes(FPR, TPR, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    title = \"ROC Curves of Collaborative Filtering Algorithms for a Stock Recommender\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=2100}\n:::\n:::\n\n\nThe figure shows that recommending random items exhibits the lowest TPR for any FPR, so it is the worst among all algorithms (which it is not surprising). Association rules, on the other hand, constitutes the best algorithm among the current selection. This result is neat because association rule mining is a computationally cheap algorithm, so we could potentially fine-tune or reestimate the model easily.\n\n## Precision-Recall Curves\n\nThe second popular approach is to plot Precision-Recall curves. The two measures are often used in information retrieval problems:\n\n* Precision = TP / (TP + FP) (i.e. correctly recommended items relative to total recommended items)\n* Recall = TP / (TP + FN) (i.e. correctly recommended items relative to total number of known useful recommendations)\n\nThe goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_tbl |>\n  ggplot(aes(x = recall, y = precision, colour = model)) +\n  geom_line() +\n  geom_label(aes(label = n))  +\n  labs(\n    x = \"Recall\", y = \"Precision\",\n    title = \"Precision-Recall Curves of Collaborative Filtering Algorithms for a Stock Recommender\",\n    colour = \"Model\"\n  ) +\n  theme(legend.position = \"right\") + \n  scale_y_continuous(labels = percent) +\n  scale_x_continuous(labels = percent)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=2100}\n:::\n:::\n\n\nAgain, proposing random items exhibits the worst performance, as for any given level of recall, this approach has the lowest precision. Association rules are also the best algorithm with this visualization approach.\n\n# Create Predictions\n\nThe final step is to create stock recommendations for investors who already have portfolios. I pick the IBCF algorithm to create such recommendations because it excelled in the analyses above. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecommender <- Recommender(binary_rating_matrix, method = \"AR\", param = list(supp = 0.01, conf = 0.1))\n```\n:::\n\n\nAs ane example, suppose you currently have a portfolio that consists of Nvidia (US67066G1040) and Apple (US0378331005). I have to transform this sample portfolio into a rating matrix with the same dimensions as the data we used as input for our training. The `predict()` function then delivers a prediction for the example portfolio. \n\n::: {.cell}\n\n```{.r .cell-code}\nsample_portfolio <- c(\"US67066G1040\", \"US0378331005\")\nsample_rating_matrix <- tibble(distinct(wikifolio_portfolios, stock)) |>\n  mutate(in_portfolio = if_else(stock %in% sample_portfolio, 1, 0)) |>\n  pivot_wider(names_from = stock,\n              values_from = in_portfolio,\n              values_fill = list(in_portfolio = 0)) |>\n  as.matrix() |>\n  as(\"binaryRatingMatrix\")\n\nprediction <- predict(recommender, sample_rating_matrix, n = 1)\nas(prediction, \"list\")[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"US5949181045\"\n```\n:::\n:::\n\n\nSo the IBCF algorithm recommends Microsoft (US5949181045) as a stock if you are already invested in Nvidia and Apple, which makes a lot of sense given the similarity in business model. Of course, this recommendation is not serious investment advice, but rather serves an illustrative purpose. ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}