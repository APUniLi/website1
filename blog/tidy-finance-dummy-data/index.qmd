---
title: "Dummy Data for Tidy Finance Readers without Access to WRDS"
author:
  - name: Christoph Scheuch
    url: https://christophscheuch.github.io/
    affiliations:
      - name: wikifolio Financial Technologies AG
date: "2023-09-20"
description: R code to generate dummy data that can be used to run the code chunks in Tidy Finance with R or Python
image: thumbnail.png
image-alt: An image of a stylized company building on a screen in the middle of a room. The room has a futuristic setting, with a backdrop of a digital, grid-like landscape symbolizing the internet. The color palette should be a combination of cool blues and warm yellow. Created with DALL-E 2.
categories: 
  - Data
---

Since we published our book [Tidy Finance with R](../../r/index.qmd), we have received feedback from readers who don't have access to WRDS that they cannot run the code we provide. To alleviate their constraints, we decided to create a dummy database that contains all tables and corresponding columns such that all code chunks in our book can be executed with this dummy database. The resulting database can be found through [this link](https://1drv.ms/u/s!ArgZyNRLSLEXgspompdvaUcBIzfekw?e=3R1L0t) (around 50 MB). Just download the database and put it into you data folder. 

We deliberately use the *dummy* label because the data is not meaningful in the sense that it allows readers to actually replicate the results of the book. For legal reasons, the data does *not* contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books. 

To generate the dummy database, we use the following packages:

```{r}
#| message: false
library(tidyverse)
library(RSQLite)
```

We use the original database as an input and initialize a dummy version where we later store the output tables. 

```{r}
tidy_finance <- dbConnect(
  SQLite(),
  "../../data/tidy_finance.sqlite",
  extended_types = TRUE
)

tidy_finance_dummy <- dbConnect(
  SQLite(),
  "../../data/tidy_finance_dummy.sqlite",
  extended_types = TRUE
)
```

Since we draw random numbers for most of the columns, we also define a seed to ensure that the generated numbers are replicable. We also initialize vectors of dates of different frequencies over 10 years that we then use to create yearly, monthly, and daily data, respectively. 

```{r}
set.seed(1234)

start_date <- as.Date("2003-01-01")
end_date <- as.Date("2022-12-31")

time_series_years <- seq(year(start_date), year(end_date), 1)
time_series_months <- seq(start_date, end_date, "1 month")
time_series_days <- seq(start_date, end_date, "1 day")
```

## Create macro dummy data

We start by creating dummy data for the macroeconomic tables. The following code does this by replacing the original data with random numbers while preserving the time structure (either daily or monthly) and column names. We iterate over each table name in the `macro_tables` vector. For each table, we fetch the data from the database and store it in a `data_original` table. The code checks the column names of `data_original` to determine if the data is structured by months or by days. Depending on the structure, it sets the appropriate time series and date column name. For each relevant column (i.e., excluding columns with dates), the code generates a command to replace its values with random numbers drawn from a uniform distribution between 0 and 1. We then use the `!!!` operator to unlist and execute the list of commands. This trick actually helps us to avoid typing the same function for each column individually. Finally, each table with dummy data is written to the new database with dummy data.

Note that we do not put any meaningful structure on the macro tables because they can be freely downloaded from the original sources - check out [Accessing and Managing Financial Data](../../r/accessing-and-managing-financial-data.qmd) to replace the dummy data with the actual data.

```{r}
macro_tables <- c(
  "cpi_monthly", "factors_ff3_daily", 
  "factors_ff3_monthly", "factors_ff5_monthly", 
  "factors_q_monthly", "industries_ff_monthly", 
  "macro_predictors"
)

for (table_name in macro_tables) {
  data_original <- tbl(tidy_finance, table_name) |>
    collect() |> 
    drop_na()
  
  if ("month" %in% names(data_original)) {
    time_series <- time_series_months
    date_column <- "month"
  } else if ("date" %in% names(data_original)) {
    time_series <- time_series_days
    date_column <- "date"
  }
  
  relevant_columns <- data_original |> 
    select(-contains(c("month", "date"))) |> 
    names()
  
  commands <- unlist(
    map(
      relevant_columns, 
      ~rlang::exprs(!!..1 := runif(n()))
    )
  )
  
  data_dummy <- tibble(
    !!sym(date_column) := time_series
    ) |> 
    mutate(
      !!!commands
    )
  
  dbWriteTable(
    tidy_finance_dummy, 
    table_name,
    data_dummy,
    overwrite = TRUE
  )
}
```

## Create stock dummy data

Let us move on the core data used throughout the book: stock and firm characteristics. We first generate a table of stock identifiers with unique `permno` and `gvkey` values, as well as associated `exchcd`, `exchange`, `industry`, and `siccd` values. The generated data is based on the characteristics of stocks in the `crsp_monthly` table of the original database, ensuring that the generated stocks roughly reflect the distribution of industries and exchanges in the original data, but the identifiers and corresponding exchanges or industries do not reflect actual firms. Similarly, the `permno`-`gvkey` combinations are purely nonsensical and should not be used together with actual CRSP or Compustat data. 

```{r}
number_of_stocks <- 100

crsp_stocks <- tbl(tidy_finance, "crsp_monthly") |>
  group_by(permno) |> 
  filter(month == max(month, na.rm = TRUE)) |> 
  ungroup() |> 
  select(permno, gvkey, industry, exchange, exchcd, siccd) |> 
  collect()

industries <- crsp_stocks |> 
  filter(industry != "Missing") |> 
  count(industry) |> 
  mutate(prob = n / sum(n))

exchanges <- crsp_stocks |> 
  filter(exchange != "Other") |> 
  count(exchange) |> 
  mutate(prob = n / sum(n))

stock_identifiers <- 1:number_of_stocks |> 
  map_df(
    function(x) {
      tibble(
        permno = x,
        gvkey = as.character(x + 10000),
        exchange = sample(exchanges$exchange, 1, 
                          prob = exchanges$prob),
        industry = sample(industries$industry, 1, 
                          prob = industries$prob)
      ) |> 
        mutate(
          exchcd = case_when(
            exchange == "NYSE" ~ sample(c(1, 31), n()),
            exchange == "AMEX" ~ sample(c(2, 32), n()),
            exchange == "NASDAQ" ~ sample(c(3, 33), n())
          ),
          siccd = case_when(
            industry == "Agriculture" ~ sample(1:999, n()),
            industry == "Mining" ~ sample(1000:1499, n()),
            industry == "Construction" ~ sample(1500:1799, n()),
            industry == "Manufacturing" ~ sample(1800:3999, n()),
            industry == "Transportation" ~ sample(4000:4899, n()),
            industry == "Utilities" ~ sample(4900:4999, n()),
            industry == "Wholesale" ~ sample(5000:5199, n()),
            industry == "Retail" ~ sample(5200:5999, n()),
            industry == "Finance" ~ sample(6000:6799, n()),
            industry == "Services" ~ sample(7000:8999, n()),
            industry == "Public" ~ sample(9000:9999, n())
          )
        )
    }
  )
```

Next, we construct three panels of stock data with varying frequencies: yearly, monthly, and daily. We begin by creating the `stock_panel_yearly` panel. To achieve this, we combine the `stock_identifiers` table with a new table containing the variable `year` from `time_series_years`. The `crossing()` function ensures that we get all possible combinations of the two tables. After combining, we select only the `gvkey` and `year` columns for our final yearly panel.

Next, we construct the `stock_panel_monthly` panel. Similar to the yearly panel, we use the `crossing()` function to combine `stock_identifiers` with a new table that has the `month` variable from `time_series_months`. After merging, we select the columns `permno`, `gvkey`, `month`, `siccd`, `industry`, `exchcd`, and `exchange` to form our monthly panel.

Lastly, we create the `stock_panel_daily` panel. We combine `stock_identifiers` with a table containing the `date` variable from `time_series_days`. After merging, we retain only the `permno` and `date` columns for our daily panel.

```{r}
stock_panel_yearly <- crossing(
  stock_identifiers, 
  tibble(year = time_series_years)
) |> 
  select(gvkey, year)

stock_panel_monthly <- crossing(
  stock_identifiers, 
  tibble(month = time_series_months)
) |> 
  select(permno, gvkey, month, siccd, industry, exchcd, exchange)

stock_panel_daily <- crossing(
  stock_identifiers, 
  tibble(date = time_series_days)
)|> 
  select(permno, date)
```

### Dummy `beta` table

We then proceed to simulate beta values for our `stock_panel_monthly` table. We generate simulated monthly beta values `beta_monthly` using the `rnorm()` function with a mean and standard deviation of 1. For daily beta values `beta_daily`, we take the simulated monthly beta and add a small random noise to it. This noise is generated again using the `rnorm()` function, but this time we divide the random values by 100 to ensure they are small deviations from the monthly beta.

```{r}
beta_dummy <- stock_panel_monthly |> 
  mutate(
    beta_monthly = rnorm(n(), mean = 1, sd = 1),
    beta_daily = beta_monthly + rnorm(n()) / 100
  )

dbWriteTable(
  tidy_finance_dummy,
  "beta", 
  beta_dummy, 
  overwrite = TRUE
)
```

### Dummy `compustat` table

To create dummy firm characteristics, we take all columns from the `compustat` table and create random numbers between 0 and 1 using the same trick as with the macro data tables. For simplicity, we set the `datadate` for each firm-year observation to the last day of the year, although it is empirically not the case. 

```{r}
relevant_columns <- tbl(tidy_finance, "compustat") |> 
  select(-c(gvkey, datadate, year)) |> 
  names()

commands <- unlist(
  map(
    relevant_columns, 
    ~rlang::exprs(!!..1 := runif(n()))
  )
)

compustat_dummy <- stock_panel_yearly |> 
  mutate(
    datadate = ymd(str_c(year, "12", "31")),
    !!!commands
  )

dbWriteTable(
  tidy_finance_dummy, 
  "compustat", 
  compustat_dummy,
  overwrite = TRUE
)
```

### Dummy `crsp_monthly` table

The `crsp_monthly` table only lacks a few more columns compared to `stock_panel_monthly`: the return `ret` drawn from a normal distribution, the excess return `ret_excess` with small deviations from the return, the shares outstanding `shrout` and the last price per month `altprc` both drawn from uniform distributions, and the market capitalization `mktcap` as the product of `shrout` and `altprc`.

```{r}
tbl(tidy_finance, "crsp_monthly")

crsp_monthly_dummy <- stock_panel_monthly |> 
  mutate(
    date = ceiling_date(month, "month") - 1,
    ret = pmax(rnorm(n()), -1),
    ret_excess = pmax(ret - runif(n(), 0, 0.0025), -1),
    shrout = runif(n(), 1, 50) * 1000,
    altprc = runif(n(), 0, 1000),
    mktcap = shrout * altprc
  ) |> 
  group_by(permno) |> 
  arrange(month) |> 
  mutate(mktcap_lag = lag(mktcap)) |> 
  ungroup()

dbWriteTable(
  tidy_finance_dummy, 
  "crsp_monthly",
  crsp_monthly_dummy,
  overwrite = TRUE
)
```

### Dummy `crsp_daily` table

The `crsp_daily` table only contains a `month` column and the daily excess returns `ret_excess` as additional columns to `stock_panel_daily`.  

```{r}
crsp_daily_dummy <- stock_panel_daily |> 
  mutate(
    month = floor_date(date, "month"),
    ret_excess = pmax(rnorm(n()), -1)
  )

dbWriteTable(
  tidy_finance_dummy,
  "crsp_daily",
  crsp_daily_dummy, 
  overwrite = TRUE
)
```

## Create bond dummy data

Lastly, we move to the bond data that we use in our books. 

### Dummy `mergent` data

To simulate data with the structure of Mergent FISD, we calculate the empirical probabilities of actual bonds for several variables: `maturity`, `offering_amt`, `interest_frequency`, `coupon`, and `sic_code`. We use these prbabilities to sample a small cross section of bonds with completely made up `complete_cusip`, `issue_id` and `issuer_id`. 

```{r}
number_of_bonds <- 100

mergent <- tbl(tidy_finance, "mergent") |> 
  collect()

maturities <- mergent |> 
  count(days = maturity - offering_date) |> 
  mutate(probability = n / sum(n))

amounts <- mergent |> 
  count(offering_amt) |> 
  mutate(probability = n / sum(n))

frequencies <- mergent |> 
  count(interest_frequency) |> 
  mutate(probability = n / sum(n))

coupons <- mergent |> 
  count(coupon) |> 
  mutate(probability = n / sum(n))

sic <- mergent |> 
  count(sic_code) |> 
  mutate(probability = n / sum(n))

mergent_dummy <- 1:number_of_bonds |> 
  map_df(
    function(x) {
      tibble(
        complete_cusip = str_to_upper(
          str_c(
            sample(c(letters, 0:9), 12, replace = TRUE), 
            collapse = ""
          )
        ),
      )
    }
  ) |> 
  mutate(
    maturity = sample(time_series_days, n(), replace = TRUE),
    offering_amt = sample(
      amounts$offering_amt, n(), 
      prob = amounts$probability, 
      replace = TRUE
    ),
    offering_date = maturity - sample(
      maturities$days, n(),
      prob = maturities$probability, 
      replace = TRUE
    ),
    dated_date = offering_date - sample(-10:10, n(), replace = TRUE),
    interest_frequency = sample(
      frequencies$interest_frequency, n(), 
      prob = frequencies$probability, 
      replace = TRUE
    ),
    coupon = sample(
      coupons$coupon, n(), 
      prob = coupons$probability, 
      replace = TRUE
    ),
    last_interest_date = pmax(maturity, offering_date, dated_date),
    issue_id = row_number(),
    issuer_id = sample(1:250, n(), replace = TRUE),
    sic_code = sample(
      sic$sic_code, n(), 
      prob = sic$probability, 
      replace = TRUE
    )
  )
  
dbWriteTable(
  tidy_finance_dummy, 
  "mergent", 
  mergent_dummy, 
  overwrite = TRUE
)
```

### Dummy `trace_enhanced` data

Finally, we create a dummy bond transaction data for the fictional CUSIPs of the dummy `mergent` data. We take the date range that we also analyze in the book and ensure that we have at least five transactions per day to fulfill a filtering step in the book. 

```{r}
start_date <- as.Date("2014-01-01")
end_date <- as.Date("2016-11-30")

bonds_panel <- crossing(
  mergent_dummy |> 
    select(cusip_id = complete_cusip),
  tibble(
    trd_exctn_dt = seq(start_date, end_date, "1 day")
  )
)

trace_enhanced_dummy <- bind_rows(
  bonds_panel, bonds_panel, 
  bonds_panel, bonds_panel, 
  bonds_panel) |> 
  mutate(
    trd_exctn_tm = str_c(
      sample(0:24, n(), replace = TRUE), ":", 
      sample(0:60, n(), replace = TRUE), ":", 
      sample(0:60, n(), replace = TRUE)
    ),
    rptd_pr = runif(n(), 10, 200),
    entrd_vol_qt = sample(1:20, n(), replace = TRUE) * 1000,
    yld_pt = runif(n(), -10, 10),
    rpt_side_cd = sample(c("B", "S"), n(), replace = TRUE),
    cntra_mp_id = sample(c("C", "D"), n(), replace = TRUE)
  ) 
  
dbWriteTable(
  tidy_finance_dummy, 
  "trace_enhanced", 
  trace_enhanced_dummy, 
  overwrite = TRUE
)
```

As stated in the introduction, the data does *not* contain any samples of the original data. We merely generate random numbers for all columns of the tables that we use throughout the books. You can find the database with the dummy data [here](https://1drv.ms/u/s!ArgZyNRLSLEXgspompdvaUcBIzfekw?e=3R1L0t).