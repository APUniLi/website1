---
title: "Tidy Market Microstructure"
author:
  - name: Björn Hagströmer
    url: https://hagstromer.org/
    affiliations:
      - name: Stockholm Business School
  - name: Niklas Landsberg
    url: https://www.kuleuven.be/wieiswie/en/person/00167120
    affiliations:
      - name: KU Leuven 
date: "2023-02-15"
description: A beginner's guide to market quality measurement in high-frequency data
image: thumbnail.png # TODO
image-alt: Description of the image # TODO
categories: 
  - Microstructure
  - R
---

<!-- TODO: Generally, I advice against setting warnings = FALSE in each chunk. We have global settings for the execution which should only be overwritten if necessary. I think for teaching it would be useful to comment on warnings should they occur (which I don't think is the case in your application). -->
Anyone active in market microstructure research *knows* that the devil is in the details. When clocks tick in microseconds and prices move in cents, a tiny delay or a small fee discount can make a huge difference for traders. In fact, they can even be the business model of an exchange. Although the nitty-gritty details are part of the fascination in our field, the empirical measurement of even the most conventional concepts can be diabolic. Referring to the *interest of brevity*, many journal articles defer the implementation details to an appendix, and even there they are often vague or incomplete. With the additional challenge of vast data requirements, new entrants into the field face a steep challenge. 

This blog post provides a beginner's guide to market quality measurement in high-frequency data, aiming to lower the barriers to entry into empirical market microstructure. We discuss economic considerations and show step-by-step how to code the most common measures of market liquidity and market efficiency. Because virtually all securities now trade at multiple venues, we also emphasize how market quality can account for market fragmentation. 

Is this guide really needed? Well, a recent paper by Menkveld et al., (2023)[^1] shows in full clarity that small variations in methodology can lead to large bottom-line differences in market quality. The authors assigned the same set of market microstructure hypotheses and the same data to 164 research teams. The variation in results across teams, the *non-standard error*, was found to be of a magnitude similar to the standard error. Many teams included seasoned professors, but past publication performance and seniority did not reduce the non-standard error. 

Another question is if the cumbersome high-frequency data analysis is really worth the effort? After all, there are numerous liquidity proxies based on daily data. The answer depends on the research question. First, low-frequency proxies are designed for low-frequency applications. For example, Amihud's (2002)[^2] popular proxy was originally proposed to be measured as an annual average. Most microstructure applications require liquidity measures at higher frequencies than that. Furthermore, recent evidence by Jahan-Parvar and Zikes (2023)[^3] show that many low-frequency proxies capture volatility rather than liquidity.

If we convinced you to take on the high-frequency data, here's what we offer. Table 1 lists the market quality measures that we cover, as well as their underlying data type. For some measures, we include several versions and discuss the differences between them. We organize the text by the data type, as that is a natural work flow. We start with liquidity measures based on tick-by-tick quote data, followed by the liquidity measures based on both trade and quote data. Finally, we look into efficiency and volatility measures, which require equispaced quote data. 

If we convinced you to take on the high-frequency data, here's what we offer. Table 1 lists the market quality measures that we cover, as well as their underlying data type. For some measures, we include several versions and discuss the differences between them. We organize the text by the data type, as that is a natural work flow. We start with liquidity measures based on tick-by-tick quote data, followed by the liquidity measures based on both trade and quote data. Finally, we look into efficiency and volatility measures, which require equispaced quote data. 

```{r}
#| echo=FALSE
library(kableExtra)

kable(matrix(c("Quoted bid-ask spread", "Quoted depth", "Effective bid-ask spread", "Trade volume", "Price impact", "Realized spread", "Return autocorrelation", "Realized volatility", "Variance ratio", 
			   "Liquidity", "Liquidity", "Liquidity", "Volume", "Liquidity", "Liquidity", "Efficiency", "Volatility", "Efficiency", 
			   "Quote tick data", "Quote tick data", "Quote and trade tick data", "Trade tick data", "Quote and trade tick data", "Quote and trade tick data", "Equispaced quote data", "Equispaced quote data", "Equispaced quote data",
			   1, 1, 2, 2, 2, 2, 3, 3, 3), ncol = 4, dimnames = list(NULL, c("Variable name", "Variable type", "Data type", "Section"))), caption = "Market quality variables and data types",
             booktabs = TRUE,
             longtable = FALSE)
```

<!-- TODO Provide direct links to sections in the table -->

Our focus is on the *practice* of empirical market microstructure. As such, we often motivate the coding choices with economic concepts. Other than that, readers interested in the economics underlying each metric are referred to introductory texts such as Campbell, Lo and MacKinlay (1997)[^4], Foucault, Pagano & Röell (2013)[^5] and Hasbrouck (2007)[^6]. 

**Programming.**
The coding here is in R, and the full code from the code chunks below is available in an R script, [here](https://www.dropbox.com/scl/fi/1lu8um14c569j841kmtyu/tidy-market-microstructure.R?rlkey=e3oy4wp7ngxv5fcq23bz8c24f&dl=1).

<!-- TODO Update script once blog post is published -->

The choice of language is of course a matter of taste. When deciding on the tools to use, what is important to consider is that the data files in this field are often huge. How can we process them without crashing our laptops? The example data used in this guide are tiny, but we wrote the code to be efficient when the number of observations per day go from thousands to millions.^[A previous guide to microstructure programming (in SAS) is provided by Boehmer, Broussard and Kallunki (2002): Boehmer, E., Broussard, J. P., & Kallunki, J. P. (2002). *Using SAS in financial research*. SAS Publishing.]

[Scheuch et al., 2023](https://tidy-finance.org/)[^7] use the `tidyverse` as
the main workhorse of the code. Instead, we rely on `data.table`. The rationale is that
`data.table` is much faster when working with the high dimensional data that characterizes microstructure. The speed benefits show when reading and sorting data, and really shine when applying functions to several groups.^[Part of the `data.table` speed advantage is that it uses parallel processing by default. Consequently, one has to be cautious to use it in parallelization, as improper specification may lead to [over-parallelization](https://towardsdatascience.com/parallelization-caveats-in-r-1-the-basics-multiprocessing-and-multithreading-performance-eb584b7e850e) and undermine the performance benefits.]

The basic `data.table` syntax takes the form `DT[i, j, by]`, where `i` and `j` can be used to subset or reorder rows and columns much in the same way as in a `data.frame`. In addition, `j` can be used to define new variables either for the full data set or group-wise as specified in the `by` argument. We will give examples and introduce more `data.table` syntax as we go along. A more complete intro to the package can be found [here](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).

This blog post comes in two flavors. We provide all code in the well-established `data.table` syntax. We also show how to achieve essentially the same virtues in speed based on the `tidyverse` syntax based on an implementation based on `dtplyr`. `dtplyr` provides a `data.table` backend for `dplyr`. Thus, readers familiar with the ´tidyverse´ can use ´dtplyr´ to translate their code to the equivalent, but usually much faster, `data.table` code.

**Data.**
We use one week of quote and trade data for one stock (PRU, [*Prudential Financial Inc.*](https://en.wikipedia.org/wiki/Prudential_plc)). There is nothing special about that stock -- it is chosen to be representative of large-cap stocks in general. As is typical these days, PRU is traded at several exchanges as well as off-exchange. In addition to the listing venue (London Stock Exchange, LSE), the data includes trades and quotes from the competing venue Turquoise (TQE), as well as some dark pool trades. 

The data is loaded automatically in the scripts provided. It can also be downloaded manually [here](https://hagstromer.org/other/example-data/).
<!-- TODO: activate link -->
The data are extracted from a database called *Tick History*, available from the London Stock Exchange Group (LSEG). We are grateful to LSEG for giving us permission to post the example data in the public domain. **It is to be used for educational purposes only.** The data set is incomplete in that it excludes quotes and trades from numerous venues where the same stock is traded. For the illustrative purpose here, however, two exchanges suffice.

<!-- TODO: To me, it seems most natural to load the entire data (both datasets) first and to give a short overview of the columns, maybe also row number (or date range). Questions I have: Size is in 100s? What is the timezone in the Exchange and Date-Time columns (and related: is GMT Offset actually necessary then?) -->
<!-- TODO: The column names of this dataset are a great example for everything one should avoid: special symbols, spaces, ... Further, some columns (Domain and Type) seem completely redundant. I suggest to rename every column first in a useful (lowercase_style) fashion. Also: instead of ticker, venue seems more appropriate to me, for the potential case of multiple tickers in the application of the reader, we may separate the RIC column into ticker and venue. I would furthermore not stick to L or l.TQ but rather to lse and turquoise (or something similar).  -->

# Quote-based liquidity
The most well-known liquidity measure is probably the quoted bid-ask spread. It measures the cost of a hypothetical roundtrip trade, where an investor buys one share only to immediately sell it again. Even though such a trade is not economically sensible, the quoted spread offers a liquidity snapshot that is accessible at any time when the market is open. All that is required is data on the best bid and ask prices. Such data typically also comes with the number of shares that is available at each price, allowing for a similar snapshot of market depth. That is, the maximum size that can be traded at the best price. 

Quotes are expressions of interests to buy or sell securities, observable before a trading decision is made. The quotes that we access in the examples below come from limit order book (LOB) markets, but they may just as well be obtained from dealers in request for quote systems, or shouted by specialists in a trading pit. 

## Quote data inspection and preparation
Let's dive into it. The following code loads the required packages and imports the data.

::: {.panel-tabset group="language"}
## tidyverse (using dtplyr)

```{r}
#| message=FALSE,
#| warning=FALSE

# install.packages("data.table")
# install.packages("dtplyr")
# install.packages("tidyverse")
library(data.table)
library(dtplyr)
library(tidyverse)
```

```{r}
#| cache=TRUE
tv_quotes <- vroom::vroom("http://tinyurl.com/8wyfk26c", 
                          col_types = list(`Exch Time` = col_character()))

tv_quotes <- lazy_dt(tv_quotes)

tv_quotes <- tv_quotes|> 
  transmute(ticker = `#RIC`,
            date = as.Date(`Date-Time`),
            time = hms::as_hms(`Exch Time`),
            time = hms::as_hms(time + dhours(`GMT Offset`)),
            bid_price = `Bid Price`, 
            ask_price = `Ask Price`,
            bid_depth = `Bid Size`, 
            ask_depth = `Ask Size`,
  )

```

## data.table

<!-- TODO: Your code is very clean, I do not think that inline codes are necessary at all. My general suggestion would be to either describe specifics in the text or to omit an explanation.  -->

```{r}
# install.packages("data.table")
library(data.table)
```

```{r}
#| cache=TRUE
quotes_url <- "http://tinyurl.com/8wyfk26c"
quotes <- fread(quotes_url)
```

:::

<!-- TODO: Replaced the long URL for now with a tiny url such as http://tinyurl.com/8wyfk26c, later with Björns domain  -->

<!-- TODO: Are you sure that curl is necessary? I did not explicitly install it but can read the file as a data.table -->

It is good practice to start with an overview of the data. 

::: {.panel-tabset group="language"}
## tidyverse (using dtplyr)

## data.table
<!-- TODO: I suggest not to print the complete quotes data.table but to move on to the subset selection directly (removed the original chunk) -->

If the console window is not wide enough to fit the whole `data.table`, we can subset the variables in the same way as for a `data.frame`. Let's start by focusing on the variables that refer to data types, ticker, dates, and timestamps.

```{r Quote data subset}
quotes[, c("#RIC", "Domain", "Date-Time", "GMT Offset", "Type", "Exch Time")]
```

When getting to know a new data set, the `table` function is great to understand the scope of variation in categorical variables. In this case, it shows us that there are two tickers, one for LSE and one for TQE. The former is more active, with 165,261 quote updates. There is no variation at all in the `Domain` variable -- all observations take the value "Market Price". In the same way, you can find that the variable `Type` always takes the value "Quote".

```{r Quote data overview}
table(quotes$`#RIC`, quotes$Domain)
```

<!-- TODO: Not sure Domain is needed here, table(quotes$`#RIC`) is also doing the job.   -->
:::

**Dates.**
In the raw data, dates and times are embedded in the same variable, `Date-Time`, but for us it is useful to have them in separate variables. Accordingly, we now define the variable `date`.

We defer minor coding notes, such as those about `fread` above, to comments in the code blocks. In the main text, we keep notes that require more explanation, or that are of economic interest. For example, we note here that the operator `:=` is used to define a new variable (`date`) within an existing `data.table`, such as `quotes` in this example. Also, when working within a `data.table`, it suffices to refer to the variable name, `Date-Time`, when defining the new variable. This is different to a `data.frame`, for which we would have to write ```quotes$`Date-Time` ```. 

```{r Extracting dates}
quotes[, date := as.Date(`Date-Time`)]
```

The table output shows that there are five trading days in the sample. The number of quote observations per stock-date varies between roughly 16,000 and 40,000. The tendency that LSE has more quotes than TQE is consistent across trading days.

```{r Overview stocks and days}
table(quotes$`#RIC`, quotes$date)
```

**Timestamps.**
The accuracy of timestamps is important in microstructure data. Timestamps are often matched between quote and trade data (that are not necessarily generated in the same systems), or between data from exchanges in different locations. It is thus essential to be aware of latencies that may arise due to, e.g.,  geography or hardware. 

In the data at hand, there are two timestamps for each observation. The `Exch Time` variable is assigned by the exchange at the time an event is recorded in the exchange matching engine. The `Date-Time` variable is the timestamp assigned on receipt at the data vendor, which is by definition later than the `Exch Time`. Exchanges that are located at different distances from the vendor are likely to have different reporting delays. It is then up to the researcher to determine which timestamp to rely on, and the choice may depend on the research question. In our setting, as we are going to measure liquidity across venues, it is important that the time stamps across venues are comparable. Based on that each exchanges has strong incentives to assign accurate time stamps (to cater for low-latency participants), we choose to work with the `Exch Time` variable.  

For US equity markets, the timestamp may reflect the matching engine time, the time when the national best bid and offer updates, or the participant timestamp. For discussions about which of these to use, see [Bartlett & McCrary (2019)](https://doi.org/10.1016/j.finmar.2019.06.003)^[Bartlett III, R. P., & McCrary, J. (2019). How rigged are stock markets? Evidence from microsecond timestamps. *Journal of Financial Markets*, *45*, 37-60.], [Holden, Pierson & Wu (2023)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441422)^[Holden, C. W., Pierson, M., & Wu, J. (2023). In the Blink of an Eye: Exchange-to-SIP Latency and Trade Classification Accuracy. *Working paper*.], and [Schwenk-Nebbe (2021)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3984827)^[Schwenk-Nebbe, S. (2022). The Participant Timestamp: Get The Most Out Of TAQ Data. *Working paper*.].

When working with timestamps in microstructure applications, it is useful to convert them into a numeric format. Dedicated time formats (e.g., `xts`) are imprecise when it comes to sub-second units (see [here](https://stackoverflow.com/questions/62366490/timestamp-r-sequence-milliseconds) and [here](https://lubridate.tidyverse.org/reference/round_date.html)). We thus convert the timestamps to the number of seconds elapsed since midnight. For example, 8:30 am becomes 8.5 x 3,600 = 30,600, because there are 3,600 seconds per hour.

The code below converts `exchange_time` to numeric and adjusts it for daylight saving using the `GMT Offset` variable (which is measured in hours). Note the use of curly brackets `{...}` in the definition of the time variable, which allows us to temporarily define the variable `time_elements` within the call. Once the operation is complete, the temporary variable is automatically deleted. The variable that is retained should always be returned as a list, hence `list(time)`.

```{r Quote time conversion}
quotes[, time := {
	time_elements = strsplit(`Exch Time`, split = ":")
	time_elements = do.call(rbind, time_elements)
	time = as.numeric(time_elements[, 1]) * 3600 + 
		   as.numeric(time_elements[, 2]) * 60 + 
		   as.numeric(time_elements[, 3]) +
		   `GMT Offset` * 3600
	list(time)}]
```

Having made sure that dates and times are in the desired format, we can save space by dropping several variables that are not further needed. 

```{r Cleaning up quotes}
quotes[, c("Date-Time", "Domain", "Exch Time", "GMT Offset", "Type") := NULL]
```

**Prices and depths.**
There are four economic variables in the quote data: the bid and ask prices (`Bid Price` and `Ask Price`) and the depth available at those prices (`Bid Size` and `Ask Size`). A LOB holds orders at numerous different prices, but in this data set we only see the best price level on each side, the highest bid price and the lowest ask price. This is sufficient for most quote-based market quality measures. 

We prefer to use the term *depth* to refer to the number of shares *quoted*, reserving the term *size* to the number of shares in a single trade. We rename the variables accordingly. While we're at it, we remove hashes, dashes, and spaces from the variable names, making it easier to work with them in R.

```{r Renaming quote variables}
setnames(quotes, 
		 old = c("#RIC", "Bid Price", "Bid Size", "Ask Price", "Ask Size"), 
		 new = c("ticker", "bid_price", "bid_depth", "ask_price", "ask_depth"))
```

An important insight about LOB quotes is that they remain valid until cancelled, executed or modified. Whenever there is a change to the prevailing quotes, a new quote observation is added to the data. It is irrelevant if the latest quote is from the previous millisecond or from the previous minute -- it remains valid until updated. It is thus economically meaningful to forward-fill quotes that prevailed in the previous period. Trades, in contrast, are agreed upon at a fixed point in time and do not convey any information about future prices or sizes. They should not be forward-filled, see [Hagströmer and Menkveld (2023)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4356262)^[Hagströmer, B., & Menkveld, A. J. (2023). Trades, quotes, and information shares. Working paper.].

We use the `nafill` function to forward-fill, with the option `type = "locf"` (last observation carried forward) specifying the type of filling. The `.SD` inside the `lapply` command tells `data.table` to repeat the same operation for the set of variables specified by the option `.SDcols`. 

When forward-filling quote data, it is important to restrict the procedure to the same date, stock and trading venue. For example, quotes should never be forward-filled from one day to the next, and not from one venue to another. This is ensured with the `by` argument, which specifies that the operation is to be done within each combination of tickers and dates. 

In summary, whereas the `.SD` applies the same function across a set of variables (columns), the `by` applies it across categories of observations (rows). The same outcome could be achieved with for loops, but in R, that would be much slower. We discuss that further below. 

Note here how the `:=` notation can be used to define multiple variables, using a vector of variable names on the left-hand-side and a function (in this case `lapply`) that returns a list of variables on the right-hand-side. Note also that when referring to multiple variable names within the `data.table`, they are specified as a character vector. 

::: {.panel-tabset group="language"}

## tidyverse (using dtplyr)

```{r}
tv_quotes <- tv_quotes |>
  group_by(ticker, date) |>
  fill(matches("bid|ask")) |> 
  ungroup()
```

## data.table
```{r Forward-filling quotes}
lob_variables <- c("bid_price", "bid_depth", "ask_price", "ask_depth")

quotes[, (lob_variables) := lapply(.SD, nafill, type = "locf"),	.SDcols = (lob_variables), 
       by = c("ticker", "date")]
```

:::
When measuring market quality, it is common to filter out periods that may be influenced by call auctions. At the LSE, stocks trade continuously between 08:00 am and 4:30 pm, local time. The trading is opened and closed with call auctions, and there is an intraday call auction at 12 pm. To avoid the impact of the auctions, we exclude quotes before 8:01 am and after 4:29 pm. We do *not* exclude quotes recorded around the intraday call auction, but set them as missing (`NA`).  If they were instead deleted, it would give the false impression that the last observation before the excluded quotes was still valid.

::: {.panel-tabset group="language"}
## tidyverse (using dtplyr)

```{r}
tv_quotes <- tv_quotes |>
  filter(time > hms::as_hms("08:01:00"), 
         time < hms::as_hms("16:29:00"))

tv_quotes <- tv_quotes |>
  mutate(across(matches("bid|ask"), 
                ~if_else(time > hms::as_hms("11:59:00") & 
                         time < hms::as_hms("12:03:00"), NA_real_, .)))
```

## data.table
```{r Opening hours}
open_time <- 8 * 3600
close_time <- 16.5 * 3600
quotes <- quotes[time > (open_time + 60) & time < (close_time - 60)]

intraday_auction_time <- 12 * 3600
quotes[time > (intraday_auction_time - 60) & time < (intraday_auction_time + 3 * 60), 
	   (lob_variables)] <-  NA
```

::: 

**Screening.**
Before turning to the market quality measurement, it is a good habit to check that the quote observations make economic sense. One way to do that is to study the variation in the bid-ask spread. The *nominal* bid-ask spread is defined as the difference between the ask price and the bid price, $quoted\_spread^{nom}= P^A - P^B$. In our data, prices are quoted in units of 0.01 British Pounds (GBP). Due to the tick size, the nominal spread is a discrete quantitative variable. We can thus get a quick overview by plotting a histogram or by tabulating the values it takes on.

<!-- TODO: specifically define P^A and P^B -->
<!-- TODO: In our experience, visualizations are key, also within teaching. I strongly suggest we replace the histogram either with a ggplot2 visualization or at least a histogram with clear axis (it cannot be seen if there are negative values). Could also be nice to show the histogram per venue to make the follow-up table obsolete.  -->
In the output below, we note that the spread is strictly positive, as it should be whenever the market is open. The TQE occasionally has wider spreads than the LSE, but there are no extraordinarily large spreads. The maximum spread, GBP 0.11, corresponds to around 0.7% in relative terms. 

Also, it is clear from the table that the tick size, the minimum price increment that is allowed when quoting prices, is 0.5 (that is, GBP 0.005). Most spreads are quoted at one or two ticks.

::: {.panel-tabset group="language"}
## tidyverse (using dtplyr)

```{r}
tv_quotes <- tv_quotes |> transmute(ticker, spread= ask_price - bid_price) |> as_tibble()
ggplot(tv_quotes, aes(spread)) +
  geom_histogram(bins = 100) +
  labs(title = "Histogram of nominal bid-ask spread",
       x = "Nominal bid-ask spread")

```

## data.table
```{r Quoted spread histogram}
hist(quotes$ask_price - quotes$bid_price, breaks = 100, 
	 main = "Histogram of nominal bid-ask spread", xlab = "Nominal bid-ask spread")

table(quotes$ticker, quotes$ask_price - quotes$bid_price)
```

:::
**Data export.**
As the quote data processing is often computationally extensive, it is useful to save a clean version of the `quotes` to disk. 
```{r}
save(quotes, file = "temp_quotes.Rdata")
```

<!-- TODO: Suggest to remove the required data folder structure for the reader such that the code can run seamlessly. -->
<!-- TODO: Suggest to use a generally readable file format such as feather or a zipped csv (.csv.gz) -->

## Liquidity measures

With all the data preparation done, we are ready for the actual liquidity measurement. For comparisons across stocks, it is useful to relate the nominal spread to the fundamental value of the security. This is done by the *relative* quoted bid-ask spread, defined as $quoted\_spread^{rel} = (P^A - P^B)/M$, where $M$ is the midpoint (also known as the midprice; defined as the average of the best bid and the best ask prices). One can argue that the midpoint is not always representative of the fundamental value, but it has the strong advantage that it is continuously available in the quote data. 

<!-- TODO: Define M -->

```{r Fundamental value}
quotes[, midpoint := (bid_price + ask_price) / 2]
```

The quoted spread can also be measured relative to the tick size. In an open market, the spread can never be below one tick. We refer to the average number of ticks in the bid-ask spread as the *tick* spread, $quoted\_spread^{tic} = (P^A - P^B) / tick\_size$. The tick size in the example data is half a cent.

<!-- TODO: could be useful to define tick_size here. -->
<!-- TODO: I suppose the tick size is half a cent at BOTH venues? -->

Another dimension of quoted liquidity is the market depth. We measure the average depth quoted at the best bid and ask prices. It is defined as $quoted\_depth = (Q^A + Q^B)/2$, where $Q^A$ and $Q^B$ are the depths available at the bid and ask prices. 

In the code below, we store the liquidity measures in a new `data.table` named `quotes_liquidity`. This is because the new variables are averages, observed on a ticker-date frequency, as opposed to the tick-by-tick frequency of the `quotes` object. We multiply the quoted spread by 10,000 to express it in basis points, and divide the quoted depth by 100,000 to express it in thousand GBP.

The output shows that the liquidity is higher at the LSE than at the TQE. Both in nominal and relative terms, the spreads are somewhat tighter at the LSE, and there is more than three times more depth posted at the LSE.

```{r Ex ante liquidity}
tick_size <- 0.5

quotes_liquidity <- quotes[, {
	quoted_spread = ask_price - bid_price
	list(quoted_spread_nom = mean(quoted_spread, na.rm = TRUE),
		 quoted_spread_rel = mean(quoted_spread / midpoint, na.rm = TRUE),
		 quoted_spread_tic = mean(quoted_spread / tick_size, na.rm = TRUE),
		 quoted_depth = mean(bid_depth * bid_price + ask_depth * ask_price, 
		                     na.rm = TRUE) / 2)},
	by = c("ticker", "date")]

quotes_liquidity[, 
	list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),
		 quoted_spread_rel = round(mean(quoted_spread_rel) * 1e4, digits = 2),
		 quoted_spread_tic = round(mean(quoted_spread_tic), digits = 2),
		 quoted_depth = round(mean(quoted_depth) * 1e-5, digits = 2)), 
	by = "ticker"]
```

<!-- TODO: I suggest to scale by 1e4 and 1e-5 in the first step and to round all columns at once in the second step -->

**Duration-weighted liquidity.**
The output above are straight averages, implying an assumption that all quote observations are equally important. But whereas some quotes remain valid for several minutes, many don't last longer than a split-second. For this reason, it is common to either sample the quote data in fixed time intervals (such as at the end of each second), or to weight the observations by their duration. The duration is the time that a quote observation is in force. That is, the time elapsed until the next quote update arrives. We show the duration-weighted approach in the code below (for guidance on how to get the quotes at the end of each second, see Section 3.1).

Note that the `duration` variable is obtained separately for each ticker and date. Even if we are interested in the average liquidity across dates, it is important to partition by each ticker and date to avoid that duration is calculated overnight (resulting in a huge weight with negative sign). Except for replacing the `mean` function with the `weighted.mean`, the code below is very similar to that above.

<!-- TODO: confused about the "negative sign". -->
In the output, we note that the differences between the duration-weighted and the equal-weighted liquidity averages are small. Nevertheless, we consider the duration-weighted average more appropriate because it is not sensitive to shortlived price and depth fluctuations.

```{r Duration weighted ex ante liquidity}
quotes[, duration := c(diff(time), 0), by = c("ticker", "date")]

quotes_liquidity_dw <- quotes[!is.na(duration), {
	quoted_spread = ask_price - bid_price
	list(quoted_spread_nom = weighted.mean(quoted_spread,
	                                       w = duration, na.rm = TRUE),
		 quoted_spread_rel = weighted.mean(quoted_spread / midpoint, 
		                                   w = duration, na.rm = TRUE),
		 quoted_spread_tic = weighted.mean(quoted_spread / tick_size, 
		                                   w = duration, na.rm = TRUE),
		 quoted_depth = weighted.mean(
		   bid_depth * bid_price + ask_depth * ask_price, 
		 					    w = duration, na.rm = TRUE) / 2
		 )},
    by = c("ticker", "date")]

quotes_liquidity_dw[, 
	list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),
		 quoted_spread_rel = round(mean(quoted_spread_rel * 1e4), digits = 2),
		 quoted_spread_tic = round(mean(quoted_spread_tic), digits = 2),
		 quoted_depth = round(mean(quoted_depth * 1e-5), digits = 2)), 
	by = "ticker"]
```

## Consolidated liquidity in fragmented markets
With the competition between exchanges, liquidity is dispersed across venues. For example, if there is a change to the market structure at the LSE, it is typically not sufficient to analyze liquidity at LSE alone. If liquidity is reduced at the LSE, it may simultaneously be boosted at the TQE. To assess the overall market quality, which may be most relevant for welfare, it is often necessary to consider the *consolidated* liquidity. 

In Europe, the consolidated liquidity is sometimes referred to as the European Best Bid and Offer (EBBO). The terminology follows in the footsteps of the US market, where the *National* Best Bid and Offer (NBBO) is transmitted to the market on continuous basis. To obtain the EBBO, one needs to merge the LOB data from each relevant venue, and then determine the EBBO prices and depths. In the code below, we show step-by-step how to do that.

**Retaining only the last quote update in each interval.**
Quote updates tend to cluster and it is common that several observations have identical timestamps. Multiple observations at one timestamp can be due to several investors responding to the same events, or that one market order leads to several LOB updates as it is executed against multiple limit orders. When matching quotes across venues, we need to restrict the number of observations per unit of time to one. There is no sensible way to distinguish observations with identical timestamps. In lack of a better approach, we retain the last observation in each interval. 

```{r Last entry per unit of time}
quotes <- quotes[!duplicated(quotes, fromLast = TRUE, by = c("ticker", "date", "time"))]
```

**Merging quotes from different venues.**
We are now ready to match the quotes from the two exchanges. First, we create separate quote data sets for the two exchanges. Second, we merge the two by matching on date and time. Third, we forward-fill quotes from both venues, such that for each LSE quote we know the prevailing TQE quote, and vice versa. The validity of this is ensured by the option `sort = TRUE` in the `merge` function, which returns a data.table that is sorted on the matching variables.

<!-- TODO: Just in case you ever used this for teaching: I wonder if there is a simple illustration of this process. -->

```{r Merging LSE and TQE}
venues <- c("_lse", "_tqe")

quotes_lse <- quotes[ticker == "PRU.L", .SD, .SDcols = c("date", "time", lob_variables)]
quotes_tqe <- quotes[ticker == "PRUl.TQ", .SD, .SDcols = c("date", "time", lob_variables)]

quotes_ebbo <- merge(quotes_lse, quotes_tqe, 
                     by = c("date", "time"), 
                     suffixes = venues, 
                     all = TRUE, sort = TRUE)

# Forward-fill the quoted prices and depth for each exchange 
local_lob_variables <- paste0(lob_variables, rep(venues, each = 4))

quotes_ebbo[, (local_lob_variables) := lapply(.SD, nafill, type = "locf"), 
			.SDcols = (local_lob_variables),
	by = "date"]
```

The best bid price at each point in time is the *maximum* of the best bid at the LSE and the best bid at the TQE. Similarly, the best ask is the *minimum* of the best ask prices at the two venues. We calculate the best bid using the parallel maxima function, `pmax`, which returns the highest value in each row. The best ask is obtained in the same way, using the parallel minima function, `pmin`. 

Note that it would also be possible to obtain the EBBO using a `for` loop, checking row-wise which is the highest bid and lowest ask. When working with large data sets in R, however, loops become extremely slow. It is strongly encouraged to run vectorised operations for the whole column at once (like we do here), or to apply functions repeatedly to blocks of data (like we have done several times above).

We obtain the depth at the best prices by summing the depth of the individual venues. When doing this, we should only consider both venues at times when they are both at the best price. When the two venues have the same best bid, for example, we calculate the consolidated bid depth as the sum of the two. To code this, we use the feature that a logical variable (with values `FALSE` or `TRUE`; such as `bid_price_lse  == best_bid_price`) works as a binary variable (with values `0` or `1`) when used in multiplication.

```{r EBBO}
# Obtain the EBBO prices and depths
quotes_ebbo[, best_bid_price := pmax(bid_price_lse, bid_price_tqe)]
quotes_ebbo[, best_ask_price := pmin(ask_price_lse, ask_price_tqe)]
quotes_ebbo[, best_bid_depth := bid_depth_lse * (bid_price_lse == best_bid_price) + 
				bid_depth_tqe * (bid_price_tqe == best_bid_price)]
quotes_ebbo[, best_ask_depth := ask_depth_lse * (ask_price_lse == best_ask_price) + 
				ask_depth_tqe * (ask_price_tqe == best_ask_price)]

# Drop local exchange variables and objects
quotes_ebbo[, (local_lob_variables) := NULL]
rm(quotes_lse, quotes_tqe, quotes, local_lob_variables)
```

**Fundamental value.**
We can now obtain EBBO midpoints, as a proxy of fundamental value that factors in liquidity posted at multiple exchanges.

```{r EBBO fundamental value}
# Calculate EBBO midpoints
quotes_ebbo[, midpoint := (best_bid_price + best_ask_price) / 2]
```

**Screening.**
As above, we check that the EBBO quotes are economically meaningful by tabulating the counts of nominal spread levels. This exercise shows us that the consolidated spread is not strictly positive. There are numerous cases of zeroes, known as *locked* quotes, and also many negatives, referred to *crossed* quotes. This is possible because orders at the LSE and the TQE are never executed against each other -- it takes arbitrageurs to step in and act on crossed markets. Locked and crossed spreads are not uncommon in consolidated data. For an analysis of the incidence in US markets, see [Shkilko, van Ness & van Ness, 2008](https://www.sciencedirect.com/science/article/pii/S1386418107000031?casa_token=GIqJQiPKyisAAAAA:ZVbaf4SPC0IzxFFLWqgI2papSw2MrEf_lPXCL9OlT_ZnKgA6-pGTl4EYisKuIUaFdx8JY1p3d7o)^[Shkilko, A. V., Van Ness, B. F., & Van Ness, R. A. (2008). Locked and crossed markets on NASDAQ and the NYSE. *Journal of Financial Markets*, *11*(3), 308-337.].

It is also notable from the table that the maximum consolidated spread is 2.5, as compared to the spreads of up to 11 recorded in the single-venue analysis. By definition, the EBBO quoted spread is never wider than at the single venues. 

```{r EBBO quoted spread histogram}
# Output an overview of the EBBO nominal quoted bid-ask spread 
table(quotes_ebbo$best_ask_price - quotes_ebbo$best_bid_price)
```

Observations with locked or crossed quotes are usually excluded when measuring market quality. It is also common to exclude bid-ask spread observations that are unrealistically high. We have no such cases in this sample, but, for illustration, we include a filter that would capture spreads wider than 5%.

<!-- TODO: I guess you mean relative spreads wider than 500 basis points? -->

In the procedure below, we flag the problematic quotes, but we do not exclude them. If they were deleted, it would imply that the last observation before the excluded spread was still in force, which may mislead subsequent analysis. 

The output shows that 2.90% of the quote observations are locked, while 0.06% are crossed. 

```{r EBBO quote filters}
# Flag problenatic consolidated quotes
quotes_ebbo[, c("crossed", "locked", "large") := {
	quoted_spread = (best_ask_price - best_bid_price)
    list(quoted_spread < 0,
	     quoted_spread == 0,
	     quoted_spread / midpoint > 0.05)}]

# Count the incidence of the consolidated quote flags
quotes_ebbo_filters  <- quotes_ebbo[, 
    list(crossed = mean(crossed, na.rm = TRUE),
         locked = mean(locked, na.rm = TRUE),
         large = mean(large, na.rm = TRUE))]

quotes_ebbo_filters[, 
	lapply(.SD, round, digits = 4), .SDcols = c("crossed", "locked", "large")]
```

<!-- TODO: consider multiplying by 100 for interpretation in percent -->

**Consolidated liquidity measures.**
We obtain duration-weighted measures of consolidated liquidity in the same way as above. The only difference here is that we subset the quotes to filter out crossed and locked markets. 

The consolidated relative quoted bid-ask spread is 5.59 basis points, as compared to 5.70 and 6.69 basis points at LSE and TQE locally. The consolidated depth, 3.14 million GBP, is somewhat lower than the sum of the local depths seen above. This is to be expected, as some of the local depth is posted at price levels that are inferior to the EBBO.

```{r EBBO ex ante liquidity}
quotes_ebbo[, duration := c(diff(time), 0), by = "date"]

# Note that the subset used here excludes crossed and locked quotes
quotes_liquidity_ebbo_dw <- quotes_ebbo[!crossed & !locked & !large, {
	quoted_spread = best_ask_price - best_bid_price
	list(quoted_spread_nom = weighted.mean(quoted_spread, 
	                                       w = duration, na.rm = TRUE),
		 quoted_spread_rel = weighted.mean(quoted_spread / midpoint, 
		                                   w = duration, na.rm = TRUE),
		 quoted_spread_tic = weighted.mean(quoted_spread / tick_size,
		                                   w = duration, na.rm = TRUE),
		 quoted_depth = weighted.mean(best_bid_depth * best_bid_price + best_ask_depth * 
		 						  best_ask_price, 
		 						  w = duration, na.rm = TRUE) / 2)}, 
	by = "date"]

# Output the liquidity measures, averaged across the five trading days 
quotes_liquidity_ebbo_dw[, 
	list(quoted_spread_nom = round(mean(quoted_spread_nom), digits = 2),
		 quoted_spread_rel = round(mean(quoted_spread_rel * 1e4), digits = 2),
		 quoted_spread_tic = round(mean(quoted_spread_tic), digits = 2),
		 quoted_depth = round(mean(quoted_depth * 1e-6), digits = 2))]
```

## Data export
As we will reuse the consolidated quotes in the applications below, we save the `quotes_ebbo` object to disk. 
```{r EBBO data export}
# Export the consolidated quotes
save(quotes_ebbo, file = "quotes_ebbo.Rdata")
```

# Trade-based liquidity
A shortcoming of the quoted bid-ask spread is that it can only be measured for liquidity that is visible in the quote data. That is not always the full picture. For example, limit order books typically allow hidden liquidity, and dealers often offer price improvements on their quotes. Outside the exchanges, trades execute extensively at venues without quotes, such as dark pools. The effective bid-ask spread is a good alternative because it uses the trade price (the *effective* price) instead of the quoted price, and benchmarks it to the spread midpoint holding at the time of the trade. 

Whenever there is a trade, the price tends to move in the direction of the trade. This is known as *price impact*, and is an aspect of market depth. Whereas the quoted depth measure above captures depth in a mechanic sense (how much can you trade without changing the price), *price impact* also captures other traders' response to a trade. The response may be either that the LOB is refilled (if the trade is viewed as uninformative), or that liquidity is withdrawn (if the trade is viewed as a signal of more to come).

For the market makers, who makes a living from the bid-ask spread, price impact undermines the profits. A more relevant measure for them may be the *realized* spread, which accounts for price impact by evaluating the trade price relative to the midpoint some time later. The three trade-based liquidity measures are closely related: the realized spread equals the effective spread minus the price impact.

## Trade data inspection and preparation
We load the trade data and view the data structure. The ticker, date, and time variables follow the same structure as in quotes data, so we can proceed with the same date and time transformations as above.
```{r Load trade data}
# Load and view the trade data
trades_url <- "https://www.dropbox.com/scl/fi/nirm5vlyun7c6t6991hvb/PRU_trades.csv?rlkey=tapgx2hs4d6zd011oininuuep&dl=1"

trades <- fread(trades_url)

trades
```

<!-- TODO: I advice to put this part to the beginning as well.  -->
<!-- TODO: Also here I would clean the column names as a first thing while quickly explaining what they mean. Drop the ones you do not need.  -->

**Dates and timestamps.**
As above, we filter out the first and last minute of continuous trading, as well as the minutes surrounding the intraday auction. Note here that filtered trades are excluded, not just set as missing. This is OK, because we won't do any forward-filling for the trade data.

```{r Trade dates and times}
# Extract dates
trades[, date := as.Date(`Date-Time`)]

# Convert time stamps to numeric format, expressed in seconds past midnight
trades[, time := {
	time_elements = strsplit(`Exch Time`, split = ":")
	time_elements = do.call(rbind, time_elements)
	time = as.numeric(time_elements[, 1]) * 3600 + 
		   as.numeric(time_elements[, 2]) * 60 + 
		   as.numeric(time_elements[, 3]) +
		   `GMT Offset` * 3600
	list(time)}]

# TODO: Suggest to remove this second occurence of open_time, close_time, and intraday_auction_time.
open_time <- 8 * 3600
close_time <- 16.5 * 3600
intraday_auction_time <- 12 * 3600
trades <- trades[time > (open_time + 60) & 
				 time < (close_time - 60) & 
				 (time < (intraday_auction_time - 60) | 
				  time > (intraday_auction_time + 3 * 60))]

# Drop variables
trades[, c("Date-Time", "Domain", "Exch Time", "GMT Offset", "Type") := NULL]
```

**Trade variables.**
There are three more variables: `Price`, `Volume`, and `MMT Classification`. As discussed above, we refer to the number of shares executed in a trade as "size" (we reserve the term "volume" to the sum of trade sizes in a given interval). We rename the `Volume` variable accordingly, and also alter the other variable names to make them easier to work with in R.

```{r Renaming trade variables}
# Rename variables 
setnames(trades, 
	old = c("#RIC", "Price", "Volume", "MMT Classification"), 
	new = c("ticker", "price", "size", "MMT"))
```

A line plot offers a good overview of price data. In the figure below, we see that the trade price data is plagued by outliers that seem to be close to zero.

```{r Trade price plot}
# Plot the trade prices
plot(trades$price, type = "l", xlab = "Trade index", ylab = "Price", 
	 main = "Trade prices")
```

There are various ways to handle outliers, but the best way is to understand them. In trade data sets, there is often information provided about the trade circumstances (for quote observations, such information is often sparse). In the current data set, the best piece of supporting information is the MMT code. MMT, short for Market Model Typology, is a rich set of flags reported for trades in Europe in recent years. For details, see the website of the [Fix Trading Community](https://www.fixtrading.org/mmt/).

The MMT code is a 14-character string, where each position corresponds to one flag. The first character specifies the type of market mechanism. For example, "1" tells us that the trade was recorded in an LOB market, "3" indicates dark pools, "4" is for off-book trading, and "5" is for periodic auctions. The second character indicates the trading mode, where, for example, continuous trading is indicated by "2". 

An overview of the populated values shows that the LOB market with continuous trading ("12") is by far the most common combination remaining after applying the filters above, followed by dark pool continuous trading ("32").  

```{r MMT overview}
# Output an overview of the MMT codes
# The function `substr` is used here to extract the first two characters of the MMT code
table(substr(trades[, MMT], start = 1, stop = 2))
```

To see if the outliers are caused by a specific trade type, we condition the price plot on on-exchange continuous trading. The output shows that this removes the outliers.

```{r Trade filtering MMT plot}
# Define a subset with continuous trades only
LOB_continuous_trades <- substr(trades[, MMT], start = 1, stop = 2) == "12"

# Plot the prices of continuous trades
plot(trades$price[LOB_continuous_trades], type = "l", xlab = "Trade index", 
	 ylab = "Price", main = "Trade prices")
```

Further detective work reveals that the trade price outliers are not erroneous, they are just stated in GBP rather than in hundred GBP. This is clear because the outliers are priced 100 times lower than the other trades. Apparently, some dark pool and off-exchange trades follow a different price reporting convention. 

```{r Trade price outliers}
trades[price < 100]
```

The take-away from the outlier analysis is that there is often an explanation for why their prices are off. It is not always as straightforward as here, but it is worthwhile to try to find out what the cause of the deviations is. Other potential explanations are that the time stamps are off (possibly due to delayed reporting) or that the pricing is not done at the market (but in accordance to some derivative contract). 

For the analysis below, we want to focus on exchange trades. Accordingly, we filter out all trades that are not from the on-exchange continuous trading sessions. 
```{r Trade filtering MMT}
trades <- trades[LOB_continuous_trades]
```

**Matching trades to quotes.**
To evaluate the cost of trading, we want to compare the trade price to the fundamental value at the time of trade, as implied by the bid-ask quotes.

The objective of matching trades and quotes is to obtain the quotes that prevailed just before the trade. This is straightforward in settings where the trades and quotes are recorded at the same point, such that they are correctly sequenced. In other settings, the timestamps may need to be adjusted due to reporting latencies, or the trade size needs to be matched to changes in quoted depth ([Jurkatis, 2021](https://doi.org/10.1016/j.finmar.2021.100635))^[Jurkatis, S. (2022). Inferring trade directions in fast markets. *Journal of Financial Markets*, *58*, 100635.]. 

For US data, the most common approach is to match trades to the last quotes available in the millisecond or microsecond before the trade, as prescribed by [Holden and Jacobsen (2014)](https://doi.org/10.1111/jofi.12127)^[Holden, C. W., & Jacobsen, S. (2014). Liquidity measurement problems in fast, competitive markets: Expensive and cheap solutions. *Journal of Finance*, 69(4), 1747-1785.]. There is, however, an active debate *which* timestamp to use. Several recent papers advocate the use of *participant* time stamps in trade and quote matching, see references about US timestamps above.

In lack of specific guidance for stocks traded in the UK, we match trades to quotes prevailing just before the trade. Based on the assumption that the combined liquidity from LSE and TQE offers the best fundamental value approximation, we match trades from all venues to the EBBO.

The merge function in `data.table` can be called as above by `merge(dt1, dt2)` (for two data.tables named `dt1` and `dt2`), or simply `dt1[dt2]`. We use the latter approach here because it allows us to specify what to do when the timestamps do not match exactly. The option `roll = TRUE` specifies that each observation in `trades` should be matched to the `quotes_ebbo` observation with the latest timestamp that is equal or earlier than the trade timestamp. However, we don't want *equal* matches, because the quote observation should always be *before* the trade. To avoid matching to contemporaneous quotes, which may be updated to reflect the impact of the trade itself, we add one microsecond to the quote timestamps before running the merge function. 

```{r Matching trades to prevailing quotes}
load(file = "quotes_ebbo.Rdata")

# Adjust quote time stamps by one microsecond
quotes_ebbo[, time := time + 0.000001]

# Sort trades and quotes (this specifies the matching criteria for the merge function)
setkeyv(trades, cols = c("date", "time"))
setkeyv(quotes_ebbo, cols = c("date", "time"))

# Match trades to quotes prevailing at the time of trade 
# The rolling is done only for the last of the matching variables, in this case "time"
# `mult = "last"` specifies that if there are multiple matches with identical timestamps, 
# the last match is retained
trades <- quotes_ebbo[trades, roll = TRUE, mult = "last"]
```

**Further screening.**
As some trades may be matched to crossed or locked quotes, another round of data screening is required. Because such quotes are not considered reliable, we do not include those trades in the liquidity measurement. Furthermore, if there are trades that could not be matched to any quotes, or that lack information on price or size, they should be excluded too. 

The output shows that 88.5% of the trades at LSE are eligible for the liquidity analysis, and 96.8% of the trades at TQE. The criterion that drives virtually all exclusions in the sample is the locked quotes.

```{r Trade filtering}
# Flag trades that should be included
trades[, include := !crossed & !locked & !large & !is.na(size) & size > 0 &  
	   	            !is.na(price) & price > 0 & !is.na(midpoint) & midpoint > 0]

# Report trade filtering stats
trades[, 
	list(crossed = round(mean(crossed, na.rm = TRUE), digits = 3),
		 locked = round(mean(locked, na.rm = TRUE), digits = 3),
    	 large = round(mean(large, na.rm = TRUE), digits = 3),
    	 no_price = round(mean(is.na(price) | price == 0), digits = 3),
    	 no_size = round(mean(is.na(size) | size == 0), digits = 3),
    	 no_quotes = round(mean(is.na(midpoint) | midpoint <= 0), digits = 3),
    	 included = round(mean(include), digits = 3)), 
	by = "ticker"]
```

**Direction of trade.**
In empirical market microstructure, we often need to determine the direction of trade. If a trade happens following a buy market order, it is said to be buyer-initiated, and vice versa. 

The most common tool to determine the direction of trade is the algorithm prescribed by [Lee and Ready (1991)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.1991.tb02683.x)^[Lee, C. M., & Ready, M. J. (1991). Inferring trade direction from intraday data. *Journal of Finance*, 46(2), 733-746.]. They primarily recommend the *quote rule*, saying that a trade is buyer-initiated if the trade price is above the prevailing midpoint, and seller-initiated if it is below. When the price equals the midpoint, Lee and Ready propose the *tick rule*. It specifies that a trade is buyer-initiated (seller-initiated) if the price is higher (lower) than the closest previous trade with a different price.

The quote rule is straightforward to implement using the `sign` function, which returns `+1` when the price deviation from the midpoint is positive and `-1` if it is negative. The tick rule, in contrast, requires several steps of code. We create a new `data.table`, named `price_tick`, which in addition to date and time observations for each trade, indicates whether a trade is priced higher (`+1`), lower (`-1`), or the same (`0`) as the previous one. We then exclude all trades that don't imply a price change. Finally, we merge the `price_tick` and the `trades` objects, such that each trade is associated with the latest previous price change.

The direction of trade can now be determined, using primarily the quote rule, and secondarily the tick rule. The output shows that, in this sample, seller-initiated trades are somewhat more common than buyer-initiated.

```{r Direction of trade}
# Quote rule (the trade price is compared to the midpoint at the time of the trade)
trades[, quote_diff := sign(price - midpoint)]

# Tick rule (each trade is matched to the closest preceding trade price change)
price_tick <- data.table(date = trades$date,
                         time = trades$time,
                         price_change = c(NA, sign(diff(trades$price))))

# Retain trades that imply a trade price change
price_tick <- price_tick[price_change != 0]

# Merge trades and trade price changes
setkeyv(trades, c("date", "time"))
setkeyv(price_tick, c("date", "time"))
trades <- price_tick[trades, roll = TRUE, mult = "last"]

# Apply the Lee-Ready (1991) algorithm
trades[, dir := {
	# 1st step: quote rule
  	direction = quote_diff
  	# 2nd step: tick rule
  	no_direction = is.na(direction) | direction == 0
	direction[no_direction] = price_change[no_direction] 
	list(direction)},
	by = "date"]

table(trades$ticker, trades$dir)
```

## Liquidity measures

**Effective spread.**
The relative effective spread is defined as $effective\_spread^{rel} = 2 D(P - M)/M$, where $P$ is the trade price and $D$ is the direction of trade. The multiplication by two is to make the effective spread comparable to the quoted spread. As is common in the literature, we use trade size weights when calculating the average effective spread. We multiply the effective spread by 10,000 to express it in basis points.

We also calculate the dollar volume, which is simply $dollar\_volume = P \times size$, with $size$ denoting the trade size. The trading volume is commonly referred to as liquidity in popular press, but in academic papers it rarely used as a liquidity measure. One reason for that is that spikes in volume are usually due to news rather than liquidity shocks. We still include trading volume here, because it often included as a control variable in microstructure event studies. To express the dollar volume in million GBP, we multiply it by $10^{-8}$.

Consistent with the quote-based measures, the effective spread indicates that PRU is more liquid at LSE than at TQE. The LSE effective spread is 4.18 bps, more than 25% lower than the LSE quoted spread at 5.70 bps. The large difference may be due to trades executed at prices better than visible in the LOB, the effective spreads benchmarked to the consolidated midpoint rather than the respective venue midpoint, or the calculation of the effective spread at times of trade rather than continuously throughout the day. If investors time their trades to reduce trading costs, it makes sense that the effective spread is on average lower than the quoted spread. The interested reader can find out which of these differences drive the wedge between the two measures, by altering the way the average quoted spread is obtained.

```{r Effective spread vw}
# Measure average liquidity for each stock-day

# First order the table
setorder(trades, ticker, date, time)

trades_liquidity <- trades[include == TRUE, {             
  list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, 
  									  w = size),
       volume = sum(price * size))
  },
	by = c("ticker", "date")]

# Output liquidity measures, averaged across the five trading days for each ticker
trades_liquidity[, 
	list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),
	     volume = round(mean(volume * 1e-8), digits = 2)), 
	by = "ticker"]
```

Is the midpoint really a good proxy for the fundamental value of the security? [Hagströmer (2021)](https://doi.org/10.1016/j.jfineco.2021.04.018)^[Hagströmer, B. (2021). Bias in the effective bid-ask spread. *Journal of Financial Economics*, 142(1), 314-337.] shows that the reliance on the midpoint introduces bias in the effective spread. The bias is because traders are more likely to buy the security when the fundamental value is closer to the ask price, and more inclined to sell when the true value is close to the bid price. To capture this, we also consider the weighted midpoint, which is a proxy that allows the true value to lie anywhere between the best bid and ask prices. It is defined as $M_w=(P^BQ^A+P^AQ^B) / (Q^A+Q^B)$, and denoted `midpoint_w` in the code below. The weighted midpoint equals the midpoint when $Q^A=Q^B$.

In line with Hagströmer (2021), we find that the weighted midpoint version of the effective spread is lower than the conventional measure. At Turquoise, the difference is about 15\%. This is noteworthy, as it overturns the previous finding that the TQE is less liquid than the LSE. Although the quoted spread at the TQE is wider, the results indicate that the traders at TQE are better at timing their trades in accordance to the fundamental value.

<!-- TODO: I suggest you stick to either TQE or Turquoise through the entire document after defining it once. -->


```{r Depth-weighted effective spread vw}
# Measure average liquidity for each stock-day, including the effective spread 
# relative to the weighted midpoint
trades_liquidity <- trades[include == TRUE, {
  midpoint_w = (best_bid_price * best_ask_depth + best_ask_price * best_bid_depth) / 
			     (best_ask_depth + best_bid_depth)
	list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, 
								          w = size),
  		 effective_spread_w = weighted.mean(2 * dir * (price - midpoint_w) / midpoint, 
  		 							        w = size))},
    by = c("ticker", "date")]

# Output liquidity measures, averaged across the five trading days for each ticker
trades_liquidity[, 
	list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),
		 effective_spread_w = round(mean(effective_spread_w * 1e4), digits = 2)), 
	by = "ticker"]
```

**Price impact and realized spreads.**
The price impact is defined as $price\_impact^{rel} = 2 D(M_{t+\Delta}-M)/M$, where $M_{t+\Delta}$ is the midpoint holding $\Delta$ seconds after the trade. It thus captures the signed price change following a trade. 

The realized spread is defined as $realized\_spread^{rel} = 2 D(P - M_{t+\Delta})/M$. Note that the price impact and the realized spread may be viewed as two components of the effective spread, as $effective\_spread^{rel}=price\_impact^{rel}+realized\_spread^{rel}$.

The code below prepares the variables needed to calculate the realized spread and the price impact at a 60 second horizon (setting $\Delta=60$). It loads the consolidated quotes, subtracts 60 seconds to their timestamps, and merges them with the trades. In this way, we have trades that are matched to quotes prevailing just before the trade (from the previous matching) as well as to future quotes.

```{r Preparation for the effective spread decomposition}
# Load the EBBO quotes again
load(file = "quotes_ebbo.Rdata")

# Adjust quote time stamps by 60 seconds
quotes_ebbo <- quotes_ebbo[, time := time - 60]

# Rename variables to indicate that they correspond to quotes 1 minute after the trade
# The function `paste0` adds the suffix "_1min" to each variable
setnames(quotes_ebbo, 
         c("midpoint", "crossed", "locked", "large"), 
         paste0(c("midpoint", "crossed", "locked", "large"), "_1min"))

# Merge trades and quotes
setkeyv(quotes_ebbo, cols = c("date", "time"))
setkeyv(trades, cols = c("date", "time"))
trades <- quotes_ebbo[trades, roll = TRUE, mult = "last"]

# Flag valid future quotes
trades[, include_1min := !crossed_1min & !locked_1min & !large_1min]
```

The next step calculates the liquidity measures. The results show that the price impact exceeds the effective spread. The realized spread is then negative, implying that liquidity providers on average lose money. How can that be? Wouldn't the market makers who incur losses simply refrain from trading? It could be that the 60-second evaluation does not reflect the market makers' trading horizon. More likely, perhaps, is that not all traders who post limit orders are in the market to earn the bid-ask spread. It could also be liquidity traders or informed investors who use limit orders to save on transaction costs. Relative to paying the effective spread, earning a negative realized spread may well be an attractive alternative.

```{r Effective spread decomposition}
# Measure trade-based liquidity
effective_spread_decomposition <- trades[include & include_1min, {
	list(effective_spread = weighted.mean(2 * dir * (price - midpoint) / midpoint, 
								  w = size),
  		 price_impact = weighted.mean(2 * dir * (midpoint_1min - midpoint) / midpoint, 
  		 						 w = size),
  		 realized_spread = weighted.mean(2 * dir * (price - midpoint_1min) / midpoint, 
  		 						 w = size))}, 
    by = c("ticker", "date")]

# Output the average liquidity measures
effective_spread_decomposition[, 
	list(effective_spread = round(mean(effective_spread * 1e4), digits = 2),
		 price_impact = round(mean(price_impact * 1e4), digits = 2),
	     realized_spread = round(mean(realized_spread * 1e4), digits = 2)), 
	by = "ticker"]
```

The accuracy of the effective spread decomposition may be improved by replacing the midpoint by the weighted midpoint. We leave the implementation of that to the interested reader.

# Market efficiency

In efficient markets, future price moves are unpredictable, but it is well-known that frictions can cause deviations from that view. This insight is the starting point for measurement of the degree of market efficiency. We consider two measures, the autocorrelation of returns and the variance ratio. We also include the *realized volatility*, the mean squared returns, which is a common control variable in microstructure research. 

<!-- TODO: I know realized volatility as the sum of squared returns?  -->

For each of these measures, rather than the tick data that we worked with above, we need *equispaced returns*. That is, we want to calculate price changes between fixed points in time, such as second by second. As this does not correspond to the LOB updates, we need to transform the data to the desired frequency.

## Equispaced returns
To get started, we again load the consolidated quote data. For the applications below, we drop all LOB variables except the midpoint and the filtering flags.

```{r Load EBBO data}
# Load consolidated quotes
load(file = "quotes_ebbo.Rdata")

# Drop variables
quotes_ebbo[, c("best_bid_price", "best_bid_depth", "best_ask_price", 
				"best_ask_depth", "duration") := NULL]
```

To obtain equispaced observations, we create a time grid with one observation per second. As above, we exclude the first and last minute when setting the opening and closing times. We use the `expand.grid` function to create a grid of second-by-second observations for each sample date, and then convert it to a `data.table`.  
```{r Equispaced quote data, 1 sec}
# Create an equispaced time grid
sampling_freq  <- 1 # 1 second grid
open_time <- 8 * 3600
close_time <- 16.5 * 3600

# The function `seq` creates a sequence of discrete numbers
# the `by` option defines the increment of the sequence, which is here 1 second
time_grid <- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)

# Repeat the time grid for each date and sort it by date and time
dates <- unique(quotes_ebbo$date)
quotes_1sec <- expand.grid(date = dates, time = time_grid)

# Make it a data.table
quotes_1sec <- data.table(quotes_1sec, key = c("date", "time"))

# View the time grid
quotes_1sec
```

For each point in the grid, we want to find the prevailing quote. That is, the last quote update preceding or coinciding with the time in question. Because quotes are valid until cancelled, the same quote may be matched to several consecutive seconds. To avoid bid-ask bounce in the market efficiency measures, we use the midpoint rather than the bid or ask prices.

In the same way as we matched trades to quotes above, we use the rolling merge to match the grid times to quotes. Once the merge is done, it is straightforward to set prices that are flagged as problematic to `NA`, and to calculate returns. We use log-diff returns expressed in basis points (i.e., multiplied by 10,000).

```{r 1-second returns}
# Sort the quotes
setkeyv(quotes_ebbo, cols = c("date", "time"))

# Merge the time grid with the quotes
quotes_1sec <- quotes_ebbo[quotes_1sec, roll = TRUE]

# Set problematic quotes to NA
quotes_1sec$midpoint[quotes_1sec$crossed|quotes_1sec$locked| quotes_1sec$large] <- NA

# Calculate returns, expressed in basis points
# The function `diff` returns the first difference of a time series, which in this case 
# is the log of the midpoint. 
# For each date, a leading `NA` is added to make the resulting vector fit the number of 
# observations in `quotes_1sec`. 
quotes_1sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = "date"]
```

## Efficiency and volatility measures
**Return autocorrelation and realized volatility.**
We obtain the return autocorrelation by applying the `cor` function to returns and lagged returns. The latter are generated using the `shift` function with one lag. We account for missing values by specifying the option `use = "complete.obs"`. The return autocorrelation comes out at 0.02, a very low number. This is not surprising, as the sample stock is a large firm with high trading activity -- two characteristics associated with high market efficiency.

Realized volatility is defined as the mean of squared returns. We also obtain the return variance to be able to calculate the variance ratio below. Although their definitions differ, the realized volatility and the return variance are almost identical in this data set, 0.32. This is because the mean return is close to zero.

```{r 1-second return efficiency}
# Measure market efficiency and volatility 
efficiency_1sec <- quotes_1sec[order(date, time), 
	list(return_corr_1sec = cor(return, shift(return, n = 1, type = "lag"), 
						   use = "complete.obs"),
		 realized_vol_1sec = mean(return^2, na.rm = TRUE),
		 return_var_1sec = var(return, na.rm = TRUE)), 
	by = "date"]

# Output an overview of the average efficiency and volatility 
efficiency_1sec[, 
	list(return_corr_1sec = round(mean(return_corr_1sec), digits = 2),
		 realized_vol_1sec = round(mean(realized_vol_1sec), digits = 2),
		 return_var_1sec = round(mean(return_var_1sec), digits = 2))]
```

**Variance ratios.**
The variance ratio is defined as $var\_ratio_{\tau_{1}, \tau_{2}} = (Var(R_{\tau_{1}}) \tau_2) / (Var(R_{\tau_{2}})  \tau_1)$, where $\tau_{1}$ and $\tau_{2}$ are two return sampling frequencies, and $Var(R_{\tau_{i}})$  is the variance of returns sampled at frequency $\tau_{i}$. Under market efficiency, the variance ratio should be equal to one. Negative and positive deviations from unity are due to frictions. It is usually the absolute deviation from unity that is used as an efficiency measure.

We measure the variance ratio for 1-second and 10-second returns. To get the 10-second return variance, the first step is to obtain the 10-second price grid. To do so, we simply take a subset of the grid obtained for the 1-second frequency. We then proceed with the calculation of returns and the return variance in the same way as above.

```{r Equispaced quote data, 10 sec}
# Subset the 1-second price grid to get the 10-second price grid 
sampling_freq <- 10 # 10 second grid
time_grid <- seq(from = open_time + 60, to = close_time - 60, by = sampling_freq)
quotes_10sec <- quotes_1sec[time %in% time_grid,]

# Calculate returns at the 10-second frequency, expressed in basis points
quotes_10sec[, return := 1e4 * c(NA, diff(log(midpoint))), by = "date"]

# Calculate the return variance at the 10-second frequency, daily
efficiency_10sec <- quotes_10sec[, 
	list(return_var_10sec = var(return, na.rm = TRUE)), 
	by = "date"]
```

Finally, we merge the 10-second return variance with the 1-second frequency efficiency measures and calculate the variance ratio. The output shows that the 10-second return variance is slightly higher than ten times the 1-second return variance, resulting in a variance ratio that exceeds the efficiency benchmark (unity) by 8%. 

```{r Variance ratios}
# Merge the efficiency measures of different return sampling frequencies
efficiency <- efficiency_1sec[efficiency_10sec]

# Obtain the variance ratio
efficiency[, var_ratio := return_var_10sec / (10 * return_var_1sec) ]

# Output an overview of the average variance ratio
efficiency[, list(return_var_1sec = round(mean(return_var_1sec), digits = 3),
				  return_var_10sec = round(mean(return_var_10sec), digits = 3),
				  var_ratio = round(mean(var_ratio), digits = 3))]
```

# Conclusion
This blog post offers a glimpse into the world of empirical market microstructure. A world characterized by intriguing questions, vast data and implementation details that matter greatly -- both for real-world outcomes of market structure reforms and for market quality measures.
If the blog post lowers the threshold and triggers your interest to enter this fascinating world, its aim is fulfilled.

A key point throughout this guide is to try to monitor and understand the data. It is a good habit to inspect, plot, and summarize the data for each step of the processing. Outliers can often be understood with the aid of flags in the data in combination with institutional knowledge. If we know why they appear, we can make an informed decision about how to handle them. 

The format necessarily imposes limitations. We don't cover all market quality measures and not all variations in methodology, but aim to convey what we consider to be good practice when calculating the most common measures. If you disagree, find bugs, or have other feedback, don't hesitate to send us an email at [bjh@sbs.su.se](mailto:bjh@sbs.su.se) and [niklas.landsberg@kuleuven.be](mailto:niklas.landsberg@kuleuven.be). 

To cite our work, please use the following reference information:
Hagströmer, B. and Landsberg, N. (2023). Tidy Market Microstructure. URL: ...

[^1]: Menkveld, A. J. et al. (2023). “Non-standard Errors”, Journal of Finance (forthcoming). http://dx.doi.org/10.2139/ssrn.3961574
[^2]: Amihud, Y. (2002). Illiquidity and stock returns: Cross-section and time-series effects. *Journal of Financial Markets*, 5(1), 31-56. https://doi.org/10.1016/S1386-4181(01)00024-6
[^3]: Jahan-Parvar, M. R., & Zikes, F. (2023). When Do Low-Frequency Measures Really Measure Effective Spreads? Evidence from Equity and Foreign Exchange Markets. *Review of Financial Studies*, 36(10), 4190-4232. https://doi.org/10.1093/rfs/hhad028
[^4]: Campbell, J. Y., Lo, A. W., & MacKinlay, A. C. (1998). *The Econometrics of Financial Markets*. Princeton University Press.
[^5]: Foucault, T., Pagano, M., & Röell, A. (2013). *Market liquidity: Theory, evidence, and policy*. Oxford University Press, USA.
[^6]: Hasbrouck, J. (2007). *Empirical market microstructure: The institutions, economics, and econometrics of securities trading*. Oxford University Press.
[^7]: Scheuch, C., Voigt, S., & Weiss P. (2023). *Tidy Finance with {R}*. Chapman and Hall/CRC. https://doi.org/10.1201/b23237
}
